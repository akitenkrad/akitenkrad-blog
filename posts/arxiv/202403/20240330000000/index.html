<!doctype html><html><head><title>arXiv @ 2024.03.30</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.30"><meta property="og:description" content="Primary Categories astro-ph.IM (1) cond-mat.mtrl-sci (1) cond-mat.stat-mech (1) cs.AI (5) cs.AR (1) cs.CL (51) cs.CR (6) cs.CV (82) cs.CY (5) cs.DC (1) cs.DS (2) cs.ET (2) cs.GR (1) cs.GT (1) cs.HC (3) cs.IR (8) cs.IT (8) cs.LG (33) cs.LO (2) cs.NE (2) cs.NI (3) cs.RO (15) cs.SD (2) cs.SE (5) cs.SI (1) eess.IV (5) eess.SP (3) eess.SY (6) math.AT (1) math.CO (1) math.CT (1) math.NA (5) math.OC (2) math.PR (1) q-fin."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240330000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-30T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-30T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.30"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240330000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Saturday, Mar 30, 2024</p></div><div class=title><h1>arXiv @ 2024.03.30</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#cond-matstat-mech-1>cond-mat.stat-mech (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#csai-5>cs.AI (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#cscl-51>cs.CL (51)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#cscr-6>cs.CR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#cscv-82>cs.CV (82)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#cscy-5>cs.CY (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#cset-2>cs.ET (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#cshc-3>cs.HC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#csir-8>cs.IR (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#csit-8>cs.IT (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#cslg-33>cs.LG (33)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#cslo-2>cs.LO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#csni-3>cs.NI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#csro-15>cs.RO (15)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#csse-5>cs.SE (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#eessiv-5>eess.IV (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#eesssp-3>eess.SP (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#eesssy-6>eess.SY (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#mathat-1>math.AT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#mathct-1>math.CT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#mathna-5>math.NA (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#mathpr-1>math.PR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#q-fintr-1>q-fin.TR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#quant-ph-2>quant-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/#statml-1>stat.ML (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Automatic Speech Recognition</td><td>3</td><td></td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Benchmarking</td><td>14</td><td>21</td><td>7</td><td>1</td></tr><tr><td>Black Box</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Causal Intervention</td><td></td><td>1</td><td></td><td></td></tr><tr><td>ChatGPT</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Content Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Contrastive Learning</td><td>2</td><td>4</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>9</td><td>3</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>12</td><td>1</td><td></td></tr><tr><td>Counter-factual</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Dependency Parsing</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Dialogue System</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>12</td><td></td><td></td></tr><tr><td>Direct Preference Optimization</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Discrete Time</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Distribution Shift</td><td>2</td><td>2</td><td>4</td><td></td></tr><tr><td>Document Classification</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Few-shot</td><td>3</td><td>1</td><td></td><td>1</td></tr><tr><td>Few-shot Learning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>12</td><td>6</td><td>4</td><td></td></tr><tr><td>Foundation Model</td><td></td><td>2</td><td>2</td><td>1</td></tr><tr><td>GPT</td><td>5</td><td>2</td><td></td><td>1</td></tr><tr><td>GPT-2</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>GPT-3</td><td>2</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>4</td><td></td><td></td><td>1</td></tr><tr><td>GPT-4 turbo</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Gaussian Process</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Gemini</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>5</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Grammatical Error Correction</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Graph</td><td>1</td><td>4</td><td>10</td><td>1</td></tr><tr><td>Graph Attention Networks</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Graph Embedding</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td>1</td><td>8</td><td></td></tr><tr><td>Grounding</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Hate Speech Detection</td><td>2</td><td></td><td></td><td></td></tr><tr><td>High-Resource</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td>2</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>8</td><td>2</td><td></td><td>1</td></tr><tr><td>Information Retrieval</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>3</td><td>6</td><td>1</td><td></td></tr><tr><td>LLaMA</td><td></td><td>1</td><td></td><td></td></tr><tr><td>LSTM</td><td>1</td><td>2</td><td>2</td><td></td></tr><tr><td>Large Language Model</td><td>51</td><td>15</td><td>4</td><td>1</td></tr><tr><td>Low-Resource</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Masked Language Model</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Model Pruning</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>7</td><td>12</td><td>2</td><td>1</td></tr><tr><td>Mutual Information</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>9</td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td>5</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Out-of-domain</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Prompt</td><td>10</td><td>15</td><td>1</td><td>1</td></tr><tr><td>Pruning</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td>3</td><td>1</td><td>1</td></tr><tr><td>Question Answering</td><td>6</td><td>1</td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>6</td><td>3</td><td></td><td>1</td></tr><tr><td>Recommender System</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td>3</td><td>1</td><td>3</td><td>1</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>6</td><td>2</td><td></td><td></td></tr><tr><td>Self-Attention</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td>1</td><td>10</td><td></td><td>2</td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td>2</td><td>2</td><td>8</td></tr><tr><td>Simulator</td><td>1</td><td>2</td><td>2</td><td>8</td></tr><tr><td>Slot Filling</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Stance Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Stemming</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Summarization</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>1</td><td>6</td><td>2</td><td></td></tr><tr><td>T5</td><td>1</td><td></td><td></td><td></td></tr><tr><td>TF-IDF</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Text Generation</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Text Understanding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>9</td><td></td><td></td></tr><tr><td>Textual Entailment</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Tokenization</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Transformer</td><td>6</td><td>8</td><td>3</td><td>1</td></tr><tr><td>Unsupervised Learning</td><td>5</td><td>2</td><td>1</td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td>6</td><td>2</td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>5</td><td>1</td><td></td></tr><tr><td>Visual Question Answering</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Word2vec</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>4</td><td></td><td>1</td></tr><tr><td>human-in-the-loop</td><td>1</td><td></td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-51>cs.CL (51)</h2><h3 id=151--1271-factoid-factual-entailment-for-hallucination-detection-vipula-rawte-et-al-2024>(1/51 | 1/271) FACTOID: FACtual enTailment fOr hallucInation Detection (Vipula Rawte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vipula Rawte, S. M Towhidul Islam Tonmoy, Krishnav Rajbangshi, Shravani Nag, Aman Chadha, Amit P. Sheth, Amitava Das. (2024)<br><strong>FACTOID: FACtual enTailment fOr hallucInation Detection</strong><br><button class=copy-to-clipboard title="FACTOID: FACtual enTailment fOr hallucInation Detection" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 123<br>Keywords: Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, GPT, GPT-3, Grounding, Hallucination Detection, Textual Entailment, Large Language Model, Large Language Model, Prompt, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19113v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19113v1.pdf filename=2403.19113v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread adoption of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has facilitated numerous benefits. However, <b>hallucination</b> <b>is</b> a significant concern. In response, <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)</b> has emerged as a highly promising paradigm to improve <b>LLM</b> outputs by <b>grounding</b> them in factual information. <b>RAG</b> relies on <b>textual</b> <b>entailment</b> (TE) or similar methods to check if the <b>text</b> <b>produced</b> by <b>LLMs</b> is supported or contradicted, compared to retrieved documents. This paper argues that conventional TE methods are inadequate for spotting <b>hallucinations</b> <b>in</b> content generated by <b>LLMs.</b> For instance, consider a <b>prompt</b> about the &lsquo;USA&rsquo;s stance on the Ukraine war&rsquo;&rsquo;. The AI-generated <b>text</b> <b>states,</b> &mldr;U.S. President Barack Obama says the U.S. will not put troops in Ukraine&mldr;&rsquo;&rsquo; However, during the war the U.S. president is Joe Biden which contradicts factual reality. Moreover, current TE systems are unable to accurately annotate the given <b>text</b> <b>and</b> identify the exact portion that is contradicted. To address this, we introduces a new type of TE called ``Factual Entailment (FE).&rsquo;&rsquo;, aims to detect factual inaccuracies in content generated by <b>LLMs</b> while also highlighting the specific <b>text</b> <b>segment</b> that contradicts reality. We present FACTOID (FACTual enTAILment for <b>hallucInation</b> <b>Detection),</b> a <b>benchmark</b> dataset for FE. We propose a multi-task learning (MTL) framework for FE, incorporating state-of-the-art (SoTA) long <b>text</b> <b>embeddings</b> such as e5-mistral-7b-instruct, along with <b>GPT-3,</b> SpanBERT, and RoFormer. The proposed MTL architecture for FE achieves an avg. 40% improvement in accuracy on the FACTOID <b>benchmark</b> compared to SoTA TE methods. As FE automatically detects <b>hallucinations,</b> <b>we</b> assessed 15 modern <b>LLMs</b> and ranked them using our proposed Auto <b>Hallucination</b> <b>Vulnerability</b> Index (HVI_auto). This index quantifies and offers a comparative scale to evaluate and rank <b>LLMs</b> according to their hallucinations.</p></p class="citation"></blockquote><h3 id=251--2271-mfort-qa-multi-hop-few-shot-open-rich-table-question-answering-che-guan-et-al-2024>(2/51 | 2/271) MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering (Che Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Che Guan, Mengyu Huang, Peng Zhang. (2024)<br><strong>MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering</strong><br><button class=copy-to-clipboard title="MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 120<br>Keywords: Few-shot, Few-shot Learning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, ChatGPT, Question Answering, Question Answering, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19116v1.pdf filename=2403.19116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s fast-paced industry, professionals face the challenge of summarizing a <b>large</b> <b>number</b> <b>of</b> documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested hyperlinks. To address this challenge, the approach of Table <b>Question</b> <b>Answering</b> <b>(QA)</b> has been developed to extract the relevant information. However, traditional Table <b>QA</b> training tasks that provide a table and an answer(s) from a gold cell coordinate(s) for a <b>question</b> <b>may</b> not always ensure extracting the accurate answer(s). Recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have opened up new possibilities for extracting information from tabular data using <b>prompts.</b> In this paper, we introduce the Multi-hop <b>Few-shot</b> <b>Open</b> Rich Table <b>QA</b> (MFORT-QA) approach, which consists of two major steps. The first step involves <b>Few-Shot</b> <b>Learning</b> (FSL), where relevant tables and associated contexts of hyperlinks are retrieved based on a given <b>question.</b> <b>The</b> retrieved content is then used to construct <b>few-shot</b> <b>prompts</b> as inputs to an <b>LLM,</b> such as <b>ChatGPT.</b> To tackle the challenge of answering complex <b>questions,</b> <b>the</b> second step leverages Chain-of-thought (CoT) <b>prompting</b> to decompose the complex <b>question</b> <b>into</b> a sequential chain of <b>questions</b> <b>and</b> <b>reasoning</b> thoughts in a multi-hop manner. <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> enhances this process by retrieving relevant tables and contexts of hyperlinks that are relevant to the resulting <b>reasoning</b> thoughts and <b>questions.</b> <b>These</b> additional contexts are then used to supplement the <b>prompt</b> used in the first step, resulting in more accurate answers from an <b>LLM.</b> Empirical results from OTT-QA demonstrate that our abstractive <b>QA</b> approach significantly improves the accuracy of extractive Table <b>QA</b> methods.</p></p class="citation"></blockquote><h3 id=351--3271-jdocqa-japanese-document-question-answering-dataset-for-generative-language-models-eri-onami-et-al-2024>(3/51 | 3/271) JDocQA: Japanese Document Question Answering Dataset for Generative Language Models (Eri Onami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eri Onami, Shuhei Kurita, Taiki Miyanishi, Taro Watanabe. (2024)<br><strong>JDocQA: Japanese Document Question Answering Dataset for Generative Language Models</strong><br><button class=copy-to-clipboard title="JDocQA: Japanese Document Question Answering Dataset for Generative Language Models" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 86<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Question Answering, Question Answering, Visual Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Text Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19454v1.pdf filename=2403.19454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Document <b>question</b> <b>answering</b> is a task of <b>question</b> <b>answering</b> on given documents such as reports, slides, pamphlets, and websites, and it is a truly demanding task as paper and electronic forms of documents are so common in our society. This is known as a quite challenging task because it requires not only <b>text</b> <b>understanding</b> but also understanding of figures and tables, and hence <b>visual</b> <b>question</b> <b>answering</b> <b>(VQA)</b> methods are often examined in addition to textual approaches. We introduce Japanese Document <b>Question</b> <b>Answering</b> (JDocQA), a <b>large-scale</b> <b>document-based</b> <b>QA</b> dataset, essentially requiring both <b>visual</b> <b>and</b> <b>textual</b> information to answer <b>questions,</b> <b>which</b> comprises 5,504 documents in PDF format and annotated 11,600 <b>question-and-answer</b> <b>instances</b> in Japanese. Each <b>QA</b> instance includes references to the document pages and bounding boxes for the answer clues. We incorporate multiple categories of <b>questions</b> <b>and</b> unanswerable <b>questions</b> <b>from</b> the document for realistic <b>question-answering</b> <b>applications.</b> We empirically evaluate the effectiveness of our dataset with <b>text-based</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and <b>multimodal</b> models. Incorporating unanswerable <b>questions</b> <b>in</b> <b>finetuning</b> may contribute to harnessing the so-called hallucination generation.</p></p class="citation"></blockquote><h3 id=451--4271-ungrammatical-syntax-based-in-context-example-selection-for-grammatical-error-correction-chenming-tang-et-al-2024>(4/51 | 4/271) Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction (Chenming Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenming Tang, Fanyi Qu, Yunfang Wu. (2024)<br><strong>Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction</strong><br><button class=copy-to-clipboard title="Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Grammatical Error Correction, Grammatical Error Correction, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19283v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19283v1.pdf filename=2403.19283v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> <b>in-context</b> <b>learning</b> <b>(ICL)</b> stands out as an effective <b>prompting</b> strategy that explores <b>LLMs&rsquo;</b> potency across various tasks. However, applying <b>LLMs</b> to <b>grammatical</b> <b>error</b> <b>correction</b> <b>(GEC)</b> is still a challenging task. In this paper, we propose a novel ungrammatical-syntax-based <b>in-context</b> <b>example</b> selection strategy for <b>GEC.</b> Specifically, we measure similarity of sentences based on their syntactic structures with diverse algorithms, and identify optimal <b>ICL</b> examples sharing the most similar ill-formed syntax to the test input. Additionally, we carry out a two-stage process to further improve the quality of selection results. On <b>benchmark</b> English <b>GEC</b> datasets, empirical results show that our proposed ungrammatical-syntax-based strategies outperform commonly-used word-matching or semantics-based methods with multiple <b>LLMs.</b> This indicates that for a syntax-oriented task like <b>GEC,</b> paying more attention to syntactic information can effectively boost <b>LLMs&rsquo;</b> performance. Our code will be publicly available after the publication of this paper.</p></p class="citation"></blockquote><h3 id=551--5271-retrieval-enhanced-knowledge-editing-for-multi-hop-question-answering-in-language-models-yucheng-shi-et-al-2024>(5/51 | 5/271) Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models (Yucheng Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yucheng Shi, Qiaoyu Tan, Xuansheng Wu, Shaochen Zhong, Kaixiong Zhou, Ninghao Liu. (2024)<br><strong>Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models</strong><br><button class=copy-to-clipboard title="Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Mutual Information, Pruning, Question Answering, Reasoning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19631v1.pdf filename=2403.19631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown proficiency in <b>question-answering</b> <b>tasks</b> but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop <b>questions</b> <b>since</b> they require <b>LLMs</b> to update and integrate multiple knowledge pieces relevant to the <b>questions.</b> <b>To</b> tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework tailored for multi-hop <b>question</b> <b>answering.</b> RAE first retrieves edited facts and then refines the language model through <b>in-context</b> <b>learning.</b> Specifically, our retrieval approach, based on <b>mutual</b> <b>information</b> maximization, leverages the <b>reasoning</b> abilities of <b>LLMs</b> to identify chain facts that na"ive similarity-based searches might miss. Additionally, our framework incorporates a <b>pruning</b> strategy to eliminate redundant information from the retrieved facts, which enhances the editing accuracy and mitigates the hallucination problem. Our framework is supported by theoretical justification for its fact retrieval efficacy. Finally, comprehensive evaluation across various <b>LLMs</b> validates RAE&rsquo;s ability in providing accurate answers with updated knowledge.</p></p class="citation"></blockquote><h3 id=651--6271-mixed-preference-optimization-reinforcement-learning-with-data-selection-and-better-reference-model-qi-gou-et-al-2024>(6/51 | 6/271) Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model (Qi Gou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Gou, Cam-Tu Nguyen. (2024)<br><strong>Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model</strong><br><button class=copy-to-clipboard title="Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Contrastive Learning, Direct Preference Optimization, Distribution Shift, Distribution Shift, Reinforcement Learning, Reinforcement Learning from Human Feedback, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19443v1.pdf filename=2403.19443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have become increasingly popular due to their ability to process and generate natural language. However, as they are trained on massive datasets of text, <b>LLMs</b> can inherit harmful biases and produce outputs that are not aligned with human values. This paper studies two main approaches to <b>LLM</b> alignment: <b>Reinforcement</b> <b>Learning</b> with Human Feedback <b>(RLHF)</b> and <b>contrastive</b> <b>learning-based</b> methods like <b>Direct</b> <b>Preference</b> <b>Optimization</b> (DPO). By analyzing the stability and robustness of <b>RLHF</b> and DPO, we propose MPO (Mixed Preference Optimization), a novel method that mitigates the weaknesses of both approaches. Specifically, we propose a two-stage training procedure: first train DPO on an easy dataset, and then perform <b>RLHF</b> on a difficult set with DPO model being the reference model. Here, the easy and difficult sets are constructed by a well-trained reward model that splits response pairs into those with <b>large</b> <b>gaps</b> <b>of</b> reward (easy), and those with small gaps (difficult). The first stage allows us to obtain a relatively optimal policy <b>(LLM)</b> model quickly, whereas the second stage refines <b>LLM</b> with online <b>RLHF,</b> thus mitigating the <b>distribution</b> <b>shift</b> issue associated with DPO. Experiments are conducted on two public alignment datasets, namely HH-RLHF and TLDR, demonstrating the effectiveness of MPO, both in terms of <b>GPT4</b> and human evaluation.</p></p class="citation"></blockquote><h3 id=751--7271-going-beyond-word-matching-syntax-improves-in-context-example-selection-for-machine-translation-chenming-tang-et-al-2024>(7/51 | 7/271) Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation (Chenming Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenming Tang, Zhixiang Wang, Yunfang Wu. (2024)<br><strong>Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation</strong><br><button class=copy-to-clipboard title="Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Neural Machine Translation, Neural Machine Translation, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19285v1.pdf filename=2403.19285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-context</b> <b>learning</b> <b>(ICL)</b> is the trending <b>prompting</b> strategy in the era of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> where a few examples are demonstrated to evoke <b>LLMs&rsquo;</b> power for a given task. How to select informative examples remains an open issue. Previous works on <b>in-context</b> <b>example</b> selection for <b>machine</b> <b>translation</b> <b>(MT)</b> focus on superficial word-level features while ignoring deep syntax-level knowledge. In this paper, we propose a syntax-based <b>in-context</b> <b>example</b> selection method for <b>MT,</b> by computing the syntactic similarity between dependency trees using Polynomial Distance. In addition, we propose an ensemble strategy combining examples selected by both word-level and syntax-level criteria. Experimental results between English and 6 common languages indicate that syntax can effectively enhancing <b>ICL</b> for <b>MT,</b> obtaining the highest COMET scores on 11 out of 12 translation directions.</p></p class="citation"></blockquote><h3 id=851--8271-disentangling-length-from-quality-in-direct-preference-optimization-ryan-park-et-al-2024>(8/51 | 8/271) Disentangling Length from Quality in Direct Preference Optimization (Ryan Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan Park, Rafael Rafailov, Stefano Ermon, Chelsea Finn. (2024)<br><strong>Disentangling Length from Quality in Direct Preference Optimization</strong><br><button class=copy-to-clipboard title="Disentangling Length from Quality in Direct Preference Optimization" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Direct Preference Optimization, Out-of-distribution, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, GPT-4, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19159v1.pdf filename=2403.19159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> has been a crucial component in the recent success of <b>Large</b> <b>Language</b> <b>Models.</b> However, <b>RLHF</b> is know to exploit biases in human preferences, such as verbosity. A well-formatted and eloquent answer is often more highly rated by users, even when it is less helpful and objective. A number of approaches have been developed to control those biases in the classical <b>RLHF</b> literature, but the problem remains relatively under-explored for <b>Direct</b> <b>Alignment</b> <b>Algorithms</b> such as <b>Direct</b> <b>Preference</b> <b>Optimization</b> (DPO). Unlike classical <b>RLHF,</b> DPO does not train a separate reward model or use <b>reinforcement</b> <b>learning</b> <b>directly,</b> <b>so</b> <b>previous</b> approaches developed to control verbosity cannot be directly applied to this setting. Our work makes several contributions. For the first time, we study the length problem in the DPO setting, showing significant exploitation in DPO and linking it to <b>out-of-distribution</b> bootstrapping. We then develop a principled but simple regularization strategy that prevents length exploitation, while still maintaining improvements in model quality. We demonstrate these effects across datasets on <b>summarization</b> and dialogue, where we achieve up to 20% improvement in win rates when controlling for length, despite the <b>GPT4</b> judge&rsquo;s well-known verbosity bias.</p></p class="citation"></blockquote><h3 id=951--9271-multi-stage-multi-modal-pre-training-for-automatic-speech-recognition-yash-jain-et-al-2024>(9/51 | 9/271) Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition (Yash Jain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yash Jain, David Chan, Pranav Dheram, Aparna Khare, Olabanji Shonibare, Venkatesh Ravichandran, Shalini Ghosh. (2024)<br><strong>Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition</strong><br><button class=copy-to-clipboard title="Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Fine-tuning, Fine-tuning, Multi-modal, Supervised Learning, Unsupervised Learning, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19822v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19822v1.pdf filename=2403.19822v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in machine learning have demonstrated that <b>multi-modal</b> pre-training can improve <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> performance compared to randomly initialized models, even when models are <b>fine-tuned</b> on uni-modal tasks. Existing <b>multi-modal</b> pre-training methods for the <b>ASR</b> task have primarily focused on single-stage pre-training where a single <b>unsupervised</b> task is used for pre-training followed by <b>fine-tuning</b> on the downstream task. In this work, we introduce a novel method combining <b>multi-modal</b> and multi-task <b>unsupervised</b> pre-training with a translation-based <b>supervised</b> mid-training approach. We empirically demonstrate that such a multi-stage approach leads to relative word error rate (WER) improvements of up to 38.45% over baselines on both Librispeech and SUPERB. Additionally, we share several important findings for choosing pre-training methods and datasets.</p></p class="citation"></blockquote><h3 id=1051--10271-ethiomt-parallel-corpus-for-low-resource-ethiopian-languages-atnafu-lambebo-tonja-et-al-2024>(10/51 | 10/271) EthioMT: Parallel Corpus for Low-resource Ethiopian Languages (Atnafu Lambebo Tonja et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atnafu Lambebo Tonja, Olga Kolesnikova, Alexander Gelbukh, Jugal Kalita. (2024)<br><strong>EthioMT: Parallel Corpus for Low-resource Ethiopian Languages</strong><br><button class=copy-to-clipboard title="EthioMT: Parallel Corpus for Low-resource Ethiopian Languages" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, High-Resource, Low-Resource, Transformer, Neural Machine Translation, Neural Machine Translation, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19365v1.pdf filename=2403.19365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research in natural language processing (NLP) has achieved impressive performance in tasks such as <b>machine</b> <b>translation</b> <b>(MT),</b> news classification, and <b>question-answering</b> <b>in</b> <b>high-resource</b> languages. However, the performance of <b>MT</b> leaves much to be desired for <b>low-resource</b> languages. This is due to the smaller size of available parallel corpora in these languages, if such corpora are available at all. NLP in Ethiopian languages suffers from the same issues due to the unavailability of publicly accessible datasets for NLP tasks, including <b>MT.</b> To help the research community and foster research for Ethiopian languages, we introduce EthioMT &ndash; a new parallel corpus for 15 languages. We also create a new <b>benchmark</b> by collecting a dataset for better-researched languages in Ethiopia. We evaluate the newly collected corpus and the <b>benchmark</b> dataset for 23 Ethiopian languages using <b>transformer</b> and <b>fine-tuning</b> approaches.</p></p class="citation"></blockquote><h3 id=1151--11271-fine-tuning-language-models-with-reward-learning-on-policy-hao-lang-et-al-2024>(11/51 | 11/271) Fine-Tuning Language Models with Reward Learning on Policy (Hao Lang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Lang, Fei Huang, Yongbin Li. (2024)<br><strong>Fine-Tuning Language Models with Reward Learning on Policy</strong><br><button class=copy-to-clipboard title="Fine-Tuning Language Models with Reward Learning on Policy" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Unsupervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19279v1.pdf filename=2403.19279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF)</b> has emerged as an effective approach to aligning <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to human preferences. <b>RLHF</b> contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially. Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts <b>LLMs&rsquo;</b> data distribution. Repeatedly collecting new preference data from the latest <b>LLMs</b> may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize. In this paper, we propose reward learning on policy (RLP), an <b>unsupervised</b> framework that refines a reward model using policy samples to keep it on-distribution. Specifically, an <b>unsupervised</b> multi-view learning method is introduced to learn robust representations of policy samples. Meanwhile, a synthetic preference generation approach is developed to simulate high-quality preference data with policy outputs. Extensive experiments on three <b>benchmark</b> datasets show that RLP consistently outperforms the state-of-the-art. Our code is available at \url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}.</p></p class="citation"></blockquote><h3 id=1251--12271-a-benchmark-evaluation-of-clinical-named-entity-recognition-in-french-nesrine-bannour-et-al-2024>(12/51 | 12/271) A Benchmark Evaluation of Clinical Named Entity Recognition in French (Nesrine Bannour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nesrine Bannour, Christophe Servan, Aurélie Névéol, Xavier Tannier. (2024)<br><strong>A Benchmark Evaluation of Clinical Named Entity Recognition in French</strong><br><button class=copy-to-clipboard title="A Benchmark Evaluation of Clinical Named Entity Recognition in French" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL, q-bio-QM<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Transformer, Named Entity Recognition, Large Language Model, Large Language Model, Masked Language Model, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19726v1.pdf filename=2403.19726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: <b>Transformer-based</b> language models have shown strong performance on many Natural LanguageProcessing (NLP) tasks. <b>Masked</b> <b>Language</b> <b>Models</b> <b>(MLMs)</b> attract sustained interest because they can be adaptedto different languages and sub-domains through training or <b>fine-tuning</b> on specific corpora while remaining lighterthan modern <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Recently, several <b>MLMs</b> have been released for the biomedicaldomain in French, and experiments suggest that they outperform standard French counterparts. However, nosystematic evaluation comparing all models on the same corpora is available. Objective: This paper presentsan evaluation of <b>masked</b> <b>language</b> <b>models</b> for biomedical French on the task of clinical <b>named</b> <b>entity</b> <b>recognition.Material</b> and methods: We evaluate biomedical models CamemBERT-bio and DrBERT and compare them tostandard French models CamemBERT, FlauBERT and FrALBERT as well as multilingual mBERT using three publicallyavailable corpora for clinical <b>named</b> <b>entity</b> <b>recognition</b> in French. The evaluation set-up relies on gold-standardcorpora as released by the corpus developers. Results: Results suggest that CamemBERT-bio outperformsDrBERT consistently while FlauBERT offers competitive performance and FrAlBERT achieves the lowest carbonfootprint. Conclusion: This is the first <b>benchmark</b> evaluation of biomedical <b>masked</b> <b>language</b> <b>models</b> for Frenchclinical entity recognition that compares model performance consistently on nested entity recognition using metricscovering performance and environmental impact.</p></p class="citation"></blockquote><h3 id=1351--13271-developing-healthcare-language-model-embedding-spaces-niall-taylor-et-al-2024>(13/51 | 13/271) Developing Healthcare Language Model Embedding Spaces (Niall Taylor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niall Taylor, Dan Schofield, Andrey Kormilitzin, Dan W Joyce, Alejo Nevado-Holgado. (2024)<br><strong>Developing Healthcare Language Model Embedding Spaces</strong><br><button class=copy-to-clipboard title="Developing Healthcare Language Model Embedding Spaces" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Contrastive Learning, Out-of-domain, Unsupervised Learning, Document Classification, Large Language Model, Large Language Model, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19802v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19802v1.pdf filename=2403.19802v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> often struggle on <b>out-of-domain</b> datasets like healthcare focused text. We explore specialized pre-training to adapt smaller <b>LLMs</b> to different healthcare datasets. Three methods are assessed: traditional <b>masked</b> <b>language</b> <b>modeling,</b> Deep <b>Contrastive</b> <b>Learning</b> for <b>Unsupervised</b> Textual Representations (DeCLUTR), and a novel pre-training objective utilizing metadata categories from the healthcare settings. These schemes are evaluated on downstream <b>document</b> <b>classification</b> tasks for each dataset, with additional analysis of the resultant embedding spaces. Contrastively trained models outperform other approaches on the classification tasks, delivering strong performance from limited labeled data and with fewer model parameter updates required. While metadata-based pre-training does not further improve classifications across the datasets, it yields interesting embedding cluster separability. All domain adapted <b>LLMs</b> outperform their publicly available general base <b>LLM,</b> validating the importance of domain-specialization. This research illustrates efficient approaches to instill healthcare competency in compact <b>LLMs</b> even under tight computational budgets, an essential capability for responsible and sustainable deployment in local healthcare settings. We provide pre-training guidelines for specialized healthcare <b>LLMs,</b> motivate continued inquiry into <b>contrastive</b> <b>objectives,</b> and demonstrates adaptation techniques to align small <b>LLMs</b> with privacy-sensitive medical tasks.</p></p class="citation"></blockquote><h3 id=1451--14271-improving-vietnamese-english-medical-machine-translation-nhu-vo-et-al-2024>(14/51 | 14/271) Improving Vietnamese-English Medical Machine Translation (Nhu Vo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nhu Vo, Dat Quoc Nguyen, Dung D. Le, Massimo Piccardi, Wray Buntine. (2024)<br><strong>Improving Vietnamese-English Medical Machine Translation</strong><br><button class=copy-to-clipboard title="Improving Vietnamese-English Medical Machine Translation" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, ChatGPT, GPT, GPT-3, GPT-3.5, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19161v1.pdf filename=2403.19161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Machine</b> <b>translation</b> for Vietnamese-English in the medical domain is still an under-explored research area. In this paper, we introduce MedEV &ndash; a high-quality Vietnamese-English parallel dataset constructed specifically for the medical domain, comprising approximately 360K sentence pairs. We conduct extensive experiments comparing Google Translate, <b>ChatGPT</b> <b>(gpt-3.5-turbo),</b> state-of-the-art Vietnamese-English <b>neural</b> <b>machine</b> <b>translation</b> models and pre-trained bilingual/multilingual sequence-to-sequence models on our new MedEV dataset. Experimental results show that the best performance is achieved by <b>fine-tuning</b> &ldquo;vinai-translate&rdquo; for each translation direction. We publicly release our dataset to promote further research.</p></p class="citation"></blockquote><h3 id=1551--15271-interpreting-key-mechanisms-of-factual-recall-in-transformer-based-language-models-ang-lv-et-al-2024>(15/51 | 15/271) Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models (Ang Lv et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ang Lv, Kaiyi Zhang, Yuhan Chen, Yulong Wang, Lifeng Liu, Ji-Rong Wen, Jian Xie, Rui Yan. (2024)<br><strong>Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models</strong><br><button class=copy-to-clipboard title="Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, Zero-shot, GPT, GPT-2, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19521v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19521v1.pdf filename=2403.19521v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we deeply explore the mechanisms employed by <b>Transformer-based</b> language models in factual recall tasks. In <b>zero-shot</b> scenarios, given a <b>prompt</b> like &ldquo;The capital of France is,&rdquo; task-specific attention heads extract the topic entity, such as &ldquo;France,&rdquo; from the context and pass it to subsequent MLPs to recall the required answer such as &ldquo;Paris.&rdquo; We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These <b>zero-shot</b> mechanisms are also employed in <b>few-shot</b> scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in the final layer of models, which suppresses correct predictions. We mitigate this suppression by leveraging our interpretation to improve factual recall performance. Our interpretations have been evaluated across various language models, from the <b>GPT-2</b> families to 1.3B OPT, and across tasks covering different domains of factual knowledge.</p></p class="citation"></blockquote><h3 id=1651--16271-hgt-leveraging-heterogeneous-graph-enhanced-large-language-models-for-few-shot-complex-table-understanding-rihui-jin-et-al-2024>(16/51 | 16/271) HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding (Rihui Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rihui Jin, Yu Li, Guilin Qi, Nan Hu, Yuan-Fang Li, Jiaoyan Chen, Jianan Wang, Yongrui Chen, Dehai Min. (2024)<br><strong>HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding</strong><br><button class=copy-to-clipboard title="HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-DB, cs-MM, cs.CL<br>Keyword Score: 56<br>Keywords: Graph, Benchmarking, Few-shot, Self-supervised Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19723v1.pdf filename=2403.19723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures.To address these challenges, we propose HGT, a framework with a heterogeneous <b>graph</b> (HG)-enhanced <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to tackle <b>few-shot</b> TU tasks.It leverages the <b>LLM</b> by aligning the table semantics with the <b>LLM&rsquo;s</b> parametric knowledge through soft <b>prompts</b> and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity <b>self-supervised</b> HG pre-training objectives.We empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for <b>few-shot</b> complex TU on several <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1751--17271-gold-generalized-knowledge-distillation-via-out-of-distribution-guided-language-data-generation-mohsen-gholami-et-al-2024>(17/51 | 17/271) GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation (Mohsen Gholami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohsen Gholami, Mohammad Akbari, Cindy Hu, Vaden Masrani, Z. Jane Wang, Yong Zhang. (2024)<br><strong>GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation</strong><br><button class=copy-to-clipboard title="GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Out-of-distribution, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19754v1.pdf filename=2403.19754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>distillation</b> from <b>LLMs</b> is essential for the efficient deployment of language models. Prior works have proposed data generation using <b>LLMs</b> for preparing <b>distilled</b> models. We argue that generating data with <b>LLMs</b> is prone to sampling mainly from the center of original content distribution. This limitation hinders the <b>distilled</b> model from learning the true underlying data distribution and to forget the tails of the distributions (samples with lower probability). To this end, we propose GOLD, a task-agnostic data generation and <b>knowledge</b> <b>distillation</b> framework, which employs an iterative <b>out-of-distribution-guided</b> feedback mechanism for the <b>LLM.</b> As a result, the generated data improves the generalizability of <b>distilled</b> models. An energy-based OOD evaluation approach is also introduced to deal with noisy generated data. Our extensive experiments on 10 different classification and sequence-to-sequence tasks in NLP show that GOLD respectively outperforms prior arts and the <b>LLM</b> with an average improvement of 5% and 14%. We will also show that the proposed method is applicable to less explored and novel tasks. The code is available.</p></p class="citation"></blockquote><h3 id=1851--18271-waterjudge-quality-detection-trade-off-when-watermarking-large-language-models-piotr-molenda-et-al-2024>(18/51 | 18/271) WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models (Piotr Molenda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Piotr Molenda, Adian Liusie, Mark J. F. Gales. (2024)<br><strong>WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models</strong><br><button class=copy-to-clipboard title="WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Generative AI, Natural Language Generation, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19548v1.pdf filename=2403.19548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Watermarking <b>generative-AI</b> <b>systems,</b> such as <b>LLMs,</b> has gained considerable interest, driven by their enhanced capabilities across a wide range of tasks. Although current approaches have demonstrated that small, context-dependent shifts in the word distributions can be used to apply and detect watermarks, there has been little work in analyzing the impact that these perturbations have on the quality of generated texts. Balancing high detectability with minimal performance degradation is crucial in terms of selecting the appropriate watermarking setting; therefore this paper proposes a simple analysis framework where comparative assessment, a flexible <b>NLG</b> evaluation framework, is used to assess the quality degradation caused by a particular watermark setting. We demonstrate that our framework provides easy visualization of the quality-detection trade-off of watermark settings, enabling a simple solution to find an <b>LLM</b> watermark operating point that provides a well-balanced performance. This approach is applied to two different <b>summarization</b> systems and a translation system, enabling cross-model analysis for a task, and cross-task analysis.</p></p class="citation"></blockquote><h3 id=1951--19271-checkpoint-merging-via-bayesian-optimization-in-llm-pretraining-deyuan-liu-et-al-2024>(19/51 | 19/271) Checkpoint Merging via Bayesian Optimization in LLM Pretraining (Deyuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan Li, Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui. (2024)<br><strong>Checkpoint Merging via Bayesian Optimization in LLM Pretraining</strong><br><button class=copy-to-clipboard title="Checkpoint Merging via Bayesian Optimization in LLM Pretraining" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Gemini, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19390v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19390v1.pdf filename=2403.19390v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid proliferation of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>GPT-4</b> and <b>Gemini</b> underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining <b>LLM.</b> This method utilizes <b>LLM</b> checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining.</p></p class="citation"></blockquote><h3 id=2051--20271-mugc-machine-generated-versus-user-generated-content-detection-yaqi-xie-et-al-2024>(20/51 | 20/271) MUGC: Machine Generated versus User Generated Content Detection (Yaqi Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaqi Xie, Anjali Rawal, Yujing Cen, Dixuan Zhao, Sunil K Narang, Shanu Sushmita. (2024)<br><strong>MUGC: Machine Generated versus User Generated Content Detection</strong><br><button class=copy-to-clipboard title="MUGC: Machine Generated versus User Generated Content Detection" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Generative AI, Word2vec, Content Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19725v1.pdf filename=2403.19725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As advanced modern systems like deep neural networks (DNNs) and <b>generative</b> <b>AI</b> continue to enhance their capabilities in producing convincing and realistic <b>content,</b> <b>the</b> need to distinguish between user-generated and machine generated <b>content</b> <b>is</b> becoming increasingly evident. In this research, we undertake a comparative evaluation of eight traditional machine-learning algorithms to distinguish between machine-generated and human-generated data across three diverse datasets: Poems, Abstracts, and Essays. Our results indicate that traditional methods demonstrate a high level of accuracy in identifying machine-generated data, reflecting the documented effectiveness of popular pre-trained models like RoBERT. We note that machine-generated texts tend to be shorter and exhibit less word variety compared to human-generated <b>content.</b> <b>While</b> specific domain-related keywords commonly utilized by humans, albeit disregarded by current <b>LLMs</b> <b>(Large</b> <b>Language</b> <b>Models),</b> may contribute to this high detection accuracy, we show that deeper word representations like <b>word2vec</b> can capture subtle semantic variances. Furthermore, readability, bias, moral, and affect comparisons reveal a discernible contrast between machine-generated and human generated <b>content.</b> <b>There</b> are variations in expression styles and potentially underlying biases in the data sources (human and machine-generated). This study provides valuable insights into the advancing capacities and challenges associated with machine-generated <b>content</b> <b>across</b> various domains.</p></p class="citation"></blockquote><h3 id=2151--21271-a-tulu-resource-for-machine-translation-manu-narayanan-et-al-2024>(21/51 | 21/271) A Tulu Resource for Machine Translation (Manu Narayanan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manu Narayanan, Noëmi Aepli. (2024)<br><strong>A Tulu Resource for Machine Translation</strong><br><button class=copy-to-clipboard title="A Tulu Resource for Machine Translation" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: High-Resource, Low-Resource, Transfer Learning, Neural Machine Translation, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19142v1.pdf filename=2403.19142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the first parallel dataset for English-Tulu translation. Tulu, classified within the South Dravidian linguistic family branch, is predominantly spoken by approximately 2.5 million individuals in southwestern India. Our dataset is constructed by integrating human translations into the multilingual <b>machine</b> <b>translation</b> resource FLORES-200. Furthermore, we use this dataset for evaluation purposes in developing our English-Tulu <b>machine</b> <b>translation</b> model. For the model&rsquo;s training, we leverage resources available for related South Dravidian languages. We adopt a <b>transfer</b> <b>learning</b> approach that exploits similarities between <b>high-resource</b> and <b>low-resource</b> languages. This method enables the training of a <b>machine</b> <b>translation</b> system even in the absence of parallel data between the source and target language, thereby overcoming a significant obstacle in <b>machine</b> <b>translation</b> development for <b>low-resource</b> languages. Our English-Tulu system, trained without using parallel English-Tulu data, outperforms Google Translate by 19 <b>BLEU</b> points (in September 2023). The dataset and code are available here: <a href=https://github.com/manunarayanan/Tulu-NMT>https://github.com/manunarayanan/Tulu-NMT</a>.</p></p class="citation"></blockquote><h3 id=2251--22271-compressing-large-language-models-by-streamlining-the-unimportant-layer-xiaodong-chen-et-al-2024>(22/51 | 22/271) Compressing Large Language Models by Streamlining the Unimportant Layer (Xiaodong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaodong Chen, Yuxuan Hu, Jing Zhang. (2024)<br><strong>Compressing Large Language Models by Streamlining the Unimportant Layer</strong><br><button class=copy-to-clipboard title="Compressing Large Language Models by Streamlining the Unimportant Layer" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Model Pruning, Pruning, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19135v1.pdf filename=2403.19135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLM)</b> have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the <b>large</b> <b>number</b> <b>of</b> parameters of the <b>models.</b> <b>Consequently,</b> there is an increasing emphasis on compact <b>models</b> <b>that</b> exhibit high performance. In this study, we observe that different layers in <b>LLM</b> have varying degrees of perturbation on the hidden states, which allows us to identify less important layers. Based on this phenomenon, we propose <b>LLM-Streamline,</b> which consists of two parts: layer <b>pruning,</b> where we remove a set of consecutive layers with the lowest importance in the <b>model</b> <b>according</b> to the target sparsity; and layer replacement, where we train a lightweight <b>model</b> <b>to</b> substitute the pruned layers, thereby mitigating the performance degradation caused by <b>pruning.</b> In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a <b>transformer</b> layer as lightweight <b>models</b> <b>and</b> ultimately demonstrate that a single MLP can effectively fit the pruned layers. Comprehensive experiments show that our proposed method, <b>LLM-Streamline,</b> outperforms previous state-of-the-art (SOTA) <b>model</b> <b>pruning</b> methods.</p></p class="citation"></blockquote><h3 id=2351--23271-bp4er-bootstrap-prompting-for-explicit-reasoning-in-medical-dialogue-generation-yuhong-he-et-al-2024>(23/51 | 23/271) BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation (Yuhong He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhong He, Yongqi Zhang, Shizhu He, Jun Wan. (2024)<br><strong>BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation</strong><br><button class=copy-to-clipboard title="BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19414v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19414v1.pdf filename=2403.19414v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical dialogue generation (MDG) has gained increasing attention due to its substantial practical value. Previous works typically employ a sequence-to-sequence framework to generate medical responses by modeling dialogue context as sequential text with annotated medical entities. While these methods have been successful in generating fluent responses, they fail to provide process explanations of <b>reasoning</b> and require extensive entity annotation. To address these limitations, we propose the method Bootstrap <b>Prompting</b> for Explicit <b>Reasoning</b> in MDG (BP4ER), which explicitly model MDG&rsquo;s multi-step <b>reasoning</b> process and iteratively enhance this <b>reasoning</b> process. We employ a least-to-most <b>prompting</b> strategy to guide a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> in explicit <b>reasoning,</b> breaking down MDG into simpler sub-questions. These sub-questions build on answers from previous ones. Additionally, we also introduce two distinct bootstrapping techniques for <b>prompting,</b> which autonomously correct errors and facilitate the <b>LLM&rsquo;s</b> explicit <b>reasoning.</b> This approach eliminates the need for entity annotation and increases the transparency of the MDG process by explicitly generating the intermediate <b>reasoning</b> chain. The experimental findings on the two public datasets indicate that BP4ER outperforms state-of-the-art methods in terms of both objective and subjective evaluation metrics.</p></p class="citation"></blockquote><h3 id=2451--24271-naijahate-evaluating-hate-speech-detection-on-nigerian-twitter-using-representative-data-manuel-tonneau-et-al-2024>(24/51 | 24/271) NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data (Manuel Tonneau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manuel Tonneau, Pedro Vitor Quinta de Castro, Karim Lasri, Ibrahim Farouq, Lakshminarayanan Subramanian, Victor Orozco-Olvera, Samuel Fraiberger. (2024)<br><strong>NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data</strong><br><button class=copy-to-clipboard title="NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Low-Resource, human-in-the-loop, Hate Speech Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19260v1.pdf filename=2403.19260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To address the global issue of hateful content proliferating in online platforms, <b>hate</b> <b>speech</b> <b>detection</b> (HSD) models are typically developed on datasets collected in the United States, thereby failing to generalize to English dialects from the Majority World. Furthermore, HSD models are often evaluated on curated samples, raising concerns about overestimating model performance in real-world settings. In this work, we introduce NaijaHate, the first dataset annotated for HSD which contains a representative sample of Nigerian tweets. We demonstrate that HSD evaluated on biased datasets traditionally used in the literature largely overestimates real-world performance on representative data. We also propose NaijaXLM-T, a pretrained model tailored to the Nigerian Twitter context, and establish the key role played by domain-adaptive pretraining and <b>finetuning</b> in maximizing HSD performance. Finally, we show that in this context, a <b>human-in-the-loop</b> approach to content moderation where humans review 1% of Nigerian tweets flagged as hateful would enable to moderate 60% of all hateful content. Taken together, these results pave the way towards robust HSD systems and a better protection of social media users from hateful content in <b>low-resource</b> settings.</p></p class="citation"></blockquote><h3 id=2551--25271-mitigating-misleading-chain-of-thought-reasoning-with-selective-filtering-yexin-wu-et-al-2024>(25/51 | 25/271) Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering (Yexin Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yexin Wu, Zhuosheng Zhang, Hai Zhao. (2024)<br><strong>Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering</strong><br><button class=copy-to-clipboard title="Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, T5, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19167v1.pdf filename=2403.19167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> have manifested remarkable capabilities by leveraging chain-of-thought (CoT) <b>reasoning</b> techniques to solve intricate questions through step-by-step <b>reasoning</b> chains. Despite its success, the efficacy of such <b>reasoning</b> is inherently contingent upon the quality of CoT. However, flawless CoT <b>reasoning</b> cannot be guaranteed due to the presence of indecomposable questions and the potential for erroneous <b>reasoning</b> chains, particularly in the case of small-scale language models. To tackle this challenge, we propose a novel approach called the selective filtering reasoner (SelF-Reasoner) that assesses the entailment relationship between the question and the candidate <b>reasoning</b> chain. Then, we proceed with CoT <b>reasoning</b> when the <b>reasoning</b> chain demonstrates confidence; otherwise, we opt to predict the answer directly. SelF-Reasoner improves the <b>fine-tuned</b> <b>T5</b> baseline consistently over the ScienceQA, ECQA, and LastLetter tasks. Code is available at \texttt{https://github.com/LibroWu/SelF-Reasoner}.</p></p class="citation"></blockquote><h3 id=2651--26271-learning-from-correctness-without-prompting-makes-llm-efficient-reasoner-yuxuan-yao-et-al-2024>(26/51 | 26/271) Learning From Correctness Without Prompting Makes LLM Efficient Reasoner (Yuxuan Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Yao, Han Wu, Zhijiang Guo, Biyan Zhou, Jiahui Gao, Sichun Luo, Hanxu Hou, Xiaojin Fu, Linqi Song. (2024)<br><strong>Learning From Correctness Without Prompting Makes LLM Efficient Reasoner</strong><br><button class=copy-to-clipboard title="Learning From Correctness Without Prompting Makes LLM Efficient Reasoner" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19094v1.pdf filename=2403.19094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful <b>reasoning,</b> and toxic content. One potential approach to mitigate these issues is learning from human or external feedback (e.g. tools). In this paper, we introduce an intrinsic self-correct <b>reasoning</b> framework for <b>LLMs</b> that eliminates the need for human feedback, external tools, and handcraft <b>prompts.</b> The proposed framework, based on a multi-step <b>reasoning</b> paradigm \textbf{Le}arning from \textbf{Co}rrectness (\textsc{LeCo}), improves <b>reasoning</b> performance without needing to learn from errors. This paradigm prioritizes learning from correct <b>reasoning</b> steps, and a unique method to measure confidence for each <b>reasoning</b> step based on generation logits. Experimental results across various multi-step <b>reasoning</b> tasks demonstrate the effectiveness of the framework in improving <b>reasoning</b> performance with reduced token consumption.</p></p class="citation"></blockquote><h3 id=2751--27271-risk-prediction-of-pathological-gambling-on-social-media-angelina-parfenova-et-al-2024>(27/51 | 27/271) Risk prediction of pathological gambling on social media (Angelina Parfenova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angelina Parfenova, Marianne Clausel. (2024)<br><strong>Risk prediction of pathological gambling on social media</strong><br><button class=copy-to-clipboard title="Risk prediction of pathological gambling on social media" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, BERT, LSTM, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19358v1.pdf filename=2403.19358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the problem of risk prediction on social media data, specifically focusing on the classification of Reddit users as having a pathological gambling disorder. To tackle this problem, this paper focuses on incorporating temporal and emotional features into the model. The preprocessing phase involves dealing with the time irregularity of posts by padding sequences. Two baseline architectures are used for preliminary evaluation: <b>BERT</b> classifier on concatenated posts per user and GRU with <b>LSTM</b> on sequential data. Experimental results demonstrate that the sequential models outperform the concatenation-based model. The results of the experiments conclude that the incorporation of a time decay layer (TD) and passing the emotion classification layer (EmoBERTa) through <b>LSTM</b> improves the performance significantly. Experiments concluded that the addition of a <b>self-attention</b> layer didn&rsquo;t significantly improve the performance of the model, however provided easily interpretable attention scores. The developed architecture with the inclusion of EmoBERTa and TD layers achieved a high F1 score, beating existing <b>benchmarks</b> on pathological gambling dataset. Future work may involve the early prediction of risk factors associated with pathological gambling disorder and testing models on other datasets. Overall, this research highlights the significance of the sequential processing of posts including temporal and emotional features to boost the predictive power, as well as adding an attention layer for interpretability.</p></p class="citation"></blockquote><h3 id=2851--28271-large-language-models-are-unconscious-of-unreasonability-in-math-problems-jingyuan-ma-et-al-2024>(28/51 | 28/271) Large Language Models Are Unconscious of Unreasonability in Math Problems (Jingyuan Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyuan Ma, Damai Dai, Zhifang Sui. (2024)<br><strong>Large Language Models Are Unconscious of Unreasonability in Math Problems</strong><br><button class=copy-to-clipboard title="Large Language Models Are Unconscious of Unreasonability in Math Problems" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19346v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19346v1.pdf filename=2403.19346v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> demonstrate substantial capabilities in solving math problems. However, they tend to produce hallucinations when given questions containing unreasonable errors. In this paper, we study the behavior of <b>LLMs</b> when faced with unreasonable math problems and further explore their potential to address these problems. First, we construct the Unreasonable Math Problem (UMP) <b>benchmark</b> to examine the error detection ability of <b>LLMs.</b> Experiments show that <b>LLMs</b> are able to detect unreasonable errors, but still fail in generating non-hallucinatory content. In order to improve their ability of error detection and correction, we further design a strategic <b>prompt</b> template called Critical Calculation and Conclusion(CCC). With CCC, <b>LLMs</b> can better self-evaluate and detect unreasonable errors in math questions, making them more reliable and safe in practical application scenarios.</p></p class="citation"></blockquote><h3 id=2951--29271-tablellm-enabling-tabular-data-manipulation-by-llms-in-real-office-usage-scenarios-xiaokang-zhang-et-al-2024>(29/51 | 29/271) TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios (Xiaokang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, Jifan Yu, Shu Zhao, Juanzi Li, Jie Tang. (2024)<br><strong>TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios</strong><br><button class=copy-to-clipboard title="TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19318v1.pdf filename=2403.19318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce TableLLM, a robust <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> with 13 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a <b>reasoning</b> process extension strategy, aiding in training <b>LLMs</b> to understand <b>reasoning</b> patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted a <b>benchmark</b> tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused <b>LLMs.</b> We have publicly released the model checkpoint, source code, <b>benchmarks,</b> and a web application for user interaction.</p></p class="citation"></blockquote><h3 id=3051--30271-code-comparison-tuning-for-code-large-language-models-yufan-jiang-et-al-2024>(30/51 | 30/271) Code Comparison Tuning for Code Large Language Models (Yufan Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufan Jiang, Qiaozhi He, Xiaomin Zhuang, Zhihua Wu. (2024)<br><strong>Code Comparison Tuning for Code Large Language Models</strong><br><button class=copy-to-clipboard title="Code Comparison Tuning for Code Large Language Models" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19121v1.pdf filename=2403.19121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Code Comparison Tuning (CCT), a simple and effective tuning method for code <b>large</b> <b>language</b> <b>models</b> (Code <b>LLMs)</b> to better handle subtle code errors. Specifically, we integrate the concept of comparison into <b>instruction</b> <b>tuning,</b> both at the token and sequence levels, enabling the model to discern even the slightest deviations in code. To compare the original code with an erroneous version containing manually added code errors, we use token-level preference loss for detailed token-level comparisons. Additionally, we combine code segments to create a new <b>instruction</b> <b>tuning</b> sample for sequence-level comparisons, enhancing the model&rsquo;s bug-fixing capability. Experimental results on the HumanEvalFix <b>benchmark</b> show that CCT surpasses <b>instruction</b> <b>tuning</b> in pass@1 scores by up to 4 points across diverse code <b>LLMs,</b> and extensive analysis demonstrates the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=3151--31271-kazparc-kazakh-parallel-corpus-for-machine-translation-rustem-yeshpanov-et-al-2024>(31/51 | 31/271) KazParC: Kazakh Parallel Corpus for Machine Translation (Rustem Yeshpanov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rustem Yeshpanov, Alina Polonskaya, Huseyin Atakan Varol. (2024)<br><strong>KazParC: Kazakh Parallel Corpus for Machine Translation</strong><br><button class=copy-to-clipboard title="KazParC: Kazakh Parallel Corpus for Machine Translation" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Neural Machine Translation, Neural Machine Translation, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19399v1.pdf filename=2403.19399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce KazParC, a parallel corpus designed for <b>machine</b> <b>translation</b> across Kazakh, English, Russian, and Turkish. The first and largest publicly available corpus of its kind, KazParC contains a collection of 371,902 parallel sentences covering different domains and developed with the assistance of human translators. Our research efforts also extend to the development of a <b>neural</b> <b>machine</b> <b>translation</b> model nicknamed Tilmash. Remarkably, the performance of Tilmash is on par with, and in certain instances, surpasses that of industry giants, such as Google Translate and Yandex Translate, as measured by standard evaluation metrics, such as <b>BLEU</b> and chrF. Both KazParC and Tilmash are openly available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository.</p></p class="citation"></blockquote><h3 id=3251--32271-beyond-borders-investigating-cross-jurisdiction-transfer-in-legal-case-summarization-t-y-s-s-santosh-et-al-2024>(32/51 | 32/271) Beyond Borders: Investigating Cross-Jurisdiction Transfer in Legal Case Summarization (T. Y. S. S Santosh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>T. Y. S. S Santosh, Vatsal Venkatkrishna, Saptarshi Ghosh, Matthias Grabmair. (2024)<br><strong>Beyond Borders: Investigating Cross-Jurisdiction Transfer in Legal Case Summarization</strong><br><button class=copy-to-clipboard title="Beyond Borders: Investigating Cross-Jurisdiction Transfer in Legal Case Summarization" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Unsupervised Learning, Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19317v1.pdf filename=2403.19317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Legal professionals face the challenge of managing an overwhelming volume of lengthy judgments, making automated legal case <b>summarization</b> crucial. However, prior approaches mainly focused on training and evaluating these models within the same jurisdiction. In this study, we explore the cross-jurisdictional generalizability of legal case <b>summarization</b> models.Specifically, we explore how to effectively <b>summarize</b> legal cases of a target jurisdiction where reference summaries are not available. In particular, we investigate whether supplementing models with unlabeled target jurisdiction corpus and extractive silver summaries obtained from <b>unsupervised</b> algorithms on target data enhances transfer performance. Our comprehensive study on three datasets from different jurisdictions highlights the role of pre-training in improving transfer performance. We shed light on the pivotal influence of jurisdictional similarity in selecting optimal source datasets for effective transfer. Furthermore, our findings underscore that incorporating unlabeled target data yields improvements in general pre-trained models, with additional gains when silver summaries are introduced. This augmentation is especially valuable when dealing with extractive datasets and scenarios featuring limited alignment between source and target jurisdictions. Our study provides key insights for developing adaptable legal case <b>summarization</b> systems, transcending jurisdictional boundaries.</p></p class="citation"></blockquote><h3 id=3351--33271-mateval-a-multi-agent-discussion-framework-for-advancing-open-ended-text-evaluation-yu-li-et-al-2024>(33/51 | 33/271) MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation (Yu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Li, Shenyu Zhang, Rui Wu, Xiutian Huang, Yongrui Chen, Wenhao Xu, Guilin Qi, Dehai Min. (2024)<br><strong>MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation</strong><br><button class=copy-to-clipboard title="MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: GPT, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19305v1.pdf filename=2403.19305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in generative Large Language Models(LLMs) have been remarkable, however, the quality of the text generated by these models often reveals persistent issues. Evaluating the quality of text generated by these models, especially in open-ended text, has consistently presented a significant challenge. Addressing this, recent work has explored the possibility of using <b>LLMs</b> as evaluators. While using a single <b>LLM</b> as an evaluation agent shows potential, it is filled with significant uncertainty and instability. To address these issues, we propose the MATEval: A &ldquo;Multi-Agent Text Evaluation framework&rdquo; where all agents are played by <b>LLMs</b> like <b>GPT-4.</b> The MATEval framework emulates human collaborative discussion methods, integrating multiple agents&rsquo; interactions to evaluate open-ended text. Our framework incorporates self-reflection and Chain-of-Thought (CoT) strategies, along with feedback mechanisms, enhancing the depth and breadth of the evaluation process and guiding discussions towards consensus, while the framework generates comprehensive evaluation reports, including error localization, error types and scoring. Experimental results show that our framework outperforms existing open-ended text evaluation methods and achieves the highest correlation with human evaluation, which confirms the effectiveness and advancement of our framework in addressing the uncertainties and instabilities in evaluating <b>LLMs-generated</b> text. Furthermore, our framework significantly improves the efficiency of text evaluation and model iteration in industrial scenarios.</p></p class="citation"></blockquote><h3 id=3451--34271-sdpo-dont-use-your-data-all-at-once-dahyun-kim-et-al-2024>(34/51 | 34/271) sDPO: Don&rsquo;t Use Your Data All at Once (Dahyun Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, Chanjun Park. (2024)<br><strong>sDPO: Don&rsquo;t Use Your Data All at Once</strong><br><button class=copy-to-clipboard title="sDPO: Don't Use Your Data All at Once" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Direct Preference Optimization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19270v1.pdf filename=2403.19270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As development of <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> progresses, aligning them with human preferences has become increasingly important. We propose stepwise DPO (sDPO), an extension of the recently popularized <b>direct</b> <b>preference</b> <b>optimization</b> (DPO) for alignment tuning. This approach involves dividing the available preference datasets and utilizing them in a stepwise manner, rather than employing it all at once. We demonstrate that this method facilitates the use of more precisely aligned reference models within the DPO training framework. Furthermore, sDPO trains the final model to be more performant, even outperforming other popular <b>LLMs</b> with more parameters.</p></p class="citation"></blockquote><h3 id=3551--35271-star-gate-teaching-language-models-to-ask-clarifying-questions-chinmaya-andukuri-et-al-2024>(35/51 | 35/271) STaR-GATE: Teaching Language Models to Ask Clarifying Questions (Chinmaya Andukuri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chinmaya Andukuri, Jan-Philipp Fränken, Tobias Gerstenberg, Noah D. Goodman. (2024)<br><strong>STaR-GATE: Teaching Language Models to Ask Clarifying Questions</strong><br><button class=copy-to-clipboard title="STaR-GATE: Teaching Language Models to Ask Clarifying Questions" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19154v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19154v2.pdf filename=2403.19154v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When <b>prompting</b> language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity (GATE; Li et al., 2023), models often struggle to ask good questions. We explore a language model&rsquo;s ability to self-improve (STaR; Zelikman et al., 2022) by rewarding the model for generating useful questions-a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task <b>prompts</b> to simulate conversations between a <b>pretrained</b> <b>language</b> <b>model-the</b> Questioner-and a Roleplayer whose preferences are unknown to the Questioner. By asking questions, the Questioner elicits preferences from the Roleplayer. The Questioner is iteratively <b>finetuned</b> on questions that increase the probability of high-quality responses to the task, which are generated by an Oracle with access to the Roleplayer&rsquo;s latent preferences. After two iterations of self-improvement, the Questioner asks better questions, allowing it to generate responses that are preferred over responses from the initial model on 72% of tasks. Our results indicate that teaching a language model to ask better questions leads to better personalized responses.</p></p class="citation"></blockquote><h3 id=3651--36271-jamba-a-hybrid-transformer-mamba-language-model-opher-lieber-et-al-2024>(36/51 | 36/271) Jamba: A Hybrid Transformer-Mamba Language Model (Opher Lieber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, Yoav Shoham. (2024)<br><strong>Jamba: A Hybrid Transformer-Mamba Language Model</strong><br><button class=copy-to-clipboard title="Jamba: A Hybrid Transformer-Mamba Language Model" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19887v1.pdf filename=2403.19887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Jamba, a new base <b>large</b> <b>language</b> <b>model</b> based on a novel hybrid <b>Transformer-Mamba</b> mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of <b>Transformer</b> and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at <b>large</b> <b>scale,</b> <b>Jamba</b> provides high throughput and small memory footprint compared to vanilla <b>Transformers,</b> and at the same time state-of-the-art performance on standard language model <b>benchmarks</b> and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine <b>Transformer</b> and Mamba layers, and how to mix experts, and show that some of them are crucial in <b>large</b> <b>scale</b> <b>modeling.</b> We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.</p></p class="citation"></blockquote><h3 id=3751--37271-new-semantic-task-for-the-french-spoken-language-understanding-media-benchmark-nadège-alavoine-et-al-2024>(37/51 | 37/271) New Semantic Task for the French Spoken Language Understanding MEDIA Benchmark (Nadège Alavoine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nadège Alavoine, Gaëlle Laperriere, Christophe Servan, Sahar Ghannay, Sophie Rosset. (2024)<br><strong>New Semantic Task for the French Spoken Language Understanding MEDIA Benchmark</strong><br><button class=copy-to-clipboard title="New Semantic Task for the French Spoken Language Understanding MEDIA Benchmark" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Dialogue System, Slot Filling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19727v1.pdf filename=2403.19727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intent classification and <b>slot-filling</b> <b>are</b> essential tasks of Spoken Language Understanding (SLU). In most SLUsystems, those tasks are realized by independent modules. For about fifteen years, models achieving both of themjointly and exploiting their mutual enhancement have been proposed. A multilingual module using a joint modelwas envisioned to create a touristic <b>dialogue</b> <b>system</b> for a European project, HumanE-AI-Net. A combination ofmultiple datasets, including the MEDIA dataset, was suggested for training this joint model. The MEDIA SLU datasetis a French dataset distributed since 2005 by ELRA, mainly used by the French research community and free foracademic research since 2020. Unfortunately, it is annotated only in <b>slots</b> <b>but</b> not intents. An enhanced version ofMEDIA annotated with intents has been built to extend its use to more tasks and use cases. This paper presents thesemi-automatic methodology used to obtain this enhanced version. In addition, we present the first results of SLUexperiments on this enhanced dataset using joint models for intent classification and slot-filling.</p></p class="citation"></blockquote><h3 id=3851--38271-target-span-detection-for-implicit-harmful-content-nazanin-jafari-et-al-2024>(38/51 | 38/271) Target Span Detection for Implicit Harmful Content (Nazanin Jafari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nazanin Jafari, James Allan. (2024)<br><strong>Target Span Detection for Implicit Harmful Content</strong><br><button class=copy-to-clipboard title="Target Span Detection for Implicit Harmful Content" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19836v1.pdf filename=2403.19836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying the targets of hate speech is a crucial step in grasping the nature of such speech and, ultimately, in improving the detection of offensive posts on online forums. Much harmful content on online platforms uses implicit language especially when targeting vulnerable and protected groups such as using stereotypical characteristics instead of explicit target names, making it harder to detect and mitigate the language. In this study, we focus on identifying implied targets of hate speech, essential for recognizing subtler hate speech and enhancing the detection of harmful content on digital platforms. We define a new task aimed at identifying the targets even when they are not explicitly stated. To address that task, we collect and annotate target spans in three prominent implicit hate speech datasets: SBIC, DynaHate, and IHC. We call the resulting merged collection Implicit-Target-Span. The collection is achieved using an innovative pooling method with matching scores based on human annotations and <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Our experiments indicate that Implicit-Target-Span provides a challenging test bed for target span detection methods.</p></p class="citation"></blockquote><h3 id=3951--39271-language-models-learn-rare-phenomena-from-less-rare-phenomena-the-case-of-the-missing-aanns-kanishka-misra-et-al-2024>(39/51 | 39/271) Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs (Kanishka Misra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kanishka Misra, Kyle Mahowald. (2024)<br><strong>Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs</strong><br><button class=copy-to-clipboard title="Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Counter-factual, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19827v1.pdf filename=2403.19827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models learn rare syntactic phenomena, but it has been argued that they rely on rote memorization, as opposed to grammatical generalization. Training on a corpus of human-scale in size (100M words), we iteratively trained <b>transformer</b> language models on systematically manipulated corpora and then evaluated their learning of a particular rare grammatical phenomenon: the English Article+Adjective+Numeral+Noun (AANN) construction (<code>a beautiful five days''). We first compared how well this construction was learned on the default corpus relative to a &lt;b>counterfactual&lt;/b> corpus in which the AANN sentences were removed. AANNs were still learned better than systematically perturbed variants of the construction. Using additional &lt;b>counterfactual&lt;/b> corpora, we suggest that this learning occurs through generalization from related constructions (e.g., </code>a few days&rsquo;&rsquo;). An additional experiment showed that this learning is enhanced when there is more variability in the input. Taken together, our results provide an existence proof that models learn rare grammatical phenomena by generalization from less rare phenomena. Code available at <a href=https://github.com/kanishkamisra/aannalysis>https://github.com/kanishkamisra/aannalysis</a></p></p class="citation"></blockquote><h3 id=4051--40271-dataverse-open-source-etl-extract-transform-load-pipeline-for-large-language-models-hyunbyung-park-et-al-2024>(40/51 | 40/271) Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large Language Models (Hyunbyung Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyunbyung Park, Sukyung Lee, Gyoungjin Gim, Yungi Kim, Dahyun Kim, Chanjun Park. (2024)<br><strong>Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large Language Models</strong><br><button class=copy-to-clipboard title="Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large Language Models" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19340v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19340v1.pdf filename=2403.19340v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To address the challenges associated with data processing at scale, we propose Dataverse, a unified open-source Extract-Transform-Load (ETL) pipeline for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with a user-friendly design at its core. Easy addition of custom processors with block-based interface in Dataverse allows users to readily and efficiently use Dataverse to build their own ETL pipeline. We hope that Dataverse will serve as a vital tool for <b>LLM</b> development and open source the entire library to welcome community contribution. Additionally, we provide a concise, two-minute video demonstration of our system, illustrating its capabilities and implementation.</p></p class="citation"></blockquote><h3 id=4151--41271-kazsandra-kazakh-sentiment-analysis-dataset-of-reviews-and-attitudes-rustem-yeshpanov-et-al-2024>(41/51 | 41/271) KazSAnDRA: Kazakh Sentiment Analysis Dataset of Reviews and Attitudes (Rustem Yeshpanov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rustem Yeshpanov, Huseyin Atakan Varol. (2024)<br><strong>KazSAnDRA: Kazakh Sentiment Analysis Dataset of Reviews and Attitudes</strong><br><button class=copy-to-clipboard title="KazSAnDRA: Kazakh Sentiment Analysis Dataset of Reviews and Attitudes" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19335v1.pdf filename=2403.19335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents KazSAnDRA, a dataset developed for Kazakh <b>sentiment</b> <b>analysis</b> that is the first and largest publicly available dataset of its kind. KazSAnDRA comprises an extensive collection of 180,064 reviews obtained from various sources and includes numerical ratings ranging from 1 to 5, providing a quantitative representation of customer attitudes. The study also pursued the automation of Kazakh <b>sentiment</b> <b>classification</b> through the development and evaluation of four machine learning models trained for both polarity classification and score classification. Experimental analysis included evaluation of the results considering both balanced and imbalanced scenarios. The most successful model attained an F1-score of 0.81 for polarity classification and 0.39 for score classification on the test sets. The dataset and <b>fine-tuned</b> models are open access and available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository.</p></p class="citation"></blockquote><h3 id=4251--42271-knowledge-boundary-and-persona-dynamic-shape-a-better-social-media-agent-junkai-zhou-et-al-2024>(42/51 | 42/271) Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent (Junkai Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junkai Zhou, Liang Pang, Ya Jing, Jia Gu, Huawei Shen, Xueqi Cheng. (2024)<br><strong>Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent</strong><br><button class=copy-to-clipboard title="Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19275v1.pdf filename=2403.19275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Constructing personalized and anthropomorphic agents holds significant importance in the <b>simulation</b> of social networks. However, there are still two key problems in existing works: the agent possesses world knowledge that does not belong to its personas, and it cannot eliminate the interference of diverse persona information on current actions, which reduces the personalization and anthropomorphism of the agent. To solve the above problems, we construct the social media agent based on personalized knowledge and dynamic persona information. For personalized knowledge, we add external knowledge sources and match them with the persona information of agents, thereby giving the agent personalized world knowledge. For dynamic persona information, we use current action information to internally retrieve the persona information of the agent, thereby reducing the interference of diverse persona information on the current action. To make the agent suitable for social media, we design five basic modules for it: persona, planning, action, memory and reflection. To provide an interaction and verification environment for the agent, we build a social media <b>simulation</b> sandbox. In the experimental verification, automatic and human evaluations demonstrated the effectiveness of the agent we constructed.</p></p class="citation"></blockquote><h3 id=4351--43271-collaborative-knowledge-infusion-for-low-resource-stance-detection-ming-yan-et-al-2024>(43/51 | 43/271) Collaborative Knowledge Infusion for Low-resource Stance Detection (Ming Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Yan, Joey Tianyi Zhou, Ivor W. Tsang. (2024)<br><strong>Collaborative Knowledge Infusion for Low-resource Stance Detection</strong><br><button class=copy-to-clipboard title="Collaborative Knowledge Infusion for Low-resource Stance Detection" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Low-Resource, Stance Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19219v1.pdf filename=2403.19219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Stance</b> <b>detection</b> is the view towards a specific target by a given context (\textit{e.g.} tweets, commercial reviews). Target-related knowledge is often needed to assist <b>stance</b> <b>detection</b> models in understanding the target well and making detection correctly. However, prevailing works for knowledge-infused <b>stance</b> <b>detection</b> predominantly incorporate target knowledge from a singular source that lacks knowledge verification in limited domain knowledge. The <b>low-resource</b> training data further increases the challenge for the data-driven large models in this task. To address those challenges, we propose a collaborative knowledge infusion approach for <b>low-resource</b> <b>stance</b> <b>detection</b> tasks, employing a combination of aligned knowledge enhancement and efficient parameter learning techniques. Specifically, our <b>stance</b> <b>detection</b> approach leverages target background knowledge collaboratively from different knowledge sources with the help of knowledge alignment. Additionally, we also introduce the parameter-efficient collaborative adaptor with a staged optimization algorithm, which collaboratively addresses the challenges associated with <b>low-resource</b> <b>stance</b> <b>detection</b> tasks from both network structure and learning perspectives. To assess the effectiveness of our method, we conduct extensive experiments on three public <b>stance</b> <b>detection</b> datasets, including <b>low-resource</b> and cross-target settings. The results demonstrate significant performance improvements compared to the existing <b>stance</b> <b>detection</b> approaches.</p></p class="citation"></blockquote><h3 id=4451--44271-empirical-analysis-for-unsupervised-universal-dependency-parse-tree-aggregation-adithya-kulkarni-et-al-2024>(44/51 | 44/271) Empirical Analysis for Unsupervised Universal Dependency Parse Tree Aggregation (Adithya Kulkarni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adithya Kulkarni, Oliver Eulenstein, Qi Li. (2024)<br><strong>Empirical Analysis for Unsupervised Universal Dependency Parse Tree Aggregation</strong><br><button class=copy-to-clipboard title="Empirical Analysis for Unsupervised Universal Dependency Parse Tree Aggregation" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Dependency Parsing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19183v1.pdf filename=2403.19183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Dependency</b> <b>parsing</b> is an essential task in NLP, and the quality of <b>dependency</b> <b>parsers</b> is crucial for many downstream tasks. Parsers&rsquo; quality often varies depending on the domain and the language involved. Therefore, it is essential to combat the issue of varying quality to achieve stable performance. In various NLP tasks, aggregation methods are used for post-processing aggregation and have been shown to combat the issue of varying quality. However, aggregation methods for post-processing aggregation have not been sufficiently studied in <b>dependency</b> <b>parsing</b> tasks. In an extensive empirical study, we compare different <b>unsupervised</b> post-processing aggregation methods to identify the most suitable <b>dependency</b> <b>tree</b> structure aggregation method.</p></p class="citation"></blockquote><h3 id=4551--45271-localizing-paragraph-memorization-in-language-models-niklas-stoehr-et-al-2024>(45/51 | 45/271) Localizing Paragraph Memorization in Language Models (Niklas Stoehr et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niklas Stoehr, Mitchell Gordon, Chiyuan Zhang, Owen Lewis. (2024)<br><strong>Localizing Paragraph Memorization in Language Models</strong><br><button class=copy-to-clipboard title="Localizing Paragraph Memorization in Language Models" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs-LG, cs.CL, stat-ML<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19851v1.pdf filename=2403.19851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Can we localize the weights and mechanisms used by a language model to memorize and recite entire paragraphs of its training data? In this paper, we show that while memorization is spread across multiple layers and model components, gradients of memorized paragraphs have a distinguishable spatial pattern, being larger in lower model layers than gradients of non-memorized examples. Moreover, the memorized examples can be unlearned by <b>fine-tuning</b> only the high-gradient weights. We localize a low-layer attention head that appears to be especially involved in paragraph memorization. This head is predominantly focusing its attention on distinctive, rare tokens that are least frequent in a corpus-level unigram distribution. Next, we study how localized memorization is across the tokens in the prefix by perturbing tokens and measuring the caused change in the decoding. A few distinctive tokens early in a prefix can often corrupt the entire continuation. Overall, memorized continuations are not only harder to unlearn, but also to corrupt than non-memorized ones.</p></p class="citation"></blockquote><h3 id=4651--46271-improving-adversarial-data-collection-by-supporting-annotators-lessons-from-gahd-a-german-hate-speech-dataset-janis-goldzycher-et-al-2024>(46/51 | 46/271) Improving Adversarial Data Collection by Supporting Annotators: Lessons from GAHD, a German Hate Speech Dataset (Janis Goldzycher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Janis Goldzycher, Paul Röttger, Gerold Schneider. (2024)<br><strong>Improving Adversarial Data Collection by Supporting Annotators: Lessons from GAHD, a German Hate Speech Dataset</strong><br><button class=copy-to-clipboard title="Improving Adversarial Data Collection by Supporting Annotators: Lessons from GAHD, a German Hate Speech Dataset" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Hate Speech Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19559v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19559v1.pdf filename=2403.19559v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Hate</b> <b>speech</b> <b>detection</b> models are only as good as the data they are trained on. Datasets sourced from social media suffer from systematic gaps and biases, leading to unreliable models with simplistic decision boundaries. Adversarial datasets, collected by exploiting model weaknesses, promise to fix this problem. However, adversarial data collection can be slow and costly, and individual annotators have limited creativity. In this paper, we introduce GAHD, a new German Adversarial <b>Hate</b> <b>speech</b> <b>Dataset</b> comprising ca.\ 11k examples. During data collection, we explore new strategies for supporting annotators, to create more diverse adversarial examples more efficiently and provide a manual analysis of annotator disagreements for each strategy. Our experiments show that the resulting dataset is challenging even for state-of-the-art <b>hate</b> <b>speech</b> <b>detection</b> models, and that training on GAHD clearly improves model robustness. Further, we find that mixing multiple support strategies is most advantageous. We make GAHD publicly available at <a href=https://github.com/jagol/gahd>https://github.com/jagol/gahd</a>.</p></p class="citation"></blockquote><h3 id=4751--47271-phonetic-segmentation-of-the-ucla-phonetics-lab-archive-eleanor-chodroff-et-al-2024>(47/51 | 47/271) Phonetic Segmentation of the UCLA Phonetics Lab Archive (Eleanor Chodroff et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eleanor Chodroff, Blaž Pažon, Annie Baker, Steven Moran. (2024)<br><strong>Phonetic Segmentation of the UCLA Phonetics Lab Archive</strong><br><button class=copy-to-clipboard title="Phonetic Segmentation of the UCLA Phonetics Lab Archive" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 10<br>Keywords: Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19509v1.pdf filename=2403.19509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research in speech technologies and comparative linguistics depends on access to diverse and accessible speech data. The UCLA Phonetics Lab Archive is one of the earliest multilingual speech corpora, with long-form audio recordings and phonetic transcriptions for 314 languages (Ladefoged et al., 2009). Recently, 95 of these languages were time-aligned with word-level phonetic transcriptions (Li et al., 2021). Here we present VoxAngeles, a corpus of audited phonetic transcriptions and phone-level alignments of the UCLA Phonetics Lab Archive, which uses the 95-language CMU re-release as our starting point. VoxAngeles also includes word- and phone-level segmentations from the original UCLA corpus, as well as phonetic measurements of word and phone durations, vowel formants, and vowel f0. This corpus enhances the usability of the original data, particularly for quantitative phonetic typology, as demonstrated through a case study of vowel intrinsic f0. We also discuss the utility of the VoxAngeles corpus for general research and pedagogy in crosslinguistic phonetics, as well as for <b>low-resource</b> and multilingual speech technologies. VoxAngeles is free to download and use under a CC-BY-NC 4.0 license.</p></p class="citation"></blockquote><h3 id=4851--48271-a-diverse-multilingual-news-headlines-dataset-from-around-the-world-felix-leeb-et-al-2024>(48/51 | 48/271) A diverse Multilingual News Headlines Dataset from around the World (Felix Leeb et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felix Leeb, Bernhard Schölkopf. (2024)<br><strong>A diverse Multilingual News Headlines Dataset from around the World</strong><br><button class=copy-to-clipboard title="A diverse Multilingual News Headlines Dataset from around the World" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: TF-IDF<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19352v1.pdf filename=2403.19352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Babel Briefings is a novel dataset featuring 4.7 million news headlines from August 2020 to November 2021, across 30 languages and 54 locations worldwide with English translations of all articles included. Designed for natural language processing and media studies, it serves as a high-quality dataset for training or evaluating language models as well as offering a simple, accessible collection of articles, for example, to analyze global news coverage and cultural narratives. As a simple demonstration of the analyses facilitated by this dataset, we use a basic procedure using a <b>TF-IDF</b> weighted similarity metric to group articles into clusters about the same event. We then visualize the \emph{event signatures} of the event showing articles of which languages appear over time, revealing intuitive features based on the proximity of the event and unexpectedness of the event. The dataset is available on \href{https://www.kaggle.com/datasets/felixludos/babel-briefings}{Kaggle} and \href{https://huggingface.co/datasets/felixludos/babel-briefings}{HuggingFace} with accompanying \href{https://github.com/felixludos/babel-briefings}{GitHub} code.</p></p class="citation"></blockquote><h3 id=4951--49271-mineland-simulating-large-scale-multi-agent-interactions-with-limited-multimodal-senses-and-physical-needs-xianhao-yu-et-al-2024>(49/51 | 49/271) MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs (Xianhao Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianhao Yu, Jiaqi Fu, Renjia Deng, Wenjuan Han. (2024)<br><strong>MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs</strong><br><button class=copy-to-clipboard title="MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 9<br>Keywords: Benchmarking, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19267v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19267v1.pdf filename=2403.19267v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional multi-agent simulators often assume perfect information and limitless capabilities, hindering the ecological validity of social interactions. We propose a multi-agent Minecraft simulator, MineLand, that bridges this gap by introducing limited <b>multimodal</b> senses and physical needs. Our simulator supports up to 48 agents with limited visual, auditory, and environmental awareness, forcing them to actively communicate and collaborate to fulfill physical needs like food and resources. This fosters dynamic and valid multi-agent interactions. We further introduce an AI agent framework, Alex, inspired by multitasking theory, enabling agents to handle intricate coordination and scheduling. Our experiments demonstrate that the simulator, the corresponding <b>benchmark,</b> and the AI agent framework contribute to more ecological and nuanced collective behavior. The source code of MineLand and Alex is openly available at <a href=https://github.com/cocacola-lab/MineLand>https://github.com/cocacola-lab/MineLand</a>.</p></p class="citation"></blockquote><h3 id=5051--50271-j-cre3-a-japanese-conversation-dataset-for-real-world-reference-resolution-nobuhiro-ueda-et-al-2024>(50/51 | 50/271) J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution (Nobuhiro Ueda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nobuhiro Ueda, Hideko Habe, Yoko Matsui, Akishige Yuguchi, Seiya Kawano, Yasutomo Kawanishi, Sadao Kurohashi, Koichiro Yoshino. (2024)<br><strong>J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution</strong><br><button class=copy-to-clipboard title="J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19259v1.pdf filename=2403.19259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding expressions that refer to the physical world is crucial for such human-assisting systems in the real world, as robots that must perform actions that are expected by users. In real-world reference resolution, a system must ground the verbal information that appears in user interactions to the visual information observed in egocentric views. To this end, we propose a <b>multimodal</b> reference resolution task and construct a Japanese Conversation dataset for Real-world Reference Resolution (J-CRe3). Our dataset contains egocentric video and dialogue audio of real-world conversations between two people acting as a master and an assistant robot at home. The dataset is annotated with crossmodal tags between phrases in the utterances and the object bounding boxes in the video frames. These tags include indirect reference relations, such as predicate-argument structures and bridging references as well as direct reference relations. We also constructed an experimental model and clarified the challenges in <b>multimodal</b> reference resolution tasks.</p></p class="citation"></blockquote><h3 id=5151--51271-semantic-map-based-generation-of-navigation-instructions-chengzu-li-et-al-2024>(51/51 | 51/271) Semantic Map-based Generation of Navigation Instructions (Chengzu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengzu Li, Chao Zhang, Simone Teufel, Rama Sanand Doddipatla, Svetlana Stoyanchev. (2024)<br><strong>Semantic Map-based Generation of Navigation Instructions</strong><br><button class=copy-to-clipboard title="Semantic Map-based Generation of Navigation Instructions" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19603v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19603v1.pdf filename=2403.19603v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We are interested in the generation of navigation instructions, either in their own right or as training material for robotic navigation task. In this paper, we propose a new approach to navigation instruction generation by framing the problem as an image captioning task using semantic maps as visual input. Conventional approaches employ a sequence of panorama images to generate navigation instructions. Semantic maps abstract away from visual details and fuse the information in multiple panorama images into a single top-down representation, thereby reducing computational complexity to process the input. We present a <b>benchmark</b> dataset for instruction generation using semantic maps, propose an initial model and ask human subjects to manually assess the quality of generated instructions. Our initial investigations show promise in using semantic maps for instruction generation instead of a sequence of panorama images, but there is vast scope for improvement. We release the code for data preparation and model training at <a href=https://github.com/chengzu-li/VLGen>https://github.com/chengzu-li/VLGen</a>.</p></p class="citation"></blockquote><h2 id=csir-8>cs.IR (8)</h2><h3 id=18--52271-are-large-language-models-good-at-utility-judgments-hengran-zhang-et-al-2024>(1/8 | 52/271) Are Large Language Models Good at Utility Judgments? (Hengran Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hengran Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng. (2024)<br><strong>Are Large Language Models Good at Utility Judgments?</strong><br><button class=copy-to-clipboard title="Are Large Language Models Good at Utility Judgments?" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 106<br>Keywords: Benchmarking, Benchmarking, Counter-factual, Dense Retrieval, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Question Answering, Question Answering, Large Language Model, Large Language Model, Retrieval Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19216v1.pdf filename=2403.19216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> is considered to be a promising approach to alleviate the hallucination issue of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of <b>retrieval</b> <b>models,</b> <b>the</b> success of <b>RAG</b> heavily lies on the ability of <b>LLMs</b> to identify passages with utility. Recent efforts have explored the ability of <b>LLMs</b> to assess the relevance of passages in <b>retrieval,</b> <b>but</b> <b>there</b> has been limited work on evaluating the utility of passages in supporting <b>question</b> <b>answering.</b> In this work, we conduct a comprehensive study about the capabilities of <b>LLMs</b> in utility evaluation for open-domain <b>QA.</b> Specifically, we introduce a <b>benchmarking</b> procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative <b>LLMs.</b> Our experiments reveal that: (i) well-instructed <b>LLMs</b> can distinguish between relevance and utility, and that <b>LLMs</b> are highly receptive to newly generated <b>counterfactual</b> passages. Moreover, (ii) we scrutinize key factors that affect utility judgments in the instruction design. And finally, (iii) to verify the efficacy of utility judgments in practical <b>retrieval</b> <b>augmentation</b> <b>applications,</b> we delve into <b>LLMs&rsquo;</b> <b>QA</b> capabilities using the evidence judged with utility and direct <b>dense</b> <b>retrieval</b> <b>results.</b> <b>(iv)</b> We propose a k-sampling, listwise approach to reduce the dependency of <b>LLMs</b> on the sequence of input passages, thereby facilitating subsequent answer generation. We believe that the way we formalize and study the problem along with our findings contributes to a critical assessment of <b>retrieval-augmented</b> <b>LLMs.</b> <b>Our</b> code and <b>benchmark</b> can be found at \url{https://github.com/ict-bigdatalab/utility_judgments}.</p></p class="citation"></blockquote><h3 id=28--53271-generate-then-retrieve-conversational-response-retrieval-using-llms-as-answer-and-query-generators-zahra-abbasiantaeb-et-al-2024>(2/8 | 53/271) Generate then Retrieve: Conversational Response Retrieval Using LLMs as Answer and Query Generators (Zahra Abbasiantaeb et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zahra Abbasiantaeb, Mohammad Aliannejadi. (2024)<br><strong>Generate then Retrieve: Conversational Response Retrieval Using LLMs as Answer and Query Generators</strong><br><button class=copy-to-clipboard title="Generate then Retrieve: Conversational Response Retrieval Using LLMs as Answer and Query Generators" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 93<br>Keywords: Benchmarking, Few-shot, Zero-shot, GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19302v1.pdf filename=2403.19302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>CIS is a prominent area in IR that focuses on developing interactive knowledge assistants. These systems must adeptly comprehend the user&rsquo;s information requirements within the conversational context and retrieve the relevant information. To this aim, the existing approaches model the user&rsquo;s information needs with one query called rewritten query and use this query for passage retrieval. In this paper, we propose three different methods for generating multiple queries to enhance the retrieval. In these methods, we leverage the capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in understanding the user&rsquo;s information need and generating an appropriate response, to generate multiple queries. We implement and evaluate the proposed models utilizing various <b>LLMs</b> including <b>GPT-4</b> and <b>Llama-2</b> chat in <b>zero-shot</b> and <b>few-shot</b> settings. In addition, we propose a new <b>benchmark</b> for TREC iKAT based on <b>gpt</b> 3.5 judgments. Our experiments reveal the effectiveness of our proposed models on the TREC iKAT dataset.</p></p class="citation"></blockquote><h3 id=38--54271-make-large-language-model-a-better-ranker-wenshuo-chao-et-al-2024>(3/8 | 54/271) Make Large Language Model a Better Ranker (Wenshuo Chao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenshuo Chao, Zhi Zheng, Hengshu Zhu, Hao Liu. (2024)<br><strong>Make Large Language Model a Better Ranker</strong><br><button class=copy-to-clipboard title="Make Large Language Model a Better Ranker" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 50<br>Keywords: Recommendation, Recommender System, Language Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19181v1.pdf filename=2403.19181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has significantly enhanced capabilities across various fields, leading to a paradigm shift in how <b>Recommender</b> <b>Systems</b> (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise <b>recommendation</b> paradigms. These approaches prove inefficient in <b>LLM-based</b> <b>recommenders</b> <b>due</b> to the high computational cost of utilizing <b>Large</b> <b>Language</b> <b>Models.</b> While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and <b>language</b> <b>generation.</b> To this end, this paper introduces the <b>Language</b> <b>Model</b> Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of <b>LLMs</b> and the nuanced requirements of ranking tasks within <b>recommender</b> <b>systems.</b> A key feature of ALRO is the introduction of soft lambda loss, an adaptation of lambda loss tailored to suit <b>language</b> <b>generation</b> tasks. Additionally, ALRO incorporates a permutation-sensitive learning mechanism that addresses position bias, a prevalent issue in generative models, without imposing additional computational burdens during inference. Our evaluative studies reveal that ALRO outperforms existing embedding-based <b>recommendation</b> methods and the existing <b>LLM-based</b> <b>recommendation</b> baselines, highlighting its efficacy.</p></p class="citation"></blockquote><h3 id=48--55271-instruction-based-hypergraph-pretraining-mingdai-yang-et-al-2024>(4/8 | 55/271) Instruction-based Hypergraph Pretraining (Mingdai Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingdai Yang, Zhiwei Liu, Liangwei Yang, Xiaolong Liu, Chen Wang, Hao Peng, Philip S. Yu. (2024)<br><strong>Instruction-based Hypergraph Pretraining</strong><br><button class=copy-to-clipboard title="Instruction-based Hypergraph Pretraining" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 38<br>Keywords: Graph, Convolution, Representation Learning, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19063v1.pdf filename=2403.19063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretraining has been widely explored to augment the adaptability of <b>graph</b> learning models to transfer knowledge from large datasets to a downstream task, such as link prediction or classification. However, the gap between training objectives and the discrepancy between data distributions in pretraining and downstream tasks hinders the transfer of the <b>pretrained</b> <b>knowledge.</b> <b>Inspired</b> by instruction-based <b>prompts</b> widely used in <b>pretrained</b> <b>language</b> <b>models,</b> we introduce instructions into <b>graph</b> pretraining. In this paper, we propose a novel pretraining framework named Instruction-based Hypergraph Pretraining. To overcome the discrepancy between pretraining and downstream tasks, text-based instructions are applied to provide explicit guidance on specific tasks for <b>representation</b> <b>learning.</b> Compared to learnable <b>prompts,</b> whose effectiveness depends on the quality and the diversity of training data, text-based instructions intrinsically encapsulate task information and support the model to generalize beyond the structure seen during pretraining. To capture high-order relations with task information in a context-aware manner, a novel <b>prompting</b> hypergraph <b>convolution</b> layer is devised to integrate instructions into information propagation in hypergraphs. Extensive experiments conducted on three public datasets verify the superiority of IHP in various scenarios.</p></p class="citation"></blockquote><h3 id=58--56271-dealing-with-missing-modalities-in-multimodal-recommendation-a-feature-propagation-based-approach-daniele-malitesta-et-al-2024>(5/8 | 56/271) Dealing with Missing Modalities in Multimodal Recommendation: a Feature Propagation-based Approach (Daniele Malitesta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniele Malitesta, Emanuele Rossi, Claudio Pomo, Fragkiskos D. Malliaros, Tommaso Di Noia. (2024)<br><strong>Dealing with Missing Modalities in Multimodal Recommendation: a Feature Propagation-based Approach</strong><br><button class=copy-to-clipboard title="Dealing with Missing Modalities in Multimodal Recommendation: a Feature Propagation-based Approach" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 34<br>Keywords: Graph, Multi-modal, Multi-modal, Recommendation, Recommender System, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19841v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19841v1.pdf filename=2403.19841v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>recommender</b> <b>systems</b> work by augmenting the <b>representation</b> <b>of</b> the products in the catalogue through <b>multimodal</b> features extracted from images, textual descriptions, or audio tracks characterising such products. Nevertheless, in real-world applications, only a limited percentage of products come with <b>multimodal</b> content to extract meaningful features from, making it hard to provide accurate <b>recommendations.</b> To the best of our knowledge, very few attention has been put into the problem of missing modalities in <b>multimodal</b> <b>recommendation</b> so far. To this end, our paper comes as a preliminary attempt to formalise and address such an issue. Inspired by the recent advances in <b>graph</b> <b>representation</b> <b>learning,</b> we propose to re-sketch the missing modalities problem as a problem of missing <b>graph</b> node features to apply the state-of-the-art feature propagation algorithm eventually. Technically, we first project the user-item <b>graph</b> into an item-item one based on co-interactions. Then, leveraging the <b>multimodal</b> similarities among co-interacted items, we apply a modified version of the feature propagation technique to impute the missing <b>multimodal</b> features. Adopted as a pre-processing stage for two recent <b>multimodal</b> <b>recommender</b> <b>systems,</b> our simple approach performs better than other shallower solutions on three popular datasets.</p></p class="citation"></blockquote><h3 id=68--57271-breaking-the-length-barrier-llm-enhanced-ctr-prediction-in-long-textual-user-behaviors-binzong-geng-et-al-2024>(6/8 | 57/271) Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors (Binzong Geng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie Yuan, Jun Zhou, Linjian Mo. (2024)<br><strong>Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors</strong><br><button class=copy-to-clipboard title="Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19347v1.pdf filename=2403.19347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rise of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> recent works have leveraged <b>LLMs</b> to improve the performance of click-through rate (CTR) prediction. However, we argue that a critical obstacle remains in deploying <b>LLMs</b> for practical use: the efficiency of <b>LLMs</b> when processing long textual user behaviors. As user sequences grow longer, the current efficiency of <b>LLMs</b> is inadequate for training on billions of users and items. To break through the efficiency barrier of <b>LLMs,</b> we propose Behavior Aggregated Hierarchical Encoding (BAHE) to enhance the efficiency of <b>LLM-based</b> CTR modeling. Specifically, BAHE proposes a novel hierarchical architecture that decouples the encoding of user behaviors from inter-behavior interactions. Firstly, to prevent computational redundancy from repeated encoding of identical user behaviors, BAHE employs the <b>LLM&rsquo;s</b> pre-trained shallow layers to extract embeddings of the most granular, atomic user behaviors from extensive user sequences and stores them in the offline database. Subsequently, the deeper, trainable layers of the <b>LLM</b> facilitate intricate inter-behavior interactions, thereby generating comprehensive user embeddings. This separation allows the learning of high-level user representations to be independent of low-level behavior encoding, significantly reducing computational complexity. Finally, these refined user embeddings, in conjunction with correspondingly processed item embeddings, are incorporated into the CTR model to compute the CTR scores. Extensive experimental results show that BAHE reduces training time and memory by five times for CTR models using <b>LLMs,</b> especially with longer user sequences. BAHE has been deployed in a real-world system, allowing for daily updates of 50 million CTR data on 8 A100 GPUs, making <b>LLMs</b> practical for industrial CTR prediction.</p></p class="citation"></blockquote><h3 id=78--58271-intelligent-classification-and-personalized-recommendation-of-e-commerce-products-based-on-machine-learning-kangming-xu-et-al-2024>(7/8 | 58/271) Intelligent Classification and Personalized Recommendation of E-commerce Products Based on Machine Learning (Kangming Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kangming Xu, Huiming Zhou, Haotian Zheng, Mingwei Zhu, Qi Xin. (2024)<br><strong>Intelligent Classification and Personalized Recommendation of E-commerce Products Based on Machine Learning</strong><br><button class=copy-to-clipboard title="Intelligent Classification and Personalized Recommendation of E-commerce Products Based on Machine Learning" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19345v1.pdf filename=2403.19345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid evolution of the Internet and the exponential proliferation of information, users encounter information overload and the conundrum of choice. Personalized <b>recommendation</b> systems play a pivotal role in alleviating this burden by aiding users in filtering and selecting information tailored to their preferences and requirements. Such systems not only enhance user experience and satisfaction but also furnish opportunities for businesses and platforms to augment user engagement, sales, and advertising efficacy.This paper undertakes a comparative analysis between the operational mechanisms of traditional e-commerce commodity classification systems and personalized <b>recommendation</b> systems. It delineates the significance and application of personalized <b>recommendation</b> systems across e-commerce, content information, and media domains. Furthermore, it delves into the challenges confronting personalized <b>recommendation</b> systems in e-commerce, including data privacy, algorithmic bias, scalability, and the cold start problem. Strategies to address these challenges are elucidated.Subsequently, the paper outlines a personalized <b>recommendation</b> system leveraging the <b>BERT</b> model and nearest neighbor algorithm, specifically tailored to address the exigencies of the eBay e-commerce platform. The efficacy of this <b>recommendation</b> system is substantiated through manual evaluation, and a practical application operational guide and structured output <b>recommendation</b> results are furnished to ensure the system&rsquo;s operability and scalability.</p></p class="citation"></blockquote><h3 id=88--59271-enhanced-bayesian-personalized-ranking-for-robust-hard-negative-sampling-in-recommender-systems-kexin-shi-et-al-2024>(8/8 | 59/271) Enhanced Bayesian Personalized Ranking for Robust Hard Negative Sampling in Recommender Systems (Kexin Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kexin Shi, Jing Zhang, Linjiajie Fang, Wenjia Wang, Bingyi Jing. (2024)<br><strong>Enhanced Bayesian Personalized Ranking for Robust Hard Negative Sampling in Recommender Systems</strong><br><button class=copy-to-clipboard title="Enhanced Bayesian Personalized Ranking for Robust Hard Negative Sampling in Recommender Systems" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19276v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19276v1.pdf filename=2403.19276v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In implicit collaborative filtering, hard negative mining techniques are developed to accelerate and enhance the <b>recommendation</b> model learning. However, the inadvertent selection of false negatives remains a major concern in hard negative sampling, as these false negatives can provide incorrect information and mislead the model learning. To date, only a small number of studies have been committed to solve the false negative problem, primarily focusing on designing sophisticated sampling algorithms to filter false negatives. In contrast, this paper shifts its focus to refining the loss function. We find that the original Bayesian Personalized Ranking (BPR), initially designed for uniform negative sampling, is inadequate in adapting to hard sampling scenarios. Hence, we introduce an enhanced Bayesian Personalized Ranking objective, named as Hard-BPR, which is specifically crafted for dynamic hard negative sampling to mitigate the influence of false negatives. This method is simple yet efficient for real-world deployment. Extensive experiments conducted on three real-world datasets demonstrate the effectiveness and robustness of our approach, along with the enhanced ability to distinguish false negatives.</p></p class="citation"></blockquote><h2 id=csni-3>cs.NI (3)</h2><h3 id=13--60271-chattracer-large-language-model-powered-real-time-bluetooth-device-tracking-system-qijun-wang-et-al-2024>(1/3 | 60/271) ChatTracer: Large Language Model Powered Real-time Bluetooth Device Tracking System (Qijun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qijun Wang, Shichen Zhang, Kunzhe Song, Huacheng Zeng. (2024)<br><strong>ChatTracer: Large Language Model Powered Real-time Bluetooth Device Tracking System</strong><br><button class=copy-to-clipboard title="ChatTracer: Large Language Model Powered Real-time Bluetooth Device Tracking System" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-NI, cs.NI<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Supervised Learning, Bard, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19833v1.pdf filename=2403.19833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> exemplified by OpenAI <b>ChatGPT</b> and Google <b>Bard,</b> have transformed the way we interact with cyber technologies. In this paper, we study the possibility of connecting <b>LLM</b> with wireless sensor networks (WSN). A successful design will not only extend <b>LLM&rsquo;s</b> knowledge landscape to the physical world but also revolutionize human interaction with WSN. To the end, we present ChatTracer, an <b>LLM-powered</b> real-time Bluetooth device tracking system. ChatTracer comprises three key components: an array of Bluetooth sniffing nodes, a database, and a <b>fine-tuned</b> <b>LLM.</b> ChatTracer was designed based on our experimental observation that commercial Apple/Android devices always broadcast hundreds of BLE packets per minute even in their idle status. Its novelties lie in two aspects: i) a reliable and efficient BLE packet grouping algorithm; and ii) an <b>LLM</b> <b>fine-tuning</b> strategy that combines both <b>supervised</b> <b>fine-tuning</b> (SFT) and <b>reinforcement</b> <b>learning</b> with human feedback <b>(RLHF).</b> We have built a prototype of ChatTracer with four sniffing nodes. Experimental results show that ChatTracer not only outperforms existing localization approaches, but also provides an intelligent interface for user interaction.</p></p class="citation"></blockquote><h3 id=23--61271-deep-learning-based-modulation-classification-of-practical-ofdm-signals-for-spectrum-sensing-byungjun-kim-et-al-2024>(2/3 | 61/271) Deep Learning-based Modulation Classification of Practical OFDM Signals for Spectrum Sensing (Byungjun Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Byungjun Kim, Christoph Mecklenbräuker, Peter Gerstoft. (2024)<br><strong>Deep Learning-based Modulation Classification of Practical OFDM Signals for Spectrum Sensing</strong><br><button class=copy-to-clipboard title="Deep Learning-based Modulation Classification of Practical OFDM Signals for Spectrum Sensing" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19292v1.pdf filename=2403.19292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, the modulation of symbols on OFDM subcarriers is classified for transmissions following Wi-Fi~6 and 5G downlink specifications. First, our approach estimates the OFDM symbol duration and cyclic prefix length based on the cyclic autocorrelation function. We propose a feature extraction algorithm characterizing the modulation of OFDM signals, which includes removing the effects of a synchronization error. The obtained feature is converted into a 2D histogram of phase and amplitude and this histogram is taken as input to a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)-based</b> classifier. The classifier does not require prior knowledge of protocol-specific information such as Wi-Fi preamble or resource allocation of 5G physical channels. The classifier&rsquo;s performance, evaluated using synthetic and real-world measured over-the-air (OTA) datasets, achieves a minimum accuracy of 97% accuracy with OTA data when SNR is above the value required for data transmission.</p></p class="citation"></blockquote><h3 id=33--62271-performance-evaluation-of-ieee-80211bf-protocol-in-the-sub-7-ghz-band-anirudha-sahoo-et-al-2024>(3/3 | 62/271) Performance Evaluation of IEEE 802.11bf Protocol in the sub-7 GHz Band (Anirudha Sahoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anirudha Sahoo, Tanguy Ropitault, Steve Blandino, Nada Golmie. (2024)<br><strong>Performance Evaluation of IEEE 802.11bf Protocol in the sub-7 GHz Band</strong><br><button class=copy-to-clipboard title="Performance Evaluation of IEEE 802.11bf Protocol in the sub-7 GHz Band" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19825v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19825v1.pdf filename=2403.19825v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Changes in Wi-Fi signal, using Wi-Fi sensing, have been used to detect movements in the environment and have led to development of many related applications. However, there has not been a standard way to do this until the IEEE 802.11bf standard development activity was taken up recently by the IEEE. Wi-Fi sensing is an overhead to data communication. While the IEEE 802.11bf standard has been designed with careful attention to the overhead and its impact on data communication, there has been no study done to quantify those. Therefore, in this paper, we evaluate performance of IEEE 802.11bf protocol with different system configurations corresponding to different sensing loads and the impact of sensing on data communication in those configurations. We outline some of the key findings from our <b>simulation</b> experiments which may be useful in practical operating configurations of an IEEE 802.11bf network.</p></p class="citation"></blockquote><h2 id=cscv-82>cs.CV (82)</h2><h3 id=182--63271-multi-frame-lightweight--efficient-vision-language-models-for-question-answering-in-autonomous-driving-akshay-gopalkrishnan-et-al-2024>(1/82 | 63/271) Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving (Akshay Gopalkrishnan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshay Gopalkrishnan, Ross Greer, Mohan Trivedi. (2024)<br><strong>Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving</strong><br><button class=copy-to-clipboard title="Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 83<br>Keywords: Multi-modal, Question Answering, Reasoning, Visual Question Answering, BLEU, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19838v1.pdf filename=2403.19838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-Language</b> Models (VLMs) and <b>Multi-Modal</b> Language models (MMLMs) have become prominent in autonomous driving research, as these models can provide interpretable textual <b>reasoning</b> and responses for end-to-end autonomous driving safety tasks using traffic scene images and other data modalities. However, current approaches to these systems use expensive <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary. To address these previous issues, we develop EM-VLM4AD, an efficient, lightweight, multi-frame vision language model which performs <b>Visual</b> <b>Question</b> <b>Answering</b> for autonomous driving. In comparison to previous approaches, EM-VLM4AD requires at least 10 times less memory and floating point operations, while also achieving higher <b>BLEU-4,</b> METEOR, CIDEr, and ROGUE scores than the existing baseline on the DriveLM dataset. EM-VLM4AD also exhibits the ability to extract relevant information from traffic views related to <b>prompts</b> and can answer <b>questions</b> <b>for</b> various autonomous driving subtasks. We release our code to train and evaluate our model at <a href=https://github.com/akshaygopalkr/EM-VLM4AD>https://github.com/akshaygopalkr/EM-VLM4AD</a>.</p></p class="citation"></blockquote><h3 id=282--64271-ivlmap-instance-aware-visual-language-grounding-for-consumer-robot-navigation-jiacui-huang-et-al-2024>(2/82 | 64/271) IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation (Jiacui Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacui Huang, Hongtao Zhang, Mingbo Zhao, Zhou Wu. (2024)<br><strong>IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation</strong><br><button class=copy-to-clipboard title="IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 80<br>Keywords: Simulation, Simulator, Zero-shot, Grounding, Reasoning, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19336v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19336v1.pdf filename=2403.19336v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-and-Language</b> Navigation (VLN) is a challenging task that requires a robot to navigate in photo-realistic environments with human natural language <b>promptings.</b> Recent studies aim to handle this task by constructing the semantic spatial map representation of the environment, and then leveraging the strong ability of <b>reasoning</b> in <b>large</b> <b>language</b> <b>models</b> for generalizing code for guiding the robot navigation. However, these methods face limitations in instance-level and attribute-level navigation tasks as they cannot distinguish different instances of the same object. To address this challenge, we propose a new method, namely, Instance-aware Visual Language Map (IVLMap), to empower the robot with instance-level and attribute-level semantic mapping, where it is autonomously constructed by fusing the RGBD video data collected from the robot agent with special-designed natural language map indexing in the bird&rsquo;s-in-eye view. Such indexing is instance-level and attribute-level. In particular, when integrated with a <b>large</b> <b>language</b> <b>model,</b> IVLMap demonstrates the capability to i) transform natural language into navigation targets with instance and attribute information, enabling precise localization, and ii) accomplish <b>zero-shot</b> end-to-end navigation tasks based on natural language commands. Extensive navigation experiments are conducted. <b>Simulation</b> results illustrate that our method can achieve an average improvement of 14.4% in navigation accuracy. Code and demo are released at <a href=https://ivlmap.github.io/>https://ivlmap.github.io/</a>.</p></p class="citation"></blockquote><h3 id=382--65271-text-data-centric-image-captioning-with-interactive-prompts-yiyu-wang-et-al-2024>(3/82 | 65/271) Text Data-Centric Image Captioning with Interactive Prompts (Yiyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyu Wang, Hao Luo, Jungang Xu, Yingfei Sun, Fan Wang. (2024)<br><strong>Text Data-Centric Image Captioning with Interactive Prompts</strong><br><button class=copy-to-clipboard title="Text Data-Centric Image Captioning with Interactive Prompts" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 80<br>Keywords: Supervised Learning, Unsupervised Learning, GPT, GPT-2, Image2text, Prompt, Text Embedding, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19193v1.pdf filename=2403.19193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Supervised</b> image captioning approaches have made great progress, but it is challenging to collect high-quality human-annotated <b>image-text</b> data. Recently, large-scale vision and language models (e.g., CLIP) and large-scale generative language models (e.g., <b>GPT-2)</b> have shown strong performances in various tasks, which also provide some new solutions for image captioning with web paired data, unpaired data or even <b>text-only</b> <b>data.</b> Among them, the mainstream solution is to project image embeddings into the <b>text</b> <b>embedding</b> space with the assistance of consistent representations between <b>image-text</b> pairs from the CLIP model. However, the current methods still face several challenges in adapting to the diversity of data configurations in a unified solution, accurately estimating <b>image-text</b> embedding bias, and correcting unsatisfactory prediction results in the inference stage. This paper proposes a new <b>Text</b> <b>data-centric</b> approach with Interactive <b>Prompts</b> for image Captioning, named TIPCap. 1) We consider four different settings which gradually reduce the dependence on paired data. 2) We construct a mapping module driven by multivariate Gaussian distribution to mitigate the modality gap, which is applicable to the above four different settings. 3) We propose a <b>prompt</b> interaction module that can incorporate optional <b>prompt</b> information before generating captions. Extensive experiments show that our TIPCap outperforms other weakly or <b>unsupervised</b> image captioning methods and achieves a new state-of-the-art performance on two widely used datasets, i.e., MS-COCO and Flickr30K.</p></p class="citation"></blockquote><h3 id=482--66271-plug-and-play-grounding-of-reasoning-in-multimodal-large-language-models-jiaxing-chen-et-al-2024>(4/82 | 66/271) Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models (Jiaxing Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Ziyong Feng, Yongle Zhao, Yin Xie. (2024)<br><strong>Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 79<br>Keywords: Benchmarking, Multi-modal, Multi-modal, GPT, Grounding, Instruction Following, Reasoning, Tokenization, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19322v1.pdf filename=2403.19322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The surge of <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs), given their prominent emergent capabilities in <b>instruction</b> <b>following</b> and <b>reasoning,</b> has greatly advanced the field of visual <b>reasoning.</b> However, constrained by their non-lossless image <b>tokenization,</b> most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images. To address this, we propose P2G, a novel framework for plug-and-play <b>grounding</b> of <b>reasoning</b> in MLLMs. Specifically, P2G exploits the tool-usage potential of MLLMs to employ expert agents to achieve on-the-fly <b>grounding</b> to critical visual and textual objects of image, thus achieving deliberate <b>reasoning</b> via <b>multimodal</b> <b>prompting.</b> We further create P2GB, a <b>benchmark</b> aimed at assessing MLLMs&rsquo; ability to understand inter-object relationships and text in challenging high-resolution images. Comprehensive experiments on visual <b>reasoning</b> tasks demonstrate the superiority of P2G. Noteworthy, P2G achieved comparable performance with <b>GPT-4V</b> on P2GB, with a 7B backbone. Our work highlights the potential of plug-and-play <b>grounding</b> of <b>reasoning</b> and opens up a promising alternative beyond model scaling.</p></p class="citation"></blockquote><h3 id=582--67271-automated-black-box-prompt-engineering-for-personalized-text-to-image-generation-yutong-he-et-al-2024>(5/82 | 67/271) Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation (Yutong He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutong He, Alexander Robey, Naoki Murata, Yiding Jiang, Joshua Williams, George J. Pappas, Hamed Hassani, Yuki Mitsufuji, Ruslan Salakhutdinov, J. Zico Kolter. (2024)<br><strong>Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 65<br>Keywords: Black Box, Text2image, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19103v1.pdf filename=2403.19103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt</b> engineering is effective for controlling the output of <b>text-to-image</b> (T2I) generative models, but it is also laborious due to the need for manually crafted <b>prompts.</b> This challenge has spurred the development of algorithms for automated <b>prompt</b> generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive <b>prompts.</b> In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable <b>prompts</b> that can effectively generate desired concepts given only <b>black-box</b> <b>access</b> to T2I models. Inspired by <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> jailbreaking, PRISM leverages the <b>in-context</b> <b>learning</b> ability of <b>LLMs</b> to iteratively refine the candidate <b>prompts</b> distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate <b>prompts</b> for objects, styles and images across multiple T2I models, including Stable Diffusion, DALL-E, and Midjourney.</p></p class="citation"></blockquote><h3 id=682--68271-img2loc-revisiting-image-geolocalization-using-multi-modality-foundation-models-and-image-based-retrieval-augmented-generation-zhongliang-zhou-et-al-2024>(6/82 | 68/271) Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation (Zhongliang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, Gengchen Mai. (2024)<br><strong>Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation</strong><br><button class=copy-to-clipboard title="Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Benchmarking, Foundation Model, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Information Retrieval, Text Generation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19584v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19584v1.pdf filename=2403.19584v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Geolocating precise locations from images presents a challenging problem in computer vision and <b>information</b> <b>retrieval.Traditional</b> <b>methods</b> <b>typically</b> employ either classification, which dividing the Earth surface into grid cells and classifying images accordingly, or <b>retrieval,</b> <b>which</b> <b>identifying</b> locations by matching images with a database of image-location pairs. However, classification-based approaches are limited by the cell size and cannot yield precise predictions, while <b>retrieval-based</b> <b>systems</b> <b>usually</b> suffer from poor search quality and inadequate coverage of the global landscape at varied scale and aggregation levels. To overcome these drawbacks, we present Img2Loc, a novel system that redefines image geolocalization as a <b>text</b> <b>generation</b> task. This is achieved using cutting-edge large multi-modality models like GPT4V or LLaVA with <b>retrieval</b> <b>augmented</b> <b>generation.</b> Img2Loc first employs CLIP-based representations to generate an image-based coordinate query database. It then uniquely combines query results with images itself, forming elaborate <b>prompts</b> customized for LMMs. When tested on <b>benchmark</b> datasets such as Im2GPS3k and YFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art models but does so without any model training.</p></p class="citation"></blockquote><h3 id=782--69271-patch-spatio-temporal-relation-prediction-for-video-anomaly-detection-hao-shen-et-al-2024>(7/82 | 69/271) Patch Spatio-Temporal Relation Prediction for Video Anomaly Detection (Hao Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Shen, Lu Shi, Wanru Xu, Yigang Cen, Linna Zhang, Gaoyun An. (2024)<br><strong>Patch Spatio-Temporal Relation Prediction for Video Anomaly Detection</strong><br><button class=copy-to-clipboard title="Patch Spatio-Temporal Relation Prediction for Video Anomaly Detection" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Vision Transformer, Anomaly Detection, Benchmarking, Self-supervised Learning, Self-supervised Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19111v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19111v1.pdf filename=2403.19111v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video <b>Anomaly</b> <b>Detection</b> (VAD), aiming to identify abnormalities within a specific context and timeframe, is crucial for intelligent Video Surveillance Systems. While recent deep learning-based VAD models have shown promising results by generating high-resolution frames, they often lack competence in preserving detailed spatial and temporal coherence in video frames. To tackle this issue, we propose a <b>self-supervised</b> <b>learning</b> approach for VAD through an inter-patch relationship prediction task. Specifically, we introduce a two-branch <b>vision</b> <b>transformer</b> network designed to capture deep visual features of video frames, addressing spatial and temporal dimensions responsible for modeling appearance and motion patterns, respectively. The inter-patch relationship in each dimension is decoupled into inter-patch similarity and the order information of each patch. To mitigate memory consumption, we convert the order information prediction task into a multi-label learning problem, and the inter-patch similarity prediction task into a distance matrix regression problem. Comprehensive experiments demonstrate the effectiveness of our method, surpassing pixel-generation-based methods by a significant margin across three public <b>benchmarks.</b> Additionally, our approach outperforms other <b>self-supervised</b> <b>learning-based</b> methods.</p></p class="citation"></blockquote><h3 id=882--70271-jointly-training-and-pruning-cnns-via-learnable-agent-guidance-and-alignment-alireza-ganjdanesh-et-al-2024>(8/82 | 70/271) Jointly Training and Pruning CNNs via Learnable Agent Guidance and Alignment (Alireza Ganjdanesh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alireza Ganjdanesh, Shangqian Gao, Heng Huang. (2024)<br><strong>Jointly Training and Pruning CNNs via Learnable Agent Guidance and Alignment</strong><br><button class=copy-to-clipboard title="Jointly Training and Pruning CNNs via Learnable Agent Guidance and Alignment" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Model Pruning, Pruning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19490v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19490v1.pdf filename=2403.19490v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structural <b>model</b> <b>pruning</b> is a prominent approach used for reducing the computational cost of <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> before their deployment on resource-constrained devices. Yet, the majority of proposed ideas require a pretrained <b>model</b> <b>before</b> <b>pruning,</b> which is costly to secure. In this paper, we propose a novel structural <b>pruning</b> approach to jointly learn the weights and structurally prune architectures of <b>CNN</b> <b>models.</b> <b>The</b> core element of our method is a <b>Reinforcement</b> <b>Learning</b> (RL) agent whose actions determine the <b>pruning</b> ratios of the <b>CNN</b> <b>model&rsquo;s</b> <b>layers,</b> and the resulting <b>model&rsquo;s</b> <b>accuracy</b> serves as its reward. We conduct the joint training and <b>pruning</b> by iteratively training the <b>model&rsquo;s</b> <b>weights</b> and the agent&rsquo;s policy, and we regularize the <b>model&rsquo;s</b> <b>weights</b> to align with the selected structure by the agent. The evolving <b>model&rsquo;s</b> <b>weights</b> result in a dynamic reward function for the agent, which prevents using prominent episodic RL methods with stationary environment assumption for our purpose. We address this challenge by designing a mechanism to <b>model</b> <b>the</b> complex changing dynamics of the reward function and provide a representation of it to the RL agent. To do so, we take a learnable embedding for each training epoch and employ a recurrent <b>model</b> <b>to</b> calculate a representation of the changing environment. We train the recurrent <b>model</b> <b>and</b> embeddings using a decoder <b>model</b> <b>to</b> reconstruct observed rewards. Such a design empowers our agent to effectively leverage episodic observations along with the environment representations to learn a proper policy to determine performant sub-networks of the <b>CNN</b> <b>model.</b> <b>Our</b> extensive experiments on CIFAR-10 and ImageNet using ResNets and MobileNets demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=982--71271-zero-shot-prompt-based-video-encoder-for-surgical-gesture-recognition-mingxing-rao-et-al-2024>(9/82 | 71/271) Zero-shot Prompt-based Video Encoder for Surgical Gesture Recognition (Mingxing Rao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingxing Rao, Yinhong Qin, Soheil Kolouri, Jie Ying Wu, Daniel Moyer. (2024)<br><strong>Zero-shot Prompt-based Video Encoder for Surgical Gesture Recognition</strong><br><button class=copy-to-clipboard title="Zero-shot Prompt-based Video Encoder for Surgical Gesture Recognition" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Supervised Learning, Weakly-supervised Learning, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19786v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19786v1.pdf filename=2403.19786v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose: Surgical video is an important data stream for gesture recognition. Thus, robust visual encoders for those data-streams is similarly important. Methods: Leveraging the Bridge-Prompt framework, we <b>fine-tune</b> a pre-trained vision-text model (CLIP) for gesture recognition in surgical videos. This can utilize extensive outside video data such as text, but also make use of label meta-data and weakly <b>supervised</b> contrastive losses. Results: Our experiments show that <b>prompt-based</b> video encoder outperforms standard encoders in surgical gesture recognition tasks. Notably, it displays strong performance in <b>zero-shot</b> scenarios, where gestures/tasks that were not provided during the encoder training phase are included in the prediction phase. Additionally, we measure the benefit of inclusion text descriptions in the feature extractor training schema. Conclusion: Bridge-Prompt and similar pre-trained+fine-tuned video encoder models present significant visual representation for surgical robotics, especially in gesture recognition tasks. Given the diverse range of surgical tasks (gestures), the ability of these models to <b>zero-shot</b> transfer without the need for any task (gesture) specific retraining makes them invaluable.</p></p class="citation"></blockquote><h3 id=1082--72271-rsmamba-remote-sensing-image-classification-with-state-space-model-keyan-chen-et-al-2024>(10/82 | 72/271) RSMamba: Remote Sensing Image Classification with State Space Model (Keyan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keyan Chen, Bowen Chen, Chenyang Liu, Wenyuan Li, Zhengxia Zou, Zhenwei Shi. (2024)<br><strong>RSMamba: Remote Sensing Image Classification with State Space Model</strong><br><button class=copy-to-clipboard title="RSMamba: Remote Sensing Image Classification with State Space Model" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Foundation Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19654v1.pdf filename=2403.19654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote sensing image classification forms the <b>foundation</b> <b>of</b> various understanding tasks, serving a crucial function in remote sensing image interpretation. The recent advancements of <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and <b>Transformers</b> have markedly enhanced classification accuracy. Nonetheless, remote sensing scene classification remains a significant challenge, especially given the complexity and diversity of remote sensing scenarios and the variability of spatiotemporal resolutions. The capacity for whole-image understanding can provide more precise semantic cues for scene discrimination. In this paper, we introduce RSMamba, a novel architecture for remote sensing image classification. RSMamba is based on the State Space Model (SSM) and incorporates an efficient, hardware-aware design known as the Mamba. It integrates the advantages of both a global receptive field and linear modeling complexity. To overcome the limitation of the vanilla Mamba, which can only model causal sequences and is not adaptable to two-dimensional image data, we propose a dynamic multi-path activation mechanism to augment Mamba&rsquo;s capacity to model non-causal data. Notably, RSMamba maintains the inherent modeling mechanism of the vanilla Mamba, yet exhibits superior performance across multiple remote sensing image classification datasets. This indicates that RSMamba holds significant potential to function as the backbone of future visual <b>foundation</b> <b>models.</b> The code will be available at \url{https://github.com/KyanChen/RSMamba}.</p></p class="citation"></blockquote><h3 id=1182--73271-gantastic-gan-based-transfer-of-interpretable-directions-for-disentangled-image-editing-in-text-to-image-diffusion-models-yusuf-dalva-et-al-2024>(11/82 | 73/271) GANTASTIC: GAN-based Transfer of Interpretable Directions for Disentangled Image Editing in Text-to-Image Diffusion Models (Yusuf Dalva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yusuf Dalva, Hidir Yesiltepe, Pinar Yanardag. (2024)<br><strong>GANTASTIC: GAN-based Transfer of Interpretable Directions for Disentangled Image Editing in Text-to-Image Diffusion Models</strong><br><button class=copy-to-clipboard title="GANTASTIC: GAN-based Transfer of Interpretable Directions for Disentangled Image Editing in Text-to-Image Diffusion Models" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Generative Adversarial Network, Generative Adversarial Network, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19645v1.pdf filename=2403.19645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement in image generation models has predominantly been driven by <b>diffusion</b> <b>models,</b> which have demonstrated unparalleled success in generating high-fidelity, diverse images from textual <b>prompts.</b> Despite their success, <b>diffusion</b> <b>models</b> encounter substantial challenges in the domain of image editing, particularly in executing disentangled edits-changes that target specific attributes of an image while leaving irrelevant parts untouched. In contrast, <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> have been recognized for their success in disentangled edits through their interpretable latent spaces. We introduce GANTASTIC, a novel framework that takes existing directions from pre-trained <b>GAN</b> models-representative of specific, controllable attributes-and transfers these directions into <b>diffusion-based</b> <b>models.</b> This novel approach not only maintains the <b>generative</b> <b>quality</b> <b>and</b> diversity that <b>diffusion</b> <b>models</b> are known for but also significantly enhances their capability to perform precise, targeted image edits, thereby leveraging the best of both worlds.</p></p class="citation"></blockquote><h3 id=1282--74271-enhance-image-classification-via-inter-class-image-mixup-with-diffusion-model-zhicai-wang-et-al-2024>(12/82 | 74/271) Enhance Image Classification via Inter-Class Image Mixup with Diffusion Model (Zhicai Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhicai Wang, Longhui Wei, Tan Wang, Heyu Chen, Yanbin Hao, Xiang Wang, Xiangnan He, Qi Tian. (2024)<br><strong>Enhance Image Classification via Inter-Class Image Mixup with Diffusion Model</strong><br><button class=copy-to-clipboard title="Enhance Image Classification via Inter-Class Image Mixup with Diffusion Model" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Data Augmentation, Few-shot, Image2text, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19600v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19600v1.pdf filename=2403.19600v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> (T2I) generative models have recently emerged as a powerful tool, enabling the creation of photo-realistic <b>images</b> <b>and</b> giving rise to a multitude of applications. However, the effective integration of T2I models into fundamental <b>image</b> <b>classification</b> tasks remains an open question. A prevalent strategy to bolster <b>image</b> <b>classification</b> performance is through augmenting the training set with synthetic <b>images</b> <b>generated</b> by T2I models. In this study, we scrutinize the shortcomings of both current generative and conventional <b>data</b> <b>augmentation</b> techniques. Our analysis reveals that these methods struggle to produce <b>images</b> <b>that</b> are both faithful (in terms of foreground objects) and diverse (in terms of background contexts) for domain-specific concepts. To tackle this challenge, we introduce an innovative inter-class <b>data</b> <b>augmentation</b> method known as Diff-Mix (<a href=https://github.com/Zhicaiwww/Diff-Mix)>https://github.com/Zhicaiwww/Diff-Mix)</a>, which enriches the dataset by performing <b>image</b> <b>translations</b> between classes. Our empirical results demonstrate that Diff-Mix achieves a better balance between faithfulness and diversity, leading to a marked improvement in performance across diverse <b>image</b> <b>classification</b> scenarios, including <b>few-shot,</b> conventional, and long-tail classifications for domain-specific datasets.</p></p class="citation"></blockquote><h3 id=1382--75271-clap4clip-continual-learning-with-probabilistic-finetuning-for-vision-language-models-saurav-jha-et-al-2024>(13/82 | 75/271) CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models (Saurav Jha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saurav Jha, Dong Gong, Lina Yao. (2024)<br><strong>CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models</strong><br><button class=copy-to-clipboard title="CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Continual Learning, Fine-tuning, Probabilistic Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19137v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19137v1.pdf filename=2403.19137v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> (CL) aims to help deep neural networks to learn new knowledge while retaining what has been learned. Recently, pre-trained <b>vision-language</b> models such as CLIP, with powerful generalization ability, have been gaining traction as practical CL candidates. However, the domain mismatch between the pre-training and the downstream CL tasks calls for <b>finetuning</b> of the CLIP on the latter. The deterministic nature of the existing <b>finetuning</b> methods makes them overlook the many possible interactions across the modalities and deems them unsafe for high-risk CL tasks requiring reliable uncertainty estimation. To address these, our work proposes <b>Continual</b> <b>LeArning</b> with <b>Probabilistic</b> <b>finetuning</b> (CLAP). CLAP develops <b>probabilistic</b> <b>modeling</b> over task-specific modules with visual-guided text features, providing more reliable <b>fine-tuning</b> in CL. It further alleviates forgetting by exploiting the rich pre-trained knowledge of CLIP for weight initialization and distribution regularization of task-specific modules. Cooperating with the diverse range of existing <b>prompting</b> methods, CLAP can surpass the predominant deterministic <b>finetuning</b> approaches for CL with CLIP. Lastly, we study the superior uncertainty estimation abilities of CLAP for novel data detection and exemplar selection within CL setups. Our code is available at \url{https://github.com/srvCodes/clap4clip}.</p></p class="citation"></blockquote><h3 id=1482--76271-omniparser-a-unified-framework-for-text-spotting-key-information-extraction-and-table-recognition-jianqiang-wan-et-al-2024>(14/82 | 76/271) OmniParser: A Unified Framework for Text Spotting, Key Information Extraction and Table Recognition (Jianqiang Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, Zhibo Yang. (2024)<br><strong>OmniParser: A Unified Framework for Text Spotting, Key Information Extraction and Table Recognition</strong><br><button class=copy-to-clipboard title="OmniParser: A Unified Framework for Text Spotting, Key Information Extraction and Table Recognition" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Information Retrieval, Text Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19128v1.pdf filename=2403.19128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, visually-situated <b>text</b> <b>parsing</b> (VsTP) has experienced notable advancements, driven by the increasing demand for automated document understanding and the emergence of Generative <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> capable of processing document-based questions. Various methods have been proposed to address the challenging problem of VsTP. However, due to the diversified targets and heterogeneous schemas, previous works usually design task-specific architectures and objectives for individual tasks, which inadvertently leads to modal isolation and complex workflow. In this paper, we propose a unified paradigm for parsing visually-situated <b>text</b> <b>across</b> diverse scenarios. Specifically, we devise a universal model, called OmniParser, which can simultaneously handle three typical visually-situated <b>text</b> <b>parsing</b> tasks: <b>text</b> <b>spotting,</b> key <b>information</b> <b>extraction,</b> and table recognition. In OmniParser, all tasks share the unified encoder-decoder architecture, the unified objective: point-conditioned <b>text</b> <b>generation,</b> and the unified input & output representation: <b>prompt</b> & structured sequences. Extensive experiments demonstrate that the proposed OmniParser achieves state-of-the-art (SOTA) or highly competitive performances on 7 datasets for the three visually-situated <b>text</b> <b>parsing</b> tasks, despite its unified, concise design. The code is available at <a href=https://github.com/AlibabaResearch/AdvancedLiterateMachinery>https://github.com/AlibabaResearch/AdvancedLiterateMachinery</a>.</p></p class="citation"></blockquote><h3 id=1582--77271-de-confounded-data-free-knowledge-distillation-for-handling-distribution-shifts-yuzheng-wang-et-al-2024>(15/82 | 77/271) De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts (Yuzheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuzheng Wang, Dingkang Yang, Zhaoyu Chen, Yang Liu, Siao Liu, Wenqiang Zhang, Lihua Zhang, Lizhe Qi. (2024)<br><strong>De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts</strong><br><button class=copy-to-clipboard title="De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Graph, Causal Intervention, Distribution Shift, Distribution Shift, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19539v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19539v1.pdf filename=2403.19539v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data-Free <b>Knowledge</b> <b>Distillation</b> (DFKD) is a promising task to train high-performance small models to enhance actual deployment without relying on the original training data. Existing methods commonly avoid relying on private data by utilizing synthetic or sampled data. However, a long-overlooked issue is that the severe <b>distribution</b> <b>shifts</b> between their substitution and original data, which manifests as huge differences in the quality of images and class proportions. The harmful shifts are essentially the confounder that significantly causes performance bottlenecks. To tackle the issue, this paper proposes a novel perspective with <b>causal</b> <b>inference</b> to disentangle the student models from the impact of such shifts. By designing a customized <b>causal</b> <b>graph,</b> we first reveal the causalities among the variables in the DFKD task. Subsequently, we propose a <b>Knowledge</b> <b>Distillation</b> <b>Causal</b> <b>Intervention</b> (KDCI) framework based on the backdoor adjustment to de-confound the confounder. KDCI can be flexibly combined with most existing state-of-the-art baselines. Experiments in combination with six representative DFKD methods demonstrate the effectiveness of our KDCI, which can obviously help existing methods under almost all settings, \textit{e.g.}, improving the baseline by up to 15.54% accuracy on the CIFAR-100 dataset.</p></p class="citation"></blockquote><h3 id=1682--78271-interdreamer-zero-shot-text-to-3d-dynamic-human-object-interaction-sirui-xu-et-al-2024>(16/82 | 78/271) InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction (Sirui Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sirui Xu, Ziyin Wang, Yu-Xiong Wang, Liang-Yan Gui. (2024)<br><strong>InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction</strong><br><button class=copy-to-clipboard title="InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Supervised Learning, Zero-shot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19652v1.pdf filename=2403.19652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-conditioned human motion generation has experienced significant advancements with <b>diffusion</b> <b>models</b> trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of <b>large-scale</b> <b>interaction</b> <b>data</b> and comprehensive descriptions that align with these interactions. This paper takes the initiative and showcases the potential of generating human-object interactions without direct training on text-interaction pair data. Our key insight in achieving this is that interaction semantics and dynamics can be decoupled. Being unable to learn interaction semantics through <b>supervised</b> training, we instead leverage pre-trained <b>large</b> <b>models,</b> <b>synergizing</b> knowledge from a <b>large</b> <b>language</b> <b>model</b> and a text-to-motion model. While such knowledge offers high-level control over interaction semantics, it cannot grasp the intricacies of low-level interaction dynamics. To overcome this issue, we further introduce a world model designed to comprehend simple physics, modeling how human actions influence object motion. By integrating these components, our novel framework, InterDreamer, is able to generate text-aligned 3D HOI sequences in a <b>zero-shot</b> manner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our comprehensive experimental analysis demonstrates its capability to generate realistic and coherent interaction sequences that seamlessly align with the text directives.</p></p class="citation"></blockquote><h3 id=1782--79271-siamese-vision-transformers-are-scalable-audio-visual-learners-yan-bo-lin-et-al-2024>(17/82 | 79/271) Siamese Vision Transformers are Scalable Audio-visual Learners (Yan-Bo Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan-Bo Lin, Gedas Bertasius. (2024)<br><strong>Siamese Vision Transformers are Scalable Audio-visual Learners</strong><br><button class=copy-to-clipboard title="Siamese Vision Transformers are Scalable Audio-visual Learners" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-SD, cs.CV, eess-AS<br>Keyword Score: 40<br>Keywords: Vision Transformer, Contrastive Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19638v1.pdf filename=2403.19638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional audio-visual methods rely on independent audio and visual backbones, which is costly and not scalable. In this work, we investigate using an audio-visual siamese network (AVSiam) for efficient and scalable audio-visual pretraining. Our framework uses a single shared <b>vision</b> <b>transformer</b> backbone to process audio and visual inputs, improving its parameter efficiency, reducing the GPU memory footprint, and allowing us to scale our method to larger datasets and model sizes. We pretrain our model using a <b>contrastive</b> <b>audio-visual</b> matching objective with a multi-ratio random masking scheme, which enables our model to process larger audio-visual instance batches, helpful for <b>contrastive</b> <b>learning.</b> Unlike prior audio-visual methods, our method can robustly handle audio, visual, and audio-visual inputs with a single shared ViT backbone. Furthermore, despite using the shared backbone for both modalities, AVSiam achieves competitive or even better results than prior methods on AudioSet and VGGSound for audio-visual classification and retrieval. Our code is available at <a href=https://github.com/GenjiB/AVSiam>https://github.com/GenjiB/AVSiam</a></p></p class="citation"></blockquote><h3 id=1882--80271-crkd-enhanced-camera-radar-object-detection-with-cross-modality-knowledge-distillation-lingjun-zhao-et-al-2024>(18/82 | 80/271) CRKD: Enhanced Camera-Radar Object Detection with Cross-modality Knowledge Distillation (Lingjun Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingjun Zhao, Jingyu Song, Katherine A. Skinner. (2024)<br><strong>CRKD: Enhanced Camera-Radar Object Detection with Cross-modality Knowledge Distillation</strong><br><button class=copy-to-clipboard title="CRKD: Enhanced Camera-Radar Object Detection with Cross-modality Knowledge Distillation" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19104v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19104v1.pdf filename=2403.19104v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of 3D <b>object</b> <b>detection</b> for autonomous driving, LiDAR-Camera (LC) fusion is the top-performing sensor configuration. Still, LiDAR is relatively high cost, which hinders adoption of this technology for consumer automobiles. Alternatively, camera and radar are commonly deployed on vehicles already on the road today, but performance of Camera-Radar (CR) fusion falls behind LC fusion. In this work, we propose Camera-Radar <b>Knowledge</b> <b>Distillation</b> (CRKD) to bridge the performance gap between LC and CR detectors with a novel cross-modality <b>KD</b> framework. We use the Bird&rsquo;s-Eye-View (BEV) representation as the shared feature space to enable effective <b>knowledge</b> <b>distillation.</b> To accommodate the unique cross-modality <b>KD</b> path, we propose four <b>distillation</b> losses to help the student learn crucial features from the teacher model. We present extensive evaluations on the nuScenes dataset to demonstrate the effectiveness of the proposed CRKD framework. The project page for CRKD is <a href=https://song-jingyu.github.io/CRKD>https://song-jingyu.github.io/CRKD</a>.</p></p class="citation"></blockquote><h3 id=1982--81271-mveb-self-supervised-learning-with-multi-view-entropy-bottleneck-liangjian-wen-et-al-2024>(19/82 | 81/271) MVEB: Self-Supervised Learning with Multi-View Entropy Bottleneck (Liangjian Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liangjian Wen, Xiasi Wang, Jianzhuang Liu, Zenglin Xu. (2024)<br><strong>MVEB: Self-Supervised Learning with Multi-View Entropy Bottleneck</strong><br><button class=copy-to-clipboard title="MVEB: Self-Supervised Learning with Multi-View Entropy Bottleneck" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Mutual Information, Self-supervised Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19078v1.pdf filename=2403.19078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> aims to learn representation that can be effectively generalized to downstream tasks. Many <b>self-supervised</b> <b>approaches</b> regard two views of an image as both the input and the <b>self-supervised</b> <b>signals,</b> assuming that either view contains the same task-relevant information and the shared information is (approximately) sufficient for predicting downstream tasks. Recent studies show that discarding superfluous information not shared between the views can improve generalization. Hence, the ideal representation is sufficient for downstream tasks and contains minimal superfluous information, termed minimal sufficient representation. One can learn this representation by maximizing the <b>mutual</b> <b>information</b> between the representation and the <b>supervised</b> view while eliminating superfluous information. Nevertheless, the computation of <b>mutual</b> <b>information</b> is notoriously intractable. In this work, we propose an objective termed multi-view entropy bottleneck (MVEB) to learn minimal sufficient representation effectively. MVEB simplifies the minimal sufficient learning to maximizing both the agreement between the embeddings of two views and the differential entropy of the embedding distribution. Our experiments confirm that MVEB significantly improves performance. For example, it achieves top-1 accuracy of 76.9% on ImageNet with a vanilla ResNet-50 backbone on linear evaluation. To the best of our knowledge, this is the new state-of-the-art result with ResNet-50.</p></p class="citation"></blockquote><h3 id=2082--82271-low-rank-rescaled-vision-transformer-fine-tuning-a-residual-design-approach-wei-dong-et-al-2024>(20/82 | 82/271) Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach (Wei Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Dong, Xing Zhang, Bihui Chen, Dawei Yan, Zhijun Lin, Qingsen Yan, Peng Wang, Yang Yang. (2024)<br><strong>Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach</strong><br><button class=copy-to-clipboard title="Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Vision Transformer, Fine-tuning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19067v1.pdf filename=2403.19067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient <b>fine-tuning</b> for pre-trained <b>Vision</b> <b>Transformers</b> aims to adeptly tailor a model to downstream tasks by learning a minimal set of new adaptation parameters while preserving the frozen majority of pre-trained parameters. Striking a balance between retaining the generalizable representation capacity of the pre-trained model and acquiring task-specific features poses a key challenge. Currently, there is a lack of focus on guiding this delicate trade-off. In this study, we approach the problem from the perspective of Singular Value Decomposition (SVD) of pre-trained parameter matrices, providing insights into the tuning dynamics of existing methods. Building upon this understanding, we propose a Residual-based Low-Rank Rescaling (RLRR) <b>fine-tuning</b> strategy. This strategy not only enhances flexibility in parameter tuning but also ensures that new parameters do not deviate excessively from the pre-trained model through a residual design. Extensive experiments demonstrate that our method achieves competitive performance across various downstream image classification tasks, all while maintaining comparable new parameters. We believe this work takes a step forward in offering a unified perspective for interpreting existing methods and serves as motivation for the development of new approaches that move closer to effectively considering the crucial trade-off mentioned above. Our code is available at \href{https://github.com/zstarN70/RLRR.git}{https://github.com/zstarN70/RLRR.git}.</p></p class="citation"></blockquote><h3 id=2182--83271-magiclens-self-supervised-image-retrieval-with-open-ended-instructions-kai-zhang-et-al-2024>(21/82 | 83/271) MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions (Kai Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, Ming-Wei Chang. (2024)<br><strong>MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions</strong><br><button class=copy-to-clipboard title="MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-IR, cs-MM, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Self-supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19651v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19651v1.pdf filename=2403.19651v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent work leverages text instructions to allow users to more freely express their search intents. However, existing work primarily focuses on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of <b>self-supervised</b> image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions via <b>large</b> <b>multimodal</b> <b>models</b> (LMMs) and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Trained on 36.7M (query image, instruction, target image) triplets with rich semantic relations mined from the web, MagicLens achieves comparable or better results on eight <b>benchmarks</b> of various image retrieval tasks than prior state-of-the-art (SOTA) methods. Remarkably, it outperforms previous SOTA but with a 50X smaller model size on multiple <b>benchmarks.</b> Additional human analyses on a 1.4M-image unseen corpus further demonstrate the diversity of search intents supported by MagicLens.</p></p class="citation"></blockquote><h3 id=2282--84271-the-bad-batches-enhancing-self-supervised-learning-in-image-classification-through-representative-batch-curation-ozgu-goksu-et-al-2024>(22/82 | 84/271) The Bad Batches: Enhancing Self-Supervised Learning in Image Classification Through Representative Batch Curation (Ozgu Goksu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ozgu Goksu, Nicolas Pugeault. (2024)<br><strong>The Bad Batches: Enhancing Self-Supervised Learning in Image Classification Through Representative Batch Curation</strong><br><button class=copy-to-clipboard title="The Bad Batches: Enhancing Self-Supervised Learning in Image Classification Through Representative Batch Curation" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Contrastive Learning, Representation Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19579v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19579v1.pdf filename=2403.19579v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pursuit of learning robust <b>representations</b> <b>without</b> human supervision is a longstanding challenge. The recent advancements in <b>self-supervised</b> <b>contrastive</b> <b>learning</b> approaches have demonstrated high performance across various <b>representation</b> <b>learning</b> challenges. However, current methods depend on the random transformation of training examples, resulting in some cases of unrepresentative positive pairs that can have a large impact on learning. This limitation not only impedes the convergence of the learning process but the robustness of the learnt <b>representation</b> <b>as</b> well as requiring larger batch sizes to improve robustness to such bad batches. This paper attempts to alleviate the influence of false positive and false negative pairs by employing pairwise similarity calculations through the Fr'echet ResNet Distance (FRD), thereby obtaining robust <b>representations</b> <b>from</b> unlabelled data. The effectiveness of the proposed method is substantiated by empirical results, where a linear classifier trained on <b>self-supervised</b> <b>contrastive</b> <b>representations</b> <b>achieved</b> an impressive 87.74% top-1 accuracy on STL10 and 99.31% on the Flower102 dataset. These results emphasize the potential of the proposed approach in pushing the boundaries of the state-of-the-art in <b>self-supervised</b> <b>contrastive</b> <b>learning,</b> particularly for image classification tasks.</p></p class="citation"></blockquote><h3 id=2382--85271-enet-21-an-optimized-light-cnn-structure-for-lane-detection-seyed-rasoul-hosseini-et-al-2024>(23/82 | 85/271) ENet-21: An Optimized light CNN Structure for Lane Detection (Seyed Rasoul Hosseini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seyed Rasoul Hosseini, Mohammad Teshnehlab. (2024)<br><strong>ENet-21: An Optimized light CNN Structure for Lane Detection</strong><br><button class=copy-to-clipboard title="ENet-21: An Optimized light CNN Structure for Lane Detection" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Clustering, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19782v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19782v1.pdf filename=2403.19782v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lane detection for autonomous vehicles is an important concept, yet it is a challenging issue of driver assistance systems in modern vehicles. The emergence of deep learning leads to significant progress in self-driving cars. Conventional deep learning-based methods handle lane detection problems as a binary segmentation task and determine whether a pixel belongs to a line. These methods rely on the assumption of a fixed number of lanes, which does not always work. This study aims to develop an optimal structure for the lane detection problem, offering a promising solution for driver assistance features in modern vehicles by utilizing a machine learning method consisting of binary segmentation and Affinity Fields that can manage varying numbers of lanes and lane change scenarios. In this approach, the <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN),</b> is selected as a feature extractor, and the final output is obtained through <b>clustering</b> of the semantic segmentation and Affinity Field outputs. Our method uses less complex <b>CNN</b> architecture than exi</p></p class="citation"></blockquote><h3 id=2482--86271-test-time-domain-generalization-for-face-anti-spoofing-qianyu-zhou-et-al-2024>(24/82 | 86/271) Test-Time Domain Generalization for Face Anti-Spoofing (Qianyu Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianyu Zhou, Ke-Yue Zhang, Taiping Yao, Xuequan Lu, Shouhong Ding, Lizhuang Ma. (2024)<br><strong>Test-Time Domain Generalization for Face Anti-Spoofing</strong><br><button class=copy-to-clipboard title="Test-Time Domain Generalization for Face Anti-Spoofing" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19334v1.pdf filename=2403.19334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Face Anti-Spoofing (FAS) is pivotal in safeguarding facial recognition systems against presentation attacks. While domain generalization (DG) methods have been developed to enhance FAS performance, they predominantly focus on learning domain-invariant features during training, which may not guarantee generalizability to unseen data that differs largely from the source distributions. Our insight is that testing data can serve as a valuable resource to enhance the generalizability beyond mere evaluation for DG FAS. In this paper, we introduce a novel Test-Time Domain Generalization (TTDG) framework for FAS, which leverages the testing data to boost the model&rsquo;s generalizability. Our method, consisting of Test-Time Style Projection (TTSP) and Diverse Style Shifts <b>Simulation</b> (DSSS), effectively projects the unseen data to the seen domain space. In particular, we first introduce the innovative TTSP to project the styles of the arbitrarily unseen samples of the testing distribution to the known source space of the training distributions. We then design the efficient DSSS to synthesize diverse style shifts via learnable style bases with two specifically designed losses in a hyperspherical feature space. Our method eliminates the need for model updates at the test time and can be seamlessly integrated into not only the <b>CNN</b> but also ViT backbones. Comprehensive experiments on widely used cross-domain FAS <b>benchmarks</b> demonstrate our method&rsquo;s state-of-the-art performance and effectiveness.</p></p class="citation"></blockquote><h3 id=2582--87271-is-synthetic-image-useful-for-transfer-learning-an-investigation-into-data-generation-volume-and-utilization-yuhang-li-et-al-2024>(25/82 | 87/271) Is Synthetic Image Useful for Transfer Learning? An Investigation into Data Generation, Volume, and Utilization (Yuhang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhang Li, Xin Dong, Chen Chen, Jingtao Li, Yuxin Wen, Michael Spranger, Lingjuan Lyu. (2024)<br><strong>Is Synthetic Image Useful for Transfer Learning? An Investigation into Data Generation, Volume, and Utilization</strong><br><button class=copy-to-clipboard title="Is Synthetic Image Useful for Transfer Learning? An Investigation into Data Generation, Volume, and Utilization" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Transfer Learning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19866v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19866v1.pdf filename=2403.19866v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Synthetic image data generation represents a promising avenue for training deep learning models, particularly in the realm of <b>transfer</b> <b>learning,</b> where obtaining real images within a specific domain can be prohibitively expensive due to privacy and intellectual property considerations. This work delves into the generation and utilization of synthetic images derived from <b>text-to-image</b> generative models in facilitating <b>transfer</b> <b>learning</b> paradigms. Despite the high visual fidelity of the generated images, we observe that their naive incorporation into existing real-image datasets does not consistently enhance model performance due to the inherent distribution gap between synthetic and real images. To address this issue, we introduce a novel two-stage framework called bridged <b>transfer,</b> <b>which</b> initially employs synthetic images for <b>fine-tuning</b> a pre-trained model to improve its transferability and subsequently uses real data for rapid adaptation. Alongside, We propose dataset style inversion strategy to improve the stylistic alignment between synthetic and real images. Our proposed methods are evaluated across 10 different datasets and 5 distinct models, demonstrating consistent improvements, with up to 30% accuracy increase on classification tasks. Intriguingly, we note that the enhancements were not yet saturated, indicating that the benefits may further increase with an expanded volume of synthetic data.</p></p class="citation"></blockquote><h3 id=2682--88271-x-mic-cross-modal-instance-conditioning-for-egocentric-action-generalization-anna-kukleva-et-al-2024>(26/82 | 88/271) X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization (Anna Kukleva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Kukleva, Fadime Sener, Edoardo Remelli, Bugra Tekin, Eric Sauser, Bernt Schiele, Shugao Ma. (2024)<br><strong>X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization</strong><br><button class=copy-to-clipboard title="X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Zero-shot, Text Embedding, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19811v1.pdf filename=2403.19811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lately, there has been growing interest in adapting <b>vision-language</b> models (VLMs) to image and third-person video classification due to their success in <b>zero-shot</b> recognition. However, the adaptation of these models to egocentric videos has been largely unexplored. To address this gap, we propose a simple yet effective cross-modal adaptation framework, which we call X-MIC. Using a video adapter, our pipeline learns to align frozen <b>text</b> <b>embeddings</b> to each egocentric video directly in the shared embedding space. Our novel adapter architecture retains and improves generalization of the pre-trained VLMs by disentangling learnable temporal modeling and frozen visual encoder. This results in an enhanced alignment of <b>text</b> <b>embeddings</b> to each egocentric video, leading to a significant improvement in cross-dataset generalization. We evaluate our approach on the Epic-Kitchens, Ego4D, and EGTEA datasets for fine-grained cross-dataset action generalization, demonstrating the effectiveness of our method. Code is available at <a href=https://github.com/annusha/xmic>https://github.com/annusha/xmic</a></p></p class="citation"></blockquote><h3 id=2782--89271-mist-mitigating-intersectional-bias-with-disentangled-cross-attention-editing-in-text-to-image-diffusion-models-hidir-yesiltepe-et-al-2024>(27/82 | 89/271) MIST: Mitigating Intersectional Bias with Disentangled Cross-Attention Editing in Text-to-Image Diffusion Models (Hidir Yesiltepe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hidir Yesiltepe, Kiymet Akdemir, Pinar Yanardag. (2024)<br><strong>MIST: Mitigating Intersectional Bias with Disentangled Cross-Attention Editing in Text-to-Image Diffusion Models</strong><br><button class=copy-to-clipboard title="MIST: Mitigating Intersectional Bias with Disentangled Cross-Attention Editing in Text-to-Image Diffusion Models" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fairness, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19738v1.pdf filename=2403.19738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion-based</b> <b>text-to-image</b> models have rapidly gained popularity for their ability to generate detailed and realistic images from textual descriptions. However, these models often reflect the biases present in their training data, especially impacting marginalized groups. While prior efforts to debias language models have focused on addressing specific biases, such as racial or gender biases, efforts to tackle intersectional bias have been limited. Intersectional bias refers to the unique form of bias experienced by individuals at the intersection of multiple social identities. Addressing intersectional bias is crucial because it amplifies the negative effects of discrimination based on race, gender, and other identities. In this paper, we introduce a method that addresses intersectional bias in <b>diffusion-based</b> <b>text-to-image</b> models by modifying cross-attention maps in a disentangled manner. Our approach utilizes a pre-trained Stable <b>Diffusion</b> <b>model,</b> eliminates the need for an additional set of reference images, and preserves the original quality for unaltered concepts. Comprehensive experiments demonstrate that our method surpasses existing approaches in mitigating both single and intersectional biases across various attributes. We make our source code and debiased models for various attributes available to encourage <b>fairness</b> in generative models and to support further research.</p></p class="citation"></blockquote><h3 id=2882--90271-densenets-reloaded-paradigm-shift-beyond-resnets-and-vits-donghyun-kim-et-al-2024>(28/82 | 90/271) DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs (Donghyun Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donghyun Kim, Byeongho Heo, Dongyoon Han. (2024)<br><strong>DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs</strong><br><button class=copy-to-clipboard title="DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-NE, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19588v1.pdf filename=2403.19588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper revives Densely Connected <b>Convolutional</b> <b>Networks</b> (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets&rsquo; potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin <b>Transformer,</b> ConvNeXt, and DeiT-III - key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing with the very recent models and downstream tasks, ADE20k semantic segmentation, and COCO object detection/instance segmentation. Finally, we provide empirical analyses that uncover the merits of the concatenation over additive shortcuts, steering a renewed preference towards DenseNet-style designs. Our code is available at <a href=https://github.com/naver-ai/rdnet>https://github.com/naver-ai/rdnet</a>.</p></p class="citation"></blockquote><h3 id=2982--91271-break-for-make-modular-low-rank-adaptations-for-composable-content-style-customization-yu-xu-et-al-2024>(29/82 | 91/271) Break-for-Make: Modular Low-Rank Adaptations for Composable Content-Style Customization (Yu Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Xu, Fan Tang, Juan Cao, Yuxin Zhang, Oliver Deussen, Weiming Dong, Jintao Li, Tong-Yee Lee. (2024)<br><strong>Break-for-Make: Modular Low-Rank Adaptations for Composable Content-Style Customization</strong><br><button class=copy-to-clipboard title="Break-for-Make: Modular Low-Rank Adaptations for Composable Content-Style Customization" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs-MM, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19456v1.pdf filename=2403.19456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personalized generation paradigms empower designers to customize visual intellectual properties with the help of textual descriptions by tuning or adapting pre-trained <b>text-to-image</b> models on a few images. Recent works explore approaches for concurrently customizing both content and detailed visual style appearance. However, these existing approaches often generate images where the content and style are entangled. In this study, we reconsider the customization of content and style concepts from the perspective of parameter space construction. Unlike existing methods that utilize a shared parameter space for content and style, we propose a learning framework that separates the parameter space to facilitate individual learning of content and style, thereby enabling disentangled content and style. To achieve this goal, we introduce &ldquo;partly learnable projection&rdquo; (PLP) matrices to separate the original adapters into divided sub-parameter spaces. We propose &ldquo;break-for-make&rdquo; customization learning pipeline based on PLP, which is simple yet effective. We break the original adapters into &ldquo;up projection&rdquo; and &ldquo;down projection&rdquo;, train content and style PLPs individually with the guidance of corresponding textual <b>prompts</b> in the separate adapters, and maintain generalization by employing a multi-correspondence projection learning strategy. Based on the adapters broken apart for separate training content and style, we then make the entity parameter space by reconstructing the content and style PLPs matrices, followed by <b>fine-tuning</b> the combined adapter to generate the target object with the desired appearance. Experiments on various styles, including textures, materials, and artistic style, show that our method outperforms state-of-the-art single/multiple concept learning pipelines in terms of content-style-prompt alignment.</p></p class="citation"></blockquote><h3 id=3082--92271-hypergraph-based-multi-view-action-recognition-using-event-cameras-yue-gao-et-al-2024>(30/82 | 92/271) Hypergraph-based Multi-View Action Recognition using Event Cameras (Yue Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Gao, Jiaxuan Lu, Siqi Li, Yipeng Li, Shaoyi Du. (2024)<br><strong>Hypergraph-based Multi-View Action Recognition using Event Cameras</strong><br><button class=copy-to-clipboard title="Hypergraph-based Multi-View Action Recognition using Event Cameras" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19316v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19316v1.pdf filename=2403.19316v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Action recognition from video data forms a cornerstone with wide-ranging applications. Single-view action recognition faces limitations due to its reliance on a single viewpoint. In contrast, multi-view approaches capture complementary information from various viewpoints for improved accuracy. Recently, event cameras have emerged as innovative bio-inspired sensors, leading to advancements in event-based action recognition. However, existing works predominantly focus on single-view scenarios, leaving a gap in multi-view event data exploitation, particularly in challenges like information deficit and semantic misalignment. To bridge this gap, we introduce HyperMV, a multi-view event-based action recognition framework. HyperMV converts discrete event data into frame-like representations and extracts view-related features using a shared <b>convolutional</b> <b>network.</b> By treating segments as vertices and constructing hyperedges using rule-based and KNN-based strategies, a multi-view hypergraph neural network that captures relationships across viewpoint and temporal features is established. The vertex attention hypergraph propagation is also introduced for enhanced feature fusion. To <b>prompt</b> research in this area, we present the largest multi-view event-based action dataset $\text{THU}^{\text{MV-EACT}}\text{-50}$, comprising 50 actions from 6 viewpoints, which surpasses existing datasets by over tenfold. Experimental results show that HyperMV significantly outperforms baselines in both cross-subject and cross-view scenarios, and also exceeds the state-of-the-arts in frame-based multi-view action recognition.</p></p class="citation"></blockquote><h3 id=3182--93271-sparse-generation-making-pseudo-labels-sparse-for-weakly-supervision-with-points-tian-ma-et-al-2024>(31/82 | 93/271) Sparse Generation: Making Pseudo Labels Sparse for weakly supervision with points (Tian Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Ma, Chuyang Shang, Wanzhu Ren, Yuancheng Li, Jiiayi Yang, Jiali Qian. (2024)<br><strong>Sparse Generation: Making Pseudo Labels Sparse for weakly supervision with points</strong><br><button class=copy-to-clipboard title="Sparse Generation: Making Pseudo Labels Sparse for weakly supervision with points" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19306v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19306v1.pdf filename=2403.19306v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, research on point weakly <b>supervised</b> <b>object</b> <b>detection</b> (PWSOD) methods in the field of computer vision has attracted people&rsquo;s attention. However, existing pseudo labels generation methods perform poorly in a small amount of <b>supervised</b> annotation data and dense <b>object</b> <b>detection</b> tasks. We consider the generation of weakly <b>supervised</b> pseudo labels as the result of model&rsquo;s sparse output, and propose a method called Sparse Generation to make pseudo labels sparse. It constructs dense tensors through the relationship between data and detector model, optimizes three of its parameters, and obtains a sparse tensor via coordinated calculation, thereby indirectly obtaining higher quality pseudo labels, and solving the model&rsquo;s density problem in the situation of only a small amount of <b>supervised</b> annotation data can be used. On two broadly used open-source datasets (RSOD, SIMD) and a self-built dataset (Bullet-Hole), the experimental results showed that the proposed method has a significant advantage in terms of overall performance metrics, comparing to that state-of-the-art method.</p></p class="citation"></blockquote><h3 id=3282--94271-taming-lookup-tables-for-efficient-image-retouching-sidi-yang-et-al-2024>(32/82 | 94/271) Taming Lookup Tables for Efficient Image Retouching (Sidi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sidi Yang, Binxiao Huang, Mingdeng Cao, Yatai Ji, Hanzhong Guo, Ngai Wong, Yujiu Yang. (2024)<br><strong>Taming Lookup Tables for Efficient Image Retouching</strong><br><button class=copy-to-clipboard title="Taming Lookup Tables for Efficient Image Retouching" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV, eess-IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19238v1.pdf filename=2403.19238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread use of high-definition screens in edge devices, such as end-user cameras, smartphones, and televisions, is spurring a significant demand for image enhancement. Existing enhancement models often optimize for high performance while falling short of reducing hardware inference time and power consumption, especially on edge devices with constrained computing and storage resources. To this end, we propose Image Color Enhancement Lookup Table (ICELUT) that adopts LUTs for extremely efficient edge inference, without any <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN).</b> During training, we leverage pointwise (1x1) <b>convolution</b> to extract color information, alongside a split fully connected layer to incorporate global information. Both components are then seamlessly converted into LUTs for hardware-agnostic deployment. ICELUT achieves near-state-of-the-art performance and remarkably low power consumption. We observe that the pointwise network structure exhibits robust scalability, upkeeping the performance even with a heavily downsampled 32x32 input image. These enable ICELUT, the first-ever purely LUT-based image enhancer, to reach an unprecedented speed of 0.4ms on GPU and 7ms on CPU, at least one order faster than any <b>CNN</b> solution. Codes are available at <a href=https://github.com/Stephen0808/ICELUT>https://github.com/Stephen0808/ICELUT</a>.</p></p class="citation"></blockquote><h3 id=3382--95271-poco-a-self-supervised-approach-via-polar-transformation-based-progressive-contrastive-learning-for-ophthalmic-disease-diagnosis-jinhong-wang-et-al-2024>(33/82 | 95/271) PoCo: A Self-Supervised Approach via Polar Transformation Based Progressive Contrastive Learning for Ophthalmic Disease Diagnosis (Jinhong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinhong Wang, Tingting Chen, Jintai Chen, Yixuan Wu, Yuyang Xu, Danny Chen, Haochao Ying, Jian Wu. (2024)<br><strong>PoCo: A Self-Supervised Approach via Polar Transformation Based Progressive Contrastive Learning for Ophthalmic Disease Diagnosis</strong><br><button class=copy-to-clipboard title="PoCo: A Self-Supervised Approach via Polar Transformation Based Progressive Contrastive Learning for Ophthalmic Disease Diagnosis" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Convolution, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19124v1.pdf filename=2403.19124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic ophthalmic disease diagnosis on fundus images is important in clinical practice. However, due to complex fundus textures and limited annotated data, developing an effective automatic method for this problem is still challenging. In this paper, we present a <b>self-supervised</b> method via polar transformation based progressive <b>contrastive</b> <b>learning,</b> called PoCo, for ophthalmic disease diagnosis. Specifically, we novelly inject the polar transformation into <b>contrastive</b> <b>learning</b> to 1) promote <b>contrastive</b> <b>learning</b> pre-training to be faster and more stable and 2) naturally capture task-free and rotation-related textures, which provides insights into disease recognition on fundus images. Beneficially, simple normal translation-invariant <b>convolution</b> on transformed images can equivalently replace the complex rotation-invariant and sector <b>convolution</b> on raw images. After that, we develop a progressive <b>contrastive</b> <b>learning</b> method to efficiently utilize large unannotated images and a novel progressive hard negative sampling scheme to gradually reduce the negative sample number for efficient training and performance enhancement. Extensive experiments on three public ophthalmic disease datasets show that our PoCo achieves state-of-the-art performance with good generalization ability, validating that our method can reduce annotation efforts and provide reliable diagnosis. Codes are available at \url{https://github.com/wjh892521292/PoCo}.</p></p class="citation"></blockquote><h3 id=3482--96271-aapmt-agi-assessment-through-prompt-and-metric-transformer-benhao-huang-2024>(34/82 | 96/271) AAPMT: AGI Assessment Through Prompt and Metric Transformer (Benhao Huang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benhao Huang. (2024)<br><strong>AAPMT: AGI Assessment Through Prompt and Metric Transformer</strong><br><button class=copy-to-clipboard title="AAPMT: AGI Assessment Through Prompt and Metric Transformer" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Transformer, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19101v1.pdf filename=2403.19101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of <b>text-to-image</b> models marks a significant milestone in the evolution of AI-generated images (AGIs), expanding their use in diverse domains like design, entertainment, and more. Despite these breakthroughs, the quality of AGIs often remains suboptimal, highlighting the need for effective evaluation methods. These methods are crucial for assessing the quality of images relative to their textual descriptions, and they must accurately mirror human perception. Substantial progress has been achieved in this domain, with innovative techniques such as BLIP and DBCNN contributing significantly. However, recent studies, including AGIQA-3K, reveal a notable discrepancy between current methods and state-of-the-art (SOTA) standards. This gap emphasizes the necessity for a more sophisticated and precise evaluation metric. In response, our objective is to develop a model that could give ratings for metrics, which focuses on parameters like perceptual quality, authenticity, and the correspondence between text and image, that more closely aligns with human perception. In our paper, we introduce a range of effective methods, including <b>prompt</b> designs and the Metric <b>Transformer.</b> The Metric <b>Transformer</b> is a novel structure inspired by the complex interrelationships among various AGI quality metrics. The code is available at <a href=https://github.com/huskydoge/CS3324-Digital-Image-Processing/tree/main/Assignment1>https://github.com/huskydoge/CS3324-Digital-Image-Processing/tree/main/Assignment1</a></p></p class="citation"></blockquote><h3 id=3582--97271-towards-multimodal-video-paragraph-captioning-models-robust-to-missing-modality-sishuo-chen-et-al-2024>(35/82 | 97/271) Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality (Sishuo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sishuo Chen, Lei Li, Shuhuai Ren, Rundong Gao, Yuanxin Liu, Xiaohan Bi, Xu Sun, Lu Hou. (2024)<br><strong>Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality</strong><br><button class=copy-to-clipboard title="Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Data Augmentation, Knowledge Distillation, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19221v1.pdf filename=2403.19221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video paragraph captioning (VPC) involves generating detailed narratives for long videos, utilizing supportive modalities such as speech and event boundaries. However, the existing models are constrained by the assumption of constant availability of a single auxiliary modality, which is impractical given the diversity and unpredictable nature of real-world scenarios. To this end, we propose a Missing-Resistant framework MR-VPC that effectively harnesses all available auxiliary inputs and maintains resilience even in the absence of certain modalities. Under this framework, we propose the <b>Multimodal</b> VPC (MVPC) architecture integrating video, speech, and event boundary inputs in a unified manner to process various auxiliary inputs. Moreover, to fortify the model against incomplete <b>data,</b> <b>we</b> introduce DropAM, a <b>data</b> <b>augmentation</b> strategy that randomly omits auxiliary inputs, paired with DistillAM, a regularization target that <b>distills</b> knowledge from teacher models trained on modality-complete <b>data,</b> <b>enabling</b> efficient learning in modality-deficient environments. Through exhaustive experimentation on YouCook2 and ActivityNet Captions, MR-VPC has proven to deliver superior performance on modality-complete and modality-missing test <b>data.</b> <b>This</b> work highlights the significance of developing resilient VPC models and paves the way for more adaptive, robust <b>multimodal</b> video understanding.</p></p class="citation"></blockquote><h3 id=3682--98271-mmcert-provable-defense-against-adversarial-attacks-to-multi-modal-models-yanting-wang-et-al-2024>(36/82 | 98/271) MMCert: Provable Defense against Adversarial Attacks to Multi-modal Models (Yanting Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanting Wang, Hongye Fu, Wei Zou, Jinyuan Jia. (2024)<br><strong>MMCert: Provable Defense against Adversarial Attacks to Multi-modal Models</strong><br><button class=copy-to-clipboard title="MMCert: Provable Defense against Adversarial Attacks to Multi-modal Models" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Emotion Recognition, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19080v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19080v2.pdf filename=2403.19080v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Different from a unimodal model whose input is from a single modality, the input (called <b>multi-modal</b> input) of a <b>multi-modal</b> model is from multiple modalities such as image, 3D points, audio, text, etc. Similar to unimodal models, many existing studies show that a <b>multi-modal</b> model is also vulnerable to <b>adversarial</b> <b>perturbation,</b> where an attacker could add small perturbation to all modalities of a <b>multi-modal</b> input such that the <b>multi-modal</b> model makes incorrect predictions for it. Existing certified defenses are mostly designed for unimodal models, which achieve sub-optimal certified robustness guarantees when extended to <b>multi-modal</b> models as shown in our experimental results. In our work, we propose MMCert, the first certified defense against <b>adversarial</b> <b>attacks</b> to a <b>multi-modal</b> model. We derive a lower bound on the performance of our MMCert under arbitrary <b>adversarial</b> <b>attacks</b> with bounded perturbations to both modalities (e.g., in the context of auto-driving, we bound the number of changed pixels in both RGB image and depth image). We evaluate our MMCert using two <b>benchmark</b> datasets: one for the <b>multi-modal</b> road segmentation task and the other for the <b>multi-modal</b> <b>emotion</b> <b>recognition</b> task. Moreover, we compare our MMCert with a state-of-the-art certified defense extended from unimodal models. Our experimental results show that our MMCert outperforms the baseline.</p></p class="citation"></blockquote><h3 id=3782--99271-moditalker-motion-disentangled-diffusion-model-for-high-fidelity-talking-head-generation-seyeon-kim-et-al-2024>(37/82 | 99/271) MoDiTalker: Motion-Disentangled Diffusion Model for High-Fidelity Talking Head Generation (Seyeon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seyeon Kim, Siyoon Jin, Jihye Park, Kihong Kim, Jiyoung Kim, Jisu Nam, Seungryong Kim. (2024)<br><strong>MoDiTalker: Motion-Disentangled Diffusion Model for High-Fidelity Talking Head Generation</strong><br><button class=copy-to-clipboard title="MoDiTalker: Motion-Disentangled Diffusion Model for High-Fidelity Talking Head Generation" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19144v1.pdf filename=2403.19144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional <b>GAN-based</b> models for talking head generation often suffer from limited quality and unstable training. Recent approaches based on <b>diffusion</b> <b>models</b> aimed to address these limitations and improve fidelity. However, they still face challenges, including extensive sampling times and difficulties in maintaining temporal consistency due to the high stochasticity of <b>diffusion</b> <b>models.</b> To overcome these challenges, we propose a novel motion-disentangled <b>diffusion</b> <b>model</b> for high-quality talking head generation, dubbed MoDiTalker. We introduce the two modules: audio-to-motion (AToM), designed to generate a synchronized lip motion from audio, and motion-to-video (MToV), designed to produce high-quality head video following the generated motion. AToM excels in capturing subtle lip movements by leveraging an audio attention mechanism. In addition, MToV enhances temporal consistency by leveraging an efficient tri-plane representation. Our experiments conducted on standard <b>benchmarks</b> demonstrate that our model achieves superior performance compared to existing models. We also provide comprehensive ablation studies and user study results.</p></p class="citation"></blockquote><h3 id=3882--100271-qncd-quantization-noise-correction-for-diffusion-models-huanpeng-chu-et-al-2024>(38/82 | 100/271) QNCD: Quantization Noise Correction for Diffusion Models (Huanpeng Chu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huanpeng Chu, Wei Wu, Chengjie Zang, Kun Yuan. (2024)<br><strong>QNCD: Quantization Noise Correction for Diffusion Models</strong><br><button class=copy-to-clipboard title="QNCD: Quantization Noise Correction for Diffusion Models" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19140v1.pdf filename=2403.19140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have revolutionized image synthesis, setting new <b>benchmarks</b> in quality and creativity. However, their widespread adoption is hindered by the intensive computation required during the iterative denoising process. Post-training <b>quantization</b> (PTQ) presents a solution to accelerate sampling, aibeit at the expense of sample quality, extremely in low-bit settings. Addressing this, our study introduces a unified <b>Quantization</b> Noise Correction Scheme (QNCD), aimed at minishing <b>quantization</b> noise throughout the sampling process. We identify two primary <b>quantization</b> challenges: intra and inter <b>quantization</b> noise. Intra <b>quantization</b> noise, mainly exacerbated by embeddings in the resblock module, extends activation <b>quantization</b> ranges, increasing disturbances in each single denosing step. Besides, inter <b>quantization</b> noise stems from cumulative <b>quantization</b> deviations across the entire denoising process, altering data distributions step-by-step. QNCD combats these through embedding-derived feature smoothing for eliminating intra <b>quantization</b> noise and an effective runtime noise estimatiation module for dynamicly filtering inter <b>quantization</b> noise. Extensive experiments demonstrate that our method outperforms previous <b>quantization</b> methods for <b>diffusion</b> <b>models,</b> achieving lossless results in W4A8 and W8A8 <b>quantization</b> settings on ImageNet (LDM-4). Code is available at: <a href=https://github.com/huanpengchu/QNCD>https://github.com/huanpengchu/QNCD</a></p></p class="citation"></blockquote><h3 id=3982--101271-detecting-image-attribution-for-text-to-image-diffusion-models-in-rgb-and-beyond-katherine-xu-et-al-2024>(39/82 | 101/271) Detecting Image Attribution for Text-to-Image Diffusion Models in RGB and Beyond (Katherine Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Katherine Xu, Lingzhi Zhang, Jianbo Shi. (2024)<br><strong>Detecting Image Attribution for Text-to-Image Diffusion Models in RGB and Beyond</strong><br><button class=copy-to-clipboard title="Detecting Image Attribution for Text-to-Image Diffusion Models in RGB and Beyond" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19653v1.pdf filename=2403.19653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern <b>text-to-image</b> (T2I) <b>diffusion</b> <b>models</b> can generate images with remarkable realism and creativity. These advancements have sparked research in fake image detection and attribution, yet prior studies have not fully explored the practical and scientific dimensions of this task. In addition to attributing images to 12 state-of-the-art T2I generators, we provide extensive analyses on what inference stage hyperparameters and image modifications are discernible. Our experiments reveal that initialization seeds are highly detectable, along with other subtle variations in the image generation process to some extent. We further investigate what visual traces are leveraged in image attribution by perturbing high-frequency details and employing mid-level representations of image style and structure. Notably, altering high-frequency information causes only slight reductions in accuracy, and training an attributor on style representations outperforms training on RGB images. Our analyses underscore that fake images are detectable and attributable at various levels of visual granularity than previously explored.</p></p class="citation"></blockquote><h3 id=4082--102271-change-agent-towards-interactive-comprehensive-change-interpretation-and-analysis-from-change-detection-and-change-captioning-chenyang-liu-et-al-2024>(40/82 | 102/271) Change-Agent: Towards Interactive Comprehensive Change Interpretation and Analysis from Change Detection and Change Captioning (Chenyang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyang Liu, Keyan Chen, Haotian Zhang, Zipeng Qi, Zhengxia Zou, Zhenwei Shi. (2024)<br><strong>Change-Agent: Towards Interactive Comprehensive Change Interpretation and Analysis from Change Detection and Change Captioning</strong><br><button class=copy-to-clipboard title="Change-Agent: Towards Interactive Comprehensive Change Interpretation and Analysis from Change Detection and Change Captioning" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19646v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19646v1.pdf filename=2403.19646v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monitoring changes in the Earth&rsquo;s surface is crucial for understanding natural processes and human impacts, necessitating precise and comprehensive interpretation methodologies. Remote sensing satellite imagery offers a unique perspective for monitoring these changes, leading to the emergence of remote sensing image change interpretation (RSICI) as a significant research focus. Current RSICI technology encompasses change detection and change captioning, each with its limitations in providing comprehensive interpretation. To address this, we propose an interactive Change-Agent which integrates a multi-level change interpretation (MCI) model as eyes and a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> as the brain. Our Change-Agent can follow user instructions to achieve comprehensive change interpretation and insightful analysis according to user instructions, such as change detection and change captioning, change object counting, change cause analysis, etc. Our proposed MCI model contains two branches of pixel-level change detection and semantic-level change captioning, in which multiple BI-temporal Iterative Interaction (BI3) layers utilize Local Perception Enhancement (LPE) and the Global Difference Fusion Attention (GDFA) modules to enhance the model&rsquo;s discriminative feature representation capabilities. To train the MCI model, we build the LEVIR-MCI dataset with change masks and captions of bi-temporal images. Extensive experiments demonstrate the effectiveness of the proposed change interpretation model and highlight the promising potential of our Change-Agent in facilitating comprehensive and intelligent interpretation of surface changes. We will make our dataset and codebase of the change interpretation model and Change-Agent publicly available to facilitate future research at <a href=https://github.com/Chen-Yang-Liu/Change-Agent>https://github.com/Chen-Yang-Liu/Change-Agent</a></p></p class="citation"></blockquote><h3 id=4182--103271-situation-awareness-for-driver-centric-driving-style-adaptation-johann-haselberger-et-al-2024>(41/82 | 103/271) Situation Awareness for Driver-Centric Driving Style Adaptation (Johann Haselberger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johann Haselberger, Bonifaz Stuhr, Bernhard Schick, Steffen Müller. (2024)<br><strong>Situation Awareness for Driver-Centric Driving Style Adaptation</strong><br><button class=copy-to-clipboard title="Situation Awareness for Driver-Centric Driving Style Adaptation" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-SY, cs.CV, eess-SY<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19595v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19595v1.pdf filename=2403.19595v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is evidence that the driving style of an autonomous vehicle is important to increase the acceptance and trust of the passengers. The driving situation has been found to have a significant influence on human driving behavior. However, current driving style models only partially incorporate driving environment information, limiting the alignment between an agent and the given situation. Therefore, we propose a situation-aware driving style model based on different visual feature encoders pretrained on fleet data, as well as driving behavior predictors, which are adapted to the driving style of a specific driver. Our experiments show that the proposed method outperforms static driving styles significantly and forms plausible situation clusters. Furthermore, we found that feature encoders pretrained on our dataset lead to more precise driving behavior modeling. In contrast, feature encoders pretrained <b>supervised</b> and <b>unsupervised</b> on different data sources lead to more specific situation clusters, which can be utilized to constrain and control the driving style adaptation for specific situations. Moreover, in a real-world setting, where driving style adaptation is happening iteratively, we found the MLP-based behavior predictors achieve good performance initially but suffer from catastrophic forgetting. In contrast, behavior predictors based on situationdependent statistics can learn iteratively from continuous data streams by design. Overall, our experiments show that important information for driving behavior prediction is contained within the visual feature encoder. The dataset is publicly available at huggingface.co/datasets/jHaselberger/SADC-Situation-Awareness-for-Driver-Centric-Driving-Style-Adaptation.</p></p class="citation"></blockquote><h3 id=4282--104271-cross-attention-is-not-always-needed-dynamic-cross-attention-for-audio-visual-dimensional-emotion-recognition-r-gnana-praveen-et-al-2024>(42/82 | 104/271) Cross-Attention is Not Always Needed: Dynamic Cross-Attention for Audio-Visual Dimensional Emotion Recognition (R. Gnana Praveen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>R. Gnana Praveen, Jahangir Alam. (2024)<br><strong>Cross-Attention is Not Always Needed: Dynamic Cross-Attention for Audio-Visual Dimensional Emotion Recognition</strong><br><button class=copy-to-clipboard title="Cross-Attention is Not Always Needed: Dynamic Cross-Attention for Audio-Visual Dimensional Emotion Recognition" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Graph Attention Networks, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19554v1.pdf filename=2403.19554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In video-based <b>emotion</b> <b>recognition,</b> audio and visual modalities are often expected to have a complementary relationship, which is widely explored using cross-attention. However, they may also exhibit weak complementary relationships, resulting in poor representations of audio-visual features, thus degrading the performance of the system. To address this issue, we propose Dynamic Cross-Attention (DCA) that can dynamically select cross-attended or unattended features on the fly based on their strong or weak complementary relationship with each other, respectively. Specifically, a simple yet efficient <b>gating</b> layer is designed to evaluate the contribution of the cross-attention mechanism and choose cross-attended features only when they exhibit a strong complementary relationship, otherwise unattended features. We evaluate the performance of the proposed approach on the challenging RECOLA and Aff-Wild2 datasets. We also compare the proposed approach with other variants of cross-attention and show that the proposed model consistently improves the performance on both datasets.</p></p class="citation"></blockquote><h3 id=4382--105271-bamm-bidirectional-autoregressive-motion-model-ekkasit-pinyoanuntapong-et-al-2024>(43/82 | 105/271) BAMM: Bidirectional Autoregressive Motion Model (Ekkasit Pinyoanuntapong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Pu Wang, Minwoo Lee, Srijan Das, Chen Chen. (2024)<br><strong>BAMM: Bidirectional Autoregressive Motion Model</strong><br><button class=copy-to-clipboard title="BAMM: Bidirectional Autoregressive Motion Model" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19435v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19435v2.pdf filename=2403.19435v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating human motion from text has been dominated by denoising motion models either through diffusion or generative masking process. However, these models face great limitations in usability by requiring prior knowledge of the motion length. Conversely, autoregressive motion models address this limitation by adaptively predicting motion endpoints, at the cost of degraded generation quality and editing capabilities. To address these challenges, we propose Bidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion generation framework. BAMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into discrete tokens in latent space, and (2) a masked <b>self-attention</b> <b>transformer</b> that autoregressively predicts randomly masked tokens via a hybrid attention masking strategy. By unifying generative masked modeling and autoregressive modeling, BAMM captures rich and bidirectional dependencies among motion tokens, while learning the probabilistic mapping from textual inputs to motion outputs with dynamically-adjusted motion sequence length. This feature enables BAMM to simultaneously achieving high-quality motion generation with enhanced usability and built-in motion editability. Extensive experiments on HumanML3D and KIT-ML datasets demonstrate that BAMM surpasses current state-of-the-art methods in both qualitative and quantitative measures. Our project page is available at <a href=https://github.com/exitudio/BAMM-page>https://github.com/exitudio/BAMM-page</a>.</p></p class="citation"></blockquote><h3 id=4482--106271-oakink2-a-dataset-of-bimanual-hands-object-manipulation-in-complex-task-completion-xinyu-zhan-et-al-2024>(44/82 | 106/271) OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion (Xinyu Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Zhan, Lixin Yang, Yifei Zhao, Kangrui Mao, Hanlin Xu, Zenan Lin, Kailin Li, Cewu Lu. (2024)<br><strong>OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion</strong><br><button class=copy-to-clipboard title="OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19417v1.pdf filename=2403.19417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present OAKINK2, a dataset of bimanual object manipulation tasks for complex daily activities. In pursuit of constructing the complex tasks into a structured representation, OAKINK2 introduces three level of abstraction to organize the manipulation tasks: Affordance, Primitive Task, and Complex Task. OAKINK2 features on an object-centric perspective for decoding the complex tasks, treating them as a sequence of object affordance fulfillment. The first level, Affordance, outlines the functionalities that objects in the scene can afford, the second level, Primitive Task, describes the minimal interaction units that humans interact with the object to achieve its affordance, and the third level, Complex Task, illustrates how Primitive Tasks are composed and interdependent. OAKINK2 dataset provides multi-view image streams and precise pose annotations for the human body, hands and various interacting objects. This extensive collection supports applications such as interaction reconstruction and motion synthesis. Based on the 3-level abstraction of OAKINK2, we explore a task-oriented framework for Complex Task Completion (CTC). CTC aims to generate a sequence of bimanual manipulation to achieve task objectives. Within the CTC framework, we employ <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to decompose the complex task objectives into sequences of Primitive Tasks and have developed a Motion Fulfillment Model that generates bimanual hand motion for each Primitive Task. OAKINK2 datasets and models are available at <a href=https://oakink.net/v2>https://oakink.net/v2</a>.</p></p class="citation"></blockquote><h3 id=4582--107271-a-simple-and-effective-point-based-network-for-event-camera-6-dofs-pose-relocalization-hongwei-ren-et-al-2024>(45/82 | 107/271) A Simple and Effective Point-based Network for Event Camera 6-DOFs Pose Relocalization (Hongwei Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongwei Ren, Jiadong Zhu, Yue Zhou, Haotian FU, Yulong Huang, Bojun Cheng. (2024)<br><strong>A Simple and Effective Point-based Network for Event Camera 6-DOFs Pose Relocalization</strong><br><button class=copy-to-clipboard title="A Simple and Effective Point-based Network for Event Camera 6-DOFs Pose Relocalization" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19412v1.pdf filename=2403.19412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event cameras exhibit remarkable attributes such as high dynamic range, asynchronicity, and low latency, making them highly suitable for vision tasks that involve high-speed motion in challenging lighting conditions. These cameras implicitly capture movement and depth information in events, making them appealing sensors for Camera Pose Relocalization (CPR) tasks. Nevertheless, existing CPR networks based on events neglect the pivotal fine-grained temporal information in events, resulting in unsatisfactory performance. Moreover, the energy-efficient features are further compromised by the use of excessively complex models, hindering efficient deployment on edge devices. In this paper, we introduce PEPNet, a simple and effective point-based network designed to regress six degrees of freedom (6-DOFs) event camera poses. We rethink the relationship between the event camera and CPR tasks, leveraging the raw Point Cloud directly as network input to harness the high-temporal resolution and inherent sparsity of events. PEPNet is adept at abstracting the spatial and implicit temporal features through hierarchical structure and explicit temporal features by Attentive Bi-directional <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(A-Bi-LSTM).</b> By employing a carefully crafted lightweight design, PEPNet delivers state-of-the-art (SOTA) performance on both indoor and outdoor datasets with meager computational resources. Specifically, PEPNet attains a significant 38% and 33% performance improvement on the random split IJRR and M3ED datasets, respectively. Moreover, the lightweight design version PEPNet$_{tiny}$ accomplishes results comparable to the SOTA while employing a mere 0.5% of the parameters.</p></p class="citation"></blockquote><h3 id=4682--108271-cat-exploiting-inter-class-dynamics-for-domain-adaptive-object-detection-mikhail-kennerley-et-al-2024>(46/82 | 108/271) CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection (Mikhail Kennerley et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mikhail Kennerley, Jian-Gang Wang, Bharadwaj Veeravalli, Robby T. Tan. (2024)<br><strong>CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection</strong><br><button class=copy-to-clipboard title="CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19278v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19278v1.pdf filename=2403.19278v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Domain</b> <b>adaptive</b> <b>object</b> <b>detection</b> aims to adapt detection models to <b>domains</b> <b>where</b> annotated data is unavailable. Existing methods have been proposed to address the <b>domain</b> <b>gap</b> using the semi-supervised student-teacher framework. However, a fundamental issue arises from the class imbalance in the labelled training set, which can result in inaccurate pseudo-labels. The relationship between classes, especially where one class is a majority and the other minority, has a large impact on class bias. We propose Class-Aware Teacher (CAT) to address the class bias issue in the <b>domain</b> <b>adaptation</b> setting. In our work, we approximate the class relationships with our Inter-Class Relation module (ICRm) and exploit it to reduce the bias within the model. In this way, we are able to apply augmentations to highly related classes, both inter- and intra-domain, to boost the performance of minority classes while having minimal impact on majority classes. We further reduce the bias by implementing a class-relation weight to our classification loss. Experiments conducted on various datasets and ablation studies show that our method is able to address the class bias in the <b>domain</b> <b>adaptation</b> setting. On the Cityscapes to Foggy Cityscapes dataset, we attained a 52.5 mAP, a substantial improvement over the 51.2 mAP achieved by the state-of-the-art method.</p></p class="citation"></blockquote><h3 id=4782--109271-dreamsalon-a-staged-diffusion-framework-for-preserving-identity-context-in-editable-face-generation-haonan-lin-et-al-2024>(47/82 | 109/271) DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context in Editable Face Generation (Haonan Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haonan Lin, Mengmeng Wang, Yan Chen, Wenbin An, Yuzhe Yao, Guang Dai, Qianying Wang, Yong Liu, Jingdong Wang. (2024)<br><strong>DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context in Editable Face Generation</strong><br><button class=copy-to-clipboard title="DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context in Editable Face Generation" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19235v1.pdf filename=2403.19235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While large-scale pre-trained <b>text-to-image</b> models can synthesize diverse and high-quality human-centered images, novel challenges arise with a nuanced task of &ldquo;identity fine editing&rdquo;: precisely modifying specific features of a subject while maintaining its inherent identity and context. Existing personalization methods either require time-consuming optimization or learning additional encoders, adept in &ldquo;identity re-contextualization&rdquo;. However, they often struggle with detailed and sensitive tasks like human face editing. To address these challenges, we introduce DreamSalon, a noise-guided, staged-editing framework, uniquely focusing on detailed image manipulations and identity-context preservation. By discerning editing and boosting stages via the frequency and gradient of predicted noises, DreamSalon first performs detailed manipulations on specific features in the editing stage, guided by high-frequency information, and then employs stochastic denoising in the boosting stage to improve image quality. For more precise editing, DreamSalon semantically mixes source and target textual <b>prompts,</b> guided by differences in their embedding covariances, to direct the model&rsquo;s focus on specific manipulation areas. Our experiments demonstrate DreamSalon&rsquo;s ability to efficiently and faithfully edit fine details on human faces, outperforming existing methods both qualitatively and quantitatively.</p></p class="citation"></blockquote><h3 id=4882--110271-uncertainty-aware-deep-video-compression-with-ensembles-wufei-ma-et-al-2024>(48/82 | 110/271) Uncertainty-Aware Deep Video Compression with Ensembles (Wufei Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wufei Ma, Jiahao Li, Bin Li, Yan Lu. (2024)<br><strong>Uncertainty-Aware Deep Video Compression with Ensembles</strong><br><button class=copy-to-clipboard title="Uncertainty-Aware Deep Video Compression with Ensembles" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19158v1.pdf filename=2403.19158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based video compression is a challenging task, and many previous state-of-the-art learning-based video codecs use optical flows to exploit the temporal correlation between successive frames and then compress the residual error. Although these two-stage models are end-to-end optimized, the epistemic uncertainty in the motion estimation and the aleatoric uncertainty from the <b>quantization</b> operation lead to errors in the intermediate representations and introduce artifacts in the reconstructed frames. This inherent flaw limits the potential for higher bit rate savings. To address this issue, we propose an uncertainty-aware video compression model that can effectively capture the predictive uncertainty with deep ensembles. Additionally, we introduce an ensemble-aware loss to encourage the diversity among ensemble members and investigate the benefits of incorporating <b>adversarial</b> <b>training</b> in the video compression task. Experimental results on 1080p sequences show that our model can effectively save bits by more than 20% compared to DVC Pro.</p></p class="citation"></blockquote><h3 id=4982--111271-synthetic-medical-imaging-generation-with-generative-adversarial-networks-for-plain-radiographs-john-r-mcnulty-et-al-2024>(49/82 | 111/271) Synthetic Medical Imaging Generation with Generative Adversarial Networks For Plain Radiographs (John R. McNulty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John R. McNulty, Lee Kho, Alexandria L. Case, Charlie Fornaca, Drew Johnston, David Slater, Joshua M. Abzug, Sybil A. Russell. (2024)<br><strong>Synthetic Medical Imaging Generation with Generative Adversarial Networks For Plain Radiographs</strong><br><button class=copy-to-clipboard title="Synthetic Medical Imaging Generation with Generative Adversarial Networks For Plain Radiographs" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19107v1.pdf filename=2403.19107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In medical imaging, access to data is commonly limited due to patient privacy restrictions and the issue that it can be difficult to acquire enough data in the case of rare diseases.[1] The purpose of this investigation was to develop a reusable open-source synthetic image generation pipeline, the <b>GAN</b> Image Synthesis Tool (GIST), that is easy to use as well as easy to deploy. The pipeline helps to improve and standardize AI algorithms in the digital health space by generating high quality synthetic image data that is not linked to specific patients. Its image generation capabilities include the ability to generate imaging of pathologies or injuries with low incidence rates. This improvement of digital health AI algorithms could improve diagnostic accuracy, aid in patient care, decrease medicolegal claims, and ultimately decrease the overall cost of healthcare. The pipeline builds on existing <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> algorithms, and preprocessing and evaluation steps were included for completeness. For this work, we focused on ensuring the pipeline supports radiography, with a focus on synthetic knee and elbow x-ray images. In designing the pipeline, we evaluated the performance of current <b>GAN</b> architectures, studying the performance on available x-ray data. We show that the pipeline is capable of generating high quality and clinically relevant images based on a lay person&rsquo;s evaluation and the Fr'echet Inception Distance (FID) metric.</p></p class="citation"></blockquote><h3 id=5082--112271-a-real-time-framework-for-domain-adaptive-underwater-object-detection-with-image-enhancement-junjie-wen-et-al-2024>(50/82 | 112/271) A Real-Time Framework for Domain-Adaptive Underwater Object Detection with Image Enhancement (Junjie Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjie Wen, Jinqiang Cui, Benyun Zhao, Bingxin Han, Xuchen Liu, Zhi Gao, Ben M. Chen. (2024)<br><strong>A Real-Time Framework for Domain-Adaptive Underwater Object Detection with Image Enhancement</strong><br><button class=copy-to-clipboard title="A Real-Time Framework for Domain-Adaptive Underwater Object Detection with Image Enhancement" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19079v1.pdf filename=2403.19079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, significant progress has been made in the field of underwater image enhancement (UIE). However, its practical utility for high-level vision tasks, such as underwater <b>object</b> <b>detection</b> (UOD) in Autonomous Underwater Vehicles (AUVs), remains relatively unexplored. It may be attributed to several factors: (1) Existing methods typically employ UIE as a pre-processing step, which inevitably introduces considerable computational overhead and latency. (2) The process of enhancing images prior to training <b>object</b> <b>detectors</b> may not necessarily yield performance improvements. (3) The complex underwater environments can induce significant <b>domain</b> <b>shifts</b> across different scenarios, seriously deteriorating the UOD performance. To address these challenges, we introduce EnYOLO, an integrated real-time framework designed for simultaneous UIE and UOD with <b>domain-adaptation</b> <b>capability.</b> Specifically, both the UIE and UOD task heads share the same network backbone and utilize a lightweight design. Furthermore, to ensure balanced training for both tasks, we present a multi-stage training strategy aimed at consistently enhancing their performance. Additionally, we propose a novel <b>domain-adaptation</b> <b>strategy</b> to align feature embeddings originating from diverse underwater environments. Comprehensive experiments demonstrate that our framework not only achieves state-of-the-art (SOTA) performance in both UIE and UOD tasks, but also shows superior adaptability when applied to different underwater scenarios. Our efficiency analysis further highlights the substantial potential of our framework for onboard deployment.</p></p class="citation"></blockquote><h3 id=5182--113271-cdimc-net-cognitive-deep-incomplete-multi-view-clustering-network-jie-wen-et-al-2024>(51/82 | 113/271) CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network (Jie Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Wen, Zheng Zhang, Yong Xu, Bob Zhang, Lunke Fei, Guo-Sen Xie. (2024)<br><strong>CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network</strong><br><button class=copy-to-clipboard title="CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 16<br>Keywords: Graph, Graph Embedding, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19514v1.pdf filename=2403.19514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, incomplete multi-view <b>clustering,</b> which studies the challenging multi-view <b>clustering</b> problem on missing views, has received growing research interests. Although a series of methods have been proposed to address this issue, the following problems still exist: 1) Almost all of the existing methods are based on shallow models, which is difficult to obtain discriminative common representations. 2) These methods are generally sensitive to noise or outliers since the negative samples are treated equally as the important samples. In this paper, we propose a novel incomplete multi-view <b>clustering</b> network, called Cognitive Deep Incomplete Multi-view <b>Clustering</b> Network (CDIMC-net), to address these issues. Specifically, it captures the high-level features and local structure of each view by incorporating the view-specific deep encoders and <b>graph</b> <b>embedding</b> strategy into a framework. Moreover, based on the human cognition, i.e., learning from easy to hard, it introduces a self-paced strategy to select the most confident samples for model training, which can reduce the negative influence of outliers. Experimental results on several incomplete datasets show that CDIMC-net outperforms the state-of-the-art incomplete multi-view <b>clustering</b> methods.</p></p class="citation"></blockquote><h3 id=5282--114271-flowdepth-decoupling-optical-flow-for-self-supervised-monocular-depth-estimation-yiyang-sun-et-al-2024>(52/82 | 114/271) FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth Estimation (Yiyang Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyang Sun, Zhiyuan Xu, Xiaonian Wang, Jing Yao. (2024)<br><strong>FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth Estimation</strong><br><button class=copy-to-clipboard title="FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth Estimation" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 15<br>Keywords: Black Box, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19294v1.pdf filename=2403.19294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> multi-frame methods have currently achieved promising results in depth estimation. However, these methods often suffer from mismatch problems due to the moving objects, which break the static assumption. Additionally, unfairness can occur when calculating photometric errors in high-freq or low-texture regions of the images. To address these issues, existing approaches use additional semantic priori <b>black-box</b> <b>networks</b> to separate moving objects and improve the model only at the loss level. Therefore, we propose FlowDepth, where a Dynamic Motion Flow Module (DMFM) decouples the optical flow by a mechanism-based approach and warps the dynamic regions thus solving the mismatch problem. For the unfairness of photometric errors caused by high-freq and low-texture regions, we use Depth-Cue-Aware Blur (DCABlur) and Cost-Volume sparsity loss respectively at the input and the loss level to solve the problem. Experimental results on the KITTI and Cityscapes datasets show that our method outperforms the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=5382--115271-ilpo-net-network-for-the-invariant-recognition-of-arbitrary-volumetric-patterns-in-3d-dmitrii-zhemchuzhnikov-et-al-2024>(53/82 | 115/271) ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D (Dmitrii Zhemchuzhnikov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dmitrii Zhemchuzhnikov, Sergei Grudinin. (2024)<br><strong>ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D</strong><br><button class=copy-to-clipboard title="ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19612v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19612v1.pdf filename=2403.19612v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective recognition of spatial patterns and learning their hierarchy is crucial in modern spatial data analysis. Volumetric data applications seek techniques ensuring invariance not only to shifts but also to pattern rotations. While traditional methods can readily achieve translational invariance, rotational invariance possesses multiple challenges and remains an active area of research. Here, we present ILPO-Net (Invariant to Local Patterns Orientation Network), a novel approach that handles arbitrarily shaped patterns with the <b>convolutional</b> operation inherently invariant to local spatial pattern orientations using the Wigner matrix expansions. Our architecture seamlessly integrates the new <b>convolution</b> operator and, when <b>benchmarked</b> on diverse volumetric datasets such as MedMNIST and CATH, demonstrates superior performance over the baselines with significantly reduced parameter counts - up to 1000 times fewer in the case of MedMNIST. Beyond these demonstrations, ILPO-Net&rsquo;s rotational invariance paves the way for other applications across multiple disciplines. Our code is publicly available at <a href=https://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet>https://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet</a>.</p></p class="citation"></blockquote><h3 id=5482--116271-ov-uni3detr-towards-unified-open-vocabulary-3d-object-detection-via-cycle-modality-propagation-zhenyu-wang-et-al-2024>(54/82 | 116/271) OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation (Zhenyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyu Wang, Yali Li, Taichi Liu, Hengshuang Zhao, Shengjin Wang. (2024)<br><strong>OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation</strong><br><button class=copy-to-clipboard title="OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19580v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19580v1.pdf filename=2403.19580v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the current state of 3D <b>object</b> <b>detection</b> research, the severe scarcity of annotated 3D data, substantial disparities across different data modalities, and the absence of a unified architecture, have impeded the progress towards the goal of universality. In this paper, we propose \textbf{OV-Uni3DETR}, a unified open-vocabulary 3D detector via cycle-modality propagation. Compared with existing 3D detectors, OV-Uni3DETR offers distinct advantages: 1) Open-vocabulary 3D detection: During training, it leverages various accessible data, especially extensive 2D detection images, to boost training diversity. During inference, it can detect both seen and unseen classes. 2) Modality unifying: It seamlessly accommodates input data from any given modality, effectively addressing scenarios involving disparate modalities or missing sensor information, thereby supporting test-time modality switching. 3) Scene unifying: It provides a unified <b>multi-modal</b> model architecture for diverse scenes collected by distinct sensors. Specifically, we propose the cycle-modality propagation, aimed at propagating knowledge bridging 2D and 3D modalities, to support the aforementioned functionalities. 2D semantic knowledge from large-vocabulary learning guides novel class discovery in the 3D domain, and 3D geometric knowledge provides localization supervision for 2D detection images. OV-Uni3DETR achieves the state-of-the-art performance on various scenarios, surpassing existing methods by more than 6% on average. Its performance using only RGB images is on par with or even surpasses that of previous point cloud based methods. Code and pre-trained models will be released later.</p></p class="citation"></blockquote><h3 id=5582--117271-locate-assign-refine-taming-customized-image-inpainting-with-text-subject-guidance-yulin-pan-et-al-2024>(55/82 | 117/271) Locate, Assign, Refine: Taming Customized Image Inpainting with Text-Subject Guidance (Yulin Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulin Pan, Chaojie Mao, Zeyinzi Jiang, Zhen Han, Jingfeng Zhang. (2024)<br><strong>Locate, Assign, Refine: Taming Customized Image Inpainting with Text-Subject Guidance</strong><br><button class=copy-to-clipboard title="Locate, Assign, Refine: Taming Customized Image Inpainting with Text-Subject Guidance" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19534v1.pdf filename=2403.19534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior studies have made significant progress in image inpainting guided by either text or subject image. However, the research on editing with their combined guidance is still in the early stages. To tackle this challenge, we present LAR-Gen, a novel approach for image inpainting that enables seamless inpainting of masked scene images, incorporating both the textual <b>prompts</b> and specified subjects. Our approach adopts a coarse-to-fine manner to ensure subject identity preservation and local semantic coherence. The process involves (i) Locate: concatenating the noise with masked scene image to achieve precise regional editing, (ii) Assign: employing decoupled cross-attention mechanism to accommodate <b>multi-modal</b> guidance, and (iii) Refine: using a novel RefineNet to supplement subject details. Additionally, to address the issue of scarce training data, we introduce a novel data construction pipeline. This pipeline extracts substantial pairs of data consisting of local text <b>prompts</b> and corresponding visual instances from a vast image dataset, leveraging publicly available large models. Extensive experiments and varied application scenarios demonstrate the superiority of LAR-Gen in terms of both identity preservation and text semantic consistency. Project page can be found at \url{https://ali-vilab.github.io/largen-page/}.</p></p class="citation"></blockquote><h3 id=5682--118271-sg-pgm-partial-graph-matching-network-with-semantic-geometric-fusion-for-3d-scene-graph-alignment-and-its-downstream-tasks-yaxu-xie-et-al-2024>(56/82 | 118/271) SG-PGM: Partial Graph Matching Network with Semantic Geometric Fusion for 3D Scene Graph Alignment and Its Downstream Tasks (Yaxu Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaxu Xie, Alain Pagani, Didier Stricker. (2024)<br><strong>SG-PGM: Partial Graph Matching Network with Semantic Geometric Fusion for 3D Scene Graph Alignment and Its Downstream Tasks</strong><br><button class=copy-to-clipboard title="SG-PGM: Partial Graph Matching Network with Semantic Geometric Fusion for 3D Scene Graph Alignment and Its Downstream Tasks" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19474v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19474v1.pdf filename=2403.19474v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene <b>graphs</b> <b>have</b> <b>been</b> recently introduced into 3D spatial understanding as a comprehensive representation of the scene. The alignment between 3D scene <b>graphs</b> <b>is</b> <b>the</b> first step of many downstream tasks such as scene <b>graph</b> <b>aided</b> <b>point</b> cloud registration, mosaicking, overlap checking, and robot navigation. In this work, we treat 3D scene <b>graph</b> <b>alignment</b> <b>as</b> a partial <b>graph-matching</b> <b>problem</b> <b>and</b> propose to solve it with a <b>graph</b> <b>neural</b> <b>network.</b> We reuse the geometric features learned by a point cloud registration method and associate the clustered point-level geometric features with the node-level semantic feature via our designed feature fusion module. Partial matching is enabled by using a learnable method to select the top-k similar node pairs. Subsequent downstream tasks such as point cloud registration are achieved by running a pre-trained registration network within the matched regions. We further propose a point-matching rescoring method, that uses the node-wise alignment of the 3D scene <b>graph</b> <b>to</b> <b>reweight</b> the matching candidates from a pre-trained point cloud registration method. It reduces the false point correspondences estimated especially in low-overlapping cases. Experiments show that our method improves the alignment accuracy by 10~20% in low-overlap and random transformation scenarios and outperforms the existing work in multiple downstream tasks.</p></p class="citation"></blockquote><h3 id=5782--119271-beyond-talking----generating-holistic-3d-human-dyadic-motion-for-communication-mingze-sun-et-al-2024>(57/82 | 119/271) Beyond Talking &ndash; Generating Holistic 3D Human Dyadic Motion for Communication (Mingze Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingze Sun, Chao Xu, Xinyu Jiang, Yang Liu, Baigui Sun, Ruqi Huang. (2024)<br><strong>Beyond Talking &ndash; Generating Holistic 3D Human Dyadic Motion for Communication</strong><br><button class=copy-to-clipboard title="Beyond Talking -- Generating Holistic 3D Human Dyadic Motion for Communication" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19467v1.pdf filename=2403.19467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce an innovative task focused on human communication, aiming to generate 3D holistic human motions for both speakers and listeners. Central to our approach is the incorporation of factorization to decouple audio features and the combination of textual semantic information, thereby facilitating the creation of more realistic and coordinated movements. We separately train VQ-VAEs with respect to the holistic motions of both speaker and listener. We consider the real-time mutual influence between the speaker and the listener and propose a novel chain-like <b>transformer-based</b> auto-regressive model specifically designed to characterize real-world communication scenarios effectively which can generate the motions of both the speaker and the listener simultaneously. These designs ensure that the results we generate are both coordinated and diverse. Our approach demonstrates state-of-the-art performance on two <b>benchmark</b> datasets. Furthermore, we introduce the HoCo holistic communication dataset, which is a valuable resource for future research. Our HoCo dataset and code will be released for research purposes upon acceptance.</p></p class="citation"></blockquote><h3 id=5882--120271-pointcloud-text-matching-benchmark-datasets-and-a-baseline-yanglin-feng-et-al-2024>(58/82 | 120/271) PointCloud-Text Matching: Benchmark Datasets and a Baseline (Yanglin Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanglin Feng, Yang Qin, Dezhong Peng, Hongyuan Zhu, Xi Peng, Peng Hu. (2024)<br><strong>PointCloud-Text Matching: Benchmark Datasets and a Baseline</strong><br><button class=copy-to-clipboard title="PointCloud-Text Matching: Benchmark Datasets and a Baseline" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19386v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19386v1.pdf filename=2403.19386v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present and study a new instance-level retrieval task: PointCloud-Text Matching~(PTM), which aims to find the exact cross-modal instance that matches a given point-cloud query or text query. PTM could be applied to various scenarios, such as indoor/urban-canyon localization and scene retrieval. However, there exists no suitable and targeted dataset for PTM in practice. Therefore, we construct three new PTM <b>benchmark</b> datasets, namely 3D2T-SR, 3D2T-NR, and 3D2T-QA. We observe that the data is challenging and with noisy correspondence due to the sparsity, noise, or disorder of point clouds and the ambiguity, vagueness, or incompleteness of texts, which make existing cross-modal matching methods ineffective for PTM. To tackle these challenges, we propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa). RoMa consists of two modules: a Dual Attention Perception module (DAP) and a Robust Negative <b>Contrastive</b> <b>Learning</b> module (RNCL). Specifically, DAP leverages token-level and feature-level attention to adaptively focus on useful local and global features, and aggregate them into common representations, thereby reducing the adverse impact of noise and ambiguity. To handle noisy correspondence, RNCL divides negative pairs, which are much less error-prone than positive pairs, into clean and noisy subsets, and assigns them forward and reverse optimization directions respectively, thus enhancing robustness against noisy correspondence. We conduct extensive experiments on our <b>benchmarks</b> and demonstrate the superiority of our RoMa.</p></p class="citation"></blockquote><h3 id=5982--121271-recdiffusion-rectangling-for-image-stitching-with-diffusion-models-tianhao-zhou-et-al-2024>(59/82 | 121/271) RecDiffusion: Rectangling for Image Stitching with Diffusion Models (Tianhao Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianhao Zhou, Haipeng Li, Ziyi Wang, Ao Luo, Chen-Lin Zhang, Jiajun Li, Bing Zeng, Shuaicheng Liu. (2024)<br><strong>RecDiffusion: Rectangling for Image Stitching with Diffusion Models</strong><br><button class=copy-to-clipboard title="RecDiffusion: Rectangling for Image Stitching with Diffusion Models" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19164v1.pdf filename=2403.19164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image stitching from different captures often results in non-rectangular boundaries, which is often considered unappealing. To solve non-rectangular boundaries, current solutions involve cropping, which discards image content, inpainting, which can introduce unrelated content, or warping, which can distort non-linear features and introduce artifacts. To overcome these issues, we introduce a novel <b>diffusion-based</b> <b>learning</b> framework, \textbf{RecDiffusion}, for image stitching rectangling. This framework combines Motion <b>Diffusion</b> <b>Models</b> (MDM) to generate motion fields, effectively transitioning from the stitched image&rsquo;s irregular borders to a geometrically corrected intermediary. Followed by Content <b>Diffusion</b> <b>Models</b> (CDM) for image detail refinement. Notably, our sampling process utilizes a weighted map to identify regions needing correction during each iteration of CDM. Our RecDiffusion ensures geometric accuracy and overall visual appeal, surpassing all previous methods in both quantitative and qualitative measures when evaluated on public <b>benchmarks.</b> Code is released at <a href=https://github.com/lhaippp/RecDiffusion>https://github.com/lhaippp/RecDiffusion</a>.</p></p class="citation"></blockquote><h3 id=6082--122271-efficient-3d-instance-mapping-and-localization-with-neural-fields-george-tang-et-al-2024>(60/82 | 122/271) Efficient 3D Instance Mapping and Localization with Neural Fields (George Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>George Tang, Krishna Murthy Jatavallabhula, Antonio Torralba. (2024)<br><strong>Efficient 3D Instance Mapping and Localization with Neural Fields</strong><br><button class=copy-to-clipboard title="Efficient 3D Instance Mapping and Localization with Neural Fields" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19797v1.pdf filename=2403.19797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We tackle the problem of learning an implicit scene representation for 3D instance segmentation from a sequence of posed RGB images. Towards this, we introduce 3DIML, a novel framework that efficiently learns a label field that may be rendered from novel viewpoints to produce view-consistent instance segmentation masks. 3DIML significantly improves upon training and inference runtimes of existing implicit scene representation based methods. Opposed to prior art that optimizes a neural field in a <b>self-supervised</b> manner, requiring complicated training procedures and loss function design, 3DIML leverages a two-phase process. The first phase, InstanceMap, takes as input 2D segmentation masks of the image sequence generated by a frontend instance segmentation model, and associates corresponding masks across images to 3D labels. These almost view-consistent pseudolabel masks are then used in the second phase, InstanceLift, to supervise the training of a neural label field, which interpolates regions missed by InstanceMap and resolves ambiguities. Additionally, we introduce InstanceLoc, which enables near realtime localization of instance masks given a trained label field and an off-the-shelf image segmentation model by fusing outputs from both. We evaluate 3DIML on sequences from the Replica and ScanNet datasets and demonstrate 3DIML&rsquo;s effectiveness under mild assumptions for the image sequences. We achieve a 14-24x speedup over existing implicit scene representation methods with comparable quality, showcasing its potential to facilitate faster and more effective 3D scene understanding.</p></p class="citation"></blockquote><h3 id=6182--123271-shapefusion-a-3d-diffusion-model-for-localized-shape-editing-rolandos-alexandros-potamias-et-al-2024>(61/82 | 123/271) ShapeFusion: A 3D diffusion model for localized shape editing (Rolandos Alexandros Potamias et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rolandos Alexandros Potamias, Michail Tarasiou Stylianos Ploumpis, Stefanos Zafeiriou. (2024)<br><strong>ShapeFusion: A 3D diffusion model for localized shape editing</strong><br><button class=copy-to-clipboard title="ShapeFusion: A 3D diffusion model for localized shape editing" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19773v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19773v1.pdf filename=2403.19773v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of 3D computer vision, parametric models have emerged as a ground-breaking methodology for the creation of realistic and expressive 3D avatars. Traditionally, they rely on Principal Component Analysis (PCA), given its ability to decompose data to an orthonormal space that maximally captures shape variations. However, due to the orthogonality constraints and the global nature of PCA&rsquo;s decomposition, these models struggle to perform localized and disentangled editing of 3D shapes, which severely affects their use in applications requiring fine control such as face sculpting. In this paper, we leverage <b>diffusion</b> <b>models</b> to enable diverse and fully localized edits on 3D meshes, while completely preserving the un-edited regions. We propose an effective <b>diffusion</b> <b>masking</b> training strategy that, by design, facilitates localized manipulation of any shape region, without being limited to predefined regions or to sparse sets of predefined control vertices. Following our framework, a user can explicitly set their manipulation region of choice and define an arbitrary set of vertices as handles to edit a 3D mesh. Compared to the current state-of-the-art our method leads to more interpretable shape manipulations than methods relying on latent code state, greater localization and generation diversity while offering faster inference than optimization based approaches. Project page: <a href=https://rolpotamias.github.io/Shapefusion/>https://rolpotamias.github.io/Shapefusion/</a></p></p class="citation"></blockquote><h3 id=6282--124271-gaustudio-a-modular-framework-for-3d-gaussian-splatting-and-beyond-chongjie-ye-et-al-2024>(62/82 | 124/271) GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond (Chongjie Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chongjie Ye, Yinyu Nie, Jiahao Chang, Yuantao Chen, Yihao Zhi, Xiaoguang Han. (2024)<br><strong>GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond</strong><br><button class=copy-to-clipboard title="GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19632v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19632v1.pdf filename=2403.19632v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present GauStudio, a novel modular framework for modeling 3D Gaussian Splatting (3DGS) to provide standardized, plug-and-play components for users to easily customize and implement a 3DGS pipeline. Supported by our framework, we propose a hybrid Gaussian representation with foreground and skyball background models. Experiments demonstrate this representation reduces artifacts in unbounded outdoor scenes and improves novel view synthesis. Finally, we propose Gaussian Splatting Surface Reconstruction (GauS), a novel render-then-fuse approach for high-fidelity mesh reconstruction from 3DGS inputs without <b>fine-tuning.</b> Overall, our GauStudio framework, hybrid representation, and GauS approach enhance 3DGS modeling and rendering capabilities, enabling higher-quality novel view synthesis and surface reconstruction.</p></p class="citation"></blockquote><h3 id=6382--125271-frame-by-familiar-frame-understanding-replication-in-video-diffusion-models-aimon-rahman-et-al-2024>(63/82 | 125/271) Frame by Familiar Frame: Understanding Replication in Video Diffusion Models (Aimon Rahman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aimon Rahman, Malsha V. Perera, Vishal M. Patel. (2024)<br><strong>Frame by Familiar Frame: Understanding Replication in Video Diffusion Models</strong><br><button class=copy-to-clipboard title="Frame by Familiar Frame: Understanding Replication in Video Diffusion Models" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19593v1.pdf filename=2403.19593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Building on the momentum of image generation <b>diffusion</b> <b>models,</b> there is an increasing interest in video-based <b>diffusion</b> <b>models.</b> However, video generation poses greater challenges due to its higher-dimensional nature, the scarcity of training data, and the complex spatiotemporal relationships involved. Image generation models, due to their extensive data requirements, have already strained computational resources to their limits. There have been instances of these models reproducing elements from the training samples, leading to concerns and even legal disputes over sample replication. Video <b>diffusion</b> <b>models,</b> which operate with even more constrained datasets and are tasked with generating both spatial and temporal content, may be more prone to replicating samples from their training sets. Compounding the issue, these models are often evaluated using metrics that inadvertently reward replication. In our paper, we present a systematic investigation into the phenomenon of sample replication in video <b>diffusion</b> <b>models.</b> We scrutinize various recent <b>diffusion</b> <b>models</b> for video synthesis, assessing their tendency to replicate spatial and temporal content in both unconditional and conditional generation scenarios. Our study identifies strategies that are less likely to lead to replication. Furthermore, we propose new evaluation strategies that take replication into account, offering a more accurate measure of a model&rsquo;s ability to generate the original content.</p></p class="citation"></blockquote><h3 id=6482--126271-tod3cap-towards-3d-dense-captioning-in-outdoor-scenes-bu-jin-et-al-2024>(64/82 | 126/271) TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes (Bu Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bu Jin, Yupeng Zheng, Pengfei Li, Weize Li, Yuhang Zheng, Sujie Hu, Xinyu Liu, Jinwei Zhu, Zhijie Yan, Haiyang Sun, Kun Zhan, Peng Jia, Xiaoxiao Long, Yilun Chen, Hao Zhao. (2024)<br><strong>TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes</strong><br><button class=copy-to-clipboard title="TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: LLaMA<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19589v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19589v1.pdf filename=2403.19589v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D dense captioning stands as a cornerstone in achieving a comprehensive understanding of 3D scenes through natural language. It has recently witnessed remarkable achievements, particularly in indoor settings. However, the exploration of 3D dense captioning in outdoor scenes is hindered by two major challenges: 1) the \textbf{domain gap} between indoor and outdoor scenes, such as dynamics and sparse visual inputs, makes it difficult to directly adapt existing indoor methods; 2) the \textbf{lack of data} with comprehensive box-caption pair annotations specifically tailored for outdoor scenes. To this end, we introduce the new task of outdoor 3D dense captioning. As input, we assume a LiDAR point cloud and a set of RGB images captured by the panoramic camera rig. The expected output is a set of object boxes with captions. To tackle this task, we propose the TOD3Cap network, which leverages the BEV representation to generate object box proposals and integrates Relation Q-Former with <b>LLaMA-Adapter</b> to generate rich captions for these objects. We also introduce the TOD3Cap dataset, the largest one to our knowledge for 3D dense captioning in outdoor scenes, which contains 2.3M descriptions of 64.3K outdoor objects from 850 scenes. Notably, our TOD3Cap network can effectively localize and caption 3D objects in outdoor scenes, which outperforms baseline methods by a significant margin (+9.6 <a href=mailto:CiDEr@0.5IoU>CiDEr@0.5IoU</a>). Code, data, and models are publicly available at <a href=https://github.com/jxbbb/TOD3Cap>https://github.com/jxbbb/TOD3Cap</a>.</p></p class="citation"></blockquote><h3 id=6582--127271-coherentgs-sparse-novel-view-synthesis-with-coherent-3d-gaussians-avinash-paliwal-et-al-2024>(65/82 | 127/271) CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians (Avinash Paliwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, Nima Khademi Kalantari. (2024)<br><strong>CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians</strong><br><button class=copy-to-clipboard title="CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19495v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19495v1.pdf filename=2403.19495v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit <b>convolutional</b> decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes.</p></p class="citation"></blockquote><h3 id=6682--128271-burst-super-resolution-with-diffusion-models-for-improving-perceptual-quality-kyotaro-tokoro-et-al-2024>(66/82 | 128/271) Burst Super-Resolution with Diffusion Models for Improving Perceptual Quality (Kyotaro Tokoro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyotaro Tokoro, Kazutoshi Akita, Norimichi Ukita. (2024)<br><strong>Burst Super-Resolution with Diffusion Models for Improving Perceptual Quality</strong><br><button class=copy-to-clipboard title="Burst Super-Resolution with Diffusion Models for Improving Perceptual Quality" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19428v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19428v1.pdf filename=2403.19428v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While burst LR images are useful for improving the SR image quality compared with a single LR image, prior SR networks accepting the burst LR images are trained in a deterministic manner, which is known to produce a blurry SR image. In addition, it is difficult to perfectly align the burst LR images, making the SR image more blurry. Since such blurry images are perceptually degraded, we aim to reconstruct the sharp high-fidelity boundaries. Such high-fidelity images can be reconstructed by <b>diffusion</b> <b>models.</b> However, prior SR methods using the <b>diffusion</b> <b>model</b> are not properly optimized for the burst SR task. Specifically, the reverse process starting from a random sample is not optimized for image enhancement and restoration methods, including burst SR. In our proposed method, on the other hand, burst LR features are used to reconstruct the initial burst SR image that is fed into an intermediate step in the <b>diffusion</b> <b>model.</b> This reverse process from the intermediate step 1) skips <b>diffusion</b> <b>steps</b> for reconstructing the global structure of the image and 2) focuses on steps for refining detailed textures. Our experimental results demonstrate that our method can improve the scores of the perceptual quality metrics. Code: <a href=https://github.com/placerkyo/BSRD>https://github.com/placerkyo/BSRD</a></p></p class="citation"></blockquote><h3 id=6782--129271-imperceptible-protection-against-style-imitation-from-diffusion-models-namhyuk-ahn-et-al-2024>(67/82 | 129/271) Imperceptible Protection against Style Imitation from Diffusion Models (Namhyuk Ahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Namhyuk Ahn, Wonhyuk Ahn, KiYoon Yoo, Daesik Kim, Seung-Hun Nam. (2024)<br><strong>Imperceptible Protection against Style Imitation from Diffusion Models</strong><br><button class=copy-to-clipboard title="Imperceptible Protection against Style Imitation from Diffusion Models" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19254v1.pdf filename=2403.19254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent progress in <b>diffusion</b> <b>models</b> has profoundly enhanced the fidelity of image generation. However, this has raised concerns about copyright infringements. While prior methods have introduced adversarial perturbations to prevent style imitation, most are accompanied by the degradation of artworks&rsquo; visual quality. Recognizing the importance of maintaining this, we develop a visually improved protection method that preserves its protection capability. To this end, we create a perceptual map to identify areas most sensitive to human eyes. We then adjust the protection intensity guided by an instance-aware refinement. We also integrate a perceptual constraints bank to further improve the imperceptibility. Results show that our method substantially elevates the quality of the protected image without compromising on protection efficacy.</p></p class="citation"></blockquote><h3 id=6882--130271-efficient-and-effective-weakly-supervised-action-segmentation-via-action-transition-aware-boundary-alignment-angchi-xu-et-al-2024>(68/82 | 130/271) Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment (Angchi Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angchi Xu, Wei-Shi Zheng. (2024)<br><strong>Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment</strong><br><button class=copy-to-clipboard title="Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19225v1.pdf filename=2403.19225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Weakly-supervised</b> action segmentation is a task of learning to partition a long video into several action segments, where training videos are only accompanied by transcripts (ordered list of actions). Most of existing methods need to infer pseudo segmentation for training by serial alignment between all frames and the transcript, which is time-consuming and hard to be parallelized while training. In this work, we aim to escape from this inefficient alignment with massive but redundant frames, and instead to directly localize a few action transitions for pseudo segmentation generation, where a transition refers to the change from an action segment to its next adjacent one in the transcript. As the true transitions are submerged in noisy boundaries due to intra-segment visual variation, we propose a novel Action-Transition-Aware Boundary Alignment (ATBA) framework to efficiently and effectively filter out noisy boundaries and detect transitions. In addition, to boost the semantic learning in the case that noise is inevitably present in the pseudo segmentation, we also introduce video-level losses to utilize the trusted video-level supervision. Extensive experiments show the effectiveness of our approach on both performance and training speed.</p></p class="citation"></blockquote><h3 id=6982--131271-geoauxnet-towards-universal-3d-representation-learning-for-multi-sensor-point-clouds-shengjun-zhang-et-al-2024>(69/82 | 131/271) GeoAuxNet: Towards Universal 3D Representation Learning for Multi-sensor Point Clouds (Shengjun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengjun Zhang, Xin Fei, Yueqi Duan. (2024)<br><strong>GeoAuxNet: Towards Universal 3D Representation Learning for Multi-sensor Point Clouds</strong><br><button class=copy-to-clipboard title="GeoAuxNet: Towards Universal 3D Representation Learning for Multi-sensor Point Clouds" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Geometry, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19220v1.pdf filename=2403.19220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point clouds captured by different sensors such as RGB-D cameras and LiDAR possess non-negligible domain gaps. Most existing methods design different network architectures and train separately on point clouds from various sensors. Typically, point-based methods achieve outstanding performances on even-distributed dense point clouds from RGB-D cameras, while voxel-based methods are more efficient for large-range sparse LiDAR point clouds. In this paper, we propose <b>geometry-to-voxel</b> auxiliary learning to enable voxel <b>representations</b> <b>to</b> access point-level geometric information, which supports better generalisation of the voxel-based backbone with additional interpretations of multi-sensor point clouds. Specifically, we construct hierarchical <b>geometry</b> pools generated by a voxel-guided dynamic point network, which efficiently provide auxiliary fine-grained geometric information adapted to different stages of voxel features. We conduct experiments on joint multi-sensor datasets to demonstrate the effectiveness of GeoAuxNet. Enjoying elaborate geometric information, our method outperforms other models collectively trained on multi-sensor datasets, and achieve competitive results with the-state-of-art experts on each single dataset.</p></p class="citation"></blockquote><h3 id=7082--132271-rethinking-information-loss-in-medical-image-segmentation-with-various-sized-targets-tianyi-liu-et-al-2024>(70/82 | 132/271) Rethinking Information Loss in Medical Image Segmentation with Various-sized Targets (Tianyi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyi Liu, Zhaorui Tan, Kaizhu Huang, Haochuan Jiang. (2024)<br><strong>Rethinking Information Loss in Medical Image Segmentation with Various-sized Targets</strong><br><button class=copy-to-clipboard title="Rethinking Information Loss in Medical Image Segmentation with Various-sized Targets" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19177v1.pdf filename=2403.19177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image segmentation presents the challenge of segmenting various-size targets, demanding the model to effectively capture both local and global information. Despite recent efforts using <b>CNNs</b> and ViTs to predict annotations of different scales, these approaches often struggle to effectively balance the detection of targets across varying sizes. Simply utilizing local information from <b>CNNs</b> and global relationships from ViTs without considering potential significant divergence in latent feature distributions may result in substantial information loss. To address this issue, in this paper, we will introduce a novel Stagger Network (SNet) and argues that a well-designed fusion structure can mitigate the divergence in latent feature distributions between <b>CNNs</b> and ViTs, thereby reducing information loss. Specifically, to emphasize both global dependencies and local focus, we design a Parallel Module to bridge the semantic gap. Meanwhile, we propose the Stagger Module, trying to fuse the selected features that are more semantically similar. An Information Recovery Module is further adopted to recover complementary information back to the network. As a key contribution, we theoretically analyze that the proposed parallel and stagger strategies would lead to less information loss, thus certifying the SNet&rsquo;s rationale. Experimental results clearly proved that the proposed SNet excels comparisons with recent SOTAs in segmenting on the Synapse dataset where targets are in various sizes. Besides, it also demonstrates superiority on the ACDC and the MoNuSeg datasets where targets are with more consistent dimensions.</p></p class="citation"></blockquote><h3 id=7182--133271-within-the-dynamic-context-inertia-aware-3d-human-modeling-with-pose-sequence-yutong-chen-et-al-2024>(71/82 | 133/271) Within the Dynamic Context: Inertia-aware 3D Human Modeling with Pose Sequence (Yutong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutong Chen, Yifan Zhan, Zhihang Zhong, Wei Wang, Xiao Sun, Yu Qiao, Yinqiang Zheng. (2024)<br><strong>Within the Dynamic Context: Inertia-aware 3D Human Modeling with Pose Sequence</strong><br><button class=copy-to-clipboard title="Within the Dynamic Context: Inertia-aware 3D Human Modeling with Pose Sequence" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19160v1.pdf filename=2403.19160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural rendering techniques have significantly advanced 3D human body modeling. However, previous approaches often overlook dynamics induced by factors such as motion inertia, leading to challenges in scenarios like abrupt stops after rotation, where the pose remains static while the appearance changes. This limitation arises from reliance on a single pose as conditional input, resulting in ambiguity in mapping one pose to multiple appearances. In this study, we elucidate that variations in human appearance depend not only on the current frame&rsquo;s pose condition but also on past pose states. Therefore, we introduce Dyco, a novel method utilizing the delta pose sequence representation for non-rigid deformations and canonical space to effectively model temporal appearance variations. To prevent a decrease in the model&rsquo;s generalization ability to novel poses, we further propose low-dimensional global context to reduce unnecessary inter-body part dependencies and a <b>quantization</b> operation to mitigate overfitting of the delta pose sequence by the model. To validate the effectiveness of our approach, we collected a novel dataset named I3D-Human, with a focus on capturing temporal changes in clothing appearance under approximate poses. Through extensive experiments on both I3D-Human and existing datasets, our approach demonstrates superior qualitative and quantitative performance. In addition, our inertia-aware 3D human method can unprecedentedly simulate appearance changes caused by inertia at different velocities.</p></p class="citation"></blockquote><h3 id=7282--134271-total-decom-decomposed-3d-scene-reconstruction-with-minimal-interaction-xiaoyang-lyu-et-al-2024>(72/82 | 134/271) Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction (Xiaoyang Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyang Lyu, Chirui Chang, Peng Dai, Yang-tian Sun, Xiaojuang Qi. (2024)<br><strong>Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction</strong><br><button class=copy-to-clipboard title="Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 8<br>Keywords: Benchmarking, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19314v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19314v1.pdf filename=2403.19314v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene reconstruction from multi-view images is a fundamental problem in computer vision and graphics. Recent neural implicit surface reconstruction methods have achieved high-quality results; however, editing and manipulating the 3D <b>geometry</b> of reconstructed scenes remains challenging due to the absence of naturally decomposed object entities and complex object/background compositions. In this paper, we present Total-Decom, a novel method for decomposed 3D reconstruction with minimal human interaction. Our approach seamlessly integrates the Segment Anything Model (SAM) with hybrid implicit-explicit neural surface representations and a mesh-based region-growing technique for accurate 3D object decomposition. Total-Decom requires minimal human annotations while providing users with real-time control over the granularity and quality of decomposition. We extensively evaluate our method on <b>benchmark</b> datasets and demonstrate its potential for downstream applications, such as animation and scene editing. The code is available at \href{https://github.com/CVMI-Lab/Total-Decom.git}{https://github.com/CVMI-Lab/Total-Decom.git}.</p></p class="citation"></blockquote><h3 id=7382--135271-reli11d-a-comprehensive-multimodal-human-motion-dataset-and-method-ming-yan-et-al-2024>(73/82 | 135/271) RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method (Ming Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Yan, Yan Zhang, Shuqiang Cai, Shuqi Fan, Xincheng Lin, Yudi Dai, Siqi Shen, Chenglu Wen, Lan Xu, Yuexin Ma, Cheng Wang. (2024)<br><strong>RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method</strong><br><button class=copy-to-clipboard title="RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19501v1.pdf filename=2403.19501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Comprehensive capturing of human motions requires both accurate captures of complex poses and precise localization of the human within scenes. Most of the HPE datasets and methods primarily rely on RGB, LiDAR, or IMU data. However, solely using these modalities or a combination of them may not be adequate for HPE, particularly for complex and fast movements. For holistic human motion understanding, we present RELI11D, a high-quality <b>multimodal</b> human motion dataset involves LiDAR, IMU system, RGB camera, and Event camera. It records the motions of 10 actors performing 5 sports in 7 scenes, including 3.32 hours of synchronized LiDAR point clouds, IMU measurement data, RGB videos and Event steams. Through extensive experiments, we demonstrate that the RELI11D presents considerable challenges and opportunities as it contains many rapid and complex motions that require precise location. To address the challenge of integrating different modalities, we propose LEIR, a <b>multimodal</b> baseline that effectively utilizes LiDAR Point Cloud, Event stream, and RGB through our cross-attention fusion strategy. We show that LEIR exhibits promising results for rapid motions and daily motions and that utilizing the characteristics of multiple modalities can indeed improve HPE performance. Both the dataset and source code will be released publicly to the research community, fostering collaboration and enabling further exploration in this field.</p></p class="citation"></blockquote><h3 id=7482--136271-benchmarking-implicit-neural-representation-and-geometric-rendering-in-real-time-rgb-d-slam-tongyan-hua-et-al-2024>(74/82 | 136/271) Benchmarking Implicit Neural Representation and Geometric Rendering in Real-Time RGB-D SLAM (Tongyan Hua et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tongyan Hua, Lin Wang. (2024)<br><strong>Benchmarking Implicit Neural Representation and Geometric Rendering in Real-Time RGB-D SLAM</strong><br><button class=copy-to-clipboard title="Benchmarking Implicit Neural Representation and Geometric Rendering in Real-Time RGB-D SLAM" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19473v1.pdf filename=2403.19473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implicit neural representation (INR), in combination with geometric rendering, has recently been employed in real-time dense RGB-D SLAM. Despite active research endeavors being made, there lacks a unified protocol for fair evaluation, impeding the evolution of this area. In this work, we establish, to our knowledge, the first open-source <b>benchmark</b> framework to evaluate the performance of a wide spectrum of commonly used INRs and rendering functions for mapping and localization. The goal of our <b>benchmark</b> is to 1) gain an intuition of how different INRs and rendering functions impact mapping and localization and 2) establish a unified evaluation protocol w.r.t. the design choices that may impact the mapping and localization. With the framework, we conduct a large suite of experiments, offering various insights in choosing the INRs and geometric rendering functions: for example, the dense feature grid outperforms other INRs (e.g. tri-plane and hash grid), even when geometric and color features are jointly encoded for memory efficiency. To extend the findings into the practical scenario, a hybrid encoding strategy is proposed to bring the best of the accuracy and completion from the grid-based and decomposition-based INRs. We further propose explicit hybrid encoding for high-fidelity dense grid mapping to comply with the RGB-D SLAM system that puts the premise on robustness and computation efficiency.</p></p class="citation"></blockquote><h3 id=7582--137271-segmentation-tool-for-images-of-cracks-andrii-kompanets-et-al-2024>(75/82 | 137/271) Segmentation tool for images of cracks (Andrii Kompanets et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrii Kompanets, Remco Duits, Davide Leonetti, Nicky van den Berg, H. H., Snijder. (2024)<br><strong>Segmentation tool for images of cracks</strong><br><button class=copy-to-clipboard title="Segmentation tool for images of cracks" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19492v1.pdf filename=2403.19492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safety-critical infrastructures, such as bridges, are periodically inspected to check for existing damage, such as fatigue cracks and corrosion, and to guarantee the safe use of the infrastructure. Visual inspection is the most frequent type of general inspection, despite the fact that its detection capability is rather limited, especially for fatigue cracks. Machine learning algorithms can be used for augmenting the capability of classical visual inspection of bridge structures, however, the implementation of such an algorithm requires a massive annotated training dataset, which is time-consuming to produce. This paper proposes a semi-automatic crack segmentation tool that eases the manual segmentation of cracks on images needed to create a training dataset for a machine learning algorithm. Also, it can be used to measure the <b>geometry</b> of the crack. This tool makes use of an image processing algorithm, which was initially developed for the analysis of vascular systems on retinal images. The algorithm relies on a multi-orientation wavelet transform, which is applied to the image to construct the so-called &ldquo;orientation scores&rdquo;, i.e. a modified version of the image. Afterwards, the filtered orientation scores are used to formulate an optimal path problem that identifies the crack. The globally optimal path between manually selected crack endpoints is computed, using a state-of-the-art geometric tracking method. The pixel-wise segmentation is done afterwards using the obtained crack path. The proposed method outperforms fully automatic methods and shows potential to be an adequate alternative to the manual data annotation.</p></p class="citation"></blockquote><h3 id=7682--138271-clora-a-contrastive-approach-to-compose-multiple-lora-models-tuna-han-salih-meral-et-al-2024>(76/82 | 138/271) CLoRA: A Contrastive Approach to Compose Multiple LoRA Models (Tuna Han Salih Meral et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuna Han Salih Meral, Enis Simsar, Federico Tombari, Pinar Yanardag. (2024)<br><strong>CLoRA: A Contrastive Approach to Compose Multiple LoRA Models</strong><br><button class=copy-to-clipboard title="CLoRA: A Contrastive Approach to Compose Multiple LoRA Models" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19776v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19776v1.pdf filename=2403.19776v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-Rank Adaptations (LoRAs) have emerged as a powerful and popular technique in the field of image generation, offering a highly effective way to adapt and refine pre-trained deep learning models for specific tasks without the need for comprehensive retraining. By employing pre-trained LoRA models, such as those representing a specific cat and a particular dog, the objective is to generate an image that faithfully embodies both animals as defined by the LoRAs. However, the task of seamlessly blending multiple concept LoRAs to capture a variety of concepts in one image proves to be a significant challenge. Common approaches often fall short, primarily because the attention mechanisms within different LoRA models overlap, leading to scenarios where one concept may be completely ignored (e.g., omitting the dog) or where concepts are incorrectly combined (e.g., producing an image of two cats instead of one cat and one dog). To overcome these issues, CLoRA addresses them by updating the attention maps of multiple LoRA models and leveraging them to create semantic masks that facilitate the fusion of latent representations. Our method enables the creation of composite images that truly reflect the characteristics of each LoRA, successfully merging multiple concepts or styles. Our comprehensive evaluations, both qualitative and quantitative, demonstrate that our approach outperforms existing methodologies, marking a significant advancement in the field of image generation with LoRAs. Furthermore, we share our source code, <b>benchmark</b> dataset, and trained LoRA models to promote further research on this topic.</p></p class="citation"></blockquote><h3 id=7782--139271-xscale-nvs-cross-scale-novel-view-synthesis-with-hash-featurized-manifold-guangyu-wang-et-al-2024>(77/82 | 139/271) XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized Manifold (Guangyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyu Wang, Jinzhi Zhang, Fan Wang, Ruqi Huang, Lu Fang. (2024)<br><strong>XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized Manifold</strong><br><button class=copy-to-clipboard title="XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized Manifold" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19517v1.pdf filename=2403.19517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose XScale-NVS for high-fidelity cross-scale novel view synthesis of real-world large-scale scenes. Existing representations based on explicit surface suffer from discretization resolution or UV distortion, while implicit volumetric representations lack scalability for large scenes due to the dispersed weight distribution and surface ambiguity. In light of the above challenges, we introduce hash featurized manifold, a novel hash-based featurization coupled with a deferred neural rendering framework. This approach fully unlocks the expressivity of the representation by explicitly concentrating the hash entries on the 2D manifold, thus effectively representing highly detailed contents independent of the discretization resolution. We also introduce a novel dataset, namely GigaNVS, to <b>benchmark</b> cross-scale, high-resolution novel view synthesis of realworld large-scale scenes. Our method significantly outperforms competing baselines on various real-world scenes, yielding an average LPIPS that is 40% lower than prior state-of-the-art on the challenging GigaNVS <b>benchmark.</b> Please see our project page at: xscalenvs.github.io.</p></p class="citation"></blockquote><h3 id=7882--140271-towards-temporally-consistent-referring-video-object-segmentation-bo-miao-et-al-2024>(78/82 | 140/271) Towards Temporally Consistent Referring Video Object Segmentation (Bo Miao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Miao, Mohammed Bennamoun, Yongsheng Gao, Mubarak Shah, Ajmal Mian. (2024)<br><strong>Towards Temporally Consistent Referring Video Object Segmentation</strong><br><button class=copy-to-clipboard title="Towards Temporally Consistent Referring Video Object Segmentation" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19407v1.pdf filename=2403.19407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Referring Video Object Segmentation (R-VOS) methods face challenges in maintaining consistent object segmentation due to temporal context variability and the presence of other visually similar objects. We propose an end-to-end R-VOS paradigm that explicitly models temporal instance consistency alongside the referring segmentation. Specifically, we introduce a novel hybrid memory that facilitates inter-frame collaboration for robust spatio-temporal matching and propagation. Features of frames with automatically generated high-quality reference masks are propagated to segment the remaining frames based on multi-granularity association to achieve temporally consistent R-VOS. Furthermore, we propose a new Mask Consistency Score (MCS) metric to evaluate the temporal consistency of video segmentation. Extensive experiments demonstrate that our approach enhances temporal consistency by a significant margin, leading to top-ranked performance on popular R-VOS <b>benchmarks,</b> i.e., Ref-YouTube-VOS (67.1%) and Ref-DAVIS17 (65.6%).</p></p class="citation"></blockquote><h3 id=7982--141271-rtracker-recoverable-tracking-via-pn-tree-structured-memory-yuqing-huang-et-al-2024>(79/82 | 141/271) RTracker: Recoverable Tracking via PN Tree Structured Memory (Yuqing Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqing Huang, Xin Li, Zikun Zhou, Yaowei Wang, Zhenyu He, Ming-Hsuan Yang. (2024)<br><strong>RTracker: Recoverable Tracking via PN Tree Structured Memory</strong><br><button class=copy-to-clipboard title="RTracker: Recoverable Tracking via PN Tree Structured Memory" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19242v1.pdf filename=2403.19242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing tracking methods mainly focus on learning better target representation or developing more robust prediction models to improve tracking performance. While tracking performance has significantly improved, the target loss issue occurs frequently due to tracking failures, complete occlusion, or out-of-view situations. However, considerably less attention is paid to the self-recovery issue of tracking methods, which is crucial for practical applications. To this end, we propose a recoverable tracking framework, RTracker, that uses a tree-structured memory to dynamically associate a tracker and a detector to enable self-recovery ability. Specifically, we propose a Positive-Negative Tree-structured memory to chronologically store and maintain positive and negative target samples. Upon the PN tree memory, we develop corresponding walking rules for determining the state of the target and define a set of control flows to unite the tracker and the detector in different tracking scenarios. Our core idea is to use the support samples of positive and negative target categories to establish a relative distance-based criterion for a reliable assessment of target loss. The favorable performance in comparison against the state-of-the-art methods on numerous challenging <b>benchmarks</b> demonstrates the effectiveness of the proposed algorithm.</p></p class="citation"></blockquote><h3 id=8082--142271-az-nas-assembling-zero-cost-proxies-for-network-architecture-search-junghyup-lee-et-al-2024>(80/82 | 142/271) AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search (Junghyup Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junghyup Lee, Bumsub Ham. (2024)<br><strong>AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search</strong><br><button class=copy-to-clipboard title="AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19232v1.pdf filename=2403.19232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training-free network architecture search (NAS) aims to discover high-performing networks with zero-cost proxies, capturing network characteristics related to the final performance. However, network rankings estimated by previous training-free NAS methods have shown weak correlations with the performance. To address this issue, we propose AZ-NAS, a novel approach that leverages the ensemble of various zero-cost proxies to enhance the correlation between a predicted ranking of networks and the ground truth substantially in terms of the performance. To achieve this, we introduce four novel zero-cost proxies that are complementary to each other, analyzing distinct traits of architectures in the views of expressivity, progressivity, trainability, and complexity. The proxy scores can be obtained simultaneously within a single forward and backward pass, making an overall NAS process highly efficient. In order to integrate the rankings predicted by our proxies effectively, we introduce a non-linear ranking aggregation method that highlights the networks highly-ranked consistently across all the proxies. Experimental results conclusively demonstrate the efficacy and efficiency of AZ-NAS, outperforming state-of-the-art methods on standard <b>benchmarks,</b> all while maintaining a reasonable runtime cost.</p></p class="citation"></blockquote><h3 id=8182--143271-learning-multiple-representations-with-inconsistency-guided-detail-regularization-for-mask-guided-matting-weihao-jiang-et-al-2024>(81/82 | 143/271) Learning Multiple Representations with Inconsistency-Guided Detail Regularization for Mask-Guided Matting (Weihao Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weihao Jiang, Zhaozhi Xie, Yuxiang Lu, Longjie Qi, Jingyong Cai, Hiroyuki Uchiyama, Bin Chen, Yue Ding, Hongtao Lu. (2024)<br><strong>Learning Multiple Representations with Inconsistency-Guided Detail Regularization for Mask-Guided Matting</strong><br><button class=copy-to-clipboard title="Learning Multiple Representations with Inconsistency-Guided Detail Regularization for Mask-Guided Matting" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19213v1.pdf filename=2403.19213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mask-guided matting networks have achieved significant improvements and have shown great potential in practical applications in recent years. However, simply learning matting representation from synthetic and lack-of-real-world-diversity matting data, these approaches tend to overfit low-level details in wrong regions, lack generalization to objects with complex structures and real-world scenes such as shadows, as well as suffer from interference of background lines or textures. To address these challenges, in this paper, we propose a novel auxiliary learning framework for mask-guided matting models, incorporating three auxiliary tasks: semantic segmentation, edge detection, and background line detection besides matting, to learn different and effective representations from different types of data and annotations. Our framework and model introduce the following key aspects: (1) to learn real-world adaptive semantic representation for objects with diverse and complex structures under real-world scenes, we introduce extra semantic segmentation and edge detection tasks on more diverse real-world data with segmentation annotations; (2) to avoid overfitting on low-level details, we propose a module to utilize the inconsistency between learned segmentation and matting representations to regularize detail refinement; (3) we propose a novel background line detection task into our auxiliary learning framework, to suppress interference of background lines or textures. In addition, we propose a high-quality matting <b>benchmark,</b> Plant-Mat, to evaluate matting methods on complex structures. Extensively quantitative and qualitative results show that our approach outperforms state-of-the-art mask-guided methods.</p></p class="citation"></blockquote><h3 id=8282--144271-graphad-interaction-scene-graph-for-end-to-end-autonomous-driving-yunpeng-zhang-et-al-2024>(82/82 | 144/271) GraphAD: Interaction Scene Graph for End-to-end Autonomous Driving (Yunpeng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunpeng Zhang, Deheng Qian, Ding Li, Yifeng Pan, Yong Chen, Zhenbao Liang, Zhiyao Zhang, Shurui Zhang, Hongxu Li, Maolei Fu, Yun Ye, Zhujin Liang, Yi Shan, Dalong Du. (2024)<br><strong>GraphAD: Interaction Scene Graph for End-to-end Autonomous Driving</strong><br><button class=copy-to-clipboard title="GraphAD: Interaction Scene Graph for End-to-end Autonomous Driving" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19098v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19098v1.pdf filename=2403.19098v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modeling complicated interactions among the ego-vehicle, road agents, and map elements has been a crucial part for safety-critical autonomous driving. Previous works on end-to-end autonomous driving rely on the attention mechanism for handling heterogeneous interactions, which fails to capture the geometric priors and is also computationally intensive. In this paper, we propose the Interaction Scene <b>Graph</b> (ISG) as a unified method to model the interactions among the ego-vehicle, road agents, and map elements. With the representation of the ISG, the driving agents aggregate essential information from the most influential elements, including the road agents with potential collisions and the map elements to follow. Since a mass of unnecessary interactions are omitted, the more efficient scene-graph-based framework is able to focus on indispensable connections and leads to better performance. We evaluate the proposed method for end-to-end autonomous driving on the nuScenes dataset. Compared with strong baselines, our method significantly outperforms in the full-stack driving tasks, including perception, prediction, and planning. Code will be released at <a href=https://github.com/zhangyp15/GraphAD>https://github.com/zhangyp15/GraphAD</a>.</p></p class="citation"></blockquote><h2 id=cslg-33>cs.LG (33)</h2><h3 id=133--145271-dual-personalizing-adapter-for-federated-foundation-models-yiyuan-yang-et-al-2024>(1/33 | 145/271) Dual-Personalizing Adapter for Federated Foundation Models (Yiyuan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyuan Yang, Guodong Long, Tao Shen, Jing Jiang, Michael Blumenstein. (2024)<br><strong>Dual-Personalizing Adapter for Federated Foundation Models</strong><br><button class=copy-to-clipboard title="Dual-Personalizing Adapter for Federated Foundation Models" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 73<br>Keywords: Benchmarking, Distribution Shift, Distribution Shift, Federated Learning, Fine-tuning, Fine-tuning, Foundation Model, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19211v1.pdf filename=2403.19211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>foundation</b> <b>models,</b> particularly <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> have demonstrated an impressive ability to adapt to various tasks by <b>fine-tuning</b> <b>large</b> <b>amounts</b> <b>of</b> instruction data. Notably, <b>federated</b> <b>foundation</b> <b>models</b> emerge as a privacy preservation method to <b>fine-tune</b> models collaboratively under <b>federated</b> <b>learning</b> (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to <b>federated</b> <b>foundation</b> <b>models</b> for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time <b>distribution</b> <b>shifts</b> in real-world applications. Therefore, to bridge this gap, we propose a new setting, termed test-time personalization, which not only concentrates on the targeted local task but also extends to other tasks that exhibit test-time <b>distribution</b> <b>shifts.</b> To address challenges in this new setting, we explore a simple yet effective solution to learn a comprehensive <b>foundation</b> <b>model.</b> Specifically, a dual-personalizing adapter architecture (FedDPA) is proposed, comprising a global adapter and a local adapter for addressing test-time <b>distribution</b> <b>shifts</b> and personalization, respectively. Additionally, we introduce an instance-wise dynamic weighting mechanism to optimize the balance between the global and local adapters, enhancing overall performance. The effectiveness of the proposed method has been evaluated on <b>benchmark</b> datasets across different NLP tasks.</p></p class="citation"></blockquote><h3 id=233--146271-sine-activated-low-rank-matrices-for-parameter-efficient-learning-yiping-ji-et-al-2024>(2/33 | 146/271) Sine Activated Low-Rank Matrices for Parameter Efficient Learning (Yiping Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon Lucey. (2024)<br><strong>Sine Activated Low-Rank Matrices for Parameter Efficient Learning</strong><br><button class=copy-to-clipboard title="Sine Activated Low-Rank Matrices for Parameter Efficient Learning" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs-NE, cs.LG<br>Keyword Score: 50<br>Keywords: Vision Transformer, Transformer, Large Language Model, Large Language Model, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19243v1.pdf filename=2403.19243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition&rsquo;s rank, thereby enhancing model accuracy. Our method proves to be an adaptable enhancement for existing low-rank models, as evidenced by its successful application in <b>Vision</b> <b>Transformers</b> (ViT), <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> Neural Radiance Fields (NeRF), and 3D shape modeling. This demonstrates the wide-ranging potential and efficiency of our proposed technique.</p></p class="citation"></blockquote><h3 id=333--147271-exploiting-individual-graph-structures-to-enhance-ecological-momentary-assessment-ema-forecasting-mandani-ntekouli-et-al-2024>(3/33 | 147/271) Exploiting Individual Graph Structures to Enhance Ecological Momentary Assessment (EMA) Forecasting (Mandani Ntekouli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mandani Ntekouli, Gerasimos Spanakis, Lourens Waldorp, Anne Roefs. (2024)<br><strong>Exploiting Individual Graph Structures to Enhance Ecological Momentary Assessment (EMA) Forecasting</strong><br><button class=copy-to-clipboard title="Exploiting Individual Graph Structures to Enhance Ecological Momentary Assessment (EMA) Forecasting" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, LSTM, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19442v1.pdf filename=2403.19442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving field of psychopathology, the accurate assessment and forecasting of data derived from Ecological Momentary Assessment (EMA) is crucial. EMA offers contextually-rich psychopathological measurements over time, that practically lead to Multivariate Time Series <b>(MTS)</b> data. Thus, many challenges arise in analysis from the temporal complexities inherent in emotional, behavioral, and contextual EMA data as well as their inter-dependencies. To address both of these aspects, this research investigates the performance of Recurrent and Temporal <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs).</b> Overall, <b>GNNs,</b> by incorporating additional information from <b>graphs</b> <b>reflecting</b> <b>the</b> inner relationships between the variables, notably enhance the results by decreasing the Mean Squared Error (MSE) to 0.84 compared to the baseline <b>LSTM</b> model at 1.02. Therefore, the effect of constructing <b>graphs</b> <b>with</b> <b>different</b> characteristics on <b>GNN</b> performance is also explored. Additionally, <b>GNN-learned</b> <b>graphs,</b> <b>which</b> <b>are</b> dynamically refined during the training process, were evaluated. Using such <b>graphs</b> <b>showed</b> <b>a</b> similarly good performance. Thus, <b>graph</b> <b>learning</b> <b>proved</b> also promising for other <b>GNN</b> methods, potentially refining the pre-defined <b>graphs.</b></p></p class="citation"></blockquote><h3 id=433--148271-mpxgat-an-attention-based-deep-learning-model-for-multiplex-graphs-embedding-marco-bongiovanni-et-al-2024>(4/33 | 148/271) MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs Embedding (Marco Bongiovanni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Bongiovanni, Luca Gallo, Roberto Grasso, Alfredo Pulvirenti. (2024)<br><strong>MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs Embedding</strong><br><button class=copy-to-clipboard title="MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs Embedding" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DM, cs-LG, cs-SI, cs.LG<br>Keyword Score: 41<br>Keywords: Graph Attention Networks, Graph Attention Networks, Graph, Graph Embedding, Benchmarking, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19246v1.pdf filename=2403.19246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>representation</b> <b>learning</b> has rapidly emerged as a pivotal field of study. Despite its growing popularity, the majority of research has been confined to embedding single-layer <b>graphs,</b> <b>which</b> <b>fall</b> short in representing complex systems with multifaceted relationships. To bridge this gap, we introduce MPXGAT, an innovative attention-based deep learning model tailored to multiplex <b>graph</b> <b>embedding.</b> <b>Leveraging</b> the robustness of <b>Graph</b> <b>Attention</b> <b>Networks</b> <b>(GATs),</b> MPXGAT captures the structure of multiplex networks by harnessing both intra-layer and inter-layer connections. This exploitation facilitates accurate link prediction within and across the network&rsquo;s multiple layers. Our comprehensive experimental evaluation, conducted on various <b>benchmark</b> datasets, confirms that MPXGAT consistently outperforms state-of-the-art competing algorithms.</p></p class="citation"></blockquote><h3 id=533--149271-model-stock-all-we-need-is-just-a-few-fine-tuned-models-dong-hwan-jang-et-al-2024>(5/33 | 149/271) Model Stock: All we need is just a few fine-tuned models (Dong-Hwan Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dong-Hwan Jang, Sangdoo Yun, Dongyoon Han. (2024)<br><strong>Model Stock: All we need is just a few fine-tuned models</strong><br><button class=copy-to-clipboard title="Model Stock: All we need is just a few fine-tuned models" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19522v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19522v1.pdf filename=2403.19522v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces an efficient <b>fine-tuning</b> method for large pre-trained models, offering strong in-distribution (ID) and <b>out-of-distribution</b> (OOD) performance. Breaking away from traditional practices that need a multitude of <b>fine-tuned</b> models for averaging, our approach employs significantly fewer models to achieve final weights yet yield superior accuracy. Drawing from key insights in the weight space of <b>fine-tuned</b> weights, we uncover a strong link between the performance and proximity to the center of weight space. Based on this, we introduce a method that approximates a center-close weight using only two <b>fine-tuned</b> models, applicable during or after training. Our innovative layer-wise weight averaging technique surpasses state-of-the-art model methods such as Model Soup, utilizing only two <b>fine-tuned</b> models. This strategy can be aptly coined Model Stock, highlighting its reliance on selecting a minimal number of models to draw a more optimized-averaged model. We demonstrate the efficacy of Model Stock with <b>fine-tuned</b> models based upon pre-trained CLIP architectures, achieving remarkable performance on both ID and OOD tasks on the standard <b>benchmarks,</b> all while barely bringing extra computational demands. Our code and pre-trained models are available at <a href=https://github.com/naver-ai/model-stock>https://github.com/naver-ai/model-stock</a>.</p></p class="citation"></blockquote><h3 id=633--150271-topological-cycle-graph-attention-network-for-brain-functional-connectivity-jinghan-huang-et-al-2024>(6/33 | 150/271) Topological Cycle Graph Attention Network for Brain Functional Connectivity (Jinghan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinghan Huang, Nanguang Chen, Anqi Qiu. (2024)<br><strong>Topological Cycle Graph Attention Network for Brain Functional Connectivity</strong><br><button class=copy-to-clipboard title="Topological Cycle Graph Attention Network for Brain Functional Connectivity" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-NC<br>Keyword Score: 33<br>Keywords: Graph, Convolution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19149v1.pdf filename=2403.19149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study, we introduce a novel Topological Cycle <b>Graph</b> Attention Network (CycGAT), designed to delineate a functional backbone within brain functional <b>graph&ndash;key</b> pathways essential for signal transmissio&ndash;from non-essential, redundant connections that form cycles around this core structure. We first introduce a cycle incidence matrix that establishes an independent cycle basis within a <b>graph,</b> mapping its relationship with edges. We propose a cycle <b>graph</b> <b>convolution</b> that leverages a cycle adjacency matrix, derived from the cycle incidence matrix, to specifically filter edge signals in a domain of cycles. Additionally, we strengthen the representation power of the cycle <b>graph</b> <b>convolution</b> by adding an attention mechanism, which is further augmented by the introduction of edge positional encodings in cycles, to enhance the topological awareness of CycGAT. We demonstrate CycGAT&rsquo;s localization through <b>simulation</b> and its efficacy on an ABCD study&rsquo;s fMRI data (n=8765), comparing it with baseline models. CycGAT outperforms these models, identifying a functional backbone with significantly fewer cycles, crucial for understanding neural circuits related to general intelligence. Our code will be released once accepted.</p></p class="citation"></blockquote><h3 id=733--151271-tiny-graph-neural-networks-for-radio-resource-management-ahmad-ghasemi-et-al-2024>(7/33 | 151/271) Tiny Graph Neural Networks for Radio Resource Management (Ahmad Ghasemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmad Ghasemi, Hossein Pishro-Nik. (2024)<br><strong>Tiny Graph Neural Networks for Radio Resource Management</strong><br><button class=copy-to-clipboard title="Tiny Graph Neural Networks for Radio Resource Management" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NI, cs.LG, eess-SP<br>Keyword Score: 33<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19143v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19143v1.pdf filename=2403.19143v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The surge in demand for efficient radio resource management has necessitated the development of sophisticated yet compact neural network architectures. In this paper, we introduce a novel approach to <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> tailored for radio resource management by presenting a new architecture: the Low Rank Message Passing <b>Graph</b> <b>Neural</b> <b>Network</b> (LR-MPGNN). The cornerstone of LR-MPGNN is the implementation of a low-rank approximation technique that substitutes the conventional linear layers with their low-rank counterparts. This innovative design significantly reduces the model size and the number of parameters. We evaluate the performance of the proposed LR-MPGNN model based on several key metrics: model size, number of parameters, weighted sum rate of the communication system, and the distribution of eigenvalues of weight matrices. Our extensive evaluations demonstrate that the LR-MPGNN model achieves a sixtyfold decrease in model size, and the number of model parameters can be reduced by up to 98%. Performance-wise, the LR-MPGNN demonstrates robustness with a marginal 2% reduction in the best-case scenario in the normalized weighted sum rate compared to the original MPGNN model. Additionally, the distribution of eigenvalues of the weight matrices in the LR-MPGNN model is more uniform and spans a wider range, suggesting a strategic redistribution of weights.</p></p class="citation"></blockquote><h3 id=833--152271-the-new-agronomists-language-models-are-experts-in-crop-management-jing-wu-et-al-2024>(8/33 | 152/271) The New Agronomists: Language Models are Experts in Crop Management (Jing Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Wu, Zhixin Lai, Suiyao Chen, Ran Tao, Pan Zhao, Naira Hovakimyan. (2024)<br><strong>The New Agronomists: Language Models are Experts in Crop Management</strong><br><button class=copy-to-clipboard title="The New Agronomists: Language Models are Experts in Crop Management" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19839v1.pdf filename=2403.19839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crop management plays a crucial role in determining crop yield, economic profitability, and environmental sustainability. Despite the availability of management guidelines, optimizing these practices remains a complex and multifaceted challenge. In response, previous studies have explored using <b>reinforcement</b> <b>learning</b> with crop simulators, typically employing simple neural-network-based <b>reinforcement</b> <b>learning</b> (RL) agents. Building on this foundation, this paper introduces a more advanced intelligent crop management system. This system uniquely combines RL, a language model (LM), and crop <b>simulations</b> facilitated by the Decision Support System for Agrotechnology Transfer (DSSAT). We utilize deep RL, specifically a deep Q-network, to train management policies that process numerous state variables from the simulator as observations. A novel aspect of our approach is the conversion of these state variables into more informative language, facilitating the language model&rsquo;s capacity to understand states and explore optimal management practices. The empirical results reveal that the LM exhibits superior learning capabilities. Through <b>simulation</b> experiments with maize crops in Florida (US) and Zaragoza (Spain), the LM not only achieves state-of-the-art performance under various evaluation metrics but also demonstrates a remarkable improvement of over 49% in economic profit, coupled with reduced environmental impact when compared to baseline methods. Our code is available at \url{https://github.com/jingwu6/LM_AG}.</p></p class="citation"></blockquote><h3 id=933--153271-concept-based-analysis-of-neural-networks-via-vision-language-models-ravi-mangal-et-al-2024>(9/33 | 153/271) Concept-based Analysis of Neural Networks via Vision-Language Models (Ravi Mangal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ravi Mangal, Nina Narodytska, Divya Gopinath, Boyue Caroline Hu, Anirban Roy, Susmit Jha, Corina Pasareanu. (2024)<br><strong>Concept-based Analysis of Neural Networks via Vision-Language Models</strong><br><button class=copy-to-clipboard title="Concept-based Analysis of Neural Networks via Vision-Language Models" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs-LO, cs.LG<br>Keyword Score: 26<br>Keywords: Foundation Model, Multi-modal, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19837v1.pdf filename=2403.19837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging <b>multimodal,</b> <b>vision-language,</b> <b>foundation</b> <b>models</b> (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}<em>{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}</em>{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our techniques on a ResNet-based classifier trained on the RIVAL-10 dataset leveraging CLIP as the <b>multimodal</b> model.</p></p class="citation"></blockquote><h3 id=1033--154271-a-review-of-graph-neural-networks-in-epidemic-modeling-zewen-liu-et-al-2024>(10/33 | 154/271) A Review of Graph Neural Networks in Epidemic Modeling (Zewen Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zewen Liu, Guancheng Wan, B. Aditya Prakash, Max S. Y. Lau, Wei Jin. (2024)<br><strong>A Review of Graph Neural Networks in Epidemic Modeling</strong><br><button class=copy-to-clipboard title="A Review of Graph Neural Networks in Epidemic Modeling" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG, physics-soc-ph, q-bio-PE<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19852v1.pdf filename=2403.19852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of <b>GNNs</b> in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into \textit{Neural Models} and \textit{Hybrid Models}. Following this, we perform an exhaustive and systematic examination of the methodologies, encompassing both the tasks and their technical details. Furthermore, we discuss the limitations of existing methods from diverse perspectives and systematically propose future research directions. This survey aims to bridge literature gaps and promote the progression of this promising field. We hope that it will facilitate synergies between the communities of <b>GNNs</b> and epidemiology, and contribute to their collective progress.</p></p class="citation"></blockquote><h3 id=1133--155271-gegenbauer-graph-neural-networks-for-time-varying-signal-reconstruction-jhon-a-castro-correa-et-al-2024>(11/33 | 155/271) Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction (Jhon A. Castro-Correa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jhon A. Castro-Correa, Jhony H. Giraldo, Mohsen Badiey, Fragkiskos D. Malliaros. (2024)<br><strong>Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction</strong><br><button class=copy-to-clipboard title="Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19800v1.pdf filename=2403.19800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstructing time-varying <b>graph</b> <b>signals</b> <b>(or</b> <b>graph</b> <b>time-series</b> <b>imputation)</b> is a critical problem in machine learning and signal processing with broad applications, ranging from missing data imputation in sensor networks to time-series forecasting. Accurately capturing the spatio-temporal information inherent in these signals is crucial for effectively addressing these tasks. However, existing approaches relying on smoothness assumptions of temporal differences and simple convex optimization techniques have inherent limitations. To address these challenges, we propose a novel approach that incorporates a learning module to enhance the accuracy of the downstream task. To this end, we introduce the Gegenbauer-based <b>graph</b> <b>convolutional</b> <b>(GegenConv)</b> operator, which is a generalization of the conventional Chebyshev <b>graph</b> <b>convolution</b> <b>by</b> leveraging the theory of Gegenbauer polynomials. By deviating from traditional convex problems, we expand the complexity of the model and offer a more accurate solution for recovering time-varying <b>graph</b> <b>signals.</b> <b>Building</b> upon GegenConv, we design the Gegenbauer-based time <b>Graph</b> <b>Neural</b> <b>Network</b> (GegenGNN) architecture, which adopts an encoder-decoder structure. Likewise, our approach also utilizes a dedicated loss function that incorporates a mean squared error component alongside Sobolev smoothness regularization. This combination enables GegenGNN to capture both the fidelity to ground truth and the underlying smoothness properties of the signals, enhancing the reconstruction performance. We conduct extensive experiments on real datasets to evaluate the effectiveness of our proposed approach. The experimental results demonstrate that GegenGNN outperforms state-of-the-art methods, showcasing its superior capability in recovering time-varying <b>graph</b> signals.</p></p class="citation"></blockquote><h3 id=1233--156271-grind-grid-interpolation-network-for-scattered-observations-andrzej-dulny-et-al-2024>(12/33 | 156/271) GrINd: Grid Interpolation Network for Scattered Observations (Andrzej Dulny et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrzej Dulny, Paul Heinisch, Andreas Hotho, Anna Krause. (2024)<br><strong>GrINd: Grid Interpolation Network for Scattered Observations</strong><br><button class=copy-to-clipboard title="GrINd: Grid Interpolation Network for Scattered Observations" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19570v1.pdf filename=2403.19570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting the evolution of spatiotemporal physical systems from sparse and scattered observational data poses a significant challenge in various scientific domains. Traditional methods rely on dense grid-structured data, limiting their applicability in scenarios with sparse observations. To address this challenge, we introduce GrINd (Grid Interpolation Network for Scattered Observations), a novel network architecture that leverages the high-performance of grid-based models by mapping scattered observations onto a high-resolution grid using a Fourier Interpolation Layer. In the high-resolution space, a NeuralPDE-class model predicts the system&rsquo;s state at future timepoints using differentiable ODE solvers and fully <b>convolutional</b> <b>neural</b> <b>networks</b> parametrizing the system&rsquo;s dynamics. We empirically evaluate GrINd on the DynaBench <b>benchmark</b> dataset, comprising six different physical systems observed at scattered locations, demonstrating its state-of-the-art performance compared to existing models. GrINd offers a promising approach for forecasting physical systems from sparse, scattered observational data, extending the applicability of deep learning methods to real-world scenarios with limited data availability.</p></p class="citation"></blockquote><h3 id=1333--157271-graph-neural-networks-for-treatment-effect-prediction-george-panagopoulos-et-al-2024>(13/33 | 157/271) Graph Neural Networks for Treatment Effect Prediction (George Panagopoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>George Panagopoulos, Daniele Malitesta, Fragkiskos D. Malliaros, Jun Pang. (2024)<br><strong>Graph Neural Networks for Treatment Effect Prediction</strong><br><button class=copy-to-clipboard title="Graph Neural Networks for Treatment Effect Prediction" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ME<br>Keyword Score: 23<br>Keywords: Message-Passing, Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19289v1.pdf filename=2403.19289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a <b>graph</b> <b>neural</b> <b>network</b> to diminish the required training set size, relying on <b>graphs</b> <b>that</b> <b>are</b> common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying <b>message-passing</b> layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of the training set in settings with extremely low experimental budget. The framework is flexible since each step can be used separately with other models or policies. The experiments on real large-scale networks indicate a clear advantage of our methodology over the state of the art, which in many cases performs close to random underlining the need for models that can generalize with limited labeled samples to reduce experimental risks.</p></p class="citation"></blockquote><h3 id=1433--158271-denetdm-debiasing-by-network-depth-modulation-silpa-vadakkeeveetil-sreelatha-et-al-2024>(14/33 | 158/271) DeNetDM: Debiasing by Network Depth Modulation (Silpa Vadakkeeveetil Sreelatha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Silpa Vadakkeeveetil Sreelatha, Adarsh Kappiyath, Anjan Dutta. (2024)<br><strong>DeNetDM: Debiasing by Network Depth Modulation</strong><br><button class=copy-to-clipboard title="DeNetDM: Debiasing by Network Depth Modulation" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19863v1.pdf filename=2403.19863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When neural networks are trained on biased datasets, they tend to inadvertently learn spurious correlations, leading to challenges in achieving strong generalization and robustness. Current approaches to address such biases typically involve utilizing bias annotations, reweighting based on pseudo-bias labels, or enhancing diversity within bias-conflicting data points through augmentation techniques. We introduce DeNetDM, a novel debiasing method based on the observation that shallow neural networks prioritize learning core attributes, while deeper ones emphasize biases when tasked with acquiring distinct information. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then <b>distill</b> knowledge to produce the target debiased model. Extensive experiments and analyses demonstrate that our approach outperforms current debiasing techniques, achieving a notable improvement of around 5% in three datasets, encompassing both synthetic and real-world data. Remarkably, DeNetDM accomplishes this without requiring annotations pertaining to bias labels or bias types, while still delivering performance on par with <b>supervised</b> counterparts. Furthermore, our approach effectively harnesses the diversity of bias-conflicting points within the data, surpassing previous methods and obviating the need for explicit augmentation-based methods to enhance the diversity of such bias-conflicting points. The source code will be available upon acceptance.</p></p class="citation"></blockquote><h3 id=1533--159271-genetic-quantization-aware-approximation-for-non-linear-operations-in-transformers-pingcheng-dong-et-al-2024>(15/33 | 159/271) Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers (Pingcheng Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pingcheng Dong, Yonghao Tan, Dong Zhang, Tianwei Ni, Xuejiao Liu, Yu Liu, Peng Luo, Luhong Liang, Shih-Yang Liu, Xijie Huang, Huaiyu Zhu, Yun Pan, Fengwei An, Kwang-Ting Cheng. (2024)<br><strong>Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers</strong><br><button class=copy-to-clipboard title="Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AR, cs-LG, cs-NE, cs.LG<br>Keyword Score: 20<br>Keywords: Quantization, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19591v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19591v2.pdf filename=2403.19591v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Non-linear functions are prevalent in <b>Transformers</b> and their lightweight variants, incurring substantial and frequently underestimated hardware costs. Previous state-of-the-art works optimize these operations by piece-wise linear approximation and store the parameters in look-up tables (LUT), but most of them require unfriendly high-precision arithmetics such as FP/INT 32 and lack consideration of integer-only INT <b>quantization.</b> This paper proposed a genetic LUT-Approximation algorithm namely GQA-LUT that can automatically determine the parameters with <b>quantization</b> awareness. The results demonstrate that GQA-LUT achieves negligible degradation on the challenging semantic segmentation task for both vanilla and linear <b>Transformer</b> models. Besides, proposed GQA-LUT enables the employment of INT8-based LUT-Approximation that achieves an area savings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the high-precision FP/INT 32 alternatives. Code is available at https:// github.com/PingchengDong/GQA-LUT.</p></p class="citation"></blockquote><h3 id=1633--160271-fairness-in-ranking-robustness-through-randomization-without-the-protected-attribute-andrii-kliachkin-et-al-2024>(16/33 | 160/271) Fairness in Ranking: Robustness through Randomization without the Protected Attribute (Andrii Kliachkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrii Kliachkin, Eleni Psaroudaki, Jakub Marecek, Dimitris Fotakis. (2024)<br><strong>Fairness in Ranking: Robustness through Randomization without the Protected Attribute</strong><br><button class=copy-to-clipboard title="Fairness in Ranking: Robustness through Randomization without the Protected Attribute" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fairness, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19419v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19419v1.pdf filename=2403.19419v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There has been great interest in <b>fairness</b> in machine learning, especially in relation to classification problems. In ranking-related problems, such as in online advertising, <b>recommender</b> <b>systems,</b> and HR automation, much work on <b>fairness</b> remains to be done. Two complications arise: first, the protected attribute may not be available in many applications. Second, there are multiple measures of <b>fairness</b> of rankings, and optimization-based methods utilizing a single measure of <b>fairness</b> of rankings may produce rankings that are unfair with respect to other measures. In this work, we propose a randomized method for post-processing rankings, which do not require the availability of the protected attribute. In an extensive numerical study, we show the robustness of our methods with respect to P-Fairness and effectiveness with respect to Normalized Discounted Cumulative Gain (NDCG) from the baseline ranking, improving on previously proposed methods.</p></p class="citation"></blockquote><h3 id=1733--161271-artificial-intelligence-ai-based-prediction-of-mortality-for-covid-19-patients-mahbubunnabi-tamala-et-al-2024>(17/33 | 161/271) Artificial Intelligence (AI) Based Prediction of Mortality, for COVID-19 Patients (Mahbubunnabi Tamala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahbubunnabi Tamala, Mohammad Marufur Rahmanb, Maryam Alhasimc, Mobarak Al Mulhimd, Mohamed Derichee. (2024)<br><strong>Artificial Intelligence (AI) Based Prediction of Mortality, for COVID-19 Patients</strong><br><button class=copy-to-clipboard title="Artificial Intelligence (AI) Based Prediction of Mortality, for COVID-19 Patients" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: LSTM, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19355v1.pdf filename=2403.19355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For severely affected COVID-19 patients, it is crucial to identify high-risk patients and predict survival and need for intensive care (ICU). Most of the proposed models are not well reported making them less reproducible and prone to high risk of bias particularly in presence of imbalance data/class. In this study, the performances of nine machine and deep learning algorithms in combination with two widely used feature selection methods were investigated to predict last status representing mortality, ICU requirement, and ventilation days. Fivefold cross-validation was used for training and validation purposes. To minimize bias, the training and testing sets were split maintaining similar distributions. Only 10 out of 122 features were found to be useful in prediction modelling with Acute kidney injury during hospitalization feature being the most important one. The algorithms performances depend on feature numbers and data pre-processing techniques. <b>LSTM</b> performs the best in predicting last status and ICU requirement with 90%, 92%, 86% and 95% accuracy, sensitivity, specificity, and AUC respectively. DNN performs the best in predicting Ventilation days with 88% accuracy. Considering all the factors and limitations including absence of exact time point of clinical onset, <b>LSTM</b> with carefully selected features can accurately predict last status and ICU requirement. DNN performs the best in predicting Ventilation days. Appropriate machine learning algorithm with carefully selected features and balance data can accurately predict mortality, ICU requirement and ventilation support. Such model can be very useful in emergency and pandemic where <b>prompt</b> and precise</p></p class="citation"></blockquote><h3 id=1833--162271-inferring-latent-temporal-sparse-coordination-graph-for-multi-agent-reinforcement-learning-wei-duan-et-al-2024>(18/33 | 162/271) Inferring Latent Temporal Sparse Coordination Graph for Multi-Agent Reinforcement Learning (Wei Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Duan, Jie Lu, Junyu Xuan. (2024)<br><strong>Inferring Latent Temporal Sparse Coordination Graph for Multi-Agent Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Inferring Latent Temporal Sparse Coordination Graph for Multi-Agent Reinforcement Learning" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-MA, cs.LG<br>Keyword Score: 16<br>Keywords: Graph, Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19253v1.pdf filename=2403.19253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective agent coordination is crucial in cooperative Multi-Agent <b>Reinforcement</b> <b>Learning</b> (MARL). While agent cooperation can be represented by <b>graph</b> structures, prevailing <b>graph</b> learning methods in MARL are limited. They rely solely on one-step observations, neglecting crucial historical experiences, leading to deficient <b>graphs</b> that foster redundant or detrimental information exchanges. Additionally, high computational demands for action-pair calculations in dense <b>graphs</b> impede scalability. To address these challenges, we propose inferring a Latent Temporal Sparse Coordination <b>Graph</b> (LTS-CG) for MARL. The LTS-CG leverages agents&rsquo; historical observations to calculate an agent-pair probability matrix, where a sparse <b>graph</b> is sampled from and used for knowledge exchange between agents, thereby simultaneously capturing agent dependencies and relation uncertainty. The computational complexity of this procedure is only related to the number of agents. This <b>graph</b> learning process is further augmented by two innovative characteristics: Predict-Future, which enables agents to foresee upcoming observations, and Infer-Present, ensuring a thorough grasp of the environmental context from limited data. These features allow LTS-CG to construct temporal <b>graphs</b> from historical and real-time information, promoting knowledge exchange during policy learning and effective collaboration. <b>Graph</b> learning and agent training occur simultaneously in an end-to-end manner. Our demonstrated results on the StarCraft II <b>benchmark</b> underscore LTS-CG&rsquo;s superior performance.</p></p class="citation"></blockquote><h3 id=1933--163271-transparent-and-clinically-interpretable-ai-for-lung-cancer-detection-in-chest-x-rays-amy-rafferty-et-al-2024>(19/33 | 163/271) Transparent and Clinically Interpretable AI for Lung Cancer Detection in Chest X-Rays (Amy Rafferty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amy Rafferty, Rishi Ramaesh, Ajitha Rajan. (2024)<br><strong>Transparent and Clinically Interpretable AI for Lung Cancer Detection in Chest X-Rays</strong><br><button class=copy-to-clipboard title="Transparent and Clinically Interpretable AI for Lung Cancer Detection in Chest X-Rays" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Black Box, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19444v1.pdf filename=2403.19444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapidly advancing field of Explainable Artificial Intelligence (XAI) aims to tackle the issue of trust regarding the use of complex <b>black-box</b> <b>deep</b> learning models in real-world applications. Existing post-hoc XAI techniques have recently been shown to have poor performance on medical data, producing unreliable explanations which are infeasible for clinical use. To address this, we propose an ante-hoc approach based on concept bottleneck models which introduces for the first time clinical concepts into the classification pipeline, allowing the user valuable insight into the decision-making process. On a large public dataset of chest X-rays and associated medical reports, we focus on the binary classification task of lung cancer detection. Our approach yields improved classification performance in lung cancer detection when compared to baseline deep learning models (F1 > 0.9), while also generating clinically relevant and more reliable explanations than existing techniques. We evaluate our approach against post-hoc image XAI techniques LIME and SHAP, as well as CXR-LLaVA, a recent textual XAI tool which operates in the context of <b>question</b> <b>answering</b> on chest X-rays.</p></p class="citation"></blockquote><h3 id=2033--164271-sparse-feature-circuits-discovering-and-editing-interpretable-causal-graphs-in-language-models-samuel-marks-et-al-2024>(20/33 | 164/271) Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models (Samuel Marks et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, Aaron Mueller. (2024)<br><strong>Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models</strong><br><button class=copy-to-clipboard title="Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19647v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19647v1.pdf filename=2403.19647v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely <b>unsupervised</b> and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.</p></p class="citation"></blockquote><h3 id=2133--165271-tabular-learning-encoding-for-entity-and-context-embeddings-fredy-reusser-2024>(21/33 | 165/271) Tabular Learning: Encoding for Entity and Context Embeddings (Fredy Reusser, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fredy Reusser. (2024)<br><strong>Tabular Learning: Encoding for Entity and Context Embeddings</strong><br><button class=copy-to-clipboard title="Tabular Learning: Encoding for Entity and Context Embeddings" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CE, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19405v1.pdf filename=2403.19405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Examining the effect of different encoding techniques on entity and context embeddings, the goal of this work is to challenge commonly used Ordinal encoding for tabular learning. Applying different preprocessing methods and network architectures over several datasets resulted in a <b>benchmark</b> on how the encoders influence the learning outcome of the networks. By keeping the test, validation and training data consistent, results have shown that ordinal encoding is not the most suited encoder for categorical data in terms of preprocessing the data and thereafter, classifying the target variable correctly. A better outcome was achieved, encoding the features based on string similarities by computing a similarity matrix as input for the network. This is the case for both, entity and context embeddings, where the <b>transformer</b> architecture showed improved performance for Ordinal and Similarity encoding with regard to multi-label classification tasks.</p></p class="citation"></blockquote><h3 id=2233--166271-medbn-robust-test-time-adaptation-against-malicious-test-samples-hyejin-park-et-al-2024>(22/33 | 166/271) MedBN: Robust Test-Time Adaptation against Malicious Test Samples (Hyejin Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyejin Park, Jeongyeon Hwang, Sunung Mun, Sangdon Park, Jungseul Ok. (2024)<br><strong>MedBN: Robust Test-Time Adaptation against Malicious Test Samples</strong><br><button class=copy-to-clipboard title="MedBN: Robust Test-Time Adaptation against Malicious Test Samples" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19326v1.pdf filename=2403.19326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Test-time adaptation (TTA) has emerged as a promising solution to address performance decay due to unforeseen <b>distribution</b> <b>shifts</b> between training and test data. While recent TTA methods excel in adapting to test data variations, such adaptability exposes a model to vulnerability against malicious examples, an aspect that has received limited attention. Previous studies have uncovered security vulnerabilities within TTA even when a small proportion of the test batch is maliciously manipulated. In response to the emerging threat, we propose median batch normalization (MedBN), leveraging the robustness of the median for statistics estimation within the batch normalization layer during test-time inference. Our method is algorithm-agnostic, thus allowing seamless integration with existing TTA frameworks. Our experimental results on <b>benchmark</b> datasets, including CIFAR10-C, CIFAR100-C and ImageNet-C, consistently demonstrate that MedBN outperforms existing approaches in maintaining robust performance across different attack scenarios, encompassing both instant and cumulative attacks. Through extensive experiments, we show that our approach sustains the performance even in the absence of attacks, achieving a practical balance between robustness and performance.</p></p class="citation"></blockquote><h3 id=2333--167271-biased-over-the-air-federated-learning-under-wireless-heterogeneity-muhammad-faraz-ul-abrar-et-al-2024>(23/33 | 167/271) Biased Over-the-Air Federated Learning under Wireless Heterogeneity (Muhammad Faraz Ul Abrar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Faraz Ul Abrar, Nicolò Michelusi. (2024)<br><strong>Biased Over-the-Air Federated Learning under Wireless Heterogeneity</strong><br><button class=copy-to-clipboard title="Biased Over-the-Air Federated Learning under Wireless Heterogeneity" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19849v1.pdf filename=2403.19849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, Over-the-Air (OTA) computation has emerged as a promising <b>federated</b> <b>learning</b> (FL) paradigm that leverages the waveform superposition properties of the wireless channel to realize fast model updates. Prior work focused on the OTA device <code>pre-scaler" design under \emph{homogeneous} wireless conditions, in which devices experience the same average path loss, resulting in zero-bias solutions. Yet, zero-bias designs are limited by the device with the worst average path loss and hence may perform poorly in \emph{heterogeneous} wireless settings. In this scenario, there may be a benefit in designing \emph{biased} solutions, in exchange for a lower variance in the model updates. To optimize this trade-off, we study the design of OTA device pre-scalers by focusing on the OTA-FL convergence. We derive an upper bound on the model </code>optimality error", which explicitly captures the effect of bias and variance in terms of the choice of the pre-scalers. Based on this bound, we identify two solutions of interest: minimum noise variance, and minimum noise variance zero-bias solutions. Numerical evaluations show that using OTA device pre-scalers that minimize the variance of FL updates, while allowing a small bias, can provide high gains over existing schemes.</p></p class="citation"></blockquote><h3 id=2433--168271-evaluating-explanatory-capabilities-of-machine-learning-models-in-medical-diagnostics-a-human-in-the-loop-approach-josé-bobes-bascarán-et-al-2024>(24/33 | 168/271) Evaluating Explanatory Capabilities of Machine Learning Models in Medical Diagnostics: A Human-in-the-Loop Approach (José Bobes-Bascarán et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>José Bobes-Bascarán, Eduardo Mosqueira-Rey, Ángel Fernández-Leal, Elena Hernández-Pereira, David Alonso-Ríos, Vicente Moret-Bonillo, Israel Figueirido-Arnoso, Yolanda Vidal-Ínsua. (2024)<br><strong>Evaluating Explanatory Capabilities of Machine Learning Models in Medical Diagnostics: A Human-in-the-Loop Approach</strong><br><button class=copy-to-clipboard title="Evaluating Explanatory Capabilities of Machine Learning Models in Medical Diagnostics: A Human-in-the-Loop Approach" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2, cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19820v1.pdf filename=2403.19820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a comprehensive study on the evaluation of explanatory capabilities of machine learning models, with a focus on Decision Trees, Random Forest and XGBoost models using a pancreatic cancer dataset. We use <b>Human-in-the-Loop</b> related techniques and medical guidelines as a source of domain knowledge to establish the importance of the different features that are relevant to establish a pancreatic cancer treatment. These features are not only used as a dimensionality reduction approach for the machine learning models, but also as way to evaluate the explainability capabilities of the different models using agnostic and non-agnostic explainability techniques. To facilitate interpretation of explanatory results, we propose the use of similarity measures such as the Weighted Jaccard Similarity coefficient. The goal is to not only select the best performing model but also the one that can best explain its conclusions and aligns with human domain knowledge.</p></p class="citation"></blockquote><h3 id=2533--169271-feature-based-echo-state-networks-a-step-towards-interpretability-and-minimalism-in-reservoir-computer-debdipta-goswami-2024>(25/33 | 169/271) Feature-Based Echo-State Networks: A Step Towards Interpretability and Minimalism in Reservoir Computer (Debdipta Goswami, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Debdipta Goswami. (2024)<br><strong>Feature-Based Echo-State Networks: A Step Towards Interpretability and Minimalism in Reservoir Computer</strong><br><button class=copy-to-clipboard title="Feature-Based Echo-State Networks: A Step Towards Interpretability and Minimalism in Reservoir Computer" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19806v1.pdf filename=2403.19806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a novel and interpretable <b>recurrent</b> <b>neural-network</b> <b>structure</b> using the echo-state network (ESN) paradigm for time-series prediction. While the traditional ESNs perform well for dynamical systems prediction, it needs a large dynamic reservoir with increased computational complexity. It also lacks interpretability to discern contributions from different input combinations to the output. Here, a systematic reservoir architecture is developed using smaller parallel reservoirs driven by different input combinations, known as features, and then they are nonlinearly combined to produce the output. The resultant feature-based ESN (Feat-ESN) outperforms the traditional single-reservoir ESN with less reservoir nodes. The predictive capability of the proposed architecture is demonstrated on three systems: two synthetic datasets from chaotic dynamical systems and a set of real-time traffic data.</p></p class="citation"></blockquote><h3 id=2633--170271-swarm-characteristics-classification-using-neural-networks-donald-w-peltier-iii-et-al-2024>(26/33 | 170/271) Swarm Characteristics Classification Using Neural Networks (Donald W. Peltier III et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donald W. Peltier III, Isaac Kaminer, Abram Clark, Marko Orescanin. (2024)<br><strong>Swarm Characteristics Classification Using Neural Networks</strong><br><button class=copy-to-clipboard title="Swarm Characteristics Classification Using Neural Networks" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19572v1.pdf filename=2403.19572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the characteristics of swarming autonomous agents is critical for defense and security applications. This article presents a study on using <b>supervised</b> neural network time series classification (NN TSC) to predict key attributes and tactics of swarming autonomous agents for military contexts. Specifically, NN TSC is applied to infer two binary attributes - communication and proportional navigation - which combine to define four mutually exclusive swarm tactics. We identify a gap in literature on using NNs for swarm classification and demonstrate the effectiveness of NN TSC in rapidly deducing intelligence about attacking swarms to inform counter-maneuvers. Through simulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms of observation window requirements, noise robustness, and scalability to swarm size. Key findings show NNs can predict swarm behaviors with 97% accuracy using short observation windows of 20 time steps, while also demonstrating graceful degradation down to 80% accuracy under 50% noise, as well as excellent scalability to swarm sizes from 10 to 100 agents. These capabilities are promising for real-time decision-making support in defense scenarios by rapidly inferring insights about swarm behavior.</p></p class="citation"></blockquote><h3 id=2733--171271-tensor-network-constrained-kernel-machines-as-gaussian-processes-frederiek-wesel-et-al-2024>(27/33 | 171/271) Tensor Network-Constrained Kernel Machines as Gaussian Processes (Frederiek Wesel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frederiek Wesel, Kim Batselier. (2024)<br><strong>Tensor Network-Constrained Kernel Machines as Gaussian Processes</strong><br><button class=copy-to-clipboard title="Tensor Network-Constrained Kernel Machines as Gaussian Processes" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-5-0, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19500v1.pdf filename=2403.19500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tensor Networks (TNs) have recently been used to speed up kernel machines by constraining the model weights, yielding exponential computational and storage savings. In this paper we prove that the outputs of Canonical Polyadic Decomposition (CPD) and Tensor Train (TT)-constrained kernel machines recover a <b>Gaussian</b> <b>Process</b> (GP), which we fully characterize, when placing i.i.d. priors over their parameters. We analyze the convergence of both CPD and TT-constrained models, and show how TT yields models exhibiting more GP behavior compared to CPD, for the same number of model parameters. We empirically observe this behavior in two numerical experiments where we respectively analyze the convergence to the GP and the performance at prediction. We thereby establish a connection between TN-constrained kernel machines and GPs.</p></p class="citation"></blockquote><h3 id=2833--172271-client-supervised-federated-learning-towards-one-model-for-all-personalization-peng-yan-et-al-2024>(28/33 | 172/271) Client-supervised Federated Learning: Towards One-model-for-all Personalization (Peng Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Yan, Guodong Long. (2024)<br><strong>Client-supervised Federated Learning: Towards One-model-for-all Personalization</strong><br><button class=copy-to-clipboard title="Client-supervised Federated Learning: Towards One-model-for-all Personalization" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19499v1.pdf filename=2403.19499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personalized <b>Federated</b> <b>Learning</b> (PerFL) is a new machine learning paradigm that delivers personalized models for diverse clients under <b>federated</b> <b>learning</b> settings. Most PerFL methods require extra learning processes on a client to adapt a globally shared model to the client-specific personalized model using its own local data. However, the model adaptation process in PerFL is still an open challenge in the stage of model deployment and test time. This work tackles the challenge by proposing a novel <b>federated</b> <b>learning</b> framework to learn only one robust global model to achieve competitive performance to those personalized models on unseen/test clients in the FL system. Specifically, we design a new Client-Supervised <b>Federated</b> <b>Learning</b> (FedCS) to unravel clients&rsquo; bias on instances&rsquo; latent representations so that the global model can learn both client-specific and client-agnostic knowledge. Experimental study shows that the FedCS can learn a robust FL global model for the changing data distributions of unseen/test clients. The FedCS&rsquo;s global model can be directly deployed to the test clients while achieving comparable performance to other personalized FL methods that require model adaptation.</p></p class="citation"></blockquote><h3 id=2933--173271-offline-imitation-learning-from-multiple-baselines-with-applications-to-compiler-optimization-teodor-v-marinov-et-al-2024>(29/33 | 173/271) Offline Imitation Learning from Multiple Baselines with Applications to Compiler Optimization (Teodor V. Marinov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teodor V. Marinov, Alekh Agarwal, Mircea Trofin. (2024)<br><strong>Offline Imitation Learning from Multiple Baselines with Applications to Compiler Optimization</strong><br><button class=copy-to-clipboard title="Offline Imitation Learning from Multiple Baselines with Applications to Compiler Optimization" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-PL, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19462v1.pdf filename=2403.19462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work studies a <b>Reinforcement</b> <b>Learning</b> (RL) problem in which we are given a set of trajectories collected with K baseline policies. Each of these policies can be quite suboptimal in isolation, and have strong performance in complementary parts of the state space. The goal is to learn a policy which performs as well as the best combination of baselines on the entire state space. We propose a simple imitation learning based algorithm, show a sample complexity bound on its accuracy and prove that the the algorithm is minimax optimal by showing a matching lower bound. Further, we apply the algorithm in the setting of machine learning guided compiler optimization to learn policies for inlining programs with the objective of creating a small binary. We demonstrate that we can learn a policy that outperforms an initial policy learned via standard RL through a few iterations of our approach.</p></p class="citation"></blockquote><h3 id=3033--174271-an-interactive-human-machine-learning-interface-for-collecting-and-learning-from-complex-annotations-jonathan-erskine-et-al-2024>(30/33 | 174/271) An Interactive Human-Machine Learning Interface for Collecting and Learning from Complex Annotations (Jonathan Erskine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Erskine, Matt Clifford, Alexander Hepburn, Raúl Santos-Rodríguez. (2024)<br><strong>An Interactive Human-Machine Learning Interface for Collecting and Learning from Complex Annotations</strong><br><button class=copy-to-clipboard title="An Interactive Human-Machine Learning Interface for Collecting and Learning from Complex Annotations" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-HC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19339v1.pdf filename=2403.19339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human-Computer Interaction has been shown to lead to improvements in machine learning systems by boosting model performance, accelerating learning and building user confidence. In this work, we aim to alleviate the expectation that human annotators adapt to the constraints imposed by traditional labels by allowing for extra flexibility in the form that supervision information is collected. For this, we propose a human-machine learning interface for binary classification tasks which enables human annotators to utilise <b>counterfactual</b> examples to complement standard binary labels as annotations for a dataset. Finally we discuss the challenges in future extensions of this work.</p></p class="citation"></blockquote><h3 id=3133--175271-evaluating-fair-feature-selection-in-machine-learning-for-healthcare-md-rahat-shahriar-zawad-et-al-2024>(31/33 | 175/271) Evaluating Fair Feature Selection in Machine Learning for Healthcare (Md Rahat Shahriar Zawad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Rahat Shahriar Zawad, Peter Washington. (2024)<br><strong>Evaluating Fair Feature Selection in Machine Learning for Healthcare</strong><br><button class=copy-to-clipboard title="Evaluating Fair Feature Selection in Machine Learning for Healthcare" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19165v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19165v1.pdf filename=2403.19165v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the universal adoption of machine learning in healthcare, the potential for the automation of societal biases to further exacerbate health disparities poses a significant risk. We explore algorithmic <b>fairness</b> from the perspective of feature selection. Traditional feature selection methods identify features for better decision making by removing resource-intensive, correlated, or non-relevant features but overlook how these factors may differ across subgroups. To counter these issues, we evaluate a fair feature selection method that considers equal importance to all demographic groups. We jointly considered a <b>fairness</b> metric and an error metric within the feature selection process to ensure a balance between minimizing both bias and global classification error. We tested our approach on three publicly available healthcare datasets. On all three datasets, we observed improvements in <b>fairness</b> metrics coupled with a minimal degradation of balanced accuracy. Our approach addresses both distributive and procedural <b>fairness</b> within the fair machine learning context.</p></p class="citation"></blockquote><h3 id=3233--176271-towards-understanding-dual-bn-in-hybrid-adversarial-training-chenshuang-zhang-et-al-2024>(32/33 | 176/271) Towards Understanding Dual BN In Hybrid Adversarial Training (Chenshuang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenshuang Zhang, Chaoning Zhang, Kang Zhang, Axi Niu, Junmo Kim, In So Kweon. (2024)<br><strong>Towards Understanding Dual BN In Hybrid Adversarial Training</strong><br><button class=copy-to-clipboard title="Towards Understanding Dual BN In Hybrid Adversarial Training" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Adversarial Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19150v1.pdf filename=2403.19150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a growing concern about applying batch normalization (BN) in <b>adversarial</b> <b>training</b> (AT), especially when the model is trained on both <b>adversarial</b> <b>samples</b> and clean samples (termed Hybrid-AT). With the assumption that <b>adversarial</b> <b>and</b> clean samples are from two different domains, a common practice in prior works is to adopt Dual BN, where BN and BN are used for <b>adversarial</b> <b>and</b> clean branches, respectively. A popular belief for motivating Dual BN is that estimating normalization statistics of this mixture distribution is challenging and thus disentangling it for normalization achieves stronger robustness. In contrast to this belief, we reveal that disentangling statistics plays a less role than disentangling affine parameters in model training. This finding aligns with prior work (Rebuffi et al., 2023), and we build upon their research for further investigations. We demonstrate that the domain gap between <b>adversarial</b> <b>and</b> clean samples is not very large, which is counter-intuitive considering the significant influence of <b>adversarial</b> <b>perturbation</b> on the model accuracy. We further propose a two-task hypothesis which serves as the empirical foundation and a unified framework for Hybrid-AT improvement. We also investigate Dual BN in test-time and reveal that affine parameters characterize the robustness during inference. Overall, our work sheds new light on understanding the mechanism of Dual BN in Hybrid-AT and its underlying justification.</p></p class="citation"></blockquote><h3 id=3333--177271-mapl-model-agnostic-peer-to-peer-learning-sayak-mukherjee-et-al-2024>(33/33 | 177/271) MAPL: Model Agnostic Peer-to-peer Learning (Sayak Mukherjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayak Mukherjee, Andrea Simonetto, Hadi Jamali-Rad. (2024)<br><strong>MAPL: Model Agnostic Peer-to-peer Learning</strong><br><button class=copy-to-clipboard title="MAPL: Model Agnostic Peer-to-peer Learning" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19792v1.pdf filename=2403.19792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective collaboration among heterogeneous clients in a decentralized setting is a rather unexplored avenue in the literature. To structurally address this, we introduce Model Agnostic Peer-to-peer Learning (coined as MAPL) a novel approach to simultaneously learn heterogeneous personalized models as well as a collaboration <b>graph</b> through peer-to-peer communication among neighboring clients. MAPL is comprised of two main modules: (i) local-level Personalized Model Learning (PML), leveraging a combination of intra- and inter-client contrastive losses; (ii) network-wide decentralized Collaborative <b>Graph</b> Learning (CGL) dynamically refining collaboration weights in a privacy-preserving manner based on local task similarities. Our extensive experimentation demonstrates the efficacy of MAPL and its competitive (or, in most cases, superior) performance compared to its centralized model-agnostic counterparts, without relying on any central server. Our code is available and can be accessed here: <a href=https://github.com/SayakMukherjee/MAPL>https://github.com/SayakMukherjee/MAPL</a></p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=11--178271-alloybert-alloy-property-prediction-with-large-language-models-akshat-chaudhari-et-al-2024>(1/1 | 178/271) AlloyBERT: Alloy Property Prediction with Large Language Models (Akshat Chaudhari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshat Chaudhari, Chakradhar Guntuboina, Hongshuo Huang, Amir Barati Farimani. (2024)<br><strong>AlloyBERT: Alloy Property Prediction with Large Language Models</strong><br><button class=copy-to-clipboard title="AlloyBERT: Alloy Property Prediction with Large Language Models" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-LG<br>Keyword Score: 70<br>Keywords: Fine-tuning, Simulation, Simulator, RoBERTa, Transformer, Large Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19783v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19783v1.pdf filename=2403.19783v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pursuit of novel alloys tailored to specific requirements poses significant challenges for researchers in the field. This underscores the importance of developing predictive techniques for essential physical properties of alloys based on their chemical composition and processing parameters. This study introduces AlloyBERT, a <b>transformer</b> encoder-based model designed to predict properties such as elastic modulus and yield strength of alloys using textual inputs. Leveraging the pre-trained <b>RoBERTa</b> encoder model as its foundation, AlloyBERT employs <b>self-attention</b> mechanisms to establish meaningful relationships between words, enabling it to interpret human-readable input and predict target alloy properties. By combining a tokenizer trained on our textual data and a <b>RoBERTa</b> encoder pre-trained and <b>fine-tuned</b> for this specific task, we achieved a mean squared error (MSE) of 0.00015 on the Multi Principal Elemental Alloys (MPEA) data set and 0.00611 on the Refractory Alloy Yield Strength (RAYS) dataset. This surpasses the performance of shallow models, which achieved a best-case MSE of 0.00025 and 0.0076 on the MPEA and RAYS datasets respectively. Our results highlight the potential of language models in material science and establish a foundational framework for text-based prediction of alloy properties that does not rely on complex underlying representations, calculations, or <b>simulations.</b></p></p class="citation"></blockquote><h2 id=csro-15>cs.RO (15)</h2><h3 id=115--179271-keypoint-action-tokens-enable-in-context-imitation-learning-in-robotics-norman-di-palo-et-al-2024>(1/15 | 179/271) Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics (Norman Di Palo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Norman Di Palo, Edward Johns. (2024)<br><strong>Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics</strong><br><button class=copy-to-clipboard title="Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-NE, cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Few-shot, GPT, GPT-4, GPT-4 turbo, Transformer, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19578v1.pdf filename=2403.19578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that off-the-shelf text-based <b>Transformers,</b> with no additional training, can perform <b>few-shot</b> <b>in-context</b> visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator&rsquo;s behaviour. We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained <b>Transformer</b> <b>(GPT-4</b> <b>Turbo)</b> can ingest and generate, via a framework we call Keypoint Action Tokens (KAT). Despite being trained only on language, we show that these <b>Transformers</b> excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks. Rather than operating in the language domain as is typical, KAT leverages text-based <b>Transformers</b> to operate in the vision and action domains to learn general patterns in demonstration data for highly efficient imitation learning, indicating promising new avenues for repurposing natural language models for embodied tasks. Videos are available at <a href=https://www.robot-learning.uk/keypoint-action-tokens>https://www.robot-learning.uk/keypoint-action-tokens</a>.</p></p class="citation"></blockquote><h3 id=215--180271-learning-sampling-distribution-and-safety-filter-for-autonomous-driving-with-vq-vae-and-differentiable-optimization-simon-idoko-et-al-2024>(2/15 | 180/271) Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization (Simon Idoko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Idoko, Basant Sharma, Arun Kumar Singh. (2024)<br><strong>Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization</strong><br><button class=copy-to-clipboard title="Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 53<br>Keywords: Autoencoder, Multi-modal, Quantization, Self-supervised Learning, Self-supervised Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19461v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19461v1.pdf filename=2403.19461v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving. Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid). Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional <b>Variational</b> <b>Autoencoder</b> (CVAE). However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we re-imagine the distribution learning through vector <b>quantized</b> <b>variational</b> <b>autoencoder</b> (VQ-VAE), whose discrete latent-space is well equipped to capture <b>multi-modal</b> sampling distribution. The VQ-VAE is trained with demonstration data of optimal trajectories. We further propose a differentiable optimization based safety filter to minimally correct the VQVAE sampled trajectories to ensure collision avoidance. We use backpropagation through the optimization layers in a <b>self-supervised</b> <b>learning</b> set-up to learn good initialization and optimal parameters of the safety filter. We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds.</p></p class="citation"></blockquote><h3 id=315--181271-said-nerf-segmentation-aided-nerf-for-depth-completion-of-transparent-objects-avinash-ummadisingu-et-al-2024>(3/15 | 181/271) SAID-NeRF: Segmentation-AIDed NeRF for Depth Completion of Transparent Objects (Avinash Ummadisingu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Avinash Ummadisingu, Jongkeum Choi, Koki Yamane, Shimpei Masuda, Naoki Fukaya, Kuniyuki Takahashi. (2024)<br><strong>SAID-NeRF: Segmentation-AIDed NeRF for Depth Completion of Transparent Objects</strong><br><button class=copy-to-clipboard title="SAID-NeRF: Segmentation-AIDed NeRF for Depth Completion of Transparent Objects" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Foundation Model, Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19607v1.pdf filename=2403.19607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Acquiring accurate depth information of transparent objects using off-the-shelf RGB-D cameras is a well-known challenge in Computer Vision and Robotics. Depth estimation/completion methods are typically employed and trained on datasets with quality depth labels acquired from either <b>simulation,</b> additional sensors or specialized data collection setups and known 3d models. However, acquiring reliable depth information for datasets at scale is not straightforward, limiting training scalability and generalization. Neural Radiance Fields (NeRFs) are learning-free approaches and have demonstrated wide success in novel view synthesis and shape recovery. However, heuristics and controlled environments (lights, backgrounds, etc) are often required to accurately capture specular surfaces. In this paper, we propose using Visual <b>Foundation</b> <b>Models</b> (VFMs) for segmentation in a <b>zero-shot,</b> label-free way to guide the NeRF reconstruction process for these objects via the simultaneous reconstruction of semantic fields and extensions to increase robustness. Our proposed method Segmentation-AIDed NeRF (SAID-NeRF) shows significant performance on depth completion datasets for transparent objects and robotic grasping.</p></p class="citation"></blockquote><h3 id=415--182271-hierarchical-deep-learning-for-intention-estimation-of-teleoperation-manipulation-in-assembly-tasks-mingyu-cai-et-al-2024>(4/15 | 182/271) Hierarchical Deep Learning for Intention Estimation of Teleoperation Manipulation in Assembly Tasks (Mingyu Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyu Cai, Karankumar Patel, Soshi Iba, Songpo Li. (2024)<br><strong>Hierarchical Deep Learning for Intention Estimation of Teleoperation Manipulation in Assembly Tasks</strong><br><button class=copy-to-clipboard title="Hierarchical Deep Learning for Intention Estimation of Teleoperation Manipulation in Assembly Tasks" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19770v1.pdf filename=2403.19770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In human-robot collaboration, shared control presents an opportunity to teleoperate robotic manipulation to improve the efficiency of manufacturing and assembly processes. Robots are expected to assist in executing the user&rsquo;s intentions. To this end, robust and <b>prompt</b> intention estimation is needed, relying on behavioral observations. The framework presents an intention estimation technique at hierarchical levels i.e., low-level actions and high-level tasks, by incorporating multi-scale hierarchical information in neural networks. Technically, we employ hierarchical dependency loss to boost overall accuracy. Furthermore, we propose a multi-window method that assigns proper hierarchical prediction windows of input data. An analysis of the predictive power with various inputs demonstrates the predominance of the deep hierarchical model in the sense of prediction accuracy and early intention identification. We implement the algorithm on a virtual reality (VR) setup to teleoperate robotic hands in a <b>simulation</b> with various assembly tasks to show the effectiveness of online estimation.</p></p class="citation"></blockquote><h3 id=515--183271-human-compatible-driving-partners-through-data-regularized-self-play-reinforcement-learning-daphne-cornelisse-et-al-2024>(5/15 | 183/271) Human-compatible driving partners through data-regularized self-play reinforcement learning (Daphne Cornelisse et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daphne Cornelisse, Eugene Vinitsky. (2024)<br><strong>Human-compatible driving partners through data-regularized self-play reinforcement learning</strong><br><button class=copy-to-clipboard title="Human-compatible driving partners through data-regularized self-play reinforcement learning" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-MA, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19648v1.pdf filename=2403.19648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A central challenge for autonomous vehicles is coordinating with humans. Therefore, incorporating realistic human agents is essential for scalable training and evaluation of autonomous driving systems in <b>simulation.</b> <b>Simulation</b> agents are typically developed by imitating large-scale, high-quality datasets of human driving. However, pure imitation learning agents empirically have high collision rates when executed in a multi-agent closed-loop setting. To build agents that are realistic and effective in closed-loop settings, we propose Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are trained through self-play with a small penalty for deviating from a human reference policy. In contrast to prior work, our approach is RL-first and only uses 30 minutes of imperfect human demonstrations. We evaluate agents in a large set of multi-agent traffic scenes. Results show our HR-PPO agents are highly effective in achieving goals, with a success rate of 93%, an off-road rate of 3.5%, and a collision rate of 3%. At the same time, the agents drive in a human-like manner, as measured by their similarity to existing human driving logs. We also find that HR-PPO agents show considerable improvements on proxy measures for coordination with human driving, particularly in highly interactive scenarios. We open-source our code and trained agents at <a href=https://github.com/Emerge-Lab/nocturne_lab>https://github.com/Emerge-Lab/nocturne_lab</a> and provide demonstrations of agent behaviors at <a href=https://sites.google.com/view/driving-partners>https://sites.google.com/view/driving-partners</a>.</p></p class="citation"></blockquote><h3 id=615--184271-kinetostatic-analysis-for-6rus-parallel-continuum-robot-using-cosserat-rod-theory-vinayvivian-rodrigues-et-al-2024>(6/15 | 184/271) Kinetostatic Analysis for 6RUS Parallel Continuum Robot using Cosserat Rod Theory (Vinayvivian Rodrigues et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vinayvivian Rodrigues, Bingbin Yu, Christoph Stoeffler, Shivesh Kumar. (2024)<br><strong>Kinetostatic Analysis for 6RUS Parallel Continuum Robot using Cosserat Rod Theory</strong><br><button class=copy-to-clipboard title="Kinetostatic Analysis for 6RUS Parallel Continuum Robot using Cosserat Rod Theory" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19784v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19784v1.pdf filename=2403.19784v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parallel Continuum Robots (PCR) are closed-loop mechanisms but use elastic kinematic links connected in parallel between the end-effector (EE) and the base platform. PCRs are actuated primarily through large deflections of the interconnected elastic links unlike by rigid joints in rigid parallel mechanisms. In this paper, Cosserat rod theory-based forward and inverse kinetostatic models of 6RUS PCR are proposed. A set of <b>simulations</b> are performed to analyze the proposed PCR structure which includes maneuverability in 3-dimensional space through trajectory following, deformation effects due to the planar rotation of the EE platform, and axial stiffness evaluation at the EE.</p></p class="citation"></blockquote><h3 id=715--185271-lamarckian-inheritance-improves-robot-evolution-in-dynamic-environments-jie-luo-et-al-2024>(7/15 | 185/271) Lamarckian Inheritance Improves Robot Evolution in Dynamic Environments (Jie Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Luo, Karine Miras, Carlo Longhi, Oliver Weissl, Agoston E. Eiben. (2024)<br><strong>Lamarckian Inheritance Improves Robot Evolution in Dynamic Environments</strong><br><button class=copy-to-clipboard title="Lamarckian Inheritance Improves Robot Evolution in Dynamic Environments" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19545v1.pdf filename=2403.19545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study explores the integration of Lamarckian system into evolutionary robotics (ER), comparing it with the traditional Darwinian model across various environments. By adopting Lamarckian principles, where robots inherit learned traits, alongside Darwinian learning without inheritance, we investigate adaptation in dynamic settings. Our research, conducted in six distinct environmental setups, demonstrates that Lamarckian systems outperform Darwinian ones in adaptability and efficiency, particularly in challenging conditions. Our analysis highlights the critical role of the interplay between controller & morphological evolution and environment adaptation, with parent-offspring similarities and newborn &amp;survivors before and after learning providing insights into the effectiveness of trait inheritance. Our findings suggest Lamarckian principles could significantly advance autonomous system design, highlighting the potential for more adaptable and robust robotic solutions in complex, real-world applications. These theoretical insights were validated using real physical robots, bridging the gap between <b>simulation</b> and practical application.</p></p class="citation"></blockquote><h3 id=815--186271-riemann-near-real-time-se3-equivariant-robot-manipulation-without-point-cloud-segmentation-chongkai-gao-et-al-2024>(8/15 | 186/271) RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation (Chongkai Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chongkai Gao, Zhengrong Xue, Shuying Deng, Tianhai Liang, Siqi Yang, Lin Shao, Huazhe Xu. (2024)<br><strong>RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation</strong><br><button class=copy-to-clipboard title="RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19460v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19460v1.pdf filename=2403.19460v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present RiEMann, an end-to-end near Real-time SE(3)-Equivariant Robot Manipulation imitation learning framework from scene point cloud input. Compared to previous methods that rely on descriptor field matching, RiEMann directly predicts the target poses of objects for manipulation without any object segmentation. RiEMann learns a manipulation task from scratch with 5 to 10 demonstrations, generalizes to unseen SE(3) transformations and instances of target objects, resists visual interference of distracting objects, and follows the near real-time pose change of the target object. The scalable action space of RiEMann facilitates the addition of custom equivariant actions such as the direction of turning the faucet, which makes articulated object manipulation possible for RiEMann. In <b>simulation</b> and real-world 6-DOF robot manipulation experiments, we test RiEMann on 5 categories of manipulation tasks with a total of 25 variants and show that RiEMann outperforms baselines in both task success rates and SE(3) geodesic distance errors on predicted poses (reduced by 68.6%), and achieves a 5.4 frames per second (FPS) network inference speed. Code and video results are available at <a href=https://riemann-web.github.io/>https://riemann-web.github.io/</a>.</p></p class="citation"></blockquote><h3 id=915--187271-multi-agent-team-access-monitoring-environments-that-benefit-from-target-information-sharing-andrew-dudash-et-al-2024>(9/15 | 187/271) Multi-Agent Team Access Monitoring: Environments that Benefit from Target Information Sharing (Andrew Dudash et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Dudash, Scott James, Ryan Rubel. (2024)<br><strong>Multi-Agent Team Access Monitoring: Environments that Benefit from Target Information Sharing</strong><br><button class=copy-to-clipboard title="Multi-Agent Team Access Monitoring: Environments that Benefit from Target Information Sharing" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-MA, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19375v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19375v1.pdf filename=2403.19375v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic access monitoring of multiple target areas has applications including checkpoint enforcement, surveillance and containment of fire and flood hazards. Monitoring access for a single target region has been successfully modeled as a minimum-cut problem. We generalize this model to support multiple target areas using two approaches: iterating on individual targets and examining the collections of targets holistically. Through <b>simulation</b> we measure the performance of each approach on different scenarios.</p></p class="citation"></blockquote><h3 id=1015--188271-rail-robot-affordance-imagination-with-large-language-models-ceng-zhang-et-al-2024>(10/15 | 188/271) RAIL: Robot Affordance Imagination with Large Language Models (Ceng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ceng Zhang, Xin Meng, Dongchen Qi, Gregory S. Chirikjian. (2024)<br><strong>RAIL: Robot Affordance Imagination with Large Language Models</strong><br><button class=copy-to-clipboard title="RAIL: Robot Affordance Imagination with Large Language Models" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19369v1.pdf filename=2403.19369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces an automatic affordance <b>reasoning</b> paradigm tailored to minimal semantic inputs, addressing the critical challenges of classifying and manipulating unseen classes of objects in household settings. Inspired by human cognitive processes, our method integrates generative language models and physics-based simulators to foster analytical thinking and creative imagination of novel affordances. Structured with a tripartite framework consisting of analysis, imagination, and evaluation, our system &ldquo;analyzes&rdquo; the requested affordance names into interaction-based definitions, &ldquo;imagines&rdquo; the virtual scenarios, and &ldquo;evaluates&rdquo; the object affordance. If an object is recognized as possessing the requested affordance, our method also predicts the optimal pose for such functionality, and how a potential user can interact with it. Tuned on only a few synthetic examples across 3 affordance classes, our pipeline achieves a very high success rate on affordance classification and functional pose prediction of 8 classes of novel objects, outperforming learning-based baselines. Validation through real robot manipulating experiments demonstrates the practical applicability of the imagined user interaction, showcasing the system&rsquo;s ability to independently conceptualize unseen affordances and interact with new objects and scenarios in everyday settings.</p></p class="citation"></blockquote><h3 id=1115--189271-adaptive-preload-control-of-cable-driven-parallel-robots-for-handling-task-thomas-reichenbach-et-al-2024>(11/15 | 189/271) Adaptive Preload Control of Cable-Driven Parallel Robots for Handling Task (Thomas Reichenbach et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Reichenbach, Johannes Clar, Andreas Pott, Alexander Verl. (2024)<br><strong>Adaptive Preload Control of Cable-Driven Parallel Robots for Handling Task</strong><br><button class=copy-to-clipboard title="Adaptive Preload Control of Cable-Driven Parallel Robots for Handling Task" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19293v1.pdf filename=2403.19293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a method for dynamic adjustment of cable preloads based on the actuation redundancy of \acp{CDPR}, which allows increasing or decreasing the platform stiffness depending on task requirements. This is achieved by computing preload parameters with an extended nullspace formulation of the kinematics. The method facilitates the operator&rsquo;s ability to specify a defined preload within the operation space. The algorithms are implemented in a real-time environment, allowing for the use of optimization in hybrid position-force control. To validate the effectiveness of this approach, a <b>simulation</b> study is performed, and the obtained results are compared to existing methods. Furthermore, the method is investigated experimentally and compared with the conventional position-controlled operation of a cable robot. The results demonstrate the feasibility of adaptively adjusting cable preloads during platform motion and manipulation of additional objects.</p></p class="citation"></blockquote><h3 id=1215--190271-bundledslam-an-accurate-visual-slam-system-using-multiple-cameras-han-song-et-al-2024>(12/15 | 190/271) BundledSLAM: An Accurate Visual SLAM System Using Multiple Cameras (Han Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Song, Cong Liu, Huafeng Dai. (2024)<br><strong>BundledSLAM: An Accurate Visual SLAM System Using Multiple Cameras</strong><br><button class=copy-to-clipboard title="BundledSLAM: An Accurate Visual SLAM System Using Multiple Cameras" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19886v1.pdf filename=2403.19886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-camera SLAM systems offer a plethora of advantages, primarily <b>stemming</b> from their capacity to amalgamate information from a broader field of view, thereby resulting in heightened robustness and improved localization accuracy. In this research, we present a significant extension and refinement of the state-of-the-art stereo SLAM system, known as ORB-SLAM2, with the objective of attaining even higher precision.To accomplish this objective, we commence by mapping measurements from all cameras onto a virtual camera termed BundledFrame. This virtual camera is meticulously engineered to seamlessly adapt to multi-camera configurations, facilitating the effective fusion of data captured from multiple cameras. Additionally, we harness extrinsic parameters in the bundle adjustment (BA) process to achieve precise trajectory estimation.Furthermore, we conduct an extensive analysis of the role of bundle adjustment (BA) in the context of multi-camera scenarios, delving into its impact on tracking, local mapping, and global optimization. Our experimental evaluation entails comprehensive comparisons between ground truth data and the state-of-the-art SLAM system. To rigorously assess the system&rsquo;s performance, we utilize the EuRoC datasets. The consistent results of our evaluations demonstrate the superior accuracy of our system in comparison to existing approaches.</p></p class="citation"></blockquote><h3 id=1315--191271-learning-a-formally-verified-control-barrier-function-in-stochastic-environment-manan-tayal-et-al-2024>(13/15 | 191/271) Learning a Formally Verified Control Barrier Function in Stochastic Environment (Manan Tayal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manan Tayal, Hongchao Zhang, Pushpak Jagtap, Andrew Clark, Shishir Kolathaya. (2024)<br><strong>Learning a Formally Verified Control Barrier Function in Stochastic Environment</strong><br><button class=copy-to-clipboard title="Learning a Formally Verified Control Barrier Function in Stochastic Environment" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19332v1.pdf filename=2403.19332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safety is a fundamental requirement of control systems. Control Barrier Functions (CBFs) are proposed to ensure the safety of the control system by constructing safety filters or synthesizing control inputs. However, the safety guarantee and performance of safe controllers rely on the construction of valid CBFs. Inspired by universal approximatability, CBFs are represented by neural networks, known as neural CBFs (NCBFs). This paper presents an algorithm for synthesizing formally verified <b>continuous-time</b> <b>neural</b> Control Barrier Functions in stochastic environments in a single step. The proposed training process ensures efficacy across the entire state space with only a finite number of data points by constructing a sample-based learning framework for Stochastic Neural CBFs (SNCBFs). Our methodology eliminates the need for post hoc verification by enforcing Lipschitz bounds on the neural network, its Jacobian, and Hessian terms. We demonstrate the effectiveness of our approach through case studies on the inverted pendulum system and obstacle avoidance in autonomous driving, showcasing larger safe regions compared to baseline methods.</p></p class="citation"></blockquote><h3 id=1415--192271-safety-critical-planning-and-control-for-dynamic-obstacle-avoidance-using-control-barrier-functions-shuo-liu-et-al-2024>(14/15 | 192/271) Safety-Critical Planning and Control for Dynamic Obstacle Avoidance Using Control Barrier Functions (Shuo Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuo Liu, Yihui Mao. (2024)<br><strong>Safety-Critical Planning and Control for Dynamic Obstacle Avoidance Using Control Barrier Functions</strong><br><button class=copy-to-clipboard title="Safety-Critical Planning and Control for Dynamic Obstacle Avoidance Using Control Barrier Functions" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO, math-OC<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19122v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19122v1.pdf filename=2403.19122v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic obstacle avoidance is a challenging topic for optimal control and optimization-based trajectory planning problems, especially when in a tight environment. Many existing works use control barrier functions (CBFs) to enforce safety constraints within control systems. Inside these works, CBFs are usually formulated under model predictive control (MPC) framework to anticipate future states and make informed decisions, or integrated with path planning algorithms as a safety enhancement tool. However, these approaches usually require knowledge of the obstacle boundary equations or have very slow computational efficiency. In this paper, we propose a novel framework to the iterative MPC with <b>discrete-time</b> <b>CBFs</b> (DCBFs) to generate a collision-free trajectory. The DCBFs are obtained from convex polyhedra generated in sequential grid maps, without the need to know the boundary equations of obstacles. Additionally, a path planning algorithm is incorporated into this framework to ensure the global optimality of the generated trajectory. We demonstrate through numerical examples that our framework enables a unicycle robot to safely and efficiently navigate through tight and dynamically changing environments, tackling both convex and nonconvex obstacles with remarkable computing efficiency and reliability in control and trajectory generation.</p></p class="citation"></blockquote><h3 id=1515--193271-mac-maximizing-algebraic-connectivity-for-graph-sparsification-kevin-doherty-et-al-2024>(15/15 | 193/271) MAC: Maximizing Algebraic Connectivity for Graph Sparsification (Kevin Doherty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin Doherty, Alan Papalia, Yewei Huang, David Rosen, Brendan Englot, John Leonard. (2024)<br><strong>MAC: Maximizing Algebraic Connectivity for Graph Sparsification</strong><br><button class=copy-to-clipboard title="MAC: Maximizing Algebraic Connectivity for Graph Sparsification" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19879v1.pdf filename=2403.19879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Simultaneous localization and mapping (SLAM) is a critical capability in autonomous navigation, but memory and computational limits make long-term application of common SLAM techniques impractical; a robot must be able to determine what information should be retained and what can safely be forgotten. In <b>graph-based</b> SLAM, the number of edges (measurements) in a pose <b>graph</b> determines both the memory requirements of storing a robot&rsquo;s observations and the computational expense of algorithms deployed for performing state estimation using those observations, both of which can grow unbounded during long-term navigation. Motivated by these challenges, we propose a new general purpose approach to sparsify <b>graphs</b> in a manner that maximizes algebraic connectivity, a key spectral property of <b>graphs</b> which has been shown to control the estimation error of pose <b>graph</b> SLAM solutions. Our algorithm, MAC (for maximizing algebraic connectivity), is simple and computationally inexpensive, and admits formal post hoc performance guarantees on the quality of the solution that it provides. In application to the problem of pose-graph SLAM, we show on several <b>benchmark</b> datasets that our approach quickly produces high-quality sparsification results which retain the connectivity of the <b>graph</b> and, in turn, the quality of corresponding SLAM solutions.</p></p class="citation"></blockquote><h2 id=quant-ph-2>quant-ph (2)</h2><h3 id=12--194271-optimizing-quantum-convolutional-neural-network-architectures-for-arbitrary-data-dimension-changwon-lee-et-al-2024>(1/2 | 194/271) Optimizing Quantum Convolutional Neural Network Architectures for Arbitrary Data Dimension (Changwon Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changwon Lee, Israel F. Araujo, Dongha Kim, Junghan Lee, Siheon Park, Ju-Young Ryu, Daniel K. Park. (2024)<br><strong>Optimizing Quantum Convolutional Neural Network Architectures for Arbitrary Data Dimension</strong><br><button class=copy-to-clipboard title="Optimizing Quantum Convolutional Neural Network Architectures for Arbitrary Data Dimension" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 53<br>Keywords: MNIST, Benchmarking, Convolution, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19099v1.pdf filename=2403.19099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum <b>convolutional</b> <b>neural</b> <b>networks</b> (QCNNs) represent a promising approach in quantum machine learning, paving new directions for both quantum and classical data analysis. This approach is particularly attractive due to the absence of the barren plateau problem, a fundamental challenge in training quantum neural networks (QNNs), and its feasibility. However, a limitation arises when applying QCNNs to classical data. The network architecture is most natural when the number of input qubits is a power of two, as this number is reduced by a factor of two in each pooling layer. The number of input qubits determines the dimensions (i.e. the number of features) of the input data that can be processed, restricting the applicability of QCNN algorithms to real-world data. To address this issue, we propose a QCNN architecture capable of handling arbitrary input data dimensions while optimizing the allocation of quantum resources such as ancillary qubits and quantum gates. This optimization is not only important for minimizing computational resources, but also essential in noisy intermediate-scale quantum (NISQ) computing, as the size of the quantum circuits that can be executed reliably is limited. Through numerical <b>simulations,</b> we <b>benchmarked</b> the classification performance of various QCNN architectures when handling arbitrary input data dimensions on the <b>MNIST</b> and Breast Cancer datasets. The results validate that the proposed QCNN architecture achieves excellent classification performance while utilizing a minimal resource overhead, providing an optimal solution when reliable quantum computation is constrained by noise and imperfections.</p></p class="citation"></blockquote><h3 id=22--195271-natural-language-ai-and-quantum-computing-in-2024-research-ingredients-and-directions-in-qnlp-dominic-widdows-et-al-2024>(2/2 | 195/271) Natural Language, AI, and Quantum Computing in 2024: Research Ingredients and Directions in QNLP (Dominic Widdows et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dominic Widdows, Willie Aboumrad, Dohun Kim, Sayonee Ray, Jonathan Mei. (2024)<br><strong>Natural Language, AI, and Quantum Computing in 2024: Research Ingredients and Directions in QNLP</strong><br><button class=copy-to-clipboard title="Natural Language, AI, and Quantum Computing in 2024: Research Ingredients and Directions in QNLP" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-AI, cs-CL, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19758v1.pdf filename=2403.19758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language processing is at the heart of current developments in artificial intelligence, and quantum computers are becoming available at the same time. This has led to great interest in quantum natural language processing, and several early proposals and experiments. This paper surveys the state of this area, showing how NLP-related techniques including <b>word</b> <b>embeddings,</b> sequential models, attention, and grammatical parsing have been used in quantum language processing. We introduce a new quantum design for the basic task of text encoding (representing a string of characters in memory), which has not been addressed in detail before. As well as motivating new technologies, quantum theory has made key contributions to the challenging questions of &lsquo;What is uncertainty?&rsquo; and &lsquo;What is intelligence?&rsquo; As these questions are taking on fresh urgency with artificial systems, the paper also considers some of the ways facts are conceptualized and presented in language. In particular, we argue that the problem of &lsquo;hallucinations&rsquo; arises through a basic misunderstanding: language expresses any number of plausible hypotheses, only a few of which become actual, a distinction that is ignored in classical mechanics, but present (albeit confusing) in quantum mechanics.</p></p class="citation"></blockquote><h2 id=csai-5>cs.AI (5)</h2><h3 id=15--196271-llmsense-harnessing-llms-for-high-level-reasoning-over-spatiotemporal-sensor-traces-xiaomin-ouyang-et-al-2024>(1/5 | 196/271) LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces (Xiaomin Ouyang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaomin Ouyang, Mani Srivastava. (2024)<br><strong>LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces</strong><br><button class=copy-to-clipboard title="LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 50<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19857v1.pdf filename=2403.19857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most studies on machine learning in sensing systems focus on low-level perception tasks that process raw sensory data within a short time window. However, many practical applications, such as human routine modeling and occupancy tracking, require high-level <b>reasoning</b> abilities to comprehend concepts and make inferences based on long-term sensor traces. Existing machine learning-based approaches for handling such complex tasks struggle to generalize due to the limited training samples and the high dimensionality of sensor traces, necessitating the integration of human knowledge for designing first-principle models or logic <b>reasoning</b> methods. We pose a fundamental question: Can we harness the <b>reasoning</b> capabilities and world knowledge of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to recognize complex events from long-term spatiotemporal sensor traces? To answer this question, we design an effective <b>prompting</b> framework for <b>LLMs</b> on high-level <b>reasoning</b> tasks, which can handle traces from the raw sensor data as well as the low-level perception results. We also design two strategies to enhance performance with long sensor traces, including <b>summarization</b> before <b>reasoning</b> and selective inclusion of historical traces. Our framework can be implemented in an edge-cloud setup, running small <b>LLMs</b> on the edge for data <b>summarization</b> and performing high-level <b>reasoning</b> on the cloud for privacy preservation. The results show that LLMSense can achieve over 80% accuracy on two high-level <b>reasoning</b> tasks such as dementia diagnosis with behavior traces and occupancy tracking with environmental sensor traces. This paper provides a few insights and guidelines for leveraging <b>LLM</b> for high-level <b>reasoning</b> on sensor traces and highlights several directions for future work.</p></p class="citation"></blockquote><h3 id=25--197271-bespoke-large-language-models-for-digital-triage-assistance-in-mental-health-care-niall-taylor-et-al-2024>(2/5 | 197/271) Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care (Niall Taylor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niall Taylor, Andrey Kormilitzin, Isabelle Lorge, Alejo Nevado-Holgado, Dan W Joyce. (2024)<br><strong>Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care</strong><br><button class=copy-to-clipboard title="Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Recommendation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19790v1.pdf filename=2403.19790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contemporary <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> may have utility for processing unstructured, narrative free-text clinical data contained in electronic health records (EHRs) &ndash; a particularly important use-case for mental health where a majority of routinely-collected patient data lacks structured, machine-readable content. A significant problem for the the United Kingdom&rsquo;s National Health Service (NHS) are the long waiting lists for specialist mental healthcare. According to NHS data, in each month of 2023, there were between 370,000 and 470,000 individual new referrals into secondary mental healthcare services. Referrals must be triaged by clinicians, using clinical information contained in the patient&rsquo;s EHR to arrive at a decision about the most appropriate mental healthcare team to assess and potentially treat these patients. The ability to efficiently recommend a relevant team by ingesting potentially voluminous clinical notes could help services both reduce referral waiting times and with the right technology, improve the evidence available to justify triage decisions. We present and evaluate three different approaches for <b>LLM-based,</b> end-to-end ingestion of variable-length clinical EHR data to assist clinicians when triaging referrals. Our model is able to deliver triage <b>recommendations</b> consistent with existing clinical practices and it&rsquo;s architecture was implemented on a single GPU, making it practical for implementation in resource-limited NHS environments where private implementations of <b>LLM</b> technology will be necessary to ensure confidential clinical data is appropriately controlled and governed.</p></p class="citation"></blockquote><h3 id=35--198271-ime-integrating-multi-curvature-shared-and-specific-embedding-for-temporal-knowledge-graph-completion-jiapu-wang-et-al-2024>(3/5 | 198/271) IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion (Jiapu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiapu Wang, Zheng Cui, Boyue Wang, Shirui Pan, Junbin Gao, Baocai Yin, Wen Gao. (2024)<br><strong>IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion</strong><br><button class=copy-to-clipboard title="IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 28<br>Keywords: Graph, Knowledge Graph, Temporal Knowledge Graph, Temporal Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19881v1.pdf filename=2403.19881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Temporal</b> <b>Knowledge</b> <b>Graphs</b> <b>(TKGs)</b> incorporate a <b>temporal</b> <b>dimension,</b> <b>allowing</b> for a precise capture of the evolution of <b>knowledge</b> <b>and</b> reflecting the dynamic nature of the real world. Typically, <b>TKGs</b> contain complex geometric structures, with various geometric structures interwoven. However, existing <b>Temporal</b> <b>Knowledge</b> <b>Graph</b> Completion (TKGC) methods either model <b>TKGs</b> in a single space or neglect the heterogeneity of different curvature spaces, thus constraining their capacity to capture these intricate geometric structures. In this paper, we propose a novel Integrating Multi-curvature shared and specific Embedding (IME) model for TKGC tasks. Concretely, IME models <b>TKGs</b> into multi-curvature spaces, including hyperspherical, hyperbolic, and Euclidean spaces. Subsequently, IME incorporates two key properties, namely space-shared property and space-specific property. The space-shared property facilitates the learning of commonalities across different curvature spaces and alleviates the spatial gap caused by the heterogeneous nature of multi-curvature spaces, while the space-specific property captures characteristic features. Meanwhile, IME proposes an Adjustable Multi-curvature Pooling (AMP) approach to effectively retain important information. Furthermore, IME innovatively designs similarity, difference, and structure loss functions to attain the stated objective. Experimental results clearly demonstrate the superior performance of IME over existing state-of-the-art TKGC models.</p></p class="citation"></blockquote><h3 id=45--199271-leveraging-counterfactual-paths-for-contrastive-explanations-of-pomdp-policies-benjamin-kraske-et-al-2024>(4/5 | 199/271) Leveraging Counterfactual Paths for Contrastive Explanations of POMDP Policies (Benjamin Kraske et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Kraske, Zakariya Laouar, Zachary Sunberg. (2024)<br><strong>Leveraging Counterfactual Paths for Contrastive Explanations of POMDP Policies</strong><br><button class=copy-to-clipboard title="Leveraging Counterfactual Paths for Contrastive Explanations of POMDP Policies" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-HC, cs.AI<br>Keyword Score: 20<br>Keywords: Counter-factual, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19760v1.pdf filename=2403.19760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As humans come to rely on autonomous systems more, ensuring the transparency of such systems is important to their continued adoption. Explainable Artificial Intelligence (XAI) aims to reduce confusion and foster trust in systems by providing explanations of agent behavior. Partially observable Markov decision processes (POMDPs) provide a flexible framework capable of <b>reasoning</b> over transition and state uncertainty, while also being amenable to explanation. This work investigates the use of user-provided <b>counterfactuals</b> to generate contrastive explanations of POMDP policies. Feature expectations are used as a means of contrasting the performance of these policies. We demonstrate our approach in a Search and Rescue (SAR) setting. We analyze and discuss the associated challenges through two case studies.</p></p class="citation"></blockquote><h3 id=55--200271-towards-a-brazilian-history-knowledge-graph-valeria-de-paiva-et-al-2024>(5/5 | 200/271) Towards a Brazilian History Knowledge Graph (Valeria de Paiva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valeria de Paiva, Alexandre Rademaker. (2024)<br><strong>Towards a Brazilian History Knowledge Graph</strong><br><button class=copy-to-clipboard title="Towards a Brazilian History Knowledge Graph" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-DL, cs.AI<br>Keyword Score: 8<br>Keywords: Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19856v1.pdf filename=2403.19856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This short paper describes the first steps in a project to construct a <b>knowledge</b> <b>graph</b> for Brazilian history based on the Brazilian Dictionary of Historical Biographies (DHBB) and Wikipedia/Wikidata. We contend that large repositories of Brazilian-named entities (people, places, organizations, and political events and movements) would be beneficial for extracting information from Portuguese texts. We show that many of the terms/entities described in the DHBB do not have corresponding concepts (or Q items) in Wikidata, the largest structured database of entities associated with Wikipedia. We describe previous work on extracting information from the DHBB and outline the steps to construct a Wikidata-based historical <b>knowledge</b> <b>graph.</b></p></p class="citation"></blockquote><h2 id=csse-5>cs.SE (5)</h2><h3 id=15--201271-coderujb-an-executable-and-unified-java-benchmark-for-practical-programming-scenarios-zhengran-zeng-et-al-2024>(1/5 | 201/271) CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios (Zhengran Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengran Zeng, Yidong Wang, Rui Xie, Wei Ye, Shikun Zhang. (2024)<br><strong>CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios</strong><br><button class=copy-to-clipboard title="CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: 68N30 (Primary) 68T20 (Secondary), D-2-0, cs-SE, cs.SE<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19287v1.pdf filename=2403.19287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving landscape of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> tailored for software engineering, the need for <b>benchmarks</b> that accurately reflect real-world development scenarios is paramount. Current <b>benchmarks</b> are either too simplistic or fail to capture the multi-tasking nature of software development. To address this, we introduce CoderUJB, a new <b>benchmark</b> designed to evaluate <b>LLMs</b> across diverse Java programming tasks that are executable and reflective of actual development scenarios, acknowledging Java&rsquo;s prevalence in real-world software production. CoderUJB comprises 2,239 programming questions derived from 17 real open-source Java projects and spans five practical programming tasks. Our empirical study on this <b>benchmark</b> investigates the coding abilities of various open-source and closed-source <b>LLMs,</b> examining the effects of continued pre-training in specific programming languages <b>code</b> <b>and</b> instruction <b>fine-tuning</b> on their performance. The findings indicate that while <b>LLMs</b> exhibit strong potential, challenges remain, particularly in non-functional <b>code</b> <b>generation</b> (e.g., test generation and defect detection). Importantly, our results advise caution in the specific programming languages continued pre-training and instruction <b>fine-tuning,</b> as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies. CoderUJB thus marks a significant step towards more realistic evaluations of programming capabilities in <b>LLMs,</b> and our study provides valuable insights for the future development of these models in software engineering.</p></p class="citation"></blockquote><h3 id=25--202271-top-leaderboard-ranking--top-coding-proficiency-always-evoeval-evolving-coding-benchmarks-via-llm-chunqiu-steven-xia-et-al-2024>(2/5 | 202/271) Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM (Chunqiu Steven Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunqiu Steven Xia, Yinlin Deng, Lingming Zhang. (2024)<br><strong>Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM</strong><br><button class=copy-to-clipboard title="Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-LG, cs-PL, cs-SE, cs.SE<br>Keyword Score: 33<br>Keywords: Benchmarking, Code Generation, Instruction Following, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19114v1.pdf filename=2403.19114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLMs</b> have become the go-to choice for <b>code</b> <b>generation</b> tasks, with an exponential increase in the training, development, and usage of <b>LLMs</b> specifically for <b>code</b> <b>generation.</b> To evaluate the ability of <b>LLMs</b> on <b>code,</b> <b>both</b> academic and industry practitioners rely on popular handcrafted <b>benchmarks.</b> However, prior <b>benchmarks</b> contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many <b>benchmarks</b> are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing <b>benchmarks</b> reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval &ndash; a program synthesis <b>benchmark</b> suite created by evolving existing <b>benchmarks</b> into different targeted domains for a comprehensive evaluation of <b>LLM</b> coding abilities. Our study on 51 <b>LLMs</b> shows that compared to the high performance obtained on standard <b>benchmarks</b> like HumanEval, there is a significant drop in performance (on average 39.4%) when using EvoEval. Additionally, the decrease in performance can range from 19.6% to 47.7%, leading to drastic ranking changes amongst <b>LLMs</b> and showing potential overfitting of existing <b>benchmarks.</b> Furthermore, we showcase various insights, including the brittleness of <b>instruction-following</b> <b>models</b> when encountering rewording or subtle changes as well as the importance of learning problem composition and decomposition. EvoEval not only provides comprehensive <b>benchmarks,</b> but can be used to further evolve arbitrary problems to keep up with advances and the ever-changing landscape of <b>LLMs</b> for <b>code.</b> <b>We</b> have open-sourced our <b>benchmarks,</b> tools, and complete <b>LLM</b> generations at <a href=https://github.com/evo-eval/evoeval>https://github.com/evo-eval/evoeval</a></p></p class="citation"></blockquote><h3 id=35--203271-hirope-length-extrapolation-for-code-models-kechi-zhang-et-al-2024>(3/5 | 203/271) HiRoPE: Length Extrapolation for Code Models (Kechi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kechi Zhang, Ge Li, Huangzhao Zhang, Zhi Jin. (2024)<br><strong>HiRoPE: Length Extrapolation for Code Models</strong><br><button class=copy-to-clipboard title="HiRoPE: Length Extrapolation for Code Models" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19115v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19115v1.pdf filename=2403.19115v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing the limitation of context length in <b>large</b> <b>language</b> <b>models</b> for code-related tasks is the primary focus of this paper. Existing <b>LLMs</b> are constrained by their pre-trained context lengths, leading to performance issues in handling long complex code sequences. Inspired by how human programmers navigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a novel approach that enhances the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code. HiRoPE offers easy integration into existing <b>LLMs</b> without extra training costs. Our method is extensively evaluated with various <b>LLMs,</b> demonstrating stable performance in tasks such as language modeling and long code completion. We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field. Theoretically and experimentally, we find that HiRoPE also addresses the <b>out-of-distribution</b> issue in position encoding. Our HiRoPE significantly expands the context length capabilities of <b>LLMs,</b> enabling inference at lengths exponentially greater than the training length.</p></p class="citation"></blockquote><h3 id=45--204271-scale-constructing-structured-natural-language-comment-trees-for-software-vulnerability-detection-xin-cheng-wen-et-al-2024>(4/5 | 204/271) SCALE: Constructing Structured Natural Language Comment Trees for Software Vulnerability Detection (Xin-Cheng Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin-Cheng Wen, Cuiyun Gao, Shuzheng Gao, Yang Xiao, Michael R. Lyu. (2024)<br><strong>SCALE: Constructing Structured Natural Language Comment Trees for Software Vulnerability Detection</strong><br><button class=copy-to-clipboard title="SCALE: Constructing Structured Natural Language Comment Trees for Software Vulnerability Detection" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CR, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19096v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19096v1.pdf filename=2403.19096v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been a growing interest in automatic software vulnerability detection. Pre-trained model-based approaches have demonstrated superior performance than other Deep Learning (DL)-based approaches in detecting vulnerabilities. However, the existing pre-trained model-based approaches generally employ code sequences as input during prediction, and may ignore vulnerability-related structural information, as reflected in the following two aspects. First, they tend to fail to infer the semantics of the code statements with complex logic such as those containing multiple operators and pointers. Second, they are hard to comprehend various code execution sequences, which is essential for precise vulnerability detection. To mitigate the challenges, we propose a Structured Natural Language Comment tree-based vulnerAbiLity dEtection framework based on the pre-trained models, named SCALE. The proposed Structured Natural Language Comment Tree (SCT) integrates the semantics of code statements with code execution sequences based on the Abstract Syntax Trees (ASTs). Specifically, SCALE comprises three main modules: (1) Comment Tree Construction, which aims at enhancing the model&rsquo;s ability to infer the semantics of code statements by first incorporating <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for comment generation and then adding the comment node to ASTs. (2) Structured Natural Language Comment Tree Construction}, which aims at explicitly involving code execution sequence by combining the code syntax templates with the comment tree. (3) SCT-Enhanced Representation, which finally incorporates the constructed SCTs for well capturing vulnerability patterns.</p></p class="citation"></blockquote><h3 id=55--205271-clustering-mooc-programming-solutions-to-diversify-their-presentation-to-students-elizaveta-artser-et-al-2024>(5/5 | 205/271) Clustering MOOC Programming Solutions to Diversify Their Presentation to Students (Elizaveta Artser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elizaveta Artser, Anastasiia Birillo, Yaroslav Golubev, Maria Tigina, Hieke Keuning, Nikolay Vyahhi, Timofey Bryksin. (2024)<br><strong>Clustering MOOC Programming Solutions to Diversify Their Presentation to Students</strong><br><button class=copy-to-clipboard title="Clustering MOOC Programming Solutions to Diversify Their Presentation to Students" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19398v1.pdf filename=2403.19398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many MOOCs, whenever a student completes a programming task, they can see previous solutions of other students to find potentially different ways of solving the problem and learn new coding constructs. However, a lot of MOOCs simply show the most recent solutions, disregarding their diversity or quality. To solve this novel problem, we adapted the existing plagiarism detection tool JPlag to Python submissions on Hyperskill, a popular MOOC platform. However, due to the tool&rsquo;s inner algorithm, it fully processed only 46 out of 867 studied tasks. Therefore, we developed our own tool called Rhubarb. This tool first standardizes solutions that are algorithmically the same, then calculates the structure-aware edit distance between them, and then applies <b>clustering.</b> Finally, it selects one example from each of the largest clusters, taking into account their code quality. Rhubarb was able to handle all 867 tasks successfully. We compared approaches on a set of 59 tasks that both tools could process. Eight experts rated the selected solutions based on diversity, code quality, and usefulness. The default platform approach of selecting recent submissions received on average 3.12 out of 5, JPlag - 3.77, Rhubarb - 3.50. Since in the real MOOC, it is imperative to process everything, we created a system that uses JPlag on the 5.3% of tasks it fully processes and Rhubarb on the remaining 94.7%.</p></p class="citation"></blockquote><h2 id=eesssy-6>eess.SY (6)</h2><h3 id=16--206271-lane-change-in-dense-traffic-with-model-predictive-control-and-neural-networks-sangjae-bae-et-al-2024>(1/6 | 206/271) Lane-Change in Dense Traffic with Model Predictive Control and Neural Networks (Sangjae Bae et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sangjae Bae, David Isele, Alireza Nakhaei, Peng Xu, Alexandre Miranda Anon, Chiho Choi, Kikuo Fujimura, Scott Moura. (2024)<br><strong>Lane-Change in Dense Traffic with Model Predictive Control and Neural Networks</strong><br><button class=copy-to-clipboard title="Lane-Change in Dense Traffic with Model Predictive Control and Neural Networks" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 40<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19633v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19633v1.pdf filename=2403.19633v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an online smooth-path lane-change control framework. We focus on dense traffic where inter-vehicle space gaps are narrow, and cooperation with surrounding drivers is essential to achieve the lane-change maneuver. We propose a two-stage control framework that harmonizes Model Predictive Control (MPC) with <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GAN)</b> by utilizing driving intentions to generate smooth lane-change maneuvers. To improve performance in practice, the system is augmented with an adaptive safety boundary and a Kalman Filter to mitigate sensor noise. <b>Simulation</b> studies are investigated in different levels of traffic density and cooperativeness of other drivers. The <b>simulation</b> results support the effectiveness, driving comfort, and safety of the proposed method.</p></p class="citation"></blockquote><h3 id=26--207271-feedback-optimization-of-incentives-for-distribution-grid-services-guido-cavraro-et-al-2024>(2/6 | 207/271) Feedback Optimization of Incentives for Distribution Grid Services (Guido Cavraro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guido Cavraro, Joshua Comden, Andrey Bernstein. (2024)<br><strong>Feedback Optimization of Incentives for Distribution Grid Services</strong><br><button class=copy-to-clipboard title="Feedback Optimization of Incentives for Distribution Grid Services" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19616v1.pdf filename=2403.19616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Energy prices and net power injection limitations regulate the operations in distribution grids and typically ensure that operational constraints are met. Nevertheless, unexpected or prolonged abnormal events could undermine the grid&rsquo;s functioning. During contingencies, customers could contribute effectively to sustaining the network by providing services. This paper proposes an incentive mechanism that promotes users&rsquo; active participation by essentially altering the energy pricing rule. The incentives are modeled via a linear function whose parameters can be computed by the system operator (SO) by solving an optimization problem. Feedback-based optimization algorithms are then proposed to seek optimal incentives by leveraging measurements from the grid, even in the case when the SO does not have a full grid and customer information. Numerical <b>simulations</b> on a standard testbed validate the proposed approach.</p></p class="citation"></blockquote><h3 id=36--208271-expectation-maximization-aided-modified-weighted-sequential-energy-detector-for-distributed-cooperative-spectrum-sensing-mohammed-rashid-et-al-2024>(3/6 | 208/271) Expectation Maximization Aided Modified Weighted Sequential Energy Detector for Distributed Cooperative Spectrum Sensing (Mohammed Rashid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammed Rashid, Jeffrey A. Nanzer. (2024)<br><strong>Expectation Maximization Aided Modified Weighted Sequential Energy Detector for Distributed Cooperative Spectrum Sensing</strong><br><button class=copy-to-clipboard title="Expectation Maximization Aided Modified Weighted Sequential Energy Detector for Distributed Cooperative Spectrum Sensing" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19556v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19556v1.pdf filename=2403.19556v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed cooperative spectrum sensing usually involves a group of unlicensed secondary users (SUs) collaborating to detect the primary user (PU) in the channel, and thereby opportunistically utilize it without causing interference to the PU. The conventional energy detector (ED) based spectrum sensing ignores the dynamic nature of the PU by using energy statistic only from the present sensing interval for the PU detection. However, for a dynamic PU, previous studies have shown that improved detection capabilities can be achieved by aggregating both present and past energy samples in a test statistic. To this end, a weighted sequential energy detector (WSED) has been proposed, but it is based on aggregating all the collected energy samples over an observation window. For a highly dynamic PU, that involves also combining the outdated samples in the test statistic. In this paper, we propose a modified WSED (mWSED) that uses the primary user states information over the window to aggregate only the highly correlated energy samples in its test statistic. In practice, since the PU states are a priori unknown, we also develop a joint expectation-maximization and Viterbi (EM-Viterbi) algorithm based scheme to iteratively estimate the states by using the energy samples collected over the window. The estimated states are then used in mWSED to compute its test statistics, and the algorithm is referred to here as EM-mWSED. <b>Simulation</b> results are presented to demonstrate the states estimation performance of EM-Viterbi and the PU detection performance of EM-mWSED. The results show that, for both highly dynamic as well as slowly time-varying PU, these algorithms outperform the ED and WSED at PU detection, and their performances improve by either increasing the average number of neighbors per SU in the network, or by increasing the SNR or the number of samples per energy statistic.</p></p class="citation"></blockquote><h3 id=46--209271-design-and-evaluation-of-a-dc-microgrid-testbed-for-der-integration-and-power-management-gokul-krishnan-s-et-al-2024>(4/6 | 209/271) Design and Evaluation of a DC Microgrid Testbed for DER Integration and Power Management (Gokul Krishnan S et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gokul Krishnan S, Charalambos Konstantinou. (2024)<br><strong>Design and Evaluation of a DC Microgrid Testbed for DER Integration and Power Management</strong><br><button class=copy-to-clipboard title="Design and Evaluation of a DC Microgrid Testbed for DER Integration and Power Management" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19176v1.pdf filename=2403.19176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a DC microgrid testbed setup that consists of various Distributed Energy Resources (DERs) including solar Photovoltaics (PV), supercapacitors for voltage regulation, and Battery Energy Storage Systems (BESS). The DC microgrid accommodates both non-flexible and flexible loads which can be dynamically adjusted based on PV power availability. The integration of the setup with the Hyphae Autonomous Power Interchange System (APIS) framework automates energy transfer within the BESS, ensuring efficient power management and optimizing the overall efficiency of the DC microgrid. Furthermore, the setup is validated in terms of the efficacy of the proposed model via real-time <b>simulation,</b> facilitated by the Speedgoat baseline real-time target Hardware-in-the-Loop (HIL) machine. The results demonstrate the model&rsquo;s adeptness in efficiently managing power sharing, emphasizing the capabilities of the DC microgrid setup in terms of performance and reliability in dynamic energy scenarios as well as enhancing the resilience of the grid amidst PV uncertainties.</p></p class="citation"></blockquote><h3 id=56--210271-harnessing-data-for-accelerating-model-predictive-control-by-constraint-removal-zhinan-hou-et-al-2024>(5/6 | 210/271) Harnessing Data for Accelerating Model Predictive Control by Constraint Removal (Zhinan Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhinan Hou, Feiran Zhao, Keyou You. (2024)<br><strong>Harnessing Data for Accelerating Model Predictive Control by Constraint Removal</strong><br><button class=copy-to-clipboard title="Harnessing Data for Accelerating Model Predictive Control by Constraint Removal" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19126v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19126v1.pdf filename=2403.19126v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model predictive control (MPC) solves a receding-horizon optimization problem in real-time, which can be computationally demanding when there are thousands of constraints. To accelerate online computation of MPC, we utilize data to adaptively remove the constraints while maintaining the MPC policy unchanged. Specifically, we design the removal rule based on the Lipschitz continuity of the MPC policy. This removal rule can use the information of historical data according to the Lipschitz constant and the distance between the current state and historical states. In particular, we provide the explicit expression for calculating the Lipschitz constant by the model parameters. Finally, <b>simulations</b> are performed to validate the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=66--211271-gain-only-neural-operator-approximators-of-pde-backstepping-controllers-rafael-vazquez-et-al-2024>(6/6 | 211/271) Gain-Only Neural Operator Approximators of PDE Backstepping Controllers (Rafael Vazquez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rafael Vazquez, Miroslav Krstic. (2024)<br><strong>Gain-Only Neural Operator Approximators of PDE Backstepping Controllers</strong><br><button class=copy-to-clipboard title="Gain-Only Neural Operator Approximators of PDE Backstepping Controllers" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19344v1.pdf filename=2403.19344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For the recently introduced deep learning-powered approach to PDE backstepping control, we present an advancement applicable across all the results developed thus far: approximating the control gain function only (a function of one variable), rather than the entire kernel function of the backstepping transformation (a function of two variables). We introduce this idea on a couple <b>benchmark</b> (unstable) PDEs, hyperbolic and parabolic. We alter the approach of quantifying the effect of the approximation error by replacing a backstepping transformation that employs the approximated kernel (suitable for adaptive control) by a transformation that employs the exact kernel (suitable for gain scheduling). A major simplification in the target system arises, with the perturbation due to the approximation shifting from the domain to the boundary condition. This results in a significant difference in the Lyapunov analysis, which nevertheless results in a guarantee of the stability being retained with the simplified approximation approach. The approach of approximating only the control gain function simplifies the operator being approximated and the training of its neural approximation, with an expected reduction in the neural network size. The price for the savings in approximation is paid through a somewhat more intricate Lyapunov analysis, in higher Sobolev spaces for some PDEs, as well as some restrictions on initial conditions that result from higher Sobolev spaces. While the proposed approach appears inapplicable to uses in adaptive control, it is almost certainly applicable in gain scheduling applications of neural operator-approximated PDE backstepping controllers.</p></p class="citation"></blockquote><h2 id=cscy-5>cs.CY (5)</h2><h3 id=15--212271-developing-generative-ai-chatbots-conceptual-framework-for-higher-education-joshua-ebere-chukwuere-2024>(1/5 | 212/271) Developing generative AI chatbots conceptual framework for higher education (Joshua Ebere Chukwuere, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joshua Ebere Chukwuere. (2024)<br><strong>Developing generative AI chatbots conceptual framework for higher education</strong><br><button class=copy-to-clipboard title="Developing generative AI chatbots conceptual framework for higher education" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 40<br>Keywords: Generative AI, Bard, ChatGPT, Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19303v1.pdf filename=2403.19303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research explores the quickly changing field of <b>generative</b> <b>artificial</b> intelligence (GAI) <b>chatbots</b> in higher education, an industry that is undergoing major technological changes. AI <b>chatbots,</b> such as <b>ChatGPT,</b> HuggingChat, and Google <b>Bard,</b> are becoming more and more common in a variety of sectors, including education. Their acceptance is still in its early phases, with a variety of prospects and obstacles. However, their potential in higher education is particularly noteworthy, providing lecturers and students with affordable, individualized support. Creating a comprehensive framework to aid the usage of <b>generative</b> <b>AI</b> <b>chatbots</b> in higher education institutions (HEIs) is the aim of this project. The Chukwuere <b>Generative</b> <b>AI</b> <b>Chatbots</b> Acceptance Model (CGAICAM) is the result of this study&rsquo;s synthesis of elements from well-known frameworks, including the TAM, UTAUT2, TPB, and others along with variables like optimism, innovativeness, discomfort, insecurity, and others. Using a research method that encompasses a comprehensive analysis of extant literature from databases such as IEEE, ACM, ScienceDirect, and Google Scholar, the study aims to comprehend the implications of AI <b>Chatbots</b> on higher education and pinpoint critical elements for their efficacious implementation. Peer-reviewed English-language publications published between 2020 and 2023 with a focus on the use of AI <b>chatbots</b> in higher education were the main focus of the search criteria. The results demonstrate how much AI <b>chatbots</b> can do to improve student engagement, streamline the educational process, and support administrative and research duties. But there are also clear difficulties, such as unfavorable student sentiments, doubts about the veracity of material produced by AI, and unease and nervousness with new technologies.</p></p class="citation"></blockquote><h3 id=25--213271-purposeful-remixing-with-generative-ai-constructing-designer-voice-in-multimodal-composing-xiao-tan-et-al-2024>(2/5 | 213/271) Purposeful remixing with generative AI: Constructing designer voice in multimodal composing (Xiao Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Tan, Wei Xu, Chaoran Wang. (2024)<br><strong>Purposeful remixing with generative AI: Constructing designer voice in multimodal composing</strong><br><button class=copy-to-clipboard title="Purposeful remixing with generative AI: Constructing designer voice in multimodal composing" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 26<br>Keywords: Generative AI, Multi-modal, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19095v1.pdf filename=2403.19095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Voice, the discursive construction of the writer&rsquo;s identity, has been extensively studied and theorized in composition studies. In <b>multimodal</b> writing, students are able to mobilize both linguistic and non linguistic resources to express their real or imagined identities. But at the same time, when students are limited to choose from available online resources, their voices might be compromised due to the incompatibility between their authorial intentions and the existing materials. This study, therefore, investigates whether the use of <b>generative</b> <b>AI</b> tools could help student authors construct a more consistent voice in <b>multimodal</b> writing. In this study, we have designed a photo essay assignment where students recount a story in the form of photo essays and <b>prompt</b> AI image generating tools to create photos for their storytelling. Drawing on interview data, written reflection, written annotation, and <b>multimodal</b> products from seven focal participants, we have identified two remixing practices, through which students attempted to establish a coherent and unique voice in writing. The study sheds light on the intentional and discursive nature of <b>multimodal</b> writing with AI as afforded by the technological flexibility, while also highlighting the practical and ethical challenges that could be attributed to students insufficient <b>prompt</b> and <b>multimodal</b> literacy and the innate limitations of AI systems. This study provides important implications for incorporating AI tools in designing <b>multimodal</b> writing tasks.</p></p class="citation"></blockquote><h3 id=35--214271-cycling-on-the-freeway-the-perilous-state-of-open-source-neuroscience-software-britta-u-westner-et-al-2024>(3/5 | 214/271) Cycling on the Freeway: The Perilous State of Open Source Neuroscience Software (Britta U. Westner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Britta U. Westner, Daniel R. McCloy, Eric Larson, Alexandre Gramfort, Daniel S. Katz, Arfon M. Smith, invited co-signees. (2024)<br><strong>Cycling on the Freeway: The Perilous State of Open Source Neuroscience Software</strong><br><button class=copy-to-clipboard title="Cycling on the Freeway: The Perilous State of Open Source Neuroscience Software" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY, q-bio-OT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19394v1.pdf filename=2403.19394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most scientists need software to perform their research (Barker et al., 2020; Carver et al., 2022; Hettrick, 2014; Hettrick et al., 2014; Switters and Osimo, 2019), and neuroscientists are no exception. Whether we work with reaction times, electrophysiological signals, or magnetic resonance imaging data, we rely on software to acquire, analyze, and statistically evaluate the raw data we obtain - or to generate such data if we work with <b>simulations.</b> In recent years there has been a shift toward relying on free, open-source scientific software (FOSSS) for neuroscience data analysis (Poldrack et al., 2019), in line with the broader open science movement in academia (McKiernan et al., 2016) and wider industry trends (Eghbal, 2016). Importantly, FOSSS is typically developed by working scientists (not professional software developers) which sets up a precarious situation given the nature of the typical academic workplace (wherein academics, especially in their early careers, are on short and fixed term contracts). In this paper, we will argue that the existing ecosystem of neuroscientific open source software is brittle, and discuss why and how the neuroscience community needs to come together to ensure a healthy growth of our software landscape to the benefit of all.</p></p class="citation"></blockquote><h3 id=45--215271-the-use-of-chatgpt-in-higher-education-the-advantages-and-disadvantages-joshua-ebere-chukwuere-2024>(4/5 | 215/271) The use of ChatGPT in higher education: The advantages and disadvantages (Joshua Ebere Chukwuere, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joshua Ebere Chukwuere. (2024)<br><strong>The use of ChatGPT in higher education: The advantages and disadvantages</strong><br><button class=copy-to-clipboard title="The use of ChatGPT in higher education: The advantages and disadvantages" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: ChatGPT, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19245v1.pdf filename=2403.19245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Higher education scholars are interested in an artificial intelligence (AI) technology called <b>ChatGPT,</b> which was developed by OpenAI. Whether <b>ChatGPT</b> can improve learning is still a topic of debate among experts. This concise overview of the literature examines the application of <b>ChatGPT</b> in higher education to comprehend and produce high-level instruction. By examining the essential literature, this study seeks to provide a thorough assessment of the advantages and disadvantages of utilizing <b>ChatGPT</b> in higher education settings. But it&rsquo;s crucial to consider both the positive and negative elements. For this rapid review, the researcher searched Google Scholar, Scopus, and others between January 2023 and July 2023 for prior research from various publications. These studies were examined. The study found that employing <b>ChatGPT</b> in higher education is beneficial for a number of reasons. It can provide individualized instruction, and <b>prompt</b> feedback, facilitate access to learning, and promote student interaction. These benefits could improve the learning environment and make it more fun for academics and students. The cons of <b>ChatGPT</b> are equally present. These problems include the inability to comprehend emotions, the lack of social interaction chances, technological limitations, and the dangers of depending too much on <b>ChatGPT</b> for higher education. Higher education should combine <b>ChatGPT</b> with other teaching techniques to provide students and lecturers with a comprehensive education. However, it is crucial to consider the positives, negatives, and moral issues before adopting <b>ChatGPT</b> in the classroom.</p></p class="citation"></blockquote><h3 id=55--216271-genai-detection-tools-adversarial-techniques-and-implications-for-inclusivity-in-higher-education-mike-perkins-et-al-2024>(5/5 | 216/271) GenAI Detection Tools, Adversarial Techniques and Implications for Inclusivity in Higher Education (Mike Perkins et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mike Perkins, Jasper Roe, Binh H. Vu, Darius Postma, Don Hickerson, James McGaughran, Huy Q. Khuat. (2024)<br><strong>GenAI Detection Tools, Adversarial Techniques and Implications for Inclusivity in Higher Education</strong><br><button class=copy-to-clipboard title="GenAI Detection Tools, Adversarial Techniques and Implications for Inclusivity in Higher Education" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19148v1.pdf filename=2403.19148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates the efficacy of six major <b>Generative</b> <b>AI</b> (GenAI) text detectors when confronted with machine-generated content that has been modified using techniques designed to evade detection by these tools (n=805). The results demonstrate that the detectors&rsquo; already low accuracy rates (39.5%) show major reductions in accuracy (17.4%) when faced with manipulated content, with some techniques proving more effective than others in evading detection. The accuracy limitations and the potential for false accusations demonstrate that these tools cannot currently be recommended for determining whether violations of academic integrity have occurred, underscoring the challenges educators face in maintaining inclusive and fair assessment practices. However, they may have a role in supporting student learning and maintaining academic integrity when used in a non-punitive manner. These results underscore the need for a combined approach to addressing the challenges posed by GenAI in academia to promote the responsible and equitable use of these emerging technologies. The study concludes that the current limitations of AI text detectors require a critical approach for any possible implementation in HE and highlight possible alternatives to AI assessment strategies.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--217271-emotion-neural-transducer-for-fine-grained-speech-emotion-recognition-siyuan-shen-et-al-2024>(1/2 | 217/271) Emotion Neural Transducer for Fine-Grained Speech Emotion Recognition (Siyuan Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyuan Shen, Yu Gao, Feng Liu, Hanyang Wang, Aimin Zhou. (2024)<br><strong>Emotion Neural Transducer for Fine-Grained Speech Emotion Recognition</strong><br><button class=copy-to-clipboard title="Emotion Neural Transducer for Fine-Grained Speech Emotion Recognition" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 40<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19224v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19224v1.pdf filename=2403.19224v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The mainstream paradigm of <b>speech</b> <b>emotion</b> <b>recognition</b> (SER) is identifying the single <b>emotion</b> <b>label</b> of the entire utterance. This line of works neglect the <b>emotion</b> <b>dynamics</b> at fine temporal granularity and mostly fail to leverage linguistic information of <b>speech</b> <b>signal</b> explicitly. In this paper, we propose <b>Emotion</b> <b>Neural</b> Transducer for fine-grained <b>speech</b> <b>emotion</b> <b>recognition</b> with <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> joint training. We first extend typical neural transducer with <b>emotion</b> <b>joint</b> network to construct <b>emotion</b> <b>lattice</b> for fine-grained SER. Then we propose lattice max pooling on the alignment lattice to facilitate distinguishing <b>emotional</b> <b>and</b> non-emotional frames. To adapt fine-grained SER to transducer inference manner, we further make blank, the special symbol of <b>ASR,</b> serve as underlying <b>emotion</b> <b>indicator</b> as well, yielding Factorized <b>Emotion</b> <b>Neural</b> Transducer. For typical utterance-level SER, our ENT models outperform state-of-the-art methods on IEMOCAP in low word error rate. Experiments on IEMOCAP and the latest <b>speech</b> <b>emotion</b> <b>diarization</b> dataset ZED also demonstrate the superiority of fine-grained <b>emotion</b> <b>modeling.</b> Our code is available at <a href=https://github.com/ECNU-Cross-Innovation-Lab/ENT>https://github.com/ECNU-Cross-Innovation-Lab/ENT</a>.</p></p class="citation"></blockquote><h3 id=22--218271-a-novel-stochastic-transformer-based-approach-for-post-traumatic-stress-disorder-detection-using-audio-recording-of-clinical-interviews-mamadou-dia-et-al-2024>(2/2 | 218/271) A Novel Stochastic Transformer-based Approach for Post-Traumatic Stress Disorder Detection using Audio Recording of Clinical Interviews (Mamadou Dia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mamadou Dia, Ghazaleh Khodabandelou, Alice Othmani. (2024)<br><strong>A Novel Stochastic Transformer-based Approach for Post-Traumatic Stress Disorder Detection using Audio Recording of Clinical Interviews</strong><br><button class=copy-to-clipboard title="A Novel Stochastic Transformer-based Approach for Post-Traumatic Stress Disorder Detection using Audio Recording of Clinical Interviews" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19441v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19441v1.pdf filename=2403.19441v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Post-traumatic stress disorder (PTSD) is a mental disorder that can be developed after witnessing or experiencing extremely traumatic events. PTSD can affect anyone, regardless of ethnicity, or culture. An estimated one in every eleven people will experience PTSD during their lifetime. The Clinician-Administered PTSD Scale (CAPS) and the PTSD Check List for Civilians (PCL-C) interviews are gold standards in the diagnosis of PTSD. These questionnaires can be fooled by the subject&rsquo;s responses. This work proposes a deep learning-based approach that achieves state-of-the-art performances for PTSD detection using audio recordings during clinical interviews. Our approach is based on MFCC low-level features extracted from audio recordings of clinical interviews, followed by deep high-level learning using a Stochastic <b>Transformer.</b> Our proposed approach achieves state-of-the-art performances with an RMSE of 2.92 on the eDAIC dataset thanks to the stochastic depth, stochastic deep learning layers, and stochastic activation function.</p></p class="citation"></blockquote><h2 id=cscr-6>cs.CR (6)</h2><h3 id=16--219271-genos-general-in-network-unsupervised-intrusion-detection-by-rule-extraction-ruoyu-li-et-al-2024>(1/6 | 219/271) Genos: General In-Network Unsupervised Intrusion Detection by Rule Extraction (Ruoyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruoyu Li, Qing Li, Yu Zhang, Dan Zhao, Xi Xiao, Yong Jiang. (2024)<br><strong>Genos: General In-Network Unsupervised Intrusion Detection by Rule Extraction</strong><br><button class=copy-to-clipboard title="Genos: General In-Network Unsupervised Intrusion Detection by Rule Extraction" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-NI, cs.CR<br>Keyword Score: 39<br>Keywords: Clustering, Fine-tuning, Multi-modal, Multi-modal, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19248v1.pdf filename=2403.19248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Anomaly-based network intrusion detection systems (A-NIDS) use <b>unsupervised</b> models to detect unforeseen attacks. However, existing A-NIDS solutions suffer from low throughput, lack of interpretability, and high maintenance costs. Recent in-network intelligence (INI) exploits programmable switches to offer line-rate deployment of NIDS. Nevertheless, current in-network NIDS are either model-specific or only apply to <b>supervised</b> models. In this paper, we propose Genos, a general in-network framework for <b>unsupervised</b> A-NIDS by rule extraction, which consists of a Model Compiler, a Model Interpreter, and a Model Debugger. Specifically, observing benign data are <b>multimodal</b> and usually located in multiple subspaces in the feature space, we utilize a divide-and-conquer approach for model-agnostic rule extraction. In the Model Compiler, we first propose a tree-based <b>clustering</b> algorithm to partition the feature space into subspaces, then design a decision boundary estimation mechanism to approximate the source model in each subspace. The Model Interpreter interprets predictions by important attributes to aid network operators in understanding the predictions. The Model Debugger conducts incremental updating to rectify errors by only <b>fine-tuning</b> rules on affected subspaces, thus reducing maintenance costs. We implement a prototype using physical hardware, and experiments demonstrate its superior performance of 100 Gbps throughput, great interpretability, and trivial updating overhead.</p></p class="citation"></blockquote><h3 id=26--220271-detecting-financial-bots-on-the-ethereum-blockchain-thomas-niedermayer-et-al-2024>(2/6 | 220/271) Detecting Financial Bots on the Ethereum Blockchain (Thomas Niedermayer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Niedermayer, Pietro Saggese, Bernhard Haslhofer. (2024)<br><strong>Detecting Financial Bots on the Ethereum Blockchain</strong><br><button class=copy-to-clipboard title="Detecting Financial Bots on the Ethereum Blockchain" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 23<br>Keywords: Clustering, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19530v1.pdf filename=2403.19530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of bots in Distributed Ledger Technologies (DLTs) fosters efficiency and automation. However, their use is also associated with predatory trading and market manipulation, and can pose threats to system integrity. It is therefore essential to understand the extent of bot deployment in DLTs; despite this, current detection systems are predominantly rule-based and lack flexibility. In this study, we present a novel approach that utilizes machine learning for the detection of financial bots on the Ethereum platform. First, we systematize existing scientific literature and collect anecdotal evidence to establish a taxonomy for financial bots, comprising 7 categories and 24 subcategories. Next, we create a ground-truth dataset consisting of 133 human and 137 bot addresses. Third, we employ both <b>unsupervised</b> and <b>supervised</b> machine learning algorithms to detect bots deployed on Ethereum. The highest-performing <b>clustering</b> algorithm is a Gaussian Mixture Model with an average cluster purity of 82.6%, while the highest-performing model for binary classification is a Random Forest with an accuracy of 83%. Our machine learning-based detection mechanism contributes to understanding the Ethereum ecosystem dynamics by providing additional insights into the current bot landscape.</p></p class="citation"></blockquote><h3 id=36--221271-on-the-robustness-of-ldp-protocols-for-numerical-attributes-under-data-poisoning-attacks-xiaoguang-li-et-al-2024>(3/6 | 221/271) On the Robustness of LDP Protocols for Numerical Attributes under Data Poisoning Attacks (Xiaoguang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoguang Li, Zitao Li, Ninghui Li, Wenhai Sun. (2024)<br><strong>On the Robustness of LDP Protocols for Numerical Attributes under Data Poisoning Attacks</strong><br><button class=copy-to-clipboard title="On the Robustness of LDP Protocols for Numerical Attributes under Data Poisoning Attacks" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Zero-shot, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19510v1.pdf filename=2403.19510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies reveal that local <b>differential</b> <b>privacy</b> (LDP) protocols are vulnerable to data poisoning attacks where an attacker can manipulate the final estimate on the server by leveraging the characteristics of LDP and sending carefully crafted data from a small fraction of controlled local clients. This vulnerability raises concerns regarding the robustness and reliability of LDP in hostile environments. In this paper, we conduct a systematic investigation of the robustness of state-of-the-art LDP protocols for numerical attributes, i.e., categorical frequency oracles (CFOs) with binning and consistency, and distribution reconstruction. We evaluate protocol robustness through an attack-driven approach and propose new metrics for cross-protocol attack gain measurement. The results indicate that Square Wave and CFO-based protocols in the Server setting are more robust against the attack compared to the CFO-based protocols in the User setting. Our evaluation also unfolds new relationships between LDP security and its inherent design choices. We found that the hash domain size in local-hashing-based LDP has a profound impact on protocol robustness beyond the well-known effect on utility. Further, we propose a <b>zero-shot</b> attack detection by leveraging the rich reconstructed distribution information. The experiment show that our detection significantly improves the existing methods and effectively identifies data manipulation in challenging scenarios.</p></p class="citation"></blockquote><h3 id=46--222271-enhancing-trust-and-privacy-in-distributed-networks-a-comprehensive-survey-on-blockchain-based-federated-learning-ji-liu-et-al-2024>(4/6 | 222/271) Enhancing Trust and Privacy in Distributed Networks: A Comprehensive Survey on Blockchain-based Federated Learning (Ji Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ji Liu, Chunlu Chen, Yu Li, Lin Sun, Yulun Song, Jingbo Zhou, Bo Jing, Dejing Dou. (2024)<br><strong>Enhancing Trust and Privacy in Distributed Networks: A Comprehensive Survey on Blockchain-based Federated Learning</strong><br><button class=copy-to-clipboard title="Enhancing Trust and Privacy in Distributed Networks: A Comprehensive Survey on Blockchain-based Federated Learning" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-DC, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Federated Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19178v1.pdf filename=2403.19178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While centralized servers pose a risk of being a single point of failure, decentralized approaches like blockchain offer a compelling solution by implementing a consensus mechanism among multiple entities. Merging distributed computing with cryptographic techniques, decentralized technologies introduce a novel computing paradigm. Blockchain ensures secure, transparent, and tamper-proof data management by validating and recording transactions via consensus across network nodes. <b>Federated</b> <b>Learning</b> (FL), as a distributed machine learning framework, enables participants to collaboratively train models while safeguarding data privacy by avoiding direct raw data exchange. Despite the growing interest in decentralized methods, their application in FL remains underexplored. This paper presents a thorough investigation into Blockchain-based FL (BCFL), spotlighting the synergy between blockchain&rsquo;s security features and FL&rsquo;s privacy-preserving model training capabilities. First, we present the taxonomy of BCFL from three aspects, including decentralized, separate networks, and reputation-based architectures. Then, we <b>summarize</b> the general architecture of BCFL systems, providing a comprehensive perspective on FL architectures informed by blockchain. Afterward, we analyze the application of BCFL in healthcare, IoT, and other privacy-sensitive areas. Finally, we identify future research directions of BCFL.</p></p class="citation"></blockquote><h3 id=56--223271-secgraph-towards-sgx-based-efficient-and-confidentiality-preserving-graph-search-qiuhao-wang-et-al-2024>(5/6 | 223/271) SecGraph: Towards SGX-based Efficient and Confidentiality-Preserving Graph Search (Qiuhao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiuhao Wang, Xu Yang, Saiyu Qi, Yong Qi. (2024)<br><strong>SecGraph: Towards SGX-based Efficient and Confidentiality-Preserving Graph Search</strong><br><button class=copy-to-clipboard title="SecGraph: Towards SGX-based Efficient and Confidentiality-Preserving Graph Search" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DB, cs-SI, cs.CR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19531v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19531v1.pdf filename=2403.19531v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graphs</b> have more expressive power and are widely researched in various search demand scenarios, compared with traditional relational and XML models. Today, many <b>graph</b> search services have been deployed on a third-party server, which can alleviate users from the burdens of maintaining large-scale <b>graphs</b> and huge computation costs. Nevertheless, outsourcing <b>graph</b> search services to the third-party server may invade users&rsquo; privacy. PeGraph was recently proposed to achieve the encrypted search over the social <b>graph.</b> The main idea of PeGraph is to maintain two data structures XSet and TSet motivated by the OXT technology to support encrypted conductive search. However, PeGraph still has some limitations. First, PeGraph suffers from high communication and computation costs in search operations. Second, PeGraph cannot support encrypted search over dynamic <b>graphs.</b> In this paper, we propose an SGX-based efficient and confidentiality-preserving <b>graph</b> search scheme SecGraph that can support insertion and deletion operations. We first design a new proxy-token generation method to reduce the communication cost. Then, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to reduce the computation cost. Finally, we design a new dynamic version of TSet named Twin-TSet to enable encrypted search over dynamic <b>graphs.</b> We have demonstrated the confidentiality preservation property of SecGraph through rigorous security analysis. Experiment results show that SecGraph yields up to 208x improvement in search time compared with PeGraph and the communication cost in PeGraph is up to 540x larger than that in SecGraph.</p></p class="citation"></blockquote><h3 id=66--224271-assetharvester-a-static-analysis-tool-for-detecting-assets-protected-by-secrets-in-software-artifacts-setu-kumar-basak-et-al-2024>(6/6 | 224/271) AssetHarvester: A Static Analysis Tool for Detecting Assets Protected by Secrets in Software Artifacts (Setu Kumar Basak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Setu Kumar Basak, K. Virgil English, Ken Ogura, Vitesh Kambara, Bradley Reaves, Laurie Williams. (2024)<br><strong>AssetHarvester: A Static Analysis Tool for Detecting Assets Protected by Secrets in Software Artifacts</strong><br><button class=copy-to-clipboard title="AssetHarvester: A Static Analysis Tool for Detecting Assets Protected by Secrets in Software Artifacts" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SE, cs.CR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19072v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19072v1.pdf filename=2403.19072v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>GitGuardian monitored secrets exposure in public GitHub repositories and reported developers leaked over 12 million secrets (database and other credentials) in 2023, indicating a 113% surge from 2021. Despite the availability of secret detection tools, developers ignore the tools&rsquo; reported warnings because of false positives (25%-99%). However, each secret protects assets of different values accessible through asset identifiers (a DNS name and a public or private IP address). The asset information for a secret can aid developers in filtering false positives and prioritizing secret removal from the source code. However, existing secret detection tools do not provide the asset information, thus presenting difficulty to developers in filtering secrets only by looking at the secret value or finding the assets manually for each reported secret. The goal of our study is to aid software practitioners in prioritizing secrets removal by providing the assets information protected by the secrets through our novel static analysis tool. We present AssetHarvester, a static analysis tool to detect secret-asset pairs in a repository. Since the location of the asset can be distant from where the secret is defined, we investigated secret-asset co-location patterns and found four patterns. To identify the secret-asset pairs of the four patterns, we utilized three approaches (pattern matching, data flow analysis, and fast-approximation heuristics). We curated a <b>benchmark</b> of 1,791 secret-asset pairs of four database types extracted from 188 public GitHub repositories to evaluate the performance of AssetHarvester. AssetHarvester demonstrates precision of (97%), recall (90%), and F1-score (94%) in detecting secret-asset pairs. Our findings indicate that data flow analysis employed in AssetHarvester detects secret-asset pairs with 0% false positives and aids in improving the recall of secret detection tools.</p></p class="citation"></blockquote><h2 id=eessiv-5>eess.IV (5)</h2><h3 id=15--225271-debiasing-cardiac-imaging-with-controlled-latent-diffusion-models-grzegorz-skorupko-et-al-2024>(1/5 | 225/271) Debiasing Cardiac Imaging with Controlled Latent Diffusion Models (Grzegorz Skorupko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Grzegorz Skorupko, Richard Osuala, Zuzanna Szafranowska, Kaisar Kushibar, Nay Aung, Steffen E Petersen, Karim Lekadir, Polyxeni Gkontra. (2024)<br><strong>Debiasing Cardiac Imaging with Controlled Latent Diffusion Models</strong><br><button class=copy-to-clipboard title="Debiasing Cardiac Imaging with Controlled Latent Diffusion Models" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 35<br>Keywords: ControlNet, Diffusion Model, Geometry, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19508v1.pdf filename=2403.19508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The progress in deep learning solutions for disease diagnosis and prognosis based on cardiac magnetic resonance imaging is hindered by highly imbalanced and biased training data. To address this issue, we propose a method to alleviate imbalances inherent in datasets through the generation of synthetic data based on sensitive attributes such as sex, age, body mass index, and health condition. We adopt <b>ControlNet</b> based on a denoising <b>diffusion</b> <b>probabilistic</b> <b>model</b> to condition on text assembled from patient metadata and cardiac <b>geometry</b> derived from segmentation masks using a large-cohort study, specifically, the UK Biobank. We assess our method by evaluating the realism of the generated images using established quantitative metrics. Furthermore, we conduct a downstream classification task aimed at debiasing a classifier by rectifying imbalances within underrepresented groups through synthetically generated samples. Our experiments demonstrate the effectiveness of the proposed approach in mitigating dataset imbalances, such as the scarcity of younger patients or individuals with normal BMI level suffering from heart failure. This work represents a major step towards the adoption of synthetic data for the development of fair and generalizable models for medical classification tasks. Notably, we conduct all our experiments using a single, consumer-level GPU to highlight the feasibility of our approach within resource-constrained environments. Our code is available at <a href=https://github.com/faildeny/debiasing-cardiac-mri>https://github.com/faildeny/debiasing-cardiac-mri</a>.</p></p class="citation"></blockquote><h3 id=25--226271-enhancing-efficiency-in-vision-transformer-networks-design-techniques-and-insights-moein-heidari-et-al-2024>(2/5 | 226/271) Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights (Moein Heidari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Moein Heidari, Reza Azad, Sina Ghorbani Kolahi, René Arimond, Leon Niggemeier, Alaa Sulaiman, Afshin Bozorgpour, Ehsan Khodapanah Aghdam, Amirhossein Kazerouni, Ilker Hacihaliloglu, Dorit Merhof. (2024)<br><strong>Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights</strong><br><button class=copy-to-clipboard title="Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19882v1.pdf filename=2403.19882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intrigued by the inherent ability of the human visual system to identify salient regions in complex scenes, attention mechanisms have been seamlessly integrated into various Computer <b>Vision</b> <b>(CV)</b> tasks. Building upon this paradigm, <b>Vision</b> <b>Transformer</b> (ViT) networks exploit attention mechanisms for improved efficiency. This review navigates the landscape of redesigned attention mechanisms within ViTs, aiming to enhance their performance. This paper provides a comprehensive exploration of techniques and insights for designing attention mechanisms, systematically reviewing recent literature in the field of CV. This survey begins with an introduction to the theoretical foundations and fundamental concepts underlying attention mechanisms. We then present a systematic taxonomy of various attention mechanisms within ViTs, employing redesigned approaches. A multi-perspective categorization is proposed based on their application, objectives, and the type of attention applied. The analysis includes an exploration of the novelty, strengths, weaknesses, and an in-depth evaluation of the different proposed strategies. This culminates in the development of taxonomies that highlight key properties and contributions. Finally, we gather the reviewed studies along with their available open-source implementations at our \href{https://github.com/mindflow-institue/Awesome-Attention-Mechanism-in-Medical-Imaging}{GitHub}\footnote{\url{https://github.com/xmindflow/Awesome-Attention-Mechanism-in-Medical-Imaging}}. We aim to regularly update it with the most recent relevant papers.</p></p class="citation"></blockquote><h3 id=35--227271-vision-language-synthetic-data-enhances-echocardiography-downstream-tasks-pooria-ashrafian-et-al-2024>(3/5 | 227/271) Vision-Language Synthetic Data Enhances Echocardiography Downstream Tasks (Pooria Ashrafian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pooria Ashrafian, Milad Yazdani, Moein Heidari, Dena Shahriari, Ilker Hacihaliloglu. (2024)<br><strong>Vision-Language Synthetic Data Enhances Echocardiography Downstream Tasks</strong><br><button class=copy-to-clipboard title="Vision-Language Synthetic Data Enhances Echocardiography Downstream Tasks" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19880v1.pdf filename=2403.19880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-quality, large-scale data is essential for robust deep learning models in medical applications, particularly ultrasound image analysis. <b>Diffusion</b> <b>models</b> facilitate high-fidelity medical image generation, reducing the costs associated with acquiring and annotating new images. This paper utilizes recent <b>vision-language</b> models to produce diverse and realistic synthetic echocardiography image data, preserving key features of the original images guided by textual and semantic label maps. Specifically, we investigate three potential avenues: unconditional generation, generation guided by text, and a hybrid approach incorporating both textual and semantic supervision. We show that the rich contextual information present in the synthesized data potentially enhances the accuracy and interpretability of downstream tasks, such as echocardiography segmentation and classification with improved metrics and faster convergence. Our implementation with checkpoints, <b>prompts,</b> and the created synthetic dataset will be publicly available at \href{https://github.com/Pooria90/DiffEcho}{GitHub}.</p></p class="citation"></blockquote><h3 id=45--228271-single-shared-network-with-prior-inspired-loss-for-parameter-efficient-multi-modal-imaging-skin-lesion-classification-peng-tang-et-al-2024>(4/5 | 228/271) Single-Shared Network with Prior-Inspired Loss for Parameter-Efficient Multi-Modal Imaging Skin Lesion Classification (Peng Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Tang, Tobias Lasser. (2024)<br><strong>Single-Shared Network with Prior-Inspired Loss for Parameter-Efficient Multi-Modal Imaging Skin Lesion Classification</strong><br><button class=copy-to-clipboard title="Single-Shared Network with Prior-Inspired Loss for Parameter-Efficient Multi-Modal Imaging Skin Lesion Classification" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 26<br>Keywords: Convolutional Neural Network, Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19203v1.pdf filename=2403.19203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce a <b>multi-modal</b> approach that efficiently integrates multi-scale clinical and dermoscopy features within a single network, thereby substantially reducing model parameters. The proposed method includes three novel fusion schemes. Firstly, unlike current methods that usually employ two individual models for for clinical and dermoscopy modalities, we verified that <b>multimodal</b> feature can be learned by sharing the parameters of encoder while leaving the individual modal-specific classifiers. Secondly, the shared cross-attention module can replace the individual one to efficiently interact between two modalities at multiple layers. Thirdly, different from current methods that equally optimize dermoscopy and clinical branches, inspired by prior knowledge that dermoscopy images play a more significant role than clinical images, we propose a novel biased loss. This loss guides the single-shared network to prioritize dermoscopy information over clinical information, implicitly learning a better joint feature representation for the modal-specific task. Extensive experiments on a well-recognized Seven-Point Checklist (SPC) dataset and a collected dataset demonstrate the effectiveness of our method on both <b>CNN</b> and <b>Transformer</b> structures. Furthermore, our method exhibits superiority in both accuracy and model parameters compared to currently advanced methods.</p></p class="citation"></blockquote><h3 id=55--229271-brain-shift-unsupervised-pseudo-healthy-brain-synthesis-for-novel-biomarker-extraction-in-chronic-subdural-hematoma-baris-imre-et-al-2024>(5/5 | 229/271) Brain-Shift: Unsupervised Pseudo-Healthy Brain Synthesis for Novel Biomarker Extraction in Chronic Subdural Hematoma (Baris Imre et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baris Imre, Elina Thibeau-Sutre, Jorieke Reimer, Kuan Kho, Jelmer M. Wolterink. (2024)<br><strong>Brain-Shift: Unsupervised Pseudo-Healthy Brain Synthesis for Novel Biomarker Extraction in Chronic Subdural Hematoma</strong><br><button class=copy-to-clipboard title="Brain-Shift: Unsupervised Pseudo-Healthy Brain Synthesis for Novel Biomarker Extraction in Chronic Subdural Hematoma" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19415v1.pdf filename=2403.19415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chronic subdural hematoma (cSDH) is a common neurological condition characterized by the accumulation of blood between the brain and the dura mater. This accumulation of blood can exert pressure on the brain, potentially leading to fatal outcomes. Treatment options for cSDH are limited to invasive surgery or non-invasive management. Traditionally, the midline shift, hand-measured by experts from an ideal sagittal plane, and the hematoma volume have been the primary metrics for quantifying and analyzing cSDH. However, these approaches do not quantify the local 3D brain deformation caused by cSDH. We propose a novel method using anatomy-aware <b>unsupervised</b> diffeomorphic pseudo-healthy synthesis to generate brain deformation fields. The deformation fields derived from this process are utilized to extract biomarkers that quantify the shift in the brain due to cSDH. We use CT scans of 121 patients for training and validation of our method and find that our metrics allow the identification of patients who require surgery. Our results indicate that automatically obtained brain deformation fields might contain prognostic value for personalized cSDH treatment. Our implementation is available on: github.com/Barisimre/brain-morphing</p></p class="citation"></blockquote><h2 id=mathna-5>math.NA (5)</h2><h3 id=15--230271-an-ultra-high-speed-reproducing-kernel-particle-method-siavash-jafarzadeh-et-al-2024>(1/5 | 230/271) An Ultra-high-speed Reproducing Kernel Particle Method (Siavash Jafarzadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siavash Jafarzadeh, Michael Hillman. (2024)<br><strong>An Ultra-high-speed Reproducing Kernel Particle Method</strong><br><button class=copy-to-clipboard title="An Ultra-high-speed Reproducing Kernel Particle Method" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-CE, cs-NA, math-NA, math.NA<br>Keyword Score: 30<br>Keywords: Convolution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19854v1.pdf filename=2403.19854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, the fast-convolving reproducing kernel particle method (FC-RKPM) is introduced. This method is hundreds to millions of times faster than the traditional RKPM for 3D meshfree <b>simulations.</b> In this approach, the meshfree discretizations with RK approximation are expressed in terms of <b>convolution</b> sums. Fast Fourier transform (FFT) is then used to efficiently compute the <b>convolutions.</b> Certain modifications to the domain and shape functions are considered to maintain generality for complex geometries and arbitrary boundary conditions. The new method does not need to identify, store, and loop over the neighbors which is one of the bottleneck of the traditional meshfree methods. As a result, the run-times and memory allocations are independent of the number of neighbors and the shape function support size. As a model problem, the method is laid out for a Galerkin weak form of the Poisson problem with the RK approximation, and is verified in 1D, 2D, and 3D. Tables with run-times and allocated memory are presented to compare the performance of FC-RKPM with the traditional method in 3D. The performance is studied for various node numbers, support size, and approximation degree. All the implementation details and the roadmap for software development are also provided. Application of the new method to nonlinear and explicit problems are briefly discussed as well.</p></p class="citation"></blockquote><h3 id=25--231271-schrödingerisation-based-computationally-stable-algorithms-for-ill-posed-problems-in-partial-differential-equations-shi-jin-et-al-2024>(2/5 | 231/271) Schrödingerisation based computationally stable algorithms for ill-posed problems in partial differential equations (Shi Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shi Jin, Nana Liu, Chuwen Ma. (2024)<br><strong>Schrödingerisation based computationally stable algorithms for ill-posed problems in partial differential equations</strong><br><button class=copy-to-clipboard title="Schrödingerisation based computationally stable algorithms for ill-posed problems in partial differential equations" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19123v1.pdf filename=2403.19123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a simple and stable computational method for ill-posed partial differential equation (PDE) problems. The method is based on Schr"odingerisation, introduced in [S. Jin, N. Liu and Y. Yu, Phys. Rev. A, 108 (2023), 032603], which maps all linear PDEs into Schr"odinger-type equations in one higher dimension, for quantum <b>simulations</b> of these PDEs. Although the original problem is ill-posed, the Schr"odingerized equations are Hamiltonian systems and time-reversible, allowing stable computation both forward and backward in time. The original variable can be recovered by data from suitably chosen domain in the extended dimension. We will use the backward heat equation and the linear convection equation with imaginary wave speed as examples. Error analysis of these algorithms are conducted and verified numerically. The methods apply to both classical and quantum computers, and we also layout the quantum algorithms for these methods.</p></p class="citation"></blockquote><h3 id=35--232271-adaptive-optimization-of-isogeometric-multi-patch-discretizations-using-artificial-neural-networks-dany-rios-et-al-2024>(3/5 | 232/271) Adaptive optimization of isogeometric multi-patch discretizations using artificial neural networks (Dany Rios et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dany Rios, Felix Scholz, Thomas Takacs. (2024)<br><strong>Adaptive optimization of isogeometric multi-patch discretizations using artificial neural networks</strong><br><button class=copy-to-clipboard title="Adaptive optimization of isogeometric multi-patch discretizations using artificial neural networks" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N50, cs-NA, math-NA, math.NA<br>Keyword Score: 8<br>Keywords: Graph, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19286v1.pdf filename=2403.19286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In isogeometric analysis, isogeometric function spaces are employed for accurately representing the solution to a partial differential equation (PDE) on a parameterized domain. They are generated from a tensor-product spline space by composing the basis functions with the inverse of the parameterization. Depending on the <b>geometry</b> of the domain and on the data of the PDE, the solution might not have maximum Sobolev regularity, leading to a reduced convergence rate. In this case it is necessary to reduce the local mesh size close to the singularities. The classical approach is to perform adaptive h-refinement, which either leads to an unnecessarily large number of degrees of freedom or to a spline space that does not possess a tensor-product structure. Based on the concept of r-adaptivity we present a novel approach for finding a suitable isogeometric function space for a given PDE without sacrificing the tensor-product structure of the underlying spline space. In particular, we use the fact that different reparameterizations of the same computational domain lead to different isogeometric function spaces while preserving the <b>geometry.</b> Starting from a multi-patch domain consisting of bilinearly parameterized patches, we aim to find the biquadratic multi-patch parameterization that leads to the isogeometric function space with the smallest best approximation error of the solution. In order to estimate the location of the optimal control points, we employ a trained residual neural network that is applied to the <b>graph</b> surfaces of the approximated solution and its derivatives. In our experimental results, we observe that our new method results in a vast improvement of the approximation error for different PDE problems on multi-patch domains.</p></p class="citation"></blockquote><h3 id=45--233271-an-efficient-multiscale-multigrid-preconditioner-for-darcy-flow-in-high-contrast-media-changqing-ye-et-al-2024>(4/5 | 233/271) An efficient multiscale multigrid preconditioner for Darcy flow in high-contrast media (Changqing Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changqing Ye, Shubin Fu, Eric T. Chung, Jizu Huang. (2024)<br><strong>An efficient multiscale multigrid preconditioner for Darcy flow in high-contrast media</strong><br><button class=copy-to-clipboard title="An efficient multiscale multigrid preconditioner for Darcy flow in high-contrast media" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19342v1.pdf filename=2403.19342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we develop a multigrid preconditioner to solve Darcy flow in highly heterogeneous porous media. The key component of the preconditioner is to construct a sequence of nested subspaces $W_{\mathcal{L}}\subset W_{\mathcal{L}-1}\subset\cdots\subset W_1=W_h$. An appropriate spectral problem is defined in the space of $W_{i-1}$, then the eigenfunctions of the spectral problems are utilized to form $W_i$. The preconditioner is applied to solve a positive semidefinite linear system which results from discretizing the Darcy flow equation with the lowest order Raviart-Thomas spaces and adopting a trapezoidal quadrature rule. Theoretical analysis and numerical investigations of this preconditioner will be presented. In particular, we will consider several typical highly heterogeneous permeability fields whose resolutions are up to $1024^3$ and examine the computational performance of the preconditioner in several aspects, such as strong scalability, weak scalability, and robustness against the contrast of the media. We also demonstrate an application of this preconditioner for solving a two-phase flow <b>benchmark</b> problem.</p></p class="citation"></blockquote><h3 id=55--234271-a-unified-shtc-multiphase-model-of-continuum-mechanics-davide-ferrari-et-al-2024>(5/5 | 234/271) A unified SHTC multiphase model of continuum mechanics (Davide Ferrari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Davide Ferrari, Ilya Peshkov, Evgeniy Romenski, Michael Dumbser. (2024)<br><strong>A unified SHTC multiphase model of continuum mechanics</strong><br><button class=copy-to-clipboard title="A unified SHTC multiphase model of continuum mechanics" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, physics-flu-dyn<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19298v1.pdf filename=2403.19298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a unified nonequilibrium model of continuum mechanics for compressible multiphase flows. The model, which is formulated within the framework of Symmetric Hyperbolic Thermodynamically Compatible (SHTC) equations, can describe the arbitrary number of phases that can be heat-conducting inviscid and viscous fluids}, as well as elastoplastic solids. The phases are allowed to have different velocities, pressures, temperatures, and shear stresses, while the material interfaces are treated as diffuse interfaces with the volume fraction playing the role of the interface field. To relate our model to other multiphase approaches, we reformulate the SHTC governing equations in terms of the phase state parameters and put them in the form of Baer-Nunziato-type models. It is the Baer-Nunziato form of the SHTC equations which is then solved numerically using a robust second-order path-conservative MUSCL-Hancock finite volume method on Cartesian meshes. Due to the fact that the obtained governing equations are very challenging, we restrict our numerical examples to a simplified version of the model, focusing on the isentropic limit for three-phase mixtures. To address the stiffness properties of the relaxation source terms present in the model, the implemented scheme incorporates a semi-analytical time integration method specifically designed for the non-linear stiff source terms governing the strain relaxation. The validation process involves a wide range of <b>benchmarks</b> and several applications for compressible multiphase problems. Notably, results are presented for multiphase flows in all the relaxation limit cases of the model, including inviscid and viscous Newtonian fluids, as well as non-linear hyperelastic and elastoplastic solids.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--235271-tasr-a-novel-trust-aware-stackelberg-routing-algorithm-to-mitigate-traffic-congestion-doris-e-m-brown-et-al-2024>(1/1 | 235/271) TASR: A Novel Trust-Aware Stackelberg Routing Algorithm to Mitigate Traffic Congestion (Doris E. M. Brown et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Doris E. M. Brown, Venkata Sriram Siddhardh Nadendla, Sajal K. Das. (2024)<br><strong>TASR: A Novel Trust-Aware Stackelberg Routing Algorithm to Mitigate Traffic Congestion</strong><br><button class=copy-to-clipboard title="TASR: A Novel Trust-Aware Stackelberg Routing Algorithm to Mitigate Traffic Congestion" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 30<br>Keywords: Recommendation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19831v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19831v1.pdf filename=2403.19831v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stackelberg routing platforms (SRP) reduce congestion in one-shot traffic networks by proposing optimal route <b>recommendations</b> to selfish travelers. Traditionally, Stackelberg routing is cast as a partial control problem where a fraction of traveler flow complies with route <b>recommendations,</b> while the remaining respond as selfish travelers. In this paper, a novel Stackelberg routing framework is formulated where the agents exhibit \emph{probabilistic compliance} by accepting SRP&rsquo;s route <b>recommendations</b> with a \emph{trust} probability. A greedy \emph{\textbf{T}rust-\textbf{A}ware \textbf{S}tackelberg \textbf{R}outing} algorithm (in short, TASR) is proposed for SRP to compute unique path <b>recommendations</b> to each traveler flow with a unique demand. <b>Simulation</b> experiments are designed with random travel demands with diverse trust values on real road networks such as Sioux Falls, Chicago Sketch, and Sydney networks for both single-commodity and multi-commodity flows. The performance of TASR is compared with state-of-the-art Stackelberg routing methods in terms of traffic congestion and trust dynamics over repeated interaction between the SRP and the travelers. Results show that TASR improves network congestion without causing a significant reduction in trust towards the SRP, when compared to most well-known Stackelberg routing strategies.</p></p class="citation"></blockquote><h2 id=q-fintr-1>q-fin.TR (1)</h2><h3 id=11--236271-reinforcement-learning-in-agent-based-market-simulation-unveiling-realistic-stylized-facts-and-behavior-zhiyuan-yao-et-al-2024>(1/1 | 236/271) Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior (Zhiyuan Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Yao, Zheng Li, Matthew Thomas, Ionut Florescu. (2024)<br><strong>Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior</strong><br><button class=copy-to-clipboard title="Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.TR<br>Categories: cs-LG, cs-MA, q-fin-TR, q-fin.TR<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19781v1.pdf filename=2403.19781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Investors and regulators can greatly benefit from a realistic market simulator that enables them to anticipate the consequences of their decisions in real markets. However, traditional rule-based market simulators often fall short in accurately capturing the dynamic behavior of market participants, particularly in response to external market impact events or changes in the behavior of other participants. In this study, we explore an agent-based <b>simulation</b> framework employing <b>reinforcement</b> <b>learning</b> (RL) agents. We present the implementation details of these RL agents and demonstrate that the simulated market exhibits realistic stylized facts observed in real-world markets. Furthermore, we investigate the behavior of RL agents when confronted with external market impacts, such as a flash crash. Our findings shed light on the effectiveness and adaptability of RL-based agents within the <b>simulation,</b> offering insights into their response to significant market events.</p></p class="citation"></blockquote><h2 id=cshc-3>cs.HC (3)</h2><h3 id=13--237271-llms-as-academic-reading-companions-extending-hci-through-synthetic-personae-celia-chen-et-al-2024>(1/3 | 237/271) LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae (Celia Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Celia Chen, Alex Leitch. (2024)<br><strong>LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae</strong><br><button class=copy-to-clipboard title="LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Claude, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19506v1.pdf filename=2403.19506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This position paper argues that <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> constitute promising yet underutilized academic reading companions capable of enhancing learning. We detail an exploratory study examining <b>Claude.ai</b> from Anthropic, an <b>LLM-based</b> interactive assistant that helps students comprehend complex qualitative literature content. The study compares quantitative survey data and qualitative interviews assessing outcomes between a control group and an experimental group leveraging <b>Claude.ai</b> over a semester across two graduate courses. Initial findings demonstrate tangible improvements in reading comprehension and engagement among participants using the AI agent versus unsupported independent study. However, there is potential for overreliance and ethical considerations that warrant continued investigation. By documenting an early integration of an <b>LLM</b> reading companion into an educational context, this work contributes pragmatic insights to guide development of synthetic personae supporting learning. Broader impacts compel policy and industry actions to uphold responsible design in order to maximize benefits of AI integration while prioritizing student wellbeing.</p></p class="citation"></blockquote><h3 id=23--238271-im-categorizing-llm-as-a-productivity-tool-examining-ethics-of-llm-use-in-hci-research-practices-shivani-kapania-et-al-2024>(2/3 | 238/271) &lsquo;I&rsquo;m categorizing LLM as a productivity tool&rsquo;: Examining ethics of LLM use in HCI research practices (Shivani Kapania et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shivani Kapania, Ruiyi Wang, Toby Jia-Jun Li, Tianshi Li, Hong Shen. (2024)<br><strong>&lsquo;I&rsquo;m categorizing LLM as a productivity tool&rsquo;: Examining ethics of LLM use in HCI research practices</strong><br><button class=copy-to-clipboard title="'I'm categorizing LLM as a productivity tool': Examining ethics of LLM use in HCI research practices" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19876v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19876v1.pdf filename=2403.19876v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> are increasingly applied in real-world scenarios, including research and education. These models, however, come with well-known ethical issues, which may manifest in unexpected ways in human-computer interaction research due to the extensive engagement with human subjects. This paper reports on research practices related to <b>LLM</b> use, drawing on 16 semi-structured interviews and a survey conducted with 50 HCI researchers. We discuss the ways in which <b>LLMs</b> are already being utilized throughout the entire HCI research pipeline, from ideation to system development and paper writing. While researchers described nuanced understandings of ethical issues, they were rarely or only partially able to identify and address those ethical concerns in their own projects. This lack of action and reliance on workarounds was explained through the perceived lack of control and distributed responsibility in the <b>LLM</b> supply chain, the conditional nature of engaging with ethics, and competing priorities. Finally, we reflect on the implications of our findings and present opportunities to shape emerging norms of engaging with <b>large</b> <b>language</b> <b>models</b> in HCI research.</p></p class="citation"></blockquote><h3 id=33--239271-algorithmic-ways-of-seeing-using-object-detection-to-facilitate-art-exploration-louie-søs-meyer-et-al-2024>(3/3 | 239/271) Algorithmic Ways of Seeing: Using Object Detection to Facilitate Art Exploration (Louie Søs Meyer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Louie Søs Meyer, Johanne Engel Aaen, Anitamalina Regitse Tranberg, Peter Kun, Matthias Freiberger, Sebastian Risi, Anders Sundnes Løvlie. (2024)<br><strong>Algorithmic Ways of Seeing: Using Object Detection to Facilitate Art Exploration</strong><br><button class=copy-to-clipboard title="Algorithmic Ways of Seeing: Using Object Detection to Facilitate Art Exploration" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CV, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19174v1.pdf filename=2403.19174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This Research through Design paper explores how <b>object</b> <b>detection</b> may be applied to a large digital art museum collection to facilitate new ways of encountering and experiencing art. We present the design and evaluation of an interactive application called SMKExplore, which allows users to explore a museum&rsquo;s digital collection of paintings by browsing through <b>objects</b> <b>detected</b> in the images, as a novel form of open-ended exploration. We provide three contributions. First, we show how an <b>object</b> <b>detection</b> pipeline can be integrated into a design process for visual exploration. Second, we present the design and development of an app that enables exploration of an art museum&rsquo;s collection. Third, we offer reflections on future possibilities for museums and HCI researchers to incorporate <b>object</b> <b>detection</b> techniques into the digitalization of museums.</p></p class="citation"></blockquote><h2 id=eesssp-3>eess.SP (3)</h2><h3 id=13--240271-removing-the-need-for-ground-truth-uwb-data-collection-self-supervised-ranging-error-correction-using-deep-reinforcement-learning-dieter-coppens-et-al-2024>(1/3 | 240/271) Removing the need for ground truth UWB data collection: self-supervised ranging error correction using deep reinforcement learning (Dieter Coppens et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dieter Coppens, Ben Van Herbruggen, Adnan Shahid, Eli De Poorter. (2024)<br><strong>Removing the need for ground truth UWB data collection: self-supervised ranging error correction using deep reinforcement learning</strong><br><button class=copy-to-clipboard title="Removing the need for ground truth UWB data collection: self-supervised ranging error correction using deep reinforcement learning" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19262v1.pdf filename=2403.19262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Indoor positioning using UWB technology has gained interest due to its centimeter-level accuracy potential. However, multipath effects and non-line-of-sight conditions cause ranging errors between anchors and tags. Existing approaches for mitigating these ranging errors rely on collecting large labeled datasets, making them impractical for real-world deployments. This paper proposes a novel <b>self-supervised</b> deep <b>reinforcement</b> <b>learning</b> approach that does not require labeled ground truth data. A <b>reinforcement</b> <b>learning</b> agent uses the channel impulse response as a state and predicts corrections to minimize the error between corrected and estimated ranges. The agent learns, <b>self-supervised,</b> by iteratively improving corrections that are generated by combining the predictability of trajectories with filtering and smoothening. Experiments on real-world UWB measurements demonstrate comparable performance to state-of-the-art <b>supervised</b> methods, overcoming data dependency and lack of generalizability limitations. This makes <b>self-supervised</b> deep <b>reinforcement</b> <b>learning</b> a promising solution for practical and scalable UWB-ranging error correction.</p></p class="citation"></blockquote><h3 id=23--241271-optimal-pilot-design-for-otfs-in-linear-time-varying-channels-ids-van-der-werf-et-al-2024>(2/3 | 241/271) Optimal Pilot Design for OTFS in Linear Time-Varying Channels (Ids van der Werf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ids van der Werf, Richard Heusdens, Richard C. Hendriks, Geert Leus. (2024)<br><strong>Optimal Pilot Design for OTFS in Linear Time-Varying Channels</strong><br><button class=copy-to-clipboard title="Optimal Pilot Design for OTFS in Linear Time-Varying Channels" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19379v1.pdf filename=2403.19379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the positioning of the pilot symbols, as well as the power distribution between the pilot and the communication symbols in the OTFS modulation scheme. We analyze the pilot placements that minimize the mean squared error (MSE) in estimating the channel taps. In addition, we optimize the average channel capacity by adjusting the power balance. We show that this leads to a significant increase in average capacity. The results provide valuable guidance for designing the OTFS parameters to achieve maximum capacity. Numerical <b>simulations</b> are performed to validate the findings.</p></p class="citation"></blockquote><h3 id=33--242271-decentralizing-coherent-joint-transmission-precoding-via-fast-admm-with-deterministic-equivalents-xinyu-bian-et-al-2024>(3/3 | 242/271) Decentralizing Coherent Joint Transmission Precoding via Fast ADMM with Deterministic Equivalents (Xinyu Bian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Bian, Yuhao Liu, Yizhou Xu, Tianqi Hou, Wenjie Wang, Yuyi Mao, Jun Zhang. (2024)<br><strong>Decentralizing Coherent Joint Transmission Precoding via Fast ADMM with Deterministic Equivalents</strong><br><button class=copy-to-clipboard title="Decentralizing Coherent Joint Transmission Precoding via Fast ADMM with Deterministic Equivalents" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19127v1.pdf filename=2403.19127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inter-cell interference (ICI) suppression is critical for multi-cell multi-user networks. In this paper, we investigate advanced precoding techniques for coordinated multi-point (CoMP) with downlink coherent joint transmission, an effective approach for ICI suppression. Different from the centralized precoding schemes that require frequent information exchange among the cooperating base stations, we propose a decentralized scheme to minimize the total power consumption. In particular, based on the covariance matrices of global channel state information, we estimate the ICI bounds via the deterministic equivalents and decouple the original design problem into sub-problems, each of which can be solved in a decentralized manner. To solve the sub-problems at each base station, we develop a low-complexity solver based on the alternating direction method of multipliers (ADMM) in conjunction with the convex-concave procedure (CCCP). <b>Simulation</b> results demonstrate the effectiveness of our proposed decentralized precoding scheme, which achieves performance similar to the optimal centralized precoding scheme. Besides, our proposed ADMM solver can substantially reduce the computational complexity, while maintaining outstanding performance.</p></p class="citation"></blockquote><h2 id=cset-2>cs.ET (2)</h2><h3 id=12--243271-towards-reverse-engineering-the-brain-brain-derived-neuromorphic-computing-approach-with-photonic-electronic-and-ionic-dynamicity-in-3d-integrated-circuits-s-j-ben-yoo-et-al-2024>(1/2 | 243/271) Towards Reverse-Engineering the Brain: Brain-Derived Neuromorphic Computing Approach with Photonic, Electronic, and Ionic Dynamicity in 3D integrated circuits (S. J. Ben Yoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S. J. Ben Yoo, Luis El-Srouji, Suman Datta, Shimeng Yu, Jean Anne Incorvia, Alberto Salleo, Volker Sorger, Juejun Hu, Lionel C Kimerling, Kristofer Bouchard, Joy Geng, Rishidev Chaudhuri, Charan Ranganath, Randall O&rsquo;Reilly. (2024)<br><strong>Towards Reverse-Engineering the Brain: Brain-Derived Neuromorphic Computing Approach with Photonic, Electronic, and Ionic Dynamicity in 3D integrated circuits</strong><br><button class=copy-to-clipboard title="Towards Reverse-Engineering the Brain: Brain-Derived Neuromorphic Computing Approach with Photonic, Electronic, and Ionic Dynamicity in 3D integrated circuits" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cs-ET, cs-NE, cs.ET, physics-optics<br>Keyword Score: 30<br>Keywords: Self-supervised Learning, Self-supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19724v1.pdf filename=2403.19724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The human brain has immense learning capabilities at extreme energy efficiencies and scale that no artificial system has been able to match. For decades, reverse engineering the brain has been one of the top priorities of science and technology research. Despite numerous efforts, conventional electronics-based methods have failed to match the scalability, energy efficiency, and <b>self-supervised</b> <b>learning</b> capabilities of the human brain. On the other hand, very recent progress in the development of new generations of photonic and electronic memristive materials, device technologies, and 3D electronic-photonic integrated circuits (3D EPIC ) promise to realize new brain-derived neuromorphic systems with comparable connectivity, density, energy-efficiency, and scalability. When combined with bio-realistic learning algorithms and architectures, it may be possible to realize an &lsquo;artificial brain&rsquo; prototype with general self-learning capabilities. This paper argues the possibility of reverse-engineering the brain through architecting a prototype of a brain-derived neuromorphic computing system consisting of artificial electronic, ionic, photonic materials, devices, and circuits with dynamicity resembling the bio-plausible molecular, neuro/synaptic, neuro-circuit, and multi-structural hierarchical macro-circuits of the brain based on well-tested computational models. We further argue the importance of bio-plausible local learning algorithms applicable to the neuromorphic computing system that capture the flexible and adaptive <b>unsupervised</b> and <b>self-supervised</b> <b>learning</b> mechanisms central to human intelligence. Most importantly, we emphasize that the unique capabilities in brain-derived neuromorphic computing prototype systems will enable us to understand links between specific neuronal and network-level properties with system-level functioning and behavior.</p></p class="citation"></blockquote><h3 id=22--244271-a-noise-tolerant-resource-saving-probabilistic-binary-neural-network-implemented-by-the-sot-mram-compute-in-memory-system-yu-gu-et-al-2024>(2/2 | 244/271) A noise-tolerant, resource-saving probabilistic binary neural network implemented by the SOT-MRAM compute-in-memory system (Yu Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Gu, Puyang Huang, Tianhao Chen, Chenyi Fu, Aitian Chen, Shouzhong Peng, Xixiang Zhang, Xufeng Kou. (2024)<br><strong>A noise-tolerant, resource-saving probabilistic binary neural network implemented by the SOT-MRAM compute-in-memory system</strong><br><button class=copy-to-clipboard title="A noise-tolerant, resource-saving probabilistic binary neural network implemented by the SOT-MRAM compute-in-memory system" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: 94C60, B-2-4; B-3-0, cs-ET, cs-SY, cs.ET, eess-SY<br>Keyword Score: 20<br>Keywords: MNIST, Noise-tolerant<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19374v1.pdf filename=2403.19374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We report a spin-orbit torque(SOT) magnetoresistive random-access memory(MRAM)-based probabilistic binary neural network(PBNN) for resource-saving and hardware <b>noise-tolerant</b> computing applications. With the presence of thermal fluctuation, the non-destructive SOT-driven magnetization switching characteristics lead to a random weight matrix with controllable probability distribution. In the meanwhile, the proposed CIM architecture allows for the concurrent execution of the probabilistic vector-matrix multiplication (PVMM) and binarization. Furthermore, leveraging the effectiveness of random binary cells to propagate multi-bit probabilistic information, our SOT-MRAM-based PBNN system achieves a 97.78% classification accuracy under a 7.01% weight variation on the <b>MNIST</b> database through 10 sampling cycles, and the number of bit-level computation operations is reduced by a factor of 6.9 compared to that of the full-precision LeNet-5 network. Our work provides a compelling framework for the design of reliable neural networks tailored to the applications with low power consumption and limited computational resources.</p></p class="citation"></blockquote><h2 id=cond-matstat-mech-1>cond-mat.stat-mech (1)</h2><h3 id=11--245271-toward-practical-benchmarks-of-ising-machines-a-case-study-on-the-quadratic-knapsack-problem-kentaro-ohno-et-al-2024>(1/1 | 245/271) Toward Practical Benchmarks of Ising Machines: A Case Study on the Quadratic Knapsack Problem (Kentaro Ohno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kentaro Ohno, Tatsuhiko Shirai, Nozomu Togawa. (2024)<br><strong>Toward Practical Benchmarks of Ising Machines: A Case Study on the Quadratic Knapsack Problem</strong><br><button class=copy-to-clipboard title="Toward Practical Benchmarks of Ising Machines: A Case Study on the Quadratic Knapsack Problem" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.stat-mech<br>Categories: cond-mat-stat-mech, cond-mat.stat-mech, cs-ET<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19175v1.pdf filename=2403.19175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Combinatorial optimization has wide applications from industry to natural science. Ising machines bring an emerging computing paradigm for efficiently solving a combinatorial optimization problem by searching a ground state of a given Ising model. Current cutting-edge Ising machines achieve fast sampling of near-optimal solutions of the max-cut problem. However, for problems with additional constraint conditions, their advantages have been hardly shown due to difficulties in handling the constraints. The performance of Ising machines on such problems heavily depends on encoding methods of constraints into penalties, but the optimal choice is non-trivial. In this work, we focus on <b>benchmarks</b> of Ising machines on the quadratic knapsack problem (QKP). To bring out their practical performance, we propose to exploit the problem structure upon using Ising machines. Specifically, we apply fast two-stage post-processing to the outputs of Ising machines, which makes handling the constraint easier. <b>Simulation</b> on medium-sized test instances shows that the proposed method substantially improves the solving performance of Ising machines and the improvement is robust to a choice of the encoding methods. We evaluate an Ising machine called Amplify Annealing Engine with the proposed method and found that it achieves comparable results with existing heuristics.</p></p class="citation"></blockquote><h2 id=csit-8>cs.IT (8)</h2><h3 id=18--246271-integrated-communication-localization-and-sensing-in-6g-d-mimo-networks-hao-guo-et-al-2024>(1/8 | 246/271) Integrated Communication, Localization, and Sensing in 6G D-MIMO Networks (Hao Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Guo, Henk Wymeersch, Behrooz Makki, Hui Chen, Yibo Wu, Giuseppe Durisi, Musa Furkan Keskin, Mohammad H. Moghaddam, Charitha Madapatha, Han Yu, Peter Hammarberg, Hyowon Kim, Tommy Svensson. (2024)<br><strong>Integrated Communication, Localization, and Sensing in 6G D-MIMO Networks</strong><br><button class=copy-to-clipboard title="Integrated Communication, Localization, and Sensing in 6G D-MIMO Networks" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19785v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19785v1.pdf filename=2403.19785v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Future generations of mobile networks call for concurrent sensing and communication functionalities in the same hardware and/or spectrum. Compared to communication, sensing services often suffer from limited coverage, due to the high path loss of the reflected signal and the increased infrastructure requirements. To provide a more uniform quality of service, distributed multiple input multiple output (D-MIMO) systems deploy a large number of distributed nodes and efficiently control them, making distributed integrated sensing and communications (ISAC) possible. In this paper, we investigate ISAC in D-MIMO through the lens of different design architectures and deployments, revealing both conflicts and synergies. In addition, <b>simulation</b> and demonstration results reveal both opportunities and challenges towards the implementation of ISAC in D-MIMO.</p></p class="citation"></blockquote><h3 id=28--247271-transmissive-ris-transmitter-enabled-spatial-modulation-for-mimo-systems-xusheng-zhu-et-al-2024>(2/8 | 247/271) Transmissive RIS Transmitter Enabled Spatial Modulation for MIMO Systems (Xusheng Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xusheng Zhu, Qingqing Wu, Wen Chen. (2024)<br><strong>Transmissive RIS Transmitter Enabled Spatial Modulation for MIMO Systems</strong><br><button class=copy-to-clipboard title="Transmissive RIS Transmitter Enabled Spatial Modulation for MIMO Systems" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19457v1.pdf filename=2403.19457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a novel transmissive reconfigurable intelligent surface (TRIS) transmitter-enabled spatial modulation (SM) multiple-input multiple-output (MIMO) system. In the transmission phase, a column-wise activation strategy is implemented for the TRIS panel, where the specific column elements are activated per time slot. Concurrently, the receiver employs the maximum likelihood detection technique. Based on this, for the transmit signals, we derive the closed-form expressions for the upper bounds of the average bit error probability (ABEP) of the proposed scheme from different perspectives, employing both vector-based and element-based approaches. Furthermore, we provide the asymptotic closed-form expressions for the ABEP of the TRIS-SM scheme, as well as the diversity gain. To improve the performance of the proposed TRIS-SM system, we optimize ABEP with a fixed data rate. Additionally, we provide lower bounds to simplify the computational complexity of improved TRIS-SM scheme. The Monte Carlo <b>simulation</b> method is used to validate the theoretical derivations exhaustively. The results demonstrate that the proposed TRIS-SM scheme can achieve better ABEP performance compared to the conventional SM scheme. Furthermore, the improved TRIS-SM scheme outperforms the TRIS-SM scheme in terms of reliability.</p></p class="citation"></blockquote><h3 id=38--248271-o-ran-for-energy-efficient-serving-cluster-formulation-in-user-centric-cell-free-mmimo-marcin-hoffmann-et-al-2024>(3/8 | 248/271) O-RAN for Energy-Efficient Serving Cluster Formulation in User-Centric Cell-Free MMIMO (Marcin Hoffmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcin Hoffmann, Paweł Kryszkiewicz. (2024)<br><strong>O-RAN for Energy-Efficient Serving Cluster Formulation in User-Centric Cell-Free MMIMO</strong><br><button class=copy-to-clipboard title="O-RAN for Energy-Efficient Serving Cluster Formulation in User-Centric Cell-Free MMIMO" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-NI, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19449v1.pdf filename=2403.19449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The 6G Massive Multiple-Input Multiple-Output (MMIMO) networks can follow the so-called User-Centric Cell-Free (UCCF) architecture, where a single user is served by multiple Access Points (APs) coordinated by the Central Processing Unit (CPU). In this paper, we propose how O-RAN functionalities, i.e., rApp-xApp pair, can be used for energy-efficient Serving Cluster Formulation (SCF). <b>Simulation</b> studies show up to 37% gain in Energy Efficiency (EE) of the proposed solution over the state-of-the-art Network-Centric (NC) designs.</p></p class="citation"></blockquote><h3 id=48--249271-pilot-signal-and-channel-estimator-co-design-for-hybrid-field-xl-mimo-yoonseong-kang-et-al-2024>(4/8 | 249/271) Pilot Signal and Channel Estimator Co-Design for Hybrid-Field XL-MIMO (Yoonseong Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoonseong Kang, Hyowoon Seo, Wan Choi. (2024)<br><strong>Pilot Signal and Channel Estimator Co-Design for Hybrid-Field XL-MIMO</strong><br><button class=copy-to-clipboard title="Pilot Signal and Channel Estimator Co-Design for Hybrid-Field XL-MIMO" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19105v1.pdf filename=2403.19105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the intricate task of hybrid-field channel estimation in extremely large-scale MIMO (XL-MIMO) systems, critical for the progression of 6G communications. Within these systems, comprising a line-of-sight (LoS) channel component alongside far-field and near-field scattering channel components, our objective is to tackle the channel estimation challenge. We encounter two central hurdles for ensuring dependable sparse channel recovery: the design of pilot signals and channel estimators tailored for hybrid-field communications. To overcome the first challenge, we propose a method to derive optimal pilot signals, aimed at minimizing the mutual coherence of the sensing matrix within the context of compressive sensing (CS) problems. These optimal signals are derived using the alternating direction method of multipliers (ADMM), ensuring robust performance in sparse channel recovery. Additionally, leveraging the acquired optimal pilot signal, we introduce a two-stage channel estimation approach that sequentially estimates the LoS channel component and the hybrid-field scattering channel components. <b>Simulation</b> results attest to the superiority of our co-designed approach for pilot signal and channel estimation over conventional CS-based methods, providing more reliable sparse channel recovery in practical scenarios.</p></p class="citation"></blockquote><h3 id=58--250271-deep-csi-compression-for-dual-polarized-massive-mimo-channels-with-disentangled-representation-learning-suhang-fan-et-al-2024>(5/8 | 250/271) Deep CSI Compression for Dual-Polarized Massive MIMO Channels with Disentangled Representation Learning (Suhang Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suhang Fan, Wei Xu, Renjie Xie, Shi Jin, Derrick Wing Kwan Ng, Naofal Al-Dhahir. (2024)<br><strong>Deep CSI Compression for Dual-Polarized Massive MIMO Channels with Disentangled Representation Learning</strong><br><button class=copy-to-clipboard title="Deep CSI Compression for Dual-Polarized Massive MIMO Channels with Disentangled Representation Learning" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 15<br>Keywords: Quantization, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19185v1.pdf filename=2403.19185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Channel state information (CSI) feedback is critical for achieving the promised advantages of enhancing spectral and energy efficiencies in massive multiple-input multiple-output (MIMO) wireless communication systems. Deep learning (DL)-based methods have been proven effective in reducing the required signaling overhead for CSI feedback. In practical dual-polarized MIMO scenarios, channels in the vertical and horizontal polarization directions tend to exhibit high polarization correlation. To fully exploit the inherent propagation similarity within dual-polarized channels, we propose a disentangled <b>representation</b> <b>neural</b> network (NN) for CSI feedback, referred to as DiReNet. The proposed DiReNet disentangles dual-polarized CSI into three components: polarization-shared information, vertical polarization-specific information, and horizontal polarization-specific information. This disentanglement of dual-polarized CSI enables the minimization of information redundancy caused by the polarization correlation and improves the performance of CSI compression and recovery. Additionally, flexible <b>quantization</b> and network extension schemes are designed. Consequently, our method provides a pragmatic solution for CSI feedback to harness the physical MIMO polarization as a priori information. Our experimental results show that the performance of our proposed DiReNet surpasses that of existing DL-based networks, while also effectively reducing the number of network parameters by nearly one third.</p></p class="citation"></blockquote><h3 id=68--251271-cell-free-mimo-perceptive-mobile-networks-cloud-vs-edge-processing-seongah-jeong-et-al-2024>(6/8 | 251/271) Cell-Free MIMO Perceptive Mobile Networks: Cloud vs. Edge Processing (Seongah Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seongah Jeong, Jinkyu Kang, Osvaldo Simeone, Shlomo Shamai. (2024)<br><strong>Cell-Free MIMO Perceptive Mobile Networks: Cloud vs. Edge Processing</strong><br><button class=copy-to-clipboard title="Cell-Free MIMO Perceptive Mobile Networks: Cloud vs. Edge Processing" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19200v1.pdf filename=2403.19200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Perceptive mobile networks implement sensing and communication by reusing existing cellular infrastructure. Cell-free multiple-input multiple-output, thanks to the cooperation among distributed access points, supports the deployment of multistatic radar sensing, while providing high spectral efficiency for data communication services. To this end, the distributed access points communicate over fronthaul links with a central processing unit acting as a cloud processor. This work explores four different types of PMN uplink solutions based on Cell-free multiple-input multiple-output, in which the sensing and decoding functionalities are carried out at either cloud or edge. Accordingly, we investigate and compare joint cloud-based decoding and sensing (CDCS), hybrid cloud-based decoding and edge-based sensing (CDES), hybrid edge-based decoding and cloud-based sensing (EDCS) and edge-based decoding and sensing (EDES). In all cases, we target a unified design problem formulation whereby the fronthaul <b>quantization</b> of signals received in the training and data phases are jointly designed to maximize the achievable rate under sensing requirements and fronthaul capacity constraints. Via numerical results, the four implementation scenarios are compared as a function of the available fronthaul resources by highlighting the relative merits of edge- and cloud-based sensing and communications. This study provides guidelines on the optimal functional allocation in fronthaul-constrained networks implementing integrated sensing and communications.</p></p class="citation"></blockquote><h3 id=78--252271-co-designing-statistical-mimo-radar-and-in-band-full-duplex-multi-user-mimo-communications----part-ii-joint-precoder-radar-code-and-receive-filters-design-jiawei-liu-et-al-2024>(7/8 | 252/271) Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User MIMO Communications &ndash; Part II: Joint Precoder, Radar Code, and Receive Filters Design (Jiawei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Liu, Kumar Vijay Mishra, Mohammad Saquib. (2024)<br><strong>Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User MIMO Communications &ndash; Part II: Joint Precoder, Radar Code, and Receive Filters Design</strong><br><button class=copy-to-clipboard title="Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User MIMO Communications -- Part II: Joint Precoder, Radar Code, and Receive Filters Design" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19119v1.pdf filename=2403.19119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the challenge of spectral sharing between a statistical multiple-input multiple-output (MIMO) radar and an in-band full-duplex (IBFD) multi-user MIMO (MU-MIMO) communications system operating simultaneously in the same frequency band. Existing research on joint MIMO-radar-MIMO-communications (MRMC) systems has limitations, such as focusing on colocated MIMO radars, half-duplex MIMO communications, single-user scenarios, neglecting practical constraints, or employing separate transmit/receive units for MRMC coexistence. This paper, along with companion papers (Part I and III), proposes a comprehensive MRMC framework that addresses all these challenges. In the previous companion paper (Part I), we presented signal processing techniques for a distributed IBFD MRMC system. In this paper, we introduce joint design of statistical MIMO radar codes, uplink/downlink precoders, and corresponding receive filters using a novel metric called compounded-and-weighted sum <b>mutual</b> <b>information.</b> To solve the resulting highly non-convex problem, we employ a combination of block coordinate descent (BCD) and alternating projection methods. Numerical experiments show convergence of our algorithm, mitigation of uplink interference, and stable data rates under varying noise levels, channel estimate imperfections, and self-interference. The subsequent companion paper (Part III) extends the discussion to multiple targets and evaluates the tracking performance of our MRMC system.</p></p class="citation"></blockquote><h3 id=88--253271-on-the-performance-of-low-complexity-decoders-of-ldpc-and-polar-codes-qingqing-peng-et-al-2024>(8/8 | 253/271) On the Performance of Low-complexity Decoders of LDPC and Polar Codes (Qingqing Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingqing Peng, Dawei Yin, Dongxu Chang, Yuan Li, Huazi Zhang, Guiying Yan, Guanghui Wang. (2024)<br><strong>On the Performance of Low-complexity Decoders of LDPC and Polar Codes</strong><br><button class=copy-to-clipboard title="On the Performance of Low-complexity Decoders of LDPC and Polar Codes" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19266v1.pdf filename=2403.19266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient decoding is crucial to high-throughput and low-power wireless communication scenarios. A theoretical analysis of the performance-complexity tradeoff toward low-complexity decoding is required for a better understanding of the fundamental limits in the above-mentioned scenarios. This study aims to explore the performance of decoders with complexity constraints. Specifically, we investigate the performance of LDPC codes with different numbers of belief-propagation iterations and the performance of polar codes with an SSC decoder. We found that the asymptotic error rates of both polar codes and LDPC codes are functions of complexity $T$ and code length $N$, in the form of $2^{-a2^{b\frac{T}{N}}}$, where $a$ and $b$ are constants that depend on channel and coding schemes. Our analysis reveals the different performance-complexity tradeoffs for LDPC and polar codes. The results indicate that if one aims to further enhance the decoding efficiency for LDPC codes, the key lies in how to efficiently pass messages on the factor <b>graph.</b> In terms of decoding efficiency, polar codes asymptotically outperform $(J, K)$-regular LDPC codes with a code rate $R \le 1-\frac{J(J-1)}{2^J+(J-1)}$ in the low-complexity regime $(T \le O(NlogN))$.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--254271-collaborative-interactive-evolution-of-art-in-the-latent-space-of-deep-generative-models-ole-hall-et-al-2024>(1/2 | 254/271) Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models (Ole Hall et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ole Hall, Anil Yaman. (2024)<br><strong>Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models</strong><br><button class=copy-to-clipboard title="Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-CV, cs-HC, cs-LG, cs-NE, cs.NE<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19620v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19620v1.pdf filename=2403.19620v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> have shown great success in generating high quality images and are thus used as one of the main approaches to generate art images. However, usually the image generation process involves sampling from the latent space of the learned art representations, allowing little control over the output. In this work, we first employ <b>GANs</b> that are trained to produce creative images using an architecture known as Creative Adversarial Networks (CANs), then, we employ an evolutionary approach to navigate within the latent space of the models to discover images. We use automatic aesthetic and collaborative interactive human evaluation metrics to assess the generated images. In the human interactive evaluation case, we propose a collaborative evaluation based on the assessments of several participants. Furthermore, we also experiment with an intelligent mutation operator that aims to improve the quality of the images through local search based on an aesthetic measure. We evaluate the effectiveness of this approach by comparing the results produced by the automatic and collaborative interactive evolution. The results show that the proposed approach can generate highly attractive art images when the evolution is guided by collaborative human feedback.</p></p class="citation"></blockquote><h3 id=22--255271-evolving-assembly-code-in-an-adversarial-environment-irina-maliukov-et-al-2024>(2/2 | 255/271) Evolving Assembly Code in an Adversarial Environment (Irina Maliukov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Irina Maliukov, Gera Weiss, Oded Margalit, Achiya Elyasaf. (2024)<br><strong>Evolving Assembly Code in an Adversarial Environment</strong><br><button class=copy-to-clipboard title="Evolving Assembly Code in an Adversarial Environment" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19489v1.pdf filename=2403.19489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we evolve assembly code for the CodeGuru competition. The competition&rsquo;s goal is to create a survivor &ndash; an assembly program that runs the longest in shared memory, by resisting attacks from adversary survivors and finding their weaknesses. For evolving top-notch solvers, we specify a Backus Normal Form (BNF) for the assembly language and synthesize the code from scratch using Genetic Programming (GP). We evaluate the survivors by running CodeGuru games against human-written winning survivors. Our evolved programs found weaknesses in the programs they were trained against and utilized them. In addition, we compare our approach with a <b>Large-Language</b> <b>Model,</b> <b>demonstrating</b> that the latter cannot generate a survivor that can win at any competition. This work has important applications for cyber-security, as we utilize evolution to detect weaknesses in survivors. The assembly BNF is domain-independent; thus, by modifying the fitness function, it can detect code weaknesses and help fix them. Finally, the CodeGuru competition offers a novel platform for analyzing GP and code evolution in adversarial environments. To support further research in this direction, we provide a thorough qualitative analysis of the evolved survivors and the weaknesses found.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--256271-physics-informed-neural-networks-for-satellite-state-estimation-jacob-varey-et-al-2024>(1/1 | 256/271) Physics-Informed Neural Networks for Satellite State Estimation (Jacob Varey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacob Varey, Jessica D. Ruprecht, Michael Tierney, Ryan Sullenberger. (2024)<br><strong>Physics-Informed Neural Networks for Satellite State Estimation</strong><br><button class=copy-to-clipboard title="Physics-Informed Neural Networks for Satellite State Estimation" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-IM, astro-ph.IM, cs-AI, cs-LG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19736v1.pdf filename=2403.19736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Space Domain Awareness (SDA) community routinely tracks satellites in orbit by fitting an orbital state to observations made by the Space Surveillance Network (SSN). In order to fit such orbits, an accurate model of the forces that are acting on the satellite is required. Over the past several decades, high-quality, physics-based models have been developed for satellite state estimation and propagation. These models are exceedingly good at estimating and propagating orbital states for non-maneuvering satellites; however, there are several classes of anomalous accelerations that a satellite might experience which are not well-modeled, such as satellites that use low-thrust electric propulsion to modify their orbit. Physics-Informed Neural Networks (PINNs) are a valuable tool for these classes of satellites as they combine physics models with Deep Neural Networks (DNNs), which are highly expressive and versatile function approximators. By combining a physics model with a DNN, the machine learning model need not learn astrodynamics, which results in more efficient and effective utilization of machine learning resources. This paper details the application of PINNs to estimate the orbital state and a continuous, low-amplitude anomalous acceleration profile for satellites. The PINN is trained to learn the unknown acceleration by minimizing the mean square error of observations. We evaluate the performance of pure physics models with PINNs in terms of their observation residuals and their propagation accuracy beyond the fit span of the observations. For a two-day <b>simulation</b> of a GEO satellite using an unmodeled acceleration profile on the order of $10^{-8} \text{ km/s}^2$, the PINN outperformed the best-fit physics model by orders of magnitude for both observation residuals (123 arcsec vs 1.00 arcsec) as well as propagation accuracy (3860 km vs 164 km after five days).</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--257271-simulating-relational-event-histories----why-and-how-rumana-lakdawala-et-al-2024>(1/1 | 257/271) Simulating Relational Event Histories &ndash; Why and How (Rumana Lakdawala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rumana Lakdawala, Joris Mulder, Roger Leenders. (2024)<br><strong>Simulating Relational Event Histories &ndash; Why and How</strong><br><button class=copy-to-clipboard title="Simulating Relational Event Histories -- Why and How" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI, stat-ME<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19329v1.pdf filename=2403.19329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many important social phenomena result from repeated interactions among individuals over time such as email exchanges in an organization, or face-to-face interactions in a classroom. Insights into the mechanisms underlying the dynamics of these interactions can be achieved through <b>simulations</b> of networks on a fine temporal granularity. In this paper, we present statistical frameworks to simulate relational event networks under dyadic and actor-oriented relational event models. These simulators have a broad applicability in temporal social network research such as model fit assessment, theory building, network intervention planning, making predictions, understanding the impact of network structures, to name a few. We show this in three extensive applications. First, it is shown why <b>simulation-based</b> techniques are crucial for relational event model assessment, for example to investigate how past events affect future interactions in the network. Second, we demonstrate how <b>simulation</b> techniques contribute to a better understanding of the longevity of network interventions. Third, we show how <b>simulation</b> techniques are important when building and extending theories about social phenomena such as understanding social identity dynamics using optimal distinctiveness theory.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--258271-mil2-efficient-cloth-simulation-using-non-distance-barriers-and-subspace-reuse-lei-lan-et-al-2024>(1/1 | 258/271) Mil2: Efficient Cloth Simulation Using Non-distance Barriers and Subspace Reuse (Lei Lan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Lan, Zixuan Lu, Jingyi Long, Chun Yuan, Xuan Li, Xiaowei He, Huamin Wang, Chenfanfu Jiang, Yin Yang. (2024)<br><strong>Mil2: Efficient Cloth Simulation Using Non-distance Barriers and Subspace Reuse</strong><br><button class=copy-to-clipboard title="Mil2: Efficient Cloth Simulation Using Non-distance Barriers and Subspace Reuse" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19272v1.pdf filename=2403.19272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mil2 pushes the performance of high-resolution cloth <b>simulation,</b> making the <b>simulation</b> interactive (in milliseconds) for models with one million degrees of freedom (DOFs) while keeping every triangle untangled. The guarantee of being penetration-free is inspired by the interior-point method, which converts the inequality constraints to barrier potentials. Nevertheless, we propose a major overhaul of this modality by defining a novel and simple barrier formulation which does not depend on the distance between mesh primitives. Such a non-distance barrier model allows a new way to integrate collision detection into the <b>simulation</b> pipeline. Another contributor to the performance boost comes from the so-called subspace reuse strategy. This is based on the observation that low-frequency strain vibrations are near orthogonal to the deformation induced by collisions or self-collisions, often of high frequency. Subspace reuse then takes care of low-frequency residuals, while high-frequency residuals can also be effectively smoothed by GPU-based iterative solvers. We show that our method outperforms existing fast cloth simulators by nearly one order while keeping the entire <b>simulation</b> penetration-free and producing high-equality animations of high-resolution models.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--259271-dataflow-aware-pim-enabled-manycore-architecture-for-deep-learning-workloads-harsh-sharma-et-al-2024>(1/1 | 259/271) Dataflow-Aware PIM-Enabled Manycore Architecture for Deep Learning Workloads (Harsh Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harsh Sharma, Gaurav Narang, Janardhan Rao Doppa, Umit Ogras, Partha Pratim Pande. (2024)<br><strong>Dataflow-Aware PIM-Enabled Manycore Architecture for Deep Learning Workloads</strong><br><button class=copy-to-clipboard title="Dataflow-Aware PIM-Enabled Manycore Architecture for Deep Learning Workloads" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AI, cs-AR, cs-ET, cs.AR<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19073v1.pdf filename=2403.19073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Processing-in-memory (PIM) has emerged as an enabler for the energy-efficient and high-performance acceleration of deep learning (DL) workloads. Resistive random-access memory (ReRAM) is one of the most promising technologies to implement PIM. However, as the complexity of Deep <b>convolutional</b> <b>neural</b> <b>networks</b> (DNNs) grows, we need to design a manycore architecture with multiple ReRAM-based processing elements (PEs) on a single chip. Existing PIM-based architectures mostly focus on computation while ignoring the role of communication. ReRAM-based tiled manycore architectures often involve many Processing Elements (PEs), which need to be interconnected via an efficient on-chip communication infrastructure. Simply allocating more resources (ReRAMs) to speed up only computation is ineffective if the communication infrastructure cannot keep up with it. In this paper, we highlight the design principles of a dataflow-aware PIM-enabled manycore platform tailor-made for various types of DL workloads. We consider the design challenges with both 2.5D interposer- and 3D integration-enabled architectures.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--260271-algorithmic-strategies-for-finding-the-best-tsp-2-opt-move-in-average-sub-quadratic-time-giuseppe-lancia-et-al-2024>(1/2 | 260/271) Algorithmic strategies for finding the best TSP 2-OPT move in average sub-quadratic time (Giuseppe Lancia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giuseppe Lancia, Paolo Vidoni. (2024)<br><strong>Algorithmic strategies for finding the best TSP 2-OPT move in average sub-quadratic time</strong><br><button class=copy-to-clipboard title="Algorithmic strategies for finding the best TSP 2-OPT move in average sub-quadratic time" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 13<br>Keywords: Graph, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19878v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19878v1.pdf filename=2403.19878v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We describe an exact algorithm for finding the best 2-OPT move which, experimentally, was observed to be much faster than the standard quadratic approach. To analyze its average-case complexity, we introduce a family of heuristic procedures and discuss their complexity when applied to a random tour in <b>graphs</b> whose edge costs are either uniform random numbers in [0, 1] or Euclidean distances between random points in the plane. We prove that, for any probability p: (i) there is a heuristic in the family which can find the best move with probability at least p in average-time O(n^3/2) for uniform instances and O(n) for Euclidean instances; (ii) the exact algorithm take lesser time then the above heuristic on all instances on which the heuristic finds the best move. During local search, while the tour becomes less and less random, the speed of our algorithm worsens until it becomes quadratic. We then discuss how to fine tune a successful hybrid approach, made of our algorithm in the beginning followed by the usual quadratic enumeration.</p></p class="citation"></blockquote><h3 id=22--261271-finding-decision-tree-splits-in-streaming-and-massively-parallel-models-huy-pham-et-al-2024>(2/2 | 261/271) Finding Decision Tree Splits in Streaming and Massively Parallel Models (Huy Pham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huy Pham, Hoang Ta, Hoa T. Vu. (2024)<br><strong>Finding Decision Tree Splits in Streaming and Massively Parallel Models</strong><br><button class=copy-to-clipboard title="Finding Decision Tree Splits in Streaming and Massively Parallel Models" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-AI, cs-DS, cs-LG, cs.DS<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19867v1.pdf filename=2403.19867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we provide data stream algorithms that compute optimal splits in decision tree learning. In particular, given a data stream of observations $x_i$ and their labels $y_i$, the goal is to find the optimal split point $j$ that divides the data into two sets such that the mean squared error (for regression) or misclassification rate (for classification) is minimized. We provide various fast streaming algorithms that use sublinear space and a small number of passes for these problems. These algorithms can also be extended to the massively parallel computation model. Our work, while not directly comparable, complements the seminal work of Domingos and Hulten <b>(KDD</b> 2000).</p></p class="citation"></blockquote><h2 id=cslo-2>cs.LO (2)</h2><h3 id=12--262271-eda-driven-preprocessing-for-sat-solving-zhengyuan-shi-et-al-2024>(1/2 | 262/271) EDA-Driven Preprocessing for SAT Solving (Zhengyuan Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyuan Shi, Tiebing Tang, Sadaf Khan, Hui-Ling Zhen, Mingxuan Yuan, Zhufei Chu, Qiang Xu. (2024)<br><strong>EDA-Driven Preprocessing for SAT Solving</strong><br><button class=copy-to-clipboard title="EDA-Driven Preprocessing for SAT Solving" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19446v1.pdf filename=2403.19446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective formulation of problems into Conjunctive Normal Form (CNF) is critical in modern Boolean Satisfiability (SAT) solving for optimizing solver performance. Addressing the limitations of existing methods, our Electronic Design Automation (EDA)-driven preprocessing framework introduces a novel methodology for preparing SAT instances, leveraging both circuit and CNF formats for enhanced flexibility and efficiency. Central to our approach is the integration of a new logic synthesis technique, guided by a <b>reinforcement</b> <b>learning</b> agent, and a novel cost-customized LUT mapping strategy, enabling efficient handling of diverse SAT challenges. By transforming the SAT competition <b>benchmarks</b> into circuit instances, our framework demonstrates substantial performance improvements, as evidenced by a 52.42% reduction on average compared to solving directly. Moreover, our framework achieves a remarkable 96.14% runtime reduction on average for a set of logic equivalence checking problems that exhibit inherent circuit structures. These results highlight the effectiveness and versatility of our approach in handling both CNF and circuit instances. The code is available at <a href=https://github.com/cure-lab/EDA4SAT>https://github.com/cure-lab/EDA4SAT</a>.</p></p class="citation"></blockquote><h3 id=22--263271-linear-programming-in-isabellehol-julian-parsert-2024>(2/2 | 263/271) Linear Programming in Isabelle/HOL (Julian Parsert, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julian Parsert. (2024)<br><strong>Linear Programming in Isabelle/HOL</strong><br><button class=copy-to-clipboard title="Linear Programming in Isabelle/HOL" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19639v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19639v1.pdf filename=2403.19639v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Linear programming describes the problem of optimising a linear objective function over a set of constraints on its variables. In this paper we present a solver for linear programs implemented in the proof assistant Isabelle/HOL. This allows formally proving its soundness, termination, and other properties. We base these results on a previous formalisation of the simplex algorithm which does not take optimisation problems into account. Using the weak duality theorem of linear programming we obtain an algorithm for solving linear programs. Using Isabelle&rsquo;s <b>code</b> <b>generation</b> mechanism we can generate an external solver for linear programs.</p></p class="citation"></blockquote><h2 id=mathct-1>math.CT (1)</h2><h3 id=11--264271-generalized-gradient-descent-is-a-hypergraph-functor-tyler-hanks-et-al-2024>(1/1 | 264/271) Generalized Gradient Descent is a Hypergraph Functor (Tyler Hanks et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tyler Hanks, Matthew Klawonn, James Fairbanks. (2024)<br><strong>Generalized Gradient Descent is a Hypergraph Functor</strong><br><button class=copy-to-clipboard title="Generalized Gradient Descent is a Hypergraph Functor" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CT<br>Categories: cs-LG, math-CT, math.CT<br>Keyword Score: 10<br>Keywords: Parameter Sharing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19845v1.pdf filename=2403.19845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cartesian reverse derivative categories (CRDCs) provide an axiomatic generalization of the reverse derivative, which allows generalized analogues of classic optimization algorithms such as gradient descent to be applied to a broad class of problems. In this paper, we show that generalized gradient descent with respect to a given CRDC induces a hypergraph functor from a hypergraph category of optimization problems to a hypergraph category of dynamical systems. The domain of this functor consists of objective functions that are 1) general in the sense that they are defined with respect to an arbitrary CRDC, and 2) open in that they are decorated spans that can be composed with other such objective functions via variable sharing. The codomain is specified analogously as a category of general and open dynamical systems for the underlying CRDC. We describe how the hypergraph functor induces a distributed optimization algorithm for arbitrary composite problems specified in the domain. To illustrate the kinds of problems our framework can model, we show that <b>parameter</b> <b>sharing</b> models in multitask learning, a prevalent machine learning paradigm, yield a composite optimization problem for a given choice of CRDC. We then apply the gradient descent functor to this composite problem and describe the resulting distributed gradient descent algorithm for training <b>parameter</b> <b>sharing</b> models.</p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=12--265271-a-framework-for-time-varying-optimization-via-derivative-estimation-matteo-marchi-et-al-2024>(1/2 | 265/271) A Framework for Time-Varying Optimization via Derivative Estimation (Matteo Marchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Marchi, Jonathan Bunton, João Pedro Silvestre, Paulo Tabuada. (2024)<br><strong>A Framework for Time-Varying Optimization via Derivative Estimation</strong><br><button class=copy-to-clipboard title="A Framework for Time-Varying Optimization via Derivative Estimation" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19088v1.pdf filename=2403.19088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimization algorithms have a rich and fundamental relationship with ordinary differential equations given by its <b>continuous-time</b> <b>limit.</b> When the cost function varies with time &ndash; typically in response to a dynamically changing environment &ndash; online optimization becomes a <b>continuous-time</b> <b>trajectory</b> tracking problem. To accommodate these time variations, one typically requires some inherent knowledge about their nature such as a time derivative. In this paper, we propose a novel construction and analysis of a <b>continuous-time</b> <b>derivative</b> estimation scheme based on &ldquo;dirty-derivatives&rdquo;, and show how it naturally interfaces with <b>continuous-time</b> <b>optimization</b> algorithms using the language of ISS (Input-to-State Stability). More generally, we show how a simple Lyapunov redesign technique leads to provable suboptimality guarantees when composing this estimator with any well-behaved optimization algorithm for time-varying costs.</p></p class="citation"></blockquote><h3 id=22--266271-fisher-rao-gradient-flows-of-linear-programs-and-state-action-natural-policy-gradients-johannes-müller-et-al-2024>(2/2 | 266/271) Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural Policy Gradients (Johannes Müller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johannes Müller, Semih Çaycı, Guido Montúfar. (2024)<br><strong>Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural Policy Gradients</strong><br><button class=copy-to-clipboard title="Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural Policy Gradients" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: 65K05, 90C05, 90C08, 90C40, 90C53, cs-LG, cs-NA, cs-SY, eess-SY, math-NA, math-OC, math.OC, stat-ML<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19448v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19448v1.pdf filename=2403.19448v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Kakade&rsquo;s natural policy gradient method has been studied extensively in the last years showing linear convergence with and without regularization. We study another natural gradient method which is based on the Fisher information matrix of the state-action distributions and has received little attention from the theoretical side. Here, the state-action distributions follow the Fisher-Rao gradient flow inside the state-action polytope with respect to a linear potential. Therefore, we study Fisher-Rao gradient flows of linear programs more generally and show linear convergence with a rate that depends on the <b>geometry</b> of the linear program. Equivalently, this yields an estimate on the error induced by entropic regularization of the linear program which improves existing results. We extend these results and show sublinear convergence for perturbed Fisher-Rao gradient flows and natural gradient flows up to an approximation error. In particular, these general results cover the case of state-action natural policy gradients.</p></p class="citation"></blockquote><h2 id=statml-1>stat.ML (1)</h2><h3 id=11--267271-maximum-likelihood-estimation-on-stochastic-blockmodels-for-directed-graph-clustering-mihai-cucuringu-et-al-2024>(1/1 | 267/271) Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering (Mihai Cucuringu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mihai Cucuringu, Xiaowen Dong, Ning Zhang. (2024)<br><strong>Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering</strong><br><button class=copy-to-clipboard title="Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, cs-SI, math-ST, stat-ML, stat-TH, stat.ML<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19516v1.pdf filename=2403.19516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies the directed <b>graph</b> <b>clustering</b> problem through the lens of statistics, where we formulate <b>clustering</b> as estimating underlying communities in the directed stochastic block model (DSBM). We conduct the maximum likelihood estimation (MLE) on the DSBM and thereby ascertain the most probable community assignment given the observed <b>graph</b> structure. In addition to the statistical point of view, we further establish the equivalence between this MLE formulation and a novel flow optimization heuristic, which jointly considers two important directed <b>graph</b> statistics: edge density and edge orientation. Building on this new formulation of directed <b>clustering,</b> we introduce two efficient and interpretable directed <b>clustering</b> algorithms, a spectral <b>clustering</b> algorithm and a semidefinite programming based <b>clustering</b> algorithm. We provide a theoretical upper bound on the number of misclustered vertices of the spectral <b>clustering</b> algorithm using tools from matrix perturbation theory. We compare, both quantitatively and qualitatively, our proposed algorithms with existing directed <b>clustering</b> methods on both synthetic and real-world data, thus providing further ground to our theoretical contributions.</p></p class="citation"></blockquote><h2 id=mathat-1>math.AT (1)</h2><h3 id=11--268271-topological-optimal-transport-for-geometric-cycle-matching-stephen-y-zhang-et-al-2024>(1/1 | 268/271) Topological Optimal Transport for Geometric Cycle Matching (Stephen Y Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephen Y Zhang, Michael P H Stumpf, Tom Needham, Agnese Barbensi. (2024)<br><strong>Topological Optimal Transport for Geometric Cycle Matching</strong><br><button class=copy-to-clipboard title="Topological Optimal Transport for Geometric Cycle Matching" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.AT<br>Categories: 55N31, 62R40, 49Q15, 49Q22, 05C65, cs-CG, math-AT, math-MG, math.AT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19097v1.pdf filename=2403.19097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Topological data analysis is a powerful tool for describing topological signatures in real world data. An important challenge in topological data analysis is matching significant topological signals across distinct systems. In <b>geometry</b> and probability theory, optimal transport formalises notions of distance and matchings between distributions and structured objects. We propose to combine these approaches, constructing a mathematical framework for optimal transport-based matchings of topological features. Building upon recent advances in the domains of persistent homology and optimal transport for hypergraphs, we develop a transport-based methodology for topological data processing. We define measure topological networks, which integrate both geometric and topological information about a system, introduce a distance on the space of these objects, and study its metric properties, showing that it induces a geodesic metric space of non-negative curvature. The resulting Topological Optimal Transport (TpOT) framework provides a transport model on point clouds that minimises topological distortion while simultaneously yielding a geometrically informed matching between persistent homology cycles.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--269271-piercing-independent-sets-in-graphs-without-large-induced-matching-jiangdong-ai-et-al-2024>(1/1 | 269/271) Piercing independent sets in graphs without large induced matching (Jiangdong Ai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiangdong Ai, Hong Liu, Zixiang Xu, Qiang Zhou. (2024)<br><strong>Piercing independent sets in graphs without large induced matching</strong><br><button class=copy-to-clipboard title="Piercing independent sets in graphs without large induced matching" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-CG, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19737v1.pdf filename=2403.19737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a <b>graph</b> $G$, denote by $h(G)$ the smallest size of a subset of $V(G)$ which intersects every maximum independent set of $G$. We prove that any <b>graph</b> $G$ without induced matching of size $t$ satisfies $h(G)\le \omega(G)^{3t-3+o(1)}$. This resolves a conjecture of Hajebi, Li and Spirkl (Hitting all maximum stable sets in $P_{5}$-free <b>graphs,</b> JCTB 2024).</p></p class="citation"></blockquote><h2 id=mathpr-1>math.PR (1)</h2><h3 id=11--270271-random-multi-type-spanning-forests-for-synchronization-on-sparse-graphs-hugo-jaquard-et-al-2024>(1/1 | 270/271) Random Multi-Type Spanning Forests for Synchronization on Sparse Graphs (Hugo Jaquard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hugo Jaquard, Pierre-Olivier Amblard, Simon Barthelmé, Nicolas Tremblay. (2024)<br><strong>Random Multi-Type Spanning Forests for Synchronization on Sparse Graphs</strong><br><button class=copy-to-clipboard title="Random Multi-Type Spanning Forests for Synchronization on Sparse Graphs" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.PR<br>Categories: cs-DS, math-PR, math-ST, math.PR, stat-TH<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19300v1.pdf filename=2403.19300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Random diffusions are a popular tool in Monte-Carlo estimations, with well established algorithms such as Walk-on-Spheres (WoS) going back several decades. In this work, we introduce diffusion estimators for the problems of angular synchronization and smoothing on <b>graphs,</b> in the presence of a rotation associated to each edge. Unlike classical WoS algorithms, these estimators allow for global estimations by propagating along the branches of multi-type spanning forests, and we show that they can outperform standard numerical-linear-algebra solvers in challenging instances, depending on the topology and density of the <b>graph.</b></p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--271271-unifaas-programming-across-distributed-cyberinfrastructure-with-federated-function-serving-yifei-li-et-al-2024>(1/1 | 271/271) UniFaaS: Programming across Distributed Cyberinfrastructure with Federated Function Serving (Yifei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Li, Ryan Chard, Yadu Babuji, Kyle Chard, Ian Foster, Zhuozhao Li. (2024)<br><strong>UniFaaS: Programming across Distributed Cyberinfrastructure with Federated Function Serving</strong><br><button class=copy-to-clipboard title="UniFaaS: Programming across Distributed Cyberinfrastructure with Federated Function Serving" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19257v1.pdf filename=2403.19257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern scientific applications are increasingly decomposable into individual functions that may be deployed across distributed and diverse cyberinfrastructure such as supercomputers, clouds, and accelerators. Such applications call for new approaches to programming, distributed execution, and function-level management. We present UniFaaS, a parallel programming framework that relies on a federated function-as-a-service (FaaS) model to enable composition of distributed, scalable, and high-performance scientific workflows, and to support fine-grained function-level management. UniFaaS provides a unified programming interface to compose dynamic task <b>graphs</b> with transparent wide-area data management. UniFaaS exploits an observe-predict-decide approach to efficiently map workflow tasks to target heterogeneous and dynamic resources. We propose a dynamic heterogeneity-aware scheduling algorithm that employs a delay mechanism and a re-scheduling mechanism to accommodate dynamic resource capacity. Our experiments show that UniFaaS can efficiently execute workflows across computing resources with minimal scheduling overhead. We show that UniFaaS can improve the performance of a real-world drug screening workflow by as much as 22.99% when employing an additional 19.48% of resources and a montage workflow by 54.41% when employing an additional 47.83% of resources across multiple distributed clusters, in contrast to using a single cluster</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.29</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.31</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-51>cs.CL (51)</a><ul><li><a href=#151--1271-factoid-factual-entailment-for-hallucination-detection-vipula-rawte-et-al-2024>(1/51 | 1/271) FACTOID: FACtual enTailment fOr hallucInation Detection (Vipula Rawte et al., 2024)</a></li><li><a href=#251--2271-mfort-qa-multi-hop-few-shot-open-rich-table-question-answering-che-guan-et-al-2024>(2/51 | 2/271) MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering (Che Guan et al., 2024)</a></li><li><a href=#351--3271-jdocqa-japanese-document-question-answering-dataset-for-generative-language-models-eri-onami-et-al-2024>(3/51 | 3/271) JDocQA: Japanese Document Question Answering Dataset for Generative Language Models (Eri Onami et al., 2024)</a></li><li><a href=#451--4271-ungrammatical-syntax-based-in-context-example-selection-for-grammatical-error-correction-chenming-tang-et-al-2024>(4/51 | 4/271) Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction (Chenming Tang et al., 2024)</a></li><li><a href=#551--5271-retrieval-enhanced-knowledge-editing-for-multi-hop-question-answering-in-language-models-yucheng-shi-et-al-2024>(5/51 | 5/271) Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models (Yucheng Shi et al., 2024)</a></li><li><a href=#651--6271-mixed-preference-optimization-reinforcement-learning-with-data-selection-and-better-reference-model-qi-gou-et-al-2024>(6/51 | 6/271) Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model (Qi Gou et al., 2024)</a></li><li><a href=#751--7271-going-beyond-word-matching-syntax-improves-in-context-example-selection-for-machine-translation-chenming-tang-et-al-2024>(7/51 | 7/271) Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation (Chenming Tang et al., 2024)</a></li><li><a href=#851--8271-disentangling-length-from-quality-in-direct-preference-optimization-ryan-park-et-al-2024>(8/51 | 8/271) Disentangling Length from Quality in Direct Preference Optimization (Ryan Park et al., 2024)</a></li><li><a href=#951--9271-multi-stage-multi-modal-pre-training-for-automatic-speech-recognition-yash-jain-et-al-2024>(9/51 | 9/271) Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition (Yash Jain et al., 2024)</a></li><li><a href=#1051--10271-ethiomt-parallel-corpus-for-low-resource-ethiopian-languages-atnafu-lambebo-tonja-et-al-2024>(10/51 | 10/271) EthioMT: Parallel Corpus for Low-resource Ethiopian Languages (Atnafu Lambebo Tonja et al., 2024)</a></li><li><a href=#1151--11271-fine-tuning-language-models-with-reward-learning-on-policy-hao-lang-et-al-2024>(11/51 | 11/271) Fine-Tuning Language Models with Reward Learning on Policy (Hao Lang et al., 2024)</a></li><li><a href=#1251--12271-a-benchmark-evaluation-of-clinical-named-entity-recognition-in-french-nesrine-bannour-et-al-2024>(12/51 | 12/271) A Benchmark Evaluation of Clinical Named Entity Recognition in French (Nesrine Bannour et al., 2024)</a></li><li><a href=#1351--13271-developing-healthcare-language-model-embedding-spaces-niall-taylor-et-al-2024>(13/51 | 13/271) Developing Healthcare Language Model Embedding Spaces (Niall Taylor et al., 2024)</a></li><li><a href=#1451--14271-improving-vietnamese-english-medical-machine-translation-nhu-vo-et-al-2024>(14/51 | 14/271) Improving Vietnamese-English Medical Machine Translation (Nhu Vo et al., 2024)</a></li><li><a href=#1551--15271-interpreting-key-mechanisms-of-factual-recall-in-transformer-based-language-models-ang-lv-et-al-2024>(15/51 | 15/271) Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models (Ang Lv et al., 2024)</a></li><li><a href=#1651--16271-hgt-leveraging-heterogeneous-graph-enhanced-large-language-models-for-few-shot-complex-table-understanding-rihui-jin-et-al-2024>(16/51 | 16/271) HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding (Rihui Jin et al., 2024)</a></li><li><a href=#1751--17271-gold-generalized-knowledge-distillation-via-out-of-distribution-guided-language-data-generation-mohsen-gholami-et-al-2024>(17/51 | 17/271) GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation (Mohsen Gholami et al., 2024)</a></li><li><a href=#1851--18271-waterjudge-quality-detection-trade-off-when-watermarking-large-language-models-piotr-molenda-et-al-2024>(18/51 | 18/271) WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models (Piotr Molenda et al., 2024)</a></li><li><a href=#1951--19271-checkpoint-merging-via-bayesian-optimization-in-llm-pretraining-deyuan-liu-et-al-2024>(19/51 | 19/271) Checkpoint Merging via Bayesian Optimization in LLM Pretraining (Deyuan Liu et al., 2024)</a></li><li><a href=#2051--20271-mugc-machine-generated-versus-user-generated-content-detection-yaqi-xie-et-al-2024>(20/51 | 20/271) MUGC: Machine Generated versus User Generated Content Detection (Yaqi Xie et al., 2024)</a></li><li><a href=#2151--21271-a-tulu-resource-for-machine-translation-manu-narayanan-et-al-2024>(21/51 | 21/271) A Tulu Resource for Machine Translation (Manu Narayanan et al., 2024)</a></li><li><a href=#2251--22271-compressing-large-language-models-by-streamlining-the-unimportant-layer-xiaodong-chen-et-al-2024>(22/51 | 22/271) Compressing Large Language Models by Streamlining the Unimportant Layer (Xiaodong Chen et al., 2024)</a></li><li><a href=#2351--23271-bp4er-bootstrap-prompting-for-explicit-reasoning-in-medical-dialogue-generation-yuhong-he-et-al-2024>(23/51 | 23/271) BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation (Yuhong He et al., 2024)</a></li><li><a href=#2451--24271-naijahate-evaluating-hate-speech-detection-on-nigerian-twitter-using-representative-data-manuel-tonneau-et-al-2024>(24/51 | 24/271) NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data (Manuel Tonneau et al., 2024)</a></li><li><a href=#2551--25271-mitigating-misleading-chain-of-thought-reasoning-with-selective-filtering-yexin-wu-et-al-2024>(25/51 | 25/271) Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering (Yexin Wu et al., 2024)</a></li><li><a href=#2651--26271-learning-from-correctness-without-prompting-makes-llm-efficient-reasoner-yuxuan-yao-et-al-2024>(26/51 | 26/271) Learning From Correctness Without Prompting Makes LLM Efficient Reasoner (Yuxuan Yao et al., 2024)</a></li><li><a href=#2751--27271-risk-prediction-of-pathological-gambling-on-social-media-angelina-parfenova-et-al-2024>(27/51 | 27/271) Risk prediction of pathological gambling on social media (Angelina Parfenova et al., 2024)</a></li><li><a href=#2851--28271-large-language-models-are-unconscious-of-unreasonability-in-math-problems-jingyuan-ma-et-al-2024>(28/51 | 28/271) Large Language Models Are Unconscious of Unreasonability in Math Problems (Jingyuan Ma et al., 2024)</a></li><li><a href=#2951--29271-tablellm-enabling-tabular-data-manipulation-by-llms-in-real-office-usage-scenarios-xiaokang-zhang-et-al-2024>(29/51 | 29/271) TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios (Xiaokang Zhang et al., 2024)</a></li><li><a href=#3051--30271-code-comparison-tuning-for-code-large-language-models-yufan-jiang-et-al-2024>(30/51 | 30/271) Code Comparison Tuning for Code Large Language Models (Yufan Jiang et al., 2024)</a></li><li><a href=#3151--31271-kazparc-kazakh-parallel-corpus-for-machine-translation-rustem-yeshpanov-et-al-2024>(31/51 | 31/271) KazParC: Kazakh Parallel Corpus for Machine Translation (Rustem Yeshpanov et al., 2024)</a></li><li><a href=#3251--32271-beyond-borders-investigating-cross-jurisdiction-transfer-in-legal-case-summarization-t-y-s-s-santosh-et-al-2024>(32/51 | 32/271) Beyond Borders: Investigating Cross-Jurisdiction Transfer in Legal Case Summarization (T. Y. S. S Santosh et al., 2024)</a></li><li><a href=#3351--33271-mateval-a-multi-agent-discussion-framework-for-advancing-open-ended-text-evaluation-yu-li-et-al-2024>(33/51 | 33/271) MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation (Yu Li et al., 2024)</a></li><li><a href=#3451--34271-sdpo-dont-use-your-data-all-at-once-dahyun-kim-et-al-2024>(34/51 | 34/271) sDPO: Don&rsquo;t Use Your Data All at Once (Dahyun Kim et al., 2024)</a></li><li><a href=#3551--35271-star-gate-teaching-language-models-to-ask-clarifying-questions-chinmaya-andukuri-et-al-2024>(35/51 | 35/271) STaR-GATE: Teaching Language Models to Ask Clarifying Questions (Chinmaya Andukuri et al., 2024)</a></li><li><a href=#3651--36271-jamba-a-hybrid-transformer-mamba-language-model-opher-lieber-et-al-2024>(36/51 | 36/271) Jamba: A Hybrid Transformer-Mamba Language Model (Opher Lieber et al., 2024)</a></li><li><a href=#3751--37271-new-semantic-task-for-the-french-spoken-language-understanding-media-benchmark-nadège-alavoine-et-al-2024>(37/51 | 37/271) New Semantic Task for the French Spoken Language Understanding MEDIA Benchmark (Nadège Alavoine et al., 2024)</a></li><li><a href=#3851--38271-target-span-detection-for-implicit-harmful-content-nazanin-jafari-et-al-2024>(38/51 | 38/271) Target Span Detection for Implicit Harmful Content (Nazanin Jafari et al., 2024)</a></li><li><a href=#3951--39271-language-models-learn-rare-phenomena-from-less-rare-phenomena-the-case-of-the-missing-aanns-kanishka-misra-et-al-2024>(39/51 | 39/271) Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs (Kanishka Misra et al., 2024)</a></li><li><a href=#4051--40271-dataverse-open-source-etl-extract-transform-load-pipeline-for-large-language-models-hyunbyung-park-et-al-2024>(40/51 | 40/271) Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large Language Models (Hyunbyung Park et al., 2024)</a></li><li><a href=#4151--41271-kazsandra-kazakh-sentiment-analysis-dataset-of-reviews-and-attitudes-rustem-yeshpanov-et-al-2024>(41/51 | 41/271) KazSAnDRA: Kazakh Sentiment Analysis Dataset of Reviews and Attitudes (Rustem Yeshpanov et al., 2024)</a></li><li><a href=#4251--42271-knowledge-boundary-and-persona-dynamic-shape-a-better-social-media-agent-junkai-zhou-et-al-2024>(42/51 | 42/271) Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent (Junkai Zhou et al., 2024)</a></li><li><a href=#4351--43271-collaborative-knowledge-infusion-for-low-resource-stance-detection-ming-yan-et-al-2024>(43/51 | 43/271) Collaborative Knowledge Infusion for Low-resource Stance Detection (Ming Yan et al., 2024)</a></li><li><a href=#4451--44271-empirical-analysis-for-unsupervised-universal-dependency-parse-tree-aggregation-adithya-kulkarni-et-al-2024>(44/51 | 44/271) Empirical Analysis for Unsupervised Universal Dependency Parse Tree Aggregation (Adithya Kulkarni et al., 2024)</a></li><li><a href=#4551--45271-localizing-paragraph-memorization-in-language-models-niklas-stoehr-et-al-2024>(45/51 | 45/271) Localizing Paragraph Memorization in Language Models (Niklas Stoehr et al., 2024)</a></li><li><a href=#4651--46271-improving-adversarial-data-collection-by-supporting-annotators-lessons-from-gahd-a-german-hate-speech-dataset-janis-goldzycher-et-al-2024>(46/51 | 46/271) Improving Adversarial Data Collection by Supporting Annotators: Lessons from GAHD, a German Hate Speech Dataset (Janis Goldzycher et al., 2024)</a></li><li><a href=#4751--47271-phonetic-segmentation-of-the-ucla-phonetics-lab-archive-eleanor-chodroff-et-al-2024>(47/51 | 47/271) Phonetic Segmentation of the UCLA Phonetics Lab Archive (Eleanor Chodroff et al., 2024)</a></li><li><a href=#4851--48271-a-diverse-multilingual-news-headlines-dataset-from-around-the-world-felix-leeb-et-al-2024>(48/51 | 48/271) A diverse Multilingual News Headlines Dataset from around the World (Felix Leeb et al., 2024)</a></li><li><a href=#4951--49271-mineland-simulating-large-scale-multi-agent-interactions-with-limited-multimodal-senses-and-physical-needs-xianhao-yu-et-al-2024>(49/51 | 49/271) MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs (Xianhao Yu et al., 2024)</a></li><li><a href=#5051--50271-j-cre3-a-japanese-conversation-dataset-for-real-world-reference-resolution-nobuhiro-ueda-et-al-2024>(50/51 | 50/271) J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution (Nobuhiro Ueda et al., 2024)</a></li><li><a href=#5151--51271-semantic-map-based-generation-of-navigation-instructions-chengzu-li-et-al-2024>(51/51 | 51/271) Semantic Map-based Generation of Navigation Instructions (Chengzu Li et al., 2024)</a></li></ul></li><li><a href=#csir-8>cs.IR (8)</a><ul><li><a href=#18--52271-are-large-language-models-good-at-utility-judgments-hengran-zhang-et-al-2024>(1/8 | 52/271) Are Large Language Models Good at Utility Judgments? (Hengran Zhang et al., 2024)</a></li><li><a href=#28--53271-generate-then-retrieve-conversational-response-retrieval-using-llms-as-answer-and-query-generators-zahra-abbasiantaeb-et-al-2024>(2/8 | 53/271) Generate then Retrieve: Conversational Response Retrieval Using LLMs as Answer and Query Generators (Zahra Abbasiantaeb et al., 2024)</a></li><li><a href=#38--54271-make-large-language-model-a-better-ranker-wenshuo-chao-et-al-2024>(3/8 | 54/271) Make Large Language Model a Better Ranker (Wenshuo Chao et al., 2024)</a></li><li><a href=#48--55271-instruction-based-hypergraph-pretraining-mingdai-yang-et-al-2024>(4/8 | 55/271) Instruction-based Hypergraph Pretraining (Mingdai Yang et al., 2024)</a></li><li><a href=#58--56271-dealing-with-missing-modalities-in-multimodal-recommendation-a-feature-propagation-based-approach-daniele-malitesta-et-al-2024>(5/8 | 56/271) Dealing with Missing Modalities in Multimodal Recommendation: a Feature Propagation-based Approach (Daniele Malitesta et al., 2024)</a></li><li><a href=#68--57271-breaking-the-length-barrier-llm-enhanced-ctr-prediction-in-long-textual-user-behaviors-binzong-geng-et-al-2024>(6/8 | 57/271) Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors (Binzong Geng et al., 2024)</a></li><li><a href=#78--58271-intelligent-classification-and-personalized-recommendation-of-e-commerce-products-based-on-machine-learning-kangming-xu-et-al-2024>(7/8 | 58/271) Intelligent Classification and Personalized Recommendation of E-commerce Products Based on Machine Learning (Kangming Xu et al., 2024)</a></li><li><a href=#88--59271-enhanced-bayesian-personalized-ranking-for-robust-hard-negative-sampling-in-recommender-systems-kexin-shi-et-al-2024>(8/8 | 59/271) Enhanced Bayesian Personalized Ranking for Robust Hard Negative Sampling in Recommender Systems (Kexin Shi et al., 2024)</a></li></ul></li><li><a href=#csni-3>cs.NI (3)</a><ul><li><a href=#13--60271-chattracer-large-language-model-powered-real-time-bluetooth-device-tracking-system-qijun-wang-et-al-2024>(1/3 | 60/271) ChatTracer: Large Language Model Powered Real-time Bluetooth Device Tracking System (Qijun Wang et al., 2024)</a></li><li><a href=#23--61271-deep-learning-based-modulation-classification-of-practical-ofdm-signals-for-spectrum-sensing-byungjun-kim-et-al-2024>(2/3 | 61/271) Deep Learning-based Modulation Classification of Practical OFDM Signals for Spectrum Sensing (Byungjun Kim et al., 2024)</a></li><li><a href=#33--62271-performance-evaluation-of-ieee-80211bf-protocol-in-the-sub-7-ghz-band-anirudha-sahoo-et-al-2024>(3/3 | 62/271) Performance Evaluation of IEEE 802.11bf Protocol in the sub-7 GHz Band (Anirudha Sahoo et al., 2024)</a></li></ul></li><li><a href=#cscv-82>cs.CV (82)</a><ul><li><a href=#182--63271-multi-frame-lightweight--efficient-vision-language-models-for-question-answering-in-autonomous-driving-akshay-gopalkrishnan-et-al-2024>(1/82 | 63/271) Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving (Akshay Gopalkrishnan et al., 2024)</a></li><li><a href=#282--64271-ivlmap-instance-aware-visual-language-grounding-for-consumer-robot-navigation-jiacui-huang-et-al-2024>(2/82 | 64/271) IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation (Jiacui Huang et al., 2024)</a></li><li><a href=#382--65271-text-data-centric-image-captioning-with-interactive-prompts-yiyu-wang-et-al-2024>(3/82 | 65/271) Text Data-Centric Image Captioning with Interactive Prompts (Yiyu Wang et al., 2024)</a></li><li><a href=#482--66271-plug-and-play-grounding-of-reasoning-in-multimodal-large-language-models-jiaxing-chen-et-al-2024>(4/82 | 66/271) Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models (Jiaxing Chen et al., 2024)</a></li><li><a href=#582--67271-automated-black-box-prompt-engineering-for-personalized-text-to-image-generation-yutong-he-et-al-2024>(5/82 | 67/271) Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation (Yutong He et al., 2024)</a></li><li><a href=#682--68271-img2loc-revisiting-image-geolocalization-using-multi-modality-foundation-models-and-image-based-retrieval-augmented-generation-zhongliang-zhou-et-al-2024>(6/82 | 68/271) Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation (Zhongliang Zhou et al., 2024)</a></li><li><a href=#782--69271-patch-spatio-temporal-relation-prediction-for-video-anomaly-detection-hao-shen-et-al-2024>(7/82 | 69/271) Patch Spatio-Temporal Relation Prediction for Video Anomaly Detection (Hao Shen et al., 2024)</a></li><li><a href=#882--70271-jointly-training-and-pruning-cnns-via-learnable-agent-guidance-and-alignment-alireza-ganjdanesh-et-al-2024>(8/82 | 70/271) Jointly Training and Pruning CNNs via Learnable Agent Guidance and Alignment (Alireza Ganjdanesh et al., 2024)</a></li><li><a href=#982--71271-zero-shot-prompt-based-video-encoder-for-surgical-gesture-recognition-mingxing-rao-et-al-2024>(9/82 | 71/271) Zero-shot Prompt-based Video Encoder for Surgical Gesture Recognition (Mingxing Rao et al., 2024)</a></li><li><a href=#1082--72271-rsmamba-remote-sensing-image-classification-with-state-space-model-keyan-chen-et-al-2024>(10/82 | 72/271) RSMamba: Remote Sensing Image Classification with State Space Model (Keyan Chen et al., 2024)</a></li><li><a href=#1182--73271-gantastic-gan-based-transfer-of-interpretable-directions-for-disentangled-image-editing-in-text-to-image-diffusion-models-yusuf-dalva-et-al-2024>(11/82 | 73/271) GANTASTIC: GAN-based Transfer of Interpretable Directions for Disentangled Image Editing in Text-to-Image Diffusion Models (Yusuf Dalva et al., 2024)</a></li><li><a href=#1282--74271-enhance-image-classification-via-inter-class-image-mixup-with-diffusion-model-zhicai-wang-et-al-2024>(12/82 | 74/271) Enhance Image Classification via Inter-Class Image Mixup with Diffusion Model (Zhicai Wang et al., 2024)</a></li><li><a href=#1382--75271-clap4clip-continual-learning-with-probabilistic-finetuning-for-vision-language-models-saurav-jha-et-al-2024>(13/82 | 75/271) CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models (Saurav Jha et al., 2024)</a></li><li><a href=#1482--76271-omniparser-a-unified-framework-for-text-spotting-key-information-extraction-and-table-recognition-jianqiang-wan-et-al-2024>(14/82 | 76/271) OmniParser: A Unified Framework for Text Spotting, Key Information Extraction and Table Recognition (Jianqiang Wan et al., 2024)</a></li><li><a href=#1582--77271-de-confounded-data-free-knowledge-distillation-for-handling-distribution-shifts-yuzheng-wang-et-al-2024>(15/82 | 77/271) De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts (Yuzheng Wang et al., 2024)</a></li><li><a href=#1682--78271-interdreamer-zero-shot-text-to-3d-dynamic-human-object-interaction-sirui-xu-et-al-2024>(16/82 | 78/271) InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction (Sirui Xu et al., 2024)</a></li><li><a href=#1782--79271-siamese-vision-transformers-are-scalable-audio-visual-learners-yan-bo-lin-et-al-2024>(17/82 | 79/271) Siamese Vision Transformers are Scalable Audio-visual Learners (Yan-Bo Lin et al., 2024)</a></li><li><a href=#1882--80271-crkd-enhanced-camera-radar-object-detection-with-cross-modality-knowledge-distillation-lingjun-zhao-et-al-2024>(18/82 | 80/271) CRKD: Enhanced Camera-Radar Object Detection with Cross-modality Knowledge Distillation (Lingjun Zhao et al., 2024)</a></li><li><a href=#1982--81271-mveb-self-supervised-learning-with-multi-view-entropy-bottleneck-liangjian-wen-et-al-2024>(19/82 | 81/271) MVEB: Self-Supervised Learning with Multi-View Entropy Bottleneck (Liangjian Wen et al., 2024)</a></li><li><a href=#2082--82271-low-rank-rescaled-vision-transformer-fine-tuning-a-residual-design-approach-wei-dong-et-al-2024>(20/82 | 82/271) Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach (Wei Dong et al., 2024)</a></li><li><a href=#2182--83271-magiclens-self-supervised-image-retrieval-with-open-ended-instructions-kai-zhang-et-al-2024>(21/82 | 83/271) MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions (Kai Zhang et al., 2024)</a></li><li><a href=#2282--84271-the-bad-batches-enhancing-self-supervised-learning-in-image-classification-through-representative-batch-curation-ozgu-goksu-et-al-2024>(22/82 | 84/271) The Bad Batches: Enhancing Self-Supervised Learning in Image Classification Through Representative Batch Curation (Ozgu Goksu et al., 2024)</a></li><li><a href=#2382--85271-enet-21-an-optimized-light-cnn-structure-for-lane-detection-seyed-rasoul-hosseini-et-al-2024>(23/82 | 85/271) ENet-21: An Optimized light CNN Structure for Lane Detection (Seyed Rasoul Hosseini et al., 2024)</a></li><li><a href=#2482--86271-test-time-domain-generalization-for-face-anti-spoofing-qianyu-zhou-et-al-2024>(24/82 | 86/271) Test-Time Domain Generalization for Face Anti-Spoofing (Qianyu Zhou et al., 2024)</a></li><li><a href=#2582--87271-is-synthetic-image-useful-for-transfer-learning-an-investigation-into-data-generation-volume-and-utilization-yuhang-li-et-al-2024>(25/82 | 87/271) Is Synthetic Image Useful for Transfer Learning? An Investigation into Data Generation, Volume, and Utilization (Yuhang Li et al., 2024)</a></li><li><a href=#2682--88271-x-mic-cross-modal-instance-conditioning-for-egocentric-action-generalization-anna-kukleva-et-al-2024>(26/82 | 88/271) X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization (Anna Kukleva et al., 2024)</a></li><li><a href=#2782--89271-mist-mitigating-intersectional-bias-with-disentangled-cross-attention-editing-in-text-to-image-diffusion-models-hidir-yesiltepe-et-al-2024>(27/82 | 89/271) MIST: Mitigating Intersectional Bias with Disentangled Cross-Attention Editing in Text-to-Image Diffusion Models (Hidir Yesiltepe et al., 2024)</a></li><li><a href=#2882--90271-densenets-reloaded-paradigm-shift-beyond-resnets-and-vits-donghyun-kim-et-al-2024>(28/82 | 90/271) DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs (Donghyun Kim et al., 2024)</a></li><li><a href=#2982--91271-break-for-make-modular-low-rank-adaptations-for-composable-content-style-customization-yu-xu-et-al-2024>(29/82 | 91/271) Break-for-Make: Modular Low-Rank Adaptations for Composable Content-Style Customization (Yu Xu et al., 2024)</a></li><li><a href=#3082--92271-hypergraph-based-multi-view-action-recognition-using-event-cameras-yue-gao-et-al-2024>(30/82 | 92/271) Hypergraph-based Multi-View Action Recognition using Event Cameras (Yue Gao et al., 2024)</a></li><li><a href=#3182--93271-sparse-generation-making-pseudo-labels-sparse-for-weakly-supervision-with-points-tian-ma-et-al-2024>(31/82 | 93/271) Sparse Generation: Making Pseudo Labels Sparse for weakly supervision with points (Tian Ma et al., 2024)</a></li><li><a href=#3282--94271-taming-lookup-tables-for-efficient-image-retouching-sidi-yang-et-al-2024>(32/82 | 94/271) Taming Lookup Tables for Efficient Image Retouching (Sidi Yang et al., 2024)</a></li><li><a href=#3382--95271-poco-a-self-supervised-approach-via-polar-transformation-based-progressive-contrastive-learning-for-ophthalmic-disease-diagnosis-jinhong-wang-et-al-2024>(33/82 | 95/271) PoCo: A Self-Supervised Approach via Polar Transformation Based Progressive Contrastive Learning for Ophthalmic Disease Diagnosis (Jinhong Wang et al., 2024)</a></li><li><a href=#3482--96271-aapmt-agi-assessment-through-prompt-and-metric-transformer-benhao-huang-2024>(34/82 | 96/271) AAPMT: AGI Assessment Through Prompt and Metric Transformer (Benhao Huang, 2024)</a></li><li><a href=#3582--97271-towards-multimodal-video-paragraph-captioning-models-robust-to-missing-modality-sishuo-chen-et-al-2024>(35/82 | 97/271) Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality (Sishuo Chen et al., 2024)</a></li><li><a href=#3682--98271-mmcert-provable-defense-against-adversarial-attacks-to-multi-modal-models-yanting-wang-et-al-2024>(36/82 | 98/271) MMCert: Provable Defense against Adversarial Attacks to Multi-modal Models (Yanting Wang et al., 2024)</a></li><li><a href=#3782--99271-moditalker-motion-disentangled-diffusion-model-for-high-fidelity-talking-head-generation-seyeon-kim-et-al-2024>(37/82 | 99/271) MoDiTalker: Motion-Disentangled Diffusion Model for High-Fidelity Talking Head Generation (Seyeon Kim et al., 2024)</a></li><li><a href=#3882--100271-qncd-quantization-noise-correction-for-diffusion-models-huanpeng-chu-et-al-2024>(38/82 | 100/271) QNCD: Quantization Noise Correction for Diffusion Models (Huanpeng Chu et al., 2024)</a></li><li><a href=#3982--101271-detecting-image-attribution-for-text-to-image-diffusion-models-in-rgb-and-beyond-katherine-xu-et-al-2024>(39/82 | 101/271) Detecting Image Attribution for Text-to-Image Diffusion Models in RGB and Beyond (Katherine Xu et al., 2024)</a></li><li><a href=#4082--102271-change-agent-towards-interactive-comprehensive-change-interpretation-and-analysis-from-change-detection-and-change-captioning-chenyang-liu-et-al-2024>(40/82 | 102/271) Change-Agent: Towards Interactive Comprehensive Change Interpretation and Analysis from Change Detection and Change Captioning (Chenyang Liu et al., 2024)</a></li><li><a href=#4182--103271-situation-awareness-for-driver-centric-driving-style-adaptation-johann-haselberger-et-al-2024>(41/82 | 103/271) Situation Awareness for Driver-Centric Driving Style Adaptation (Johann Haselberger et al., 2024)</a></li><li><a href=#4282--104271-cross-attention-is-not-always-needed-dynamic-cross-attention-for-audio-visual-dimensional-emotion-recognition-r-gnana-praveen-et-al-2024>(42/82 | 104/271) Cross-Attention is Not Always Needed: Dynamic Cross-Attention for Audio-Visual Dimensional Emotion Recognition (R. Gnana Praveen et al., 2024)</a></li><li><a href=#4382--105271-bamm-bidirectional-autoregressive-motion-model-ekkasit-pinyoanuntapong-et-al-2024>(43/82 | 105/271) BAMM: Bidirectional Autoregressive Motion Model (Ekkasit Pinyoanuntapong et al., 2024)</a></li><li><a href=#4482--106271-oakink2-a-dataset-of-bimanual-hands-object-manipulation-in-complex-task-completion-xinyu-zhan-et-al-2024>(44/82 | 106/271) OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion (Xinyu Zhan et al., 2024)</a></li><li><a href=#4582--107271-a-simple-and-effective-point-based-network-for-event-camera-6-dofs-pose-relocalization-hongwei-ren-et-al-2024>(45/82 | 107/271) A Simple and Effective Point-based Network for Event Camera 6-DOFs Pose Relocalization (Hongwei Ren et al., 2024)</a></li><li><a href=#4682--108271-cat-exploiting-inter-class-dynamics-for-domain-adaptive-object-detection-mikhail-kennerley-et-al-2024>(46/82 | 108/271) CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection (Mikhail Kennerley et al., 2024)</a></li><li><a href=#4782--109271-dreamsalon-a-staged-diffusion-framework-for-preserving-identity-context-in-editable-face-generation-haonan-lin-et-al-2024>(47/82 | 109/271) DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context in Editable Face Generation (Haonan Lin et al., 2024)</a></li><li><a href=#4882--110271-uncertainty-aware-deep-video-compression-with-ensembles-wufei-ma-et-al-2024>(48/82 | 110/271) Uncertainty-Aware Deep Video Compression with Ensembles (Wufei Ma et al., 2024)</a></li><li><a href=#4982--111271-synthetic-medical-imaging-generation-with-generative-adversarial-networks-for-plain-radiographs-john-r-mcnulty-et-al-2024>(49/82 | 111/271) Synthetic Medical Imaging Generation with Generative Adversarial Networks For Plain Radiographs (John R. McNulty et al., 2024)</a></li><li><a href=#5082--112271-a-real-time-framework-for-domain-adaptive-underwater-object-detection-with-image-enhancement-junjie-wen-et-al-2024>(50/82 | 112/271) A Real-Time Framework for Domain-Adaptive Underwater Object Detection with Image Enhancement (Junjie Wen et al., 2024)</a></li><li><a href=#5182--113271-cdimc-net-cognitive-deep-incomplete-multi-view-clustering-network-jie-wen-et-al-2024>(51/82 | 113/271) CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network (Jie Wen et al., 2024)</a></li><li><a href=#5282--114271-flowdepth-decoupling-optical-flow-for-self-supervised-monocular-depth-estimation-yiyang-sun-et-al-2024>(52/82 | 114/271) FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth Estimation (Yiyang Sun et al., 2024)</a></li><li><a href=#5382--115271-ilpo-net-network-for-the-invariant-recognition-of-arbitrary-volumetric-patterns-in-3d-dmitrii-zhemchuzhnikov-et-al-2024>(53/82 | 115/271) ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D (Dmitrii Zhemchuzhnikov et al., 2024)</a></li><li><a href=#5482--116271-ov-uni3detr-towards-unified-open-vocabulary-3d-object-detection-via-cycle-modality-propagation-zhenyu-wang-et-al-2024>(54/82 | 116/271) OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation (Zhenyu Wang et al., 2024)</a></li><li><a href=#5582--117271-locate-assign-refine-taming-customized-image-inpainting-with-text-subject-guidance-yulin-pan-et-al-2024>(55/82 | 117/271) Locate, Assign, Refine: Taming Customized Image Inpainting with Text-Subject Guidance (Yulin Pan et al., 2024)</a></li><li><a href=#5682--118271-sg-pgm-partial-graph-matching-network-with-semantic-geometric-fusion-for-3d-scene-graph-alignment-and-its-downstream-tasks-yaxu-xie-et-al-2024>(56/82 | 118/271) SG-PGM: Partial Graph Matching Network with Semantic Geometric Fusion for 3D Scene Graph Alignment and Its Downstream Tasks (Yaxu Xie et al., 2024)</a></li><li><a href=#5782--119271-beyond-talking----generating-holistic-3d-human-dyadic-motion-for-communication-mingze-sun-et-al-2024>(57/82 | 119/271) Beyond Talking &ndash; Generating Holistic 3D Human Dyadic Motion for Communication (Mingze Sun et al., 2024)</a></li><li><a href=#5882--120271-pointcloud-text-matching-benchmark-datasets-and-a-baseline-yanglin-feng-et-al-2024>(58/82 | 120/271) PointCloud-Text Matching: Benchmark Datasets and a Baseline (Yanglin Feng et al., 2024)</a></li><li><a href=#5982--121271-recdiffusion-rectangling-for-image-stitching-with-diffusion-models-tianhao-zhou-et-al-2024>(59/82 | 121/271) RecDiffusion: Rectangling for Image Stitching with Diffusion Models (Tianhao Zhou et al., 2024)</a></li><li><a href=#6082--122271-efficient-3d-instance-mapping-and-localization-with-neural-fields-george-tang-et-al-2024>(60/82 | 122/271) Efficient 3D Instance Mapping and Localization with Neural Fields (George Tang et al., 2024)</a></li><li><a href=#6182--123271-shapefusion-a-3d-diffusion-model-for-localized-shape-editing-rolandos-alexandros-potamias-et-al-2024>(61/82 | 123/271) ShapeFusion: A 3D diffusion model for localized shape editing (Rolandos Alexandros Potamias et al., 2024)</a></li><li><a href=#6282--124271-gaustudio-a-modular-framework-for-3d-gaussian-splatting-and-beyond-chongjie-ye-et-al-2024>(62/82 | 124/271) GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond (Chongjie Ye et al., 2024)</a></li><li><a href=#6382--125271-frame-by-familiar-frame-understanding-replication-in-video-diffusion-models-aimon-rahman-et-al-2024>(63/82 | 125/271) Frame by Familiar Frame: Understanding Replication in Video Diffusion Models (Aimon Rahman et al., 2024)</a></li><li><a href=#6482--126271-tod3cap-towards-3d-dense-captioning-in-outdoor-scenes-bu-jin-et-al-2024>(64/82 | 126/271) TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes (Bu Jin et al., 2024)</a></li><li><a href=#6582--127271-coherentgs-sparse-novel-view-synthesis-with-coherent-3d-gaussians-avinash-paliwal-et-al-2024>(65/82 | 127/271) CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians (Avinash Paliwal et al., 2024)</a></li><li><a href=#6682--128271-burst-super-resolution-with-diffusion-models-for-improving-perceptual-quality-kyotaro-tokoro-et-al-2024>(66/82 | 128/271) Burst Super-Resolution with Diffusion Models for Improving Perceptual Quality (Kyotaro Tokoro et al., 2024)</a></li><li><a href=#6782--129271-imperceptible-protection-against-style-imitation-from-diffusion-models-namhyuk-ahn-et-al-2024>(67/82 | 129/271) Imperceptible Protection against Style Imitation from Diffusion Models (Namhyuk Ahn et al., 2024)</a></li><li><a href=#6882--130271-efficient-and-effective-weakly-supervised-action-segmentation-via-action-transition-aware-boundary-alignment-angchi-xu-et-al-2024>(68/82 | 130/271) Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment (Angchi Xu et al., 2024)</a></li><li><a href=#6982--131271-geoauxnet-towards-universal-3d-representation-learning-for-multi-sensor-point-clouds-shengjun-zhang-et-al-2024>(69/82 | 131/271) GeoAuxNet: Towards Universal 3D Representation Learning for Multi-sensor Point Clouds (Shengjun Zhang et al., 2024)</a></li><li><a href=#7082--132271-rethinking-information-loss-in-medical-image-segmentation-with-various-sized-targets-tianyi-liu-et-al-2024>(70/82 | 132/271) Rethinking Information Loss in Medical Image Segmentation with Various-sized Targets (Tianyi Liu et al., 2024)</a></li><li><a href=#7182--133271-within-the-dynamic-context-inertia-aware-3d-human-modeling-with-pose-sequence-yutong-chen-et-al-2024>(71/82 | 133/271) Within the Dynamic Context: Inertia-aware 3D Human Modeling with Pose Sequence (Yutong Chen et al., 2024)</a></li><li><a href=#7282--134271-total-decom-decomposed-3d-scene-reconstruction-with-minimal-interaction-xiaoyang-lyu-et-al-2024>(72/82 | 134/271) Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction (Xiaoyang Lyu et al., 2024)</a></li><li><a href=#7382--135271-reli11d-a-comprehensive-multimodal-human-motion-dataset-and-method-ming-yan-et-al-2024>(73/82 | 135/271) RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method (Ming Yan et al., 2024)</a></li><li><a href=#7482--136271-benchmarking-implicit-neural-representation-and-geometric-rendering-in-real-time-rgb-d-slam-tongyan-hua-et-al-2024>(74/82 | 136/271) Benchmarking Implicit Neural Representation and Geometric Rendering in Real-Time RGB-D SLAM (Tongyan Hua et al., 2024)</a></li><li><a href=#7582--137271-segmentation-tool-for-images-of-cracks-andrii-kompanets-et-al-2024>(75/82 | 137/271) Segmentation tool for images of cracks (Andrii Kompanets et al., 2024)</a></li><li><a href=#7682--138271-clora-a-contrastive-approach-to-compose-multiple-lora-models-tuna-han-salih-meral-et-al-2024>(76/82 | 138/271) CLoRA: A Contrastive Approach to Compose Multiple LoRA Models (Tuna Han Salih Meral et al., 2024)</a></li><li><a href=#7782--139271-xscale-nvs-cross-scale-novel-view-synthesis-with-hash-featurized-manifold-guangyu-wang-et-al-2024>(77/82 | 139/271) XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized Manifold (Guangyu Wang et al., 2024)</a></li><li><a href=#7882--140271-towards-temporally-consistent-referring-video-object-segmentation-bo-miao-et-al-2024>(78/82 | 140/271) Towards Temporally Consistent Referring Video Object Segmentation (Bo Miao et al., 2024)</a></li><li><a href=#7982--141271-rtracker-recoverable-tracking-via-pn-tree-structured-memory-yuqing-huang-et-al-2024>(79/82 | 141/271) RTracker: Recoverable Tracking via PN Tree Structured Memory (Yuqing Huang et al., 2024)</a></li><li><a href=#8082--142271-az-nas-assembling-zero-cost-proxies-for-network-architecture-search-junghyup-lee-et-al-2024>(80/82 | 142/271) AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search (Junghyup Lee et al., 2024)</a></li><li><a href=#8182--143271-learning-multiple-representations-with-inconsistency-guided-detail-regularization-for-mask-guided-matting-weihao-jiang-et-al-2024>(81/82 | 143/271) Learning Multiple Representations with Inconsistency-Guided Detail Regularization for Mask-Guided Matting (Weihao Jiang et al., 2024)</a></li><li><a href=#8282--144271-graphad-interaction-scene-graph-for-end-to-end-autonomous-driving-yunpeng-zhang-et-al-2024>(82/82 | 144/271) GraphAD: Interaction Scene Graph for End-to-end Autonomous Driving (Yunpeng Zhang et al., 2024)</a></li></ul></li><li><a href=#cslg-33>cs.LG (33)</a><ul><li><a href=#133--145271-dual-personalizing-adapter-for-federated-foundation-models-yiyuan-yang-et-al-2024>(1/33 | 145/271) Dual-Personalizing Adapter for Federated Foundation Models (Yiyuan Yang et al., 2024)</a></li><li><a href=#233--146271-sine-activated-low-rank-matrices-for-parameter-efficient-learning-yiping-ji-et-al-2024>(2/33 | 146/271) Sine Activated Low-Rank Matrices for Parameter Efficient Learning (Yiping Ji et al., 2024)</a></li><li><a href=#333--147271-exploiting-individual-graph-structures-to-enhance-ecological-momentary-assessment-ema-forecasting-mandani-ntekouli-et-al-2024>(3/33 | 147/271) Exploiting Individual Graph Structures to Enhance Ecological Momentary Assessment (EMA) Forecasting (Mandani Ntekouli et al., 2024)</a></li><li><a href=#433--148271-mpxgat-an-attention-based-deep-learning-model-for-multiplex-graphs-embedding-marco-bongiovanni-et-al-2024>(4/33 | 148/271) MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs Embedding (Marco Bongiovanni et al., 2024)</a></li><li><a href=#533--149271-model-stock-all-we-need-is-just-a-few-fine-tuned-models-dong-hwan-jang-et-al-2024>(5/33 | 149/271) Model Stock: All we need is just a few fine-tuned models (Dong-Hwan Jang et al., 2024)</a></li><li><a href=#633--150271-topological-cycle-graph-attention-network-for-brain-functional-connectivity-jinghan-huang-et-al-2024>(6/33 | 150/271) Topological Cycle Graph Attention Network for Brain Functional Connectivity (Jinghan Huang et al., 2024)</a></li><li><a href=#733--151271-tiny-graph-neural-networks-for-radio-resource-management-ahmad-ghasemi-et-al-2024>(7/33 | 151/271) Tiny Graph Neural Networks for Radio Resource Management (Ahmad Ghasemi et al., 2024)</a></li><li><a href=#833--152271-the-new-agronomists-language-models-are-experts-in-crop-management-jing-wu-et-al-2024>(8/33 | 152/271) The New Agronomists: Language Models are Experts in Crop Management (Jing Wu et al., 2024)</a></li><li><a href=#933--153271-concept-based-analysis-of-neural-networks-via-vision-language-models-ravi-mangal-et-al-2024>(9/33 | 153/271) Concept-based Analysis of Neural Networks via Vision-Language Models (Ravi Mangal et al., 2024)</a></li><li><a href=#1033--154271-a-review-of-graph-neural-networks-in-epidemic-modeling-zewen-liu-et-al-2024>(10/33 | 154/271) A Review of Graph Neural Networks in Epidemic Modeling (Zewen Liu et al., 2024)</a></li><li><a href=#1133--155271-gegenbauer-graph-neural-networks-for-time-varying-signal-reconstruction-jhon-a-castro-correa-et-al-2024>(11/33 | 155/271) Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction (Jhon A. Castro-Correa et al., 2024)</a></li><li><a href=#1233--156271-grind-grid-interpolation-network-for-scattered-observations-andrzej-dulny-et-al-2024>(12/33 | 156/271) GrINd: Grid Interpolation Network for Scattered Observations (Andrzej Dulny et al., 2024)</a></li><li><a href=#1333--157271-graph-neural-networks-for-treatment-effect-prediction-george-panagopoulos-et-al-2024>(13/33 | 157/271) Graph Neural Networks for Treatment Effect Prediction (George Panagopoulos et al., 2024)</a></li><li><a href=#1433--158271-denetdm-debiasing-by-network-depth-modulation-silpa-vadakkeeveetil-sreelatha-et-al-2024>(14/33 | 158/271) DeNetDM: Debiasing by Network Depth Modulation (Silpa Vadakkeeveetil Sreelatha et al., 2024)</a></li><li><a href=#1533--159271-genetic-quantization-aware-approximation-for-non-linear-operations-in-transformers-pingcheng-dong-et-al-2024>(15/33 | 159/271) Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers (Pingcheng Dong et al., 2024)</a></li><li><a href=#1633--160271-fairness-in-ranking-robustness-through-randomization-without-the-protected-attribute-andrii-kliachkin-et-al-2024>(16/33 | 160/271) Fairness in Ranking: Robustness through Randomization without the Protected Attribute (Andrii Kliachkin et al., 2024)</a></li><li><a href=#1733--161271-artificial-intelligence-ai-based-prediction-of-mortality-for-covid-19-patients-mahbubunnabi-tamala-et-al-2024>(17/33 | 161/271) Artificial Intelligence (AI) Based Prediction of Mortality, for COVID-19 Patients (Mahbubunnabi Tamala et al., 2024)</a></li><li><a href=#1833--162271-inferring-latent-temporal-sparse-coordination-graph-for-multi-agent-reinforcement-learning-wei-duan-et-al-2024>(18/33 | 162/271) Inferring Latent Temporal Sparse Coordination Graph for Multi-Agent Reinforcement Learning (Wei Duan et al., 2024)</a></li><li><a href=#1933--163271-transparent-and-clinically-interpretable-ai-for-lung-cancer-detection-in-chest-x-rays-amy-rafferty-et-al-2024>(19/33 | 163/271) Transparent and Clinically Interpretable AI for Lung Cancer Detection in Chest X-Rays (Amy Rafferty et al., 2024)</a></li><li><a href=#2033--164271-sparse-feature-circuits-discovering-and-editing-interpretable-causal-graphs-in-language-models-samuel-marks-et-al-2024>(20/33 | 164/271) Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models (Samuel Marks et al., 2024)</a></li><li><a href=#2133--165271-tabular-learning-encoding-for-entity-and-context-embeddings-fredy-reusser-2024>(21/33 | 165/271) Tabular Learning: Encoding for Entity and Context Embeddings (Fredy Reusser, 2024)</a></li><li><a href=#2233--166271-medbn-robust-test-time-adaptation-against-malicious-test-samples-hyejin-park-et-al-2024>(22/33 | 166/271) MedBN: Robust Test-Time Adaptation against Malicious Test Samples (Hyejin Park et al., 2024)</a></li><li><a href=#2333--167271-biased-over-the-air-federated-learning-under-wireless-heterogeneity-muhammad-faraz-ul-abrar-et-al-2024>(23/33 | 167/271) Biased Over-the-Air Federated Learning under Wireless Heterogeneity (Muhammad Faraz Ul Abrar et al., 2024)</a></li><li><a href=#2433--168271-evaluating-explanatory-capabilities-of-machine-learning-models-in-medical-diagnostics-a-human-in-the-loop-approach-josé-bobes-bascarán-et-al-2024>(24/33 | 168/271) Evaluating Explanatory Capabilities of Machine Learning Models in Medical Diagnostics: A Human-in-the-Loop Approach (José Bobes-Bascarán et al., 2024)</a></li><li><a href=#2533--169271-feature-based-echo-state-networks-a-step-towards-interpretability-and-minimalism-in-reservoir-computer-debdipta-goswami-2024>(25/33 | 169/271) Feature-Based Echo-State Networks: A Step Towards Interpretability and Minimalism in Reservoir Computer (Debdipta Goswami, 2024)</a></li><li><a href=#2633--170271-swarm-characteristics-classification-using-neural-networks-donald-w-peltier-iii-et-al-2024>(26/33 | 170/271) Swarm Characteristics Classification Using Neural Networks (Donald W. Peltier III et al., 2024)</a></li><li><a href=#2733--171271-tensor-network-constrained-kernel-machines-as-gaussian-processes-frederiek-wesel-et-al-2024>(27/33 | 171/271) Tensor Network-Constrained Kernel Machines as Gaussian Processes (Frederiek Wesel et al., 2024)</a></li><li><a href=#2833--172271-client-supervised-federated-learning-towards-one-model-for-all-personalization-peng-yan-et-al-2024>(28/33 | 172/271) Client-supervised Federated Learning: Towards One-model-for-all Personalization (Peng Yan et al., 2024)</a></li><li><a href=#2933--173271-offline-imitation-learning-from-multiple-baselines-with-applications-to-compiler-optimization-teodor-v-marinov-et-al-2024>(29/33 | 173/271) Offline Imitation Learning from Multiple Baselines with Applications to Compiler Optimization (Teodor V. Marinov et al., 2024)</a></li><li><a href=#3033--174271-an-interactive-human-machine-learning-interface-for-collecting-and-learning-from-complex-annotations-jonathan-erskine-et-al-2024>(30/33 | 174/271) An Interactive Human-Machine Learning Interface for Collecting and Learning from Complex Annotations (Jonathan Erskine et al., 2024)</a></li><li><a href=#3133--175271-evaluating-fair-feature-selection-in-machine-learning-for-healthcare-md-rahat-shahriar-zawad-et-al-2024>(31/33 | 175/271) Evaluating Fair Feature Selection in Machine Learning for Healthcare (Md Rahat Shahriar Zawad et al., 2024)</a></li><li><a href=#3233--176271-towards-understanding-dual-bn-in-hybrid-adversarial-training-chenshuang-zhang-et-al-2024>(32/33 | 176/271) Towards Understanding Dual BN In Hybrid Adversarial Training (Chenshuang Zhang et al., 2024)</a></li><li><a href=#3333--177271-mapl-model-agnostic-peer-to-peer-learning-sayak-mukherjee-et-al-2024>(33/33 | 177/271) MAPL: Model Agnostic Peer-to-peer Learning (Sayak Mukherjee et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#11--178271-alloybert-alloy-property-prediction-with-large-language-models-akshat-chaudhari-et-al-2024>(1/1 | 178/271) AlloyBERT: Alloy Property Prediction with Large Language Models (Akshat Chaudhari et al., 2024)</a></li></ul></li><li><a href=#csro-15>cs.RO (15)</a><ul><li><a href=#115--179271-keypoint-action-tokens-enable-in-context-imitation-learning-in-robotics-norman-di-palo-et-al-2024>(1/15 | 179/271) Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics (Norman Di Palo et al., 2024)</a></li><li><a href=#215--180271-learning-sampling-distribution-and-safety-filter-for-autonomous-driving-with-vq-vae-and-differentiable-optimization-simon-idoko-et-al-2024>(2/15 | 180/271) Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization (Simon Idoko et al., 2024)</a></li><li><a href=#315--181271-said-nerf-segmentation-aided-nerf-for-depth-completion-of-transparent-objects-avinash-ummadisingu-et-al-2024>(3/15 | 181/271) SAID-NeRF: Segmentation-AIDed NeRF for Depth Completion of Transparent Objects (Avinash Ummadisingu et al., 2024)</a></li><li><a href=#415--182271-hierarchical-deep-learning-for-intention-estimation-of-teleoperation-manipulation-in-assembly-tasks-mingyu-cai-et-al-2024>(4/15 | 182/271) Hierarchical Deep Learning for Intention Estimation of Teleoperation Manipulation in Assembly Tasks (Mingyu Cai et al., 2024)</a></li><li><a href=#515--183271-human-compatible-driving-partners-through-data-regularized-self-play-reinforcement-learning-daphne-cornelisse-et-al-2024>(5/15 | 183/271) Human-compatible driving partners through data-regularized self-play reinforcement learning (Daphne Cornelisse et al., 2024)</a></li><li><a href=#615--184271-kinetostatic-analysis-for-6rus-parallel-continuum-robot-using-cosserat-rod-theory-vinayvivian-rodrigues-et-al-2024>(6/15 | 184/271) Kinetostatic Analysis for 6RUS Parallel Continuum Robot using Cosserat Rod Theory (Vinayvivian Rodrigues et al., 2024)</a></li><li><a href=#715--185271-lamarckian-inheritance-improves-robot-evolution-in-dynamic-environments-jie-luo-et-al-2024>(7/15 | 185/271) Lamarckian Inheritance Improves Robot Evolution in Dynamic Environments (Jie Luo et al., 2024)</a></li><li><a href=#815--186271-riemann-near-real-time-se3-equivariant-robot-manipulation-without-point-cloud-segmentation-chongkai-gao-et-al-2024>(8/15 | 186/271) RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation (Chongkai Gao et al., 2024)</a></li><li><a href=#915--187271-multi-agent-team-access-monitoring-environments-that-benefit-from-target-information-sharing-andrew-dudash-et-al-2024>(9/15 | 187/271) Multi-Agent Team Access Monitoring: Environments that Benefit from Target Information Sharing (Andrew Dudash et al., 2024)</a></li><li><a href=#1015--188271-rail-robot-affordance-imagination-with-large-language-models-ceng-zhang-et-al-2024>(10/15 | 188/271) RAIL: Robot Affordance Imagination with Large Language Models (Ceng Zhang et al., 2024)</a></li><li><a href=#1115--189271-adaptive-preload-control-of-cable-driven-parallel-robots-for-handling-task-thomas-reichenbach-et-al-2024>(11/15 | 189/271) Adaptive Preload Control of Cable-Driven Parallel Robots for Handling Task (Thomas Reichenbach et al., 2024)</a></li><li><a href=#1215--190271-bundledslam-an-accurate-visual-slam-system-using-multiple-cameras-han-song-et-al-2024>(12/15 | 190/271) BundledSLAM: An Accurate Visual SLAM System Using Multiple Cameras (Han Song et al., 2024)</a></li><li><a href=#1315--191271-learning-a-formally-verified-control-barrier-function-in-stochastic-environment-manan-tayal-et-al-2024>(13/15 | 191/271) Learning a Formally Verified Control Barrier Function in Stochastic Environment (Manan Tayal et al., 2024)</a></li><li><a href=#1415--192271-safety-critical-planning-and-control-for-dynamic-obstacle-avoidance-using-control-barrier-functions-shuo-liu-et-al-2024>(14/15 | 192/271) Safety-Critical Planning and Control for Dynamic Obstacle Avoidance Using Control Barrier Functions (Shuo Liu et al., 2024)</a></li><li><a href=#1515--193271-mac-maximizing-algebraic-connectivity-for-graph-sparsification-kevin-doherty-et-al-2024>(15/15 | 193/271) MAC: Maximizing Algebraic Connectivity for Graph Sparsification (Kevin Doherty et al., 2024)</a></li></ul></li><li><a href=#quant-ph-2>quant-ph (2)</a><ul><li><a href=#12--194271-optimizing-quantum-convolutional-neural-network-architectures-for-arbitrary-data-dimension-changwon-lee-et-al-2024>(1/2 | 194/271) Optimizing Quantum Convolutional Neural Network Architectures for Arbitrary Data Dimension (Changwon Lee et al., 2024)</a></li><li><a href=#22--195271-natural-language-ai-and-quantum-computing-in-2024-research-ingredients-and-directions-in-qnlp-dominic-widdows-et-al-2024>(2/2 | 195/271) Natural Language, AI, and Quantum Computing in 2024: Research Ingredients and Directions in QNLP (Dominic Widdows et al., 2024)</a></li></ul></li><li><a href=#csai-5>cs.AI (5)</a><ul><li><a href=#15--196271-llmsense-harnessing-llms-for-high-level-reasoning-over-spatiotemporal-sensor-traces-xiaomin-ouyang-et-al-2024>(1/5 | 196/271) LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces (Xiaomin Ouyang et al., 2024)</a></li><li><a href=#25--197271-bespoke-large-language-models-for-digital-triage-assistance-in-mental-health-care-niall-taylor-et-al-2024>(2/5 | 197/271) Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care (Niall Taylor et al., 2024)</a></li><li><a href=#35--198271-ime-integrating-multi-curvature-shared-and-specific-embedding-for-temporal-knowledge-graph-completion-jiapu-wang-et-al-2024>(3/5 | 198/271) IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion (Jiapu Wang et al., 2024)</a></li><li><a href=#45--199271-leveraging-counterfactual-paths-for-contrastive-explanations-of-pomdp-policies-benjamin-kraske-et-al-2024>(4/5 | 199/271) Leveraging Counterfactual Paths for Contrastive Explanations of POMDP Policies (Benjamin Kraske et al., 2024)</a></li><li><a href=#55--200271-towards-a-brazilian-history-knowledge-graph-valeria-de-paiva-et-al-2024>(5/5 | 200/271) Towards a Brazilian History Knowledge Graph (Valeria de Paiva et al., 2024)</a></li></ul></li><li><a href=#csse-5>cs.SE (5)</a><ul><li><a href=#15--201271-coderujb-an-executable-and-unified-java-benchmark-for-practical-programming-scenarios-zhengran-zeng-et-al-2024>(1/5 | 201/271) CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios (Zhengran Zeng et al., 2024)</a></li><li><a href=#25--202271-top-leaderboard-ranking--top-coding-proficiency-always-evoeval-evolving-coding-benchmarks-via-llm-chunqiu-steven-xia-et-al-2024>(2/5 | 202/271) Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM (Chunqiu Steven Xia et al., 2024)</a></li><li><a href=#35--203271-hirope-length-extrapolation-for-code-models-kechi-zhang-et-al-2024>(3/5 | 203/271) HiRoPE: Length Extrapolation for Code Models (Kechi Zhang et al., 2024)</a></li><li><a href=#45--204271-scale-constructing-structured-natural-language-comment-trees-for-software-vulnerability-detection-xin-cheng-wen-et-al-2024>(4/5 | 204/271) SCALE: Constructing Structured Natural Language Comment Trees for Software Vulnerability Detection (Xin-Cheng Wen et al., 2024)</a></li><li><a href=#55--205271-clustering-mooc-programming-solutions-to-diversify-their-presentation-to-students-elizaveta-artser-et-al-2024>(5/5 | 205/271) Clustering MOOC Programming Solutions to Diversify Their Presentation to Students (Elizaveta Artser et al., 2024)</a></li></ul></li><li><a href=#eesssy-6>eess.SY (6)</a><ul><li><a href=#16--206271-lane-change-in-dense-traffic-with-model-predictive-control-and-neural-networks-sangjae-bae-et-al-2024>(1/6 | 206/271) Lane-Change in Dense Traffic with Model Predictive Control and Neural Networks (Sangjae Bae et al., 2024)</a></li><li><a href=#26--207271-feedback-optimization-of-incentives-for-distribution-grid-services-guido-cavraro-et-al-2024>(2/6 | 207/271) Feedback Optimization of Incentives for Distribution Grid Services (Guido Cavraro et al., 2024)</a></li><li><a href=#36--208271-expectation-maximization-aided-modified-weighted-sequential-energy-detector-for-distributed-cooperative-spectrum-sensing-mohammed-rashid-et-al-2024>(3/6 | 208/271) Expectation Maximization Aided Modified Weighted Sequential Energy Detector for Distributed Cooperative Spectrum Sensing (Mohammed Rashid et al., 2024)</a></li><li><a href=#46--209271-design-and-evaluation-of-a-dc-microgrid-testbed-for-der-integration-and-power-management-gokul-krishnan-s-et-al-2024>(4/6 | 209/271) Design and Evaluation of a DC Microgrid Testbed for DER Integration and Power Management (Gokul Krishnan S et al., 2024)</a></li><li><a href=#56--210271-harnessing-data-for-accelerating-model-predictive-control-by-constraint-removal-zhinan-hou-et-al-2024>(5/6 | 210/271) Harnessing Data for Accelerating Model Predictive Control by Constraint Removal (Zhinan Hou et al., 2024)</a></li><li><a href=#66--211271-gain-only-neural-operator-approximators-of-pde-backstepping-controllers-rafael-vazquez-et-al-2024>(6/6 | 211/271) Gain-Only Neural Operator Approximators of PDE Backstepping Controllers (Rafael Vazquez et al., 2024)</a></li></ul></li><li><a href=#cscy-5>cs.CY (5)</a><ul><li><a href=#15--212271-developing-generative-ai-chatbots-conceptual-framework-for-higher-education-joshua-ebere-chukwuere-2024>(1/5 | 212/271) Developing generative AI chatbots conceptual framework for higher education (Joshua Ebere Chukwuere, 2024)</a></li><li><a href=#25--213271-purposeful-remixing-with-generative-ai-constructing-designer-voice-in-multimodal-composing-xiao-tan-et-al-2024>(2/5 | 213/271) Purposeful remixing with generative AI: Constructing designer voice in multimodal composing (Xiao Tan et al., 2024)</a></li><li><a href=#35--214271-cycling-on-the-freeway-the-perilous-state-of-open-source-neuroscience-software-britta-u-westner-et-al-2024>(3/5 | 214/271) Cycling on the Freeway: The Perilous State of Open Source Neuroscience Software (Britta U. Westner et al., 2024)</a></li><li><a href=#45--215271-the-use-of-chatgpt-in-higher-education-the-advantages-and-disadvantages-joshua-ebere-chukwuere-2024>(4/5 | 215/271) The use of ChatGPT in higher education: The advantages and disadvantages (Joshua Ebere Chukwuere, 2024)</a></li><li><a href=#55--216271-genai-detection-tools-adversarial-techniques-and-implications-for-inclusivity-in-higher-education-mike-perkins-et-al-2024>(5/5 | 216/271) GenAI Detection Tools, Adversarial Techniques and Implications for Inclusivity in Higher Education (Mike Perkins et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--217271-emotion-neural-transducer-for-fine-grained-speech-emotion-recognition-siyuan-shen-et-al-2024>(1/2 | 217/271) Emotion Neural Transducer for Fine-Grained Speech Emotion Recognition (Siyuan Shen et al., 2024)</a></li><li><a href=#22--218271-a-novel-stochastic-transformer-based-approach-for-post-traumatic-stress-disorder-detection-using-audio-recording-of-clinical-interviews-mamadou-dia-et-al-2024>(2/2 | 218/271) A Novel Stochastic Transformer-based Approach for Post-Traumatic Stress Disorder Detection using Audio Recording of Clinical Interviews (Mamadou Dia et al., 2024)</a></li></ul></li><li><a href=#cscr-6>cs.CR (6)</a><ul><li><a href=#16--219271-genos-general-in-network-unsupervised-intrusion-detection-by-rule-extraction-ruoyu-li-et-al-2024>(1/6 | 219/271) Genos: General In-Network Unsupervised Intrusion Detection by Rule Extraction (Ruoyu Li et al., 2024)</a></li><li><a href=#26--220271-detecting-financial-bots-on-the-ethereum-blockchain-thomas-niedermayer-et-al-2024>(2/6 | 220/271) Detecting Financial Bots on the Ethereum Blockchain (Thomas Niedermayer et al., 2024)</a></li><li><a href=#36--221271-on-the-robustness-of-ldp-protocols-for-numerical-attributes-under-data-poisoning-attacks-xiaoguang-li-et-al-2024>(3/6 | 221/271) On the Robustness of LDP Protocols for Numerical Attributes under Data Poisoning Attacks (Xiaoguang Li et al., 2024)</a></li><li><a href=#46--222271-enhancing-trust-and-privacy-in-distributed-networks-a-comprehensive-survey-on-blockchain-based-federated-learning-ji-liu-et-al-2024>(4/6 | 222/271) Enhancing Trust and Privacy in Distributed Networks: A Comprehensive Survey on Blockchain-based Federated Learning (Ji Liu et al., 2024)</a></li><li><a href=#56--223271-secgraph-towards-sgx-based-efficient-and-confidentiality-preserving-graph-search-qiuhao-wang-et-al-2024>(5/6 | 223/271) SecGraph: Towards SGX-based Efficient and Confidentiality-Preserving Graph Search (Qiuhao Wang et al., 2024)</a></li><li><a href=#66--224271-assetharvester-a-static-analysis-tool-for-detecting-assets-protected-by-secrets-in-software-artifacts-setu-kumar-basak-et-al-2024>(6/6 | 224/271) AssetHarvester: A Static Analysis Tool for Detecting Assets Protected by Secrets in Software Artifacts (Setu Kumar Basak et al., 2024)</a></li></ul></li><li><a href=#eessiv-5>eess.IV (5)</a><ul><li><a href=#15--225271-debiasing-cardiac-imaging-with-controlled-latent-diffusion-models-grzegorz-skorupko-et-al-2024>(1/5 | 225/271) Debiasing Cardiac Imaging with Controlled Latent Diffusion Models (Grzegorz Skorupko et al., 2024)</a></li><li><a href=#25--226271-enhancing-efficiency-in-vision-transformer-networks-design-techniques-and-insights-moein-heidari-et-al-2024>(2/5 | 226/271) Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights (Moein Heidari et al., 2024)</a></li><li><a href=#35--227271-vision-language-synthetic-data-enhances-echocardiography-downstream-tasks-pooria-ashrafian-et-al-2024>(3/5 | 227/271) Vision-Language Synthetic Data Enhances Echocardiography Downstream Tasks (Pooria Ashrafian et al., 2024)</a></li><li><a href=#45--228271-single-shared-network-with-prior-inspired-loss-for-parameter-efficient-multi-modal-imaging-skin-lesion-classification-peng-tang-et-al-2024>(4/5 | 228/271) Single-Shared Network with Prior-Inspired Loss for Parameter-Efficient Multi-Modal Imaging Skin Lesion Classification (Peng Tang et al., 2024)</a></li><li><a href=#55--229271-brain-shift-unsupervised-pseudo-healthy-brain-synthesis-for-novel-biomarker-extraction-in-chronic-subdural-hematoma-baris-imre-et-al-2024>(5/5 | 229/271) Brain-Shift: Unsupervised Pseudo-Healthy Brain Synthesis for Novel Biomarker Extraction in Chronic Subdural Hematoma (Baris Imre et al., 2024)</a></li></ul></li><li><a href=#mathna-5>math.NA (5)</a><ul><li><a href=#15--230271-an-ultra-high-speed-reproducing-kernel-particle-method-siavash-jafarzadeh-et-al-2024>(1/5 | 230/271) An Ultra-high-speed Reproducing Kernel Particle Method (Siavash Jafarzadeh et al., 2024)</a></li><li><a href=#25--231271-schrödingerisation-based-computationally-stable-algorithms-for-ill-posed-problems-in-partial-differential-equations-shi-jin-et-al-2024>(2/5 | 231/271) Schrödingerisation based computationally stable algorithms for ill-posed problems in partial differential equations (Shi Jin et al., 2024)</a></li><li><a href=#35--232271-adaptive-optimization-of-isogeometric-multi-patch-discretizations-using-artificial-neural-networks-dany-rios-et-al-2024>(3/5 | 232/271) Adaptive optimization of isogeometric multi-patch discretizations using artificial neural networks (Dany Rios et al., 2024)</a></li><li><a href=#45--233271-an-efficient-multiscale-multigrid-preconditioner-for-darcy-flow-in-high-contrast-media-changqing-ye-et-al-2024>(4/5 | 233/271) An efficient multiscale multigrid preconditioner for Darcy flow in high-contrast media (Changqing Ye et al., 2024)</a></li><li><a href=#55--234271-a-unified-shtc-multiphase-model-of-continuum-mechanics-davide-ferrari-et-al-2024>(5/5 | 234/271) A unified SHTC multiphase model of continuum mechanics (Davide Ferrari et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--235271-tasr-a-novel-trust-aware-stackelberg-routing-algorithm-to-mitigate-traffic-congestion-doris-e-m-brown-et-al-2024>(1/1 | 235/271) TASR: A Novel Trust-Aware Stackelberg Routing Algorithm to Mitigate Traffic Congestion (Doris E. M. Brown et al., 2024)</a></li></ul></li><li><a href=#q-fintr-1>q-fin.TR (1)</a><ul><li><a href=#11--236271-reinforcement-learning-in-agent-based-market-simulation-unveiling-realistic-stylized-facts-and-behavior-zhiyuan-yao-et-al-2024>(1/1 | 236/271) Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior (Zhiyuan Yao et al., 2024)</a></li></ul></li><li><a href=#cshc-3>cs.HC (3)</a><ul><li><a href=#13--237271-llms-as-academic-reading-companions-extending-hci-through-synthetic-personae-celia-chen-et-al-2024>(1/3 | 237/271) LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae (Celia Chen et al., 2024)</a></li><li><a href=#23--238271-im-categorizing-llm-as-a-productivity-tool-examining-ethics-of-llm-use-in-hci-research-practices-shivani-kapania-et-al-2024>(2/3 | 238/271) &lsquo;I&rsquo;m categorizing LLM as a productivity tool&rsquo;: Examining ethics of LLM use in HCI research practices (Shivani Kapania et al., 2024)</a></li><li><a href=#33--239271-algorithmic-ways-of-seeing-using-object-detection-to-facilitate-art-exploration-louie-søs-meyer-et-al-2024>(3/3 | 239/271) Algorithmic Ways of Seeing: Using Object Detection to Facilitate Art Exploration (Louie Søs Meyer et al., 2024)</a></li></ul></li><li><a href=#eesssp-3>eess.SP (3)</a><ul><li><a href=#13--240271-removing-the-need-for-ground-truth-uwb-data-collection-self-supervised-ranging-error-correction-using-deep-reinforcement-learning-dieter-coppens-et-al-2024>(1/3 | 240/271) Removing the need for ground truth UWB data collection: self-supervised ranging error correction using deep reinforcement learning (Dieter Coppens et al., 2024)</a></li><li><a href=#23--241271-optimal-pilot-design-for-otfs-in-linear-time-varying-channels-ids-van-der-werf-et-al-2024>(2/3 | 241/271) Optimal Pilot Design for OTFS in Linear Time-Varying Channels (Ids van der Werf et al., 2024)</a></li><li><a href=#33--242271-decentralizing-coherent-joint-transmission-precoding-via-fast-admm-with-deterministic-equivalents-xinyu-bian-et-al-2024>(3/3 | 242/271) Decentralizing Coherent Joint Transmission Precoding via Fast ADMM with Deterministic Equivalents (Xinyu Bian et al., 2024)</a></li></ul></li><li><a href=#cset-2>cs.ET (2)</a><ul><li><a href=#12--243271-towards-reverse-engineering-the-brain-brain-derived-neuromorphic-computing-approach-with-photonic-electronic-and-ionic-dynamicity-in-3d-integrated-circuits-s-j-ben-yoo-et-al-2024>(1/2 | 243/271) Towards Reverse-Engineering the Brain: Brain-Derived Neuromorphic Computing Approach with Photonic, Electronic, and Ionic Dynamicity in 3D integrated circuits (S. J. Ben Yoo et al., 2024)</a></li><li><a href=#22--244271-a-noise-tolerant-resource-saving-probabilistic-binary-neural-network-implemented-by-the-sot-mram-compute-in-memory-system-yu-gu-et-al-2024>(2/2 | 244/271) A noise-tolerant, resource-saving probabilistic binary neural network implemented by the SOT-MRAM compute-in-memory system (Yu Gu et al., 2024)</a></li></ul></li><li><a href=#cond-matstat-mech-1>cond-mat.stat-mech (1)</a><ul><li><a href=#11--245271-toward-practical-benchmarks-of-ising-machines-a-case-study-on-the-quadratic-knapsack-problem-kentaro-ohno-et-al-2024>(1/1 | 245/271) Toward Practical Benchmarks of Ising Machines: A Case Study on the Quadratic Knapsack Problem (Kentaro Ohno et al., 2024)</a></li></ul></li><li><a href=#csit-8>cs.IT (8)</a><ul><li><a href=#18--246271-integrated-communication-localization-and-sensing-in-6g-d-mimo-networks-hao-guo-et-al-2024>(1/8 | 246/271) Integrated Communication, Localization, and Sensing in 6G D-MIMO Networks (Hao Guo et al., 2024)</a></li><li><a href=#28--247271-transmissive-ris-transmitter-enabled-spatial-modulation-for-mimo-systems-xusheng-zhu-et-al-2024>(2/8 | 247/271) Transmissive RIS Transmitter Enabled Spatial Modulation for MIMO Systems (Xusheng Zhu et al., 2024)</a></li><li><a href=#38--248271-o-ran-for-energy-efficient-serving-cluster-formulation-in-user-centric-cell-free-mmimo-marcin-hoffmann-et-al-2024>(3/8 | 248/271) O-RAN for Energy-Efficient Serving Cluster Formulation in User-Centric Cell-Free MMIMO (Marcin Hoffmann et al., 2024)</a></li><li><a href=#48--249271-pilot-signal-and-channel-estimator-co-design-for-hybrid-field-xl-mimo-yoonseong-kang-et-al-2024>(4/8 | 249/271) Pilot Signal and Channel Estimator Co-Design for Hybrid-Field XL-MIMO (Yoonseong Kang et al., 2024)</a></li><li><a href=#58--250271-deep-csi-compression-for-dual-polarized-massive-mimo-channels-with-disentangled-representation-learning-suhang-fan-et-al-2024>(5/8 | 250/271) Deep CSI Compression for Dual-Polarized Massive MIMO Channels with Disentangled Representation Learning (Suhang Fan et al., 2024)</a></li><li><a href=#68--251271-cell-free-mimo-perceptive-mobile-networks-cloud-vs-edge-processing-seongah-jeong-et-al-2024>(6/8 | 251/271) Cell-Free MIMO Perceptive Mobile Networks: Cloud vs. Edge Processing (Seongah Jeong et al., 2024)</a></li><li><a href=#78--252271-co-designing-statistical-mimo-radar-and-in-band-full-duplex-multi-user-mimo-communications----part-ii-joint-precoder-radar-code-and-receive-filters-design-jiawei-liu-et-al-2024>(7/8 | 252/271) Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User MIMO Communications &ndash; Part II: Joint Precoder, Radar Code, and Receive Filters Design (Jiawei Liu et al., 2024)</a></li><li><a href=#88--253271-on-the-performance-of-low-complexity-decoders-of-ldpc-and-polar-codes-qingqing-peng-et-al-2024>(8/8 | 253/271) On the Performance of Low-complexity Decoders of LDPC and Polar Codes (Qingqing Peng et al., 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--254271-collaborative-interactive-evolution-of-art-in-the-latent-space-of-deep-generative-models-ole-hall-et-al-2024>(1/2 | 254/271) Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models (Ole Hall et al., 2024)</a></li><li><a href=#22--255271-evolving-assembly-code-in-an-adversarial-environment-irina-maliukov-et-al-2024>(2/2 | 255/271) Evolving Assembly Code in an Adversarial Environment (Irina Maliukov et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--256271-physics-informed-neural-networks-for-satellite-state-estimation-jacob-varey-et-al-2024>(1/1 | 256/271) Physics-Informed Neural Networks for Satellite State Estimation (Jacob Varey et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--257271-simulating-relational-event-histories----why-and-how-rumana-lakdawala-et-al-2024>(1/1 | 257/271) Simulating Relational Event Histories &ndash; Why and How (Rumana Lakdawala et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--258271-mil2-efficient-cloth-simulation-using-non-distance-barriers-and-subspace-reuse-lei-lan-et-al-2024>(1/1 | 258/271) Mil2: Efficient Cloth Simulation Using Non-distance Barriers and Subspace Reuse (Lei Lan et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--259271-dataflow-aware-pim-enabled-manycore-architecture-for-deep-learning-workloads-harsh-sharma-et-al-2024>(1/1 | 259/271) Dataflow-Aware PIM-Enabled Manycore Architecture for Deep Learning Workloads (Harsh Sharma et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--260271-algorithmic-strategies-for-finding-the-best-tsp-2-opt-move-in-average-sub-quadratic-time-giuseppe-lancia-et-al-2024>(1/2 | 260/271) Algorithmic strategies for finding the best TSP 2-OPT move in average sub-quadratic time (Giuseppe Lancia et al., 2024)</a></li><li><a href=#22--261271-finding-decision-tree-splits-in-streaming-and-massively-parallel-models-huy-pham-et-al-2024>(2/2 | 261/271) Finding Decision Tree Splits in Streaming and Massively Parallel Models (Huy Pham et al., 2024)</a></li></ul></li><li><a href=#cslo-2>cs.LO (2)</a><ul><li><a href=#12--262271-eda-driven-preprocessing-for-sat-solving-zhengyuan-shi-et-al-2024>(1/2 | 262/271) EDA-Driven Preprocessing for SAT Solving (Zhengyuan Shi et al., 2024)</a></li><li><a href=#22--263271-linear-programming-in-isabellehol-julian-parsert-2024>(2/2 | 263/271) Linear Programming in Isabelle/HOL (Julian Parsert, 2024)</a></li></ul></li><li><a href=#mathct-1>math.CT (1)</a><ul><li><a href=#11--264271-generalized-gradient-descent-is-a-hypergraph-functor-tyler-hanks-et-al-2024>(1/1 | 264/271) Generalized Gradient Descent is a Hypergraph Functor (Tyler Hanks et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#12--265271-a-framework-for-time-varying-optimization-via-derivative-estimation-matteo-marchi-et-al-2024>(1/2 | 265/271) A Framework for Time-Varying Optimization via Derivative Estimation (Matteo Marchi et al., 2024)</a></li><li><a href=#22--266271-fisher-rao-gradient-flows-of-linear-programs-and-state-action-natural-policy-gradients-johannes-müller-et-al-2024>(2/2 | 266/271) Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural Policy Gradients (Johannes Müller et al., 2024)</a></li></ul></li><li><a href=#statml-1>stat.ML (1)</a><ul><li><a href=#11--267271-maximum-likelihood-estimation-on-stochastic-blockmodels-for-directed-graph-clustering-mihai-cucuringu-et-al-2024>(1/1 | 267/271) Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering (Mihai Cucuringu et al., 2024)</a></li></ul></li><li><a href=#mathat-1>math.AT (1)</a><ul><li><a href=#11--268271-topological-optimal-transport-for-geometric-cycle-matching-stephen-y-zhang-et-al-2024>(1/1 | 268/271) Topological Optimal Transport for Geometric Cycle Matching (Stephen Y Zhang et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--269271-piercing-independent-sets-in-graphs-without-large-induced-matching-jiangdong-ai-et-al-2024>(1/1 | 269/271) Piercing independent sets in graphs without large induced matching (Jiangdong Ai et al., 2024)</a></li></ul></li><li><a href=#mathpr-1>math.PR (1)</a><ul><li><a href=#11--270271-random-multi-type-spanning-forests-for-synchronization-on-sparse-graphs-hugo-jaquard-et-al-2024>(1/1 | 270/271) Random Multi-Type Spanning Forests for Synchronization on Sparse Graphs (Hugo Jaquard et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--271271-unifaas-programming-across-distributed-cyberinfrastructure-with-federated-function-serving-yifei-li-et-al-2024>(1/1 | 271/271) UniFaaS: Programming across Distributed Cyberinfrastructure with Federated Function Serving (Yifei Li et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>