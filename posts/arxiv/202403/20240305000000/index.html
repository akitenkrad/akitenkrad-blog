<!doctype html><html><head><title>arXiv @ 2024.03.05</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.05"><meta property="og:description" content="Primary Categories cs.AI (1) cs.AR (1) cs.CE (1) cs.CL (21) cs.CR (6) cs.CV (29) cs.CY (1) cs.DB (3) cs.DL (1) cs.DS (1) cs.HC (3) cs.IR (1) cs.IT (4) cs.LG (30) cs.MA (1) cs.PF (1) cs.RO (9) cs.SE (1) eess.AS (2) eess.IV (3) math.CO (1) math.NA (2) math.OC (2) physics.flu-dyn (1) stat.ML (2) Keywords keyword cs.CL cs.CV cs.LG Active Learning 1 Adversarial Learning 1 Attention Alignment 1 Autoencoder 1 2 BERT 2 BLEU 2 Bandit Algorithm 1 Benchmarking 3 5 2 Chain-of-thought Prompt 1 Clustering 3 1 Code Generation 1 Common-sense Reasoning 2 Continual Learning 1 Contrastive Learning 2 3 Convolution 3 1 Convolutional Neural Network 2 1 Counter-factual 1 Data Augmentation 2 1 Differential Privacy 1 Diffusion Model 2 3 Domain Adaptation 2 Explainable AI 1 Fairness 1 Federated Learning 4 Few-shot 1 Few-shot Learning 1 Fine-tuning 7 5 Foundation Model 1 GPT 2 1 GPT-3 1 GPT-3."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240305000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-05T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-05T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.05"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240305000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Tuesday, Mar 5, 2024</p></div><div class=title><h1>arXiv @ 2024.03.05</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#csai-1>cs.AI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#cscl-21>cs.CL (21)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#cscr-6>cs.CR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#cscv-29>cs.CV (29)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#csdb-3>cs.DB (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#csdl-1>cs.DL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#cshc-3>cs.HC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#csir-1>cs.IR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#csit-4>cs.IT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#cslg-30>cs.LG (30)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#cspf-1>cs.PF (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#csro-9>cs.RO (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#csse-1>cs.SE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#eessiv-3>eess.IV (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/#statml-2>stat.ML (2)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td>1</td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td></td></tr><tr><td>Attention Alignment</td><td>1</td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td>1</td><td>2</td></tr><tr><td>BERT</td><td>2</td><td></td><td></td></tr><tr><td>BLEU</td><td>2</td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>1</td></tr><tr><td>Benchmarking</td><td>3</td><td>5</td><td>2</td></tr><tr><td>Chain-of-thought Prompt</td><td></td><td>1</td><td></td></tr><tr><td>Clustering</td><td></td><td>3</td><td>1</td></tr><tr><td>Code Generation</td><td></td><td></td><td>1</td></tr><tr><td>Common-sense Reasoning</td><td>2</td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td></tr><tr><td>Contrastive Learning</td><td></td><td>2</td><td>3</td></tr><tr><td>Convolution</td><td></td><td>3</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td></td><td>2</td><td>1</td></tr><tr><td>Counter-factual</td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td>2</td><td>1</td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td>1</td></tr><tr><td>Diffusion Model</td><td></td><td>2</td><td>3</td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>2</td></tr><tr><td>Explainable AI</td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td>1</td></tr><tr><td>Federated Learning</td><td></td><td></td><td>4</td></tr><tr><td>Few-shot</td><td></td><td></td><td>1</td></tr><tr><td>Few-shot Learning</td><td></td><td></td><td>1</td></tr><tr><td>Fine-tuning</td><td>7</td><td>5</td><td></td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td></td></tr><tr><td>GPT</td><td>2</td><td>1</td><td></td></tr><tr><td>GPT-3</td><td>1</td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td></td><td></td></tr><tr><td>GPT-4</td><td></td><td>1</td><td></td></tr><tr><td>Generative AI</td><td></td><td></td><td>1</td></tr><tr><td>Geometry</td><td></td><td>1</td><td>1</td></tr><tr><td>Graph</td><td>5</td><td>1</td><td>6</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td>1</td></tr><tr><td>Graph Contrastive Learning</td><td></td><td></td><td>1</td></tr><tr><td>Graph Convolutional Network</td><td></td><td>1</td><td></td></tr><tr><td>Graph Embedding</td><td></td><td></td><td>1</td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>4</td></tr><tr><td>Grounding</td><td>1</td><td></td><td></td></tr><tr><td>High-Resource</td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>3</td><td></td><td>2</td></tr><tr><td>Knowledge Distillation</td><td>4</td><td>7</td><td></td></tr><tr><td>Knowledge Graph</td><td>6</td><td></td><td></td></tr><tr><td>LSTM</td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>22</td><td>7</td><td>6</td></tr><tr><td>Low-Resource</td><td>1</td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td>1</td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>1</td></tr><tr><td>Meta Learning</td><td></td><td></td><td>1</td></tr><tr><td>Multi-modal</td><td>4</td><td>6</td><td>1</td></tr><tr><td>Neural Machine Translation</td><td>5</td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>1</td><td></td></tr><tr><td>Open-Domain Question Answering</td><td>3</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>1</td></tr><tr><td>Out-of-domain</td><td></td><td>1</td><td></td></tr><tr><td>Pre-trained Language Model</td><td>2</td><td></td><td></td></tr><tr><td>Prompt</td><td>2</td><td>5</td><td>1</td></tr><tr><td>Pruning</td><td></td><td></td><td>1</td></tr><tr><td>Quantization</td><td></td><td></td><td>4</td></tr><tr><td>Question Answering</td><td>9</td><td></td><td></td></tr><tr><td>Reasoning</td><td>3</td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td>1</td><td>1</td><td>2</td></tr><tr><td>Reinforcement Learning</td><td></td><td></td><td>1</td></tr><tr><td>Representation Learning</td><td></td><td>2</td><td>2</td></tr><tr><td>Retrieval-Augmented Generation</td><td>6</td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td>1</td></tr><tr><td>Self-supervised Learning</td><td></td><td>7</td><td>2</td></tr><tr><td>Semi-Supervised Learning</td><td></td><td>1</td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td>3</td></tr><tr><td>Simulator</td><td></td><td></td><td>3</td></tr><tr><td>Stemming</td><td></td><td>1</td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>2</td></tr><tr><td>Summarization</td><td>1</td><td></td><td>1</td></tr><tr><td>Supervised Learning</td><td></td><td>3</td><td>4</td></tr><tr><td>Text Classification</td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>4</td><td>1</td></tr><tr><td>Transfer Learning</td><td></td><td>2</td><td>1</td></tr><tr><td>Transformer</td><td>2</td><td>7</td><td>5</td></tr><tr><td>Unsupervised Learning</td><td>1</td><td>1</td><td>3</td></tr><tr><td>Vision Transformer</td><td></td><td>4</td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>1</td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td></td></tr><tr><td>Word Embedding</td><td>1</td><td>1</td><td></td></tr><tr><td>Zero-shot</td><td>2</td><td>2</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscv-29>cs.CV (29)</h2><h3 id=129--1128-hyperspectral-image-analysis-in-single-modal-and-multimodal-setting-using-deep-learning-techniques-shivam-pande-2024>(1/29 | 1/128) Hyperspectral Image Analysis in Single-Modal and Multimodal setting using Deep Learning Techniques (Shivam Pande, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shivam Pande. (2024)<br><strong>Hyperspectral Image Analysis in Single-Modal and Multimodal setting using Deep Learning Techniques</strong><br><button class=copy-to-clipboard title="Hyperspectral Image Analysis in Single-Modal and Multimodal setting using Deep Learning Techniques" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 106<br>Keywords: Adversarial Learning, Autoencoder, Convolution, Knowledge Distillation, Knowledge Distillation, Multi-modal, Multi-modal, Self-supervised Learning, Self-supervised Learning, Semi-Supervised Learning, Recurrent Neural Network, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01546v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01546v1.pdf filename=2403.01546v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyperspectral imaging provides precise classification for land use and cover due to its exceptional spectral resolution. However, the challenges of high dimensionality and limited spatial resolution hinder its effectiveness. This study addresses these challenges by employing deep learning techniques to efficiently process, extract features, and classify data in an integrated manner. To enhance spatial resolution, we integrate information from complementary modalities such as LiDAR and SAR data through <b>multimodal</b> learning. Moreover, <b>adversarial</b> <b>learning</b> and <b>knowledge</b> <b>distillation</b> are utilized to overcome issues <b>stemming</b> from domain disparities and missing modalities. We also tailor deep learning architectures to suit the unique characteristics of HSI data, utilizing 1D <b>convolutional</b> and <b>recurrent</b> <b>neural</b> <b>networks</b> to handle its continuous spectral dimension. Techniques like visual attention and feedback connections within the architecture bolster the robustness of feature extraction. Additionally, we tackle the issue of limited training samples through <b>self-supervised</b> <b>learning</b> methods, employing <b>autoencoders</b> for dimensionality reduction and exploring <b>semi-supervised</b> <b>learning</b> techniques that leverage unlabeled data. Our proposed approaches are evaluated across various HSI datasets, consistently outperforming existing state-of-the-art techniques.</p></p class="citation"></blockquote><h3 id=229--2128-self-supervised-representation-learning-with-meta-comprehensive-regularization-huijie-guo-et-al-2024>(2/29 | 2/128) Self-Supervised Representation Learning with Meta Comprehensive Regularization (Huijie Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huijie Guo, Ying Ba, Jie Hu, Lingyu Si, Wenwen Qiang, Lei Shi. (2024)<br><strong>Self-Supervised Representation Learning with Meta Comprehensive Regularization</strong><br><button class=copy-to-clipboard title="Self-Supervised Representation Learning with Meta Comprehensive Regularization" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 58<br>Keywords: Object Detection, Benchmarking, Counter-factual, Data Augmentation, Representation Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01549v1.pdf filename=2403.01549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-Supervised</b> <b>Learning</b> (SSL) methods harness the concept of semantic invariance by utilizing <b>data</b> <b>augmentation</b> strategies to produce similar <b>representations</b> <b>for</b> different deformations of the same input. Essentially, the model captures the shared information among multiple augmented views of samples, while disregarding the non-shared information that may be beneficial for downstream tasks. To address this issue, we introduce a module called CompMod with Meta Comprehensive Regularization (MCR), embedded into existing <b>self-supervised</b> <b>frameworks,</b> to make the learned <b>representations</b> <b>more</b> comprehensive. Specifically, we update our proposed model through a bi-level optimization mechanism, enabling it to capture comprehensive features. Additionally, guided by the constrained extraction of features using maximum entropy coding, the <b>self-supervised</b> <b>learning</b> model learns more comprehensive features on top of learning consistent features. In addition, we provide theoretical support for our proposed method from information theory and causal <b>counterfactual</b> perspective. Experimental results show that our method achieves significant improvement in classification, <b>object</b> <b>detection</b> and instance segmentation tasks on multiple <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=329--3128-eagle-eigen-aggregation-learning-for-object-centric-unsupervised-semantic-segmentation-chanyoung-kim-et-al-2024>(3/29 | 3/128) EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation (Chanyoung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chanyoung Kim, Woojung Han, Dayun Ju, Seong Jae Hwang. (2024)<br><strong>EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 55<br>Keywords: Vision Transformer, Representation Learning, Self-supervised Learning, Unsupervised Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01482v1.pdf filename=2403.01482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic segmentation has innately relied on extensive pixel-level labeled annotated data, leading to the emergence of <b>unsupervised</b> methodologies. Among them, leveraging <b>self-supervised</b> <b>Vision</b> <b>Transformers</b> for <b>unsupervised</b> semantic segmentation (USS) has been making steady progress with expressive deep features. Yet, for semantically segmenting images with complex objects, a predominant challenge remains: the lack of explicit object-level semantic encoding in patch-level features. This technical limitation often leads to inadequate segmentation of complex objects with diverse structures. To address this gap, we present a novel approach, EAGLE, which emphasizes object-centric <b>representation</b> <b>learning</b> for <b>unsupervised</b> semantic segmentation. Specifically, we introduce EiCue, a spectral technique providing semantic and structural cues through an eigenbasis derived from the semantic similarity matrix of deep image features and color affinity from an image. Further, by incorporating our object-centric contrastive loss with EiCue, we guide our model to learn object-level <b>representations</b> <b>with</b> intra- and inter-image object-feature consistency, thereby enhancing semantic accuracy. Extensive experiments on COCO-Stuff, Cityscapes, and Potsdam-3 datasets demonstrate the state-of-the-art USS results of EAGLE with accurate and consistent semantic segmentation across complex scenes.</p></p class="citation"></blockquote><h3 id=429--4128-schema-state-changes-matter-for-procedure-planning-in-instructional-videos-yulei-niu-et-al-2024>(4/29 | 4/128) SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos (Yulei Niu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulei Niu, Wenliang Guo, Long Chen, Xudong Lin, Shih-Fu Chang. (2024)<br><strong>SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos</strong><br><button class=copy-to-clipboard title="SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Contrastive Learning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01599v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01599v1.pdf filename=2403.01599v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of procedure planning in instructional videos, which aims to make a goal-oriented sequence of action steps given partial visual state observations. The motivation of this problem is to learn a structured and plannable state and action space. Recent works succeeded in sequence modeling of steps with only sequence-level annotations accessible during training, which overlooked the roles of states in the procedures. In this work, we point out that State CHangEs MAtter (SCHEMA) for procedure planning in instructional videos. We aim to establish a more structured state space by investigating the causal relations between steps and states in procedures. Specifically, we explicitly represent each step as state changes and track the state changes in procedures. For step representation, we leveraged the commonsense knowledge in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to describe the state changes of steps via our designed <b>chain-of-thought</b> <b>prompting.</b> For state change tracking, we align visual state observations with language state descriptions via cross-modal <b>contrastive</b> <b>learning,</b> and explicitly model the intermediate states of the procedure using <b>LLM-generated</b> state descriptions. Experiments on CrossTask, COIN, and NIV <b>benchmark</b> datasets demonstrate that our proposed SCHEMA model achieves state-of-the-art performance and obtains explainable visualizations.</p></p class="citation"></blockquote><h3 id=529--5128-kick-back--relax-scaling-beyond-ground-truth-depth-with-slowtv--cribstv-jaime-spencer-et-al-2024>(5/29 | 5/128) Kick Back & Relax++: Scaling Beyond Ground-Truth Depth with SlowTV & CribsTV (Jaime Spencer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaime Spencer, Chris Russell, Simon Hadfield, Richard Bowden. (2024)<br><strong>Kick Back & Relax++: Scaling Beyond Ground-Truth Depth with SlowTV & CribsTV</strong><br><button class=copy-to-clipboard title="Kick Back & Relax++: Scaling Beyond Ground-Truth Depth with SlowTV & CribsTV" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 50<br>Keywords: Self-supervised Learning, Self-supervised Learning, Supervised Learning, Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01569v1.pdf filename=2403.01569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> is the key to unlocking generic computer vision systems. By eliminating the reliance on ground-truth annotations, it allows scaling to much larger data quantities. Unfortunately, <b>self-supervised</b> <b>monocular</b> depth estimation (SS-MDE) has been limited by the absence of diverse training data. Existing datasets have focused exclusively on urban driving in densely populated cities, resulting in models that fail to generalize beyond this domain. To address these limitations, this paper proposes two novel datasets: SlowTV and CribsTV. These are large-scale datasets curated from publicly available YouTube videos, containing a total of 2M training frames. They offer an incredibly diverse set of environments, ranging from snowy forests to coastal roads, luxury mansions and even underwater coral reefs. We leverage these datasets to tackle the challenging task of <b>zero-shot</b> generalization, outperforming every existing SS-MDE approach and even some state-of-the-art <b>supervised</b> methods. The generalization capabilities of our models are further enhanced by a range of components and contributions: 1) learning the camera intrinsics, 2) a stronger augmentation regime targeting aspect ratio changes, 3) support frame randomization, 4) flexible motion estimation, 5) a modern <b>transformer-based</b> architecture. We demonstrate the effectiveness of each component in extensive ablation experiments. To facilitate the development of future research, we make the datasets, code and pretrained models available to the public at <a href=https://github.com/jspenmar/slowtv_monodepth>https://github.com/jspenmar/slowtv_monodepth</a>.</p></p class="citation"></blockquote><h3 id=629--6128-guardt2i-defending-text-to-image-models-from-adversarial-prompts-yijun-yang-et-al-2024>(6/29 | 6/128) GuardT2I: Defending Text-to-Image Models from Adversarial Prompts (Yijun Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijun Yang, Ruiyuan Gao, Xiao Yang, Jianyuan Zhong, Qiang Xu. (2024)<br><strong>GuardT2I: Defending Text-to-Image Models from Adversarial Prompts</strong><br><button class=copy-to-clipboard title="GuardT2I: Defending Text-to-Image Models from Adversarial Prompts" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Text2image, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01446v1.pdf filename=2403.01446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Text-to-Image</b> (T2I) models have raised significant safety concerns about their potential misuse for generating inappropriate or Not-Safe-For-Work (NSFW) contents, despite existing countermeasures such as NSFW classifiers or model <b>fine-tuning</b> for inappropriate concept removal. Addressing this challenge, our study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance T2I models&rsquo; robustness against adversarial <b>prompts.</b> Instead of making a binary classification, GuardT2I utilizes a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> to conditionally transform text guidance embeddings within the T2I models into natural language for effective adversarial <b>prompt</b> detection, without compromising the models&rsquo; inherent performance. Our extensive experiments reveal that GuardT2I outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios.</p></p class="citation"></blockquote><h3 id=729--7128-scott-accelerating-diffusion-models-with-stochastic-consistency-distillation-hongjian-liu-et-al-2024>(7/29 | 7/128) SCott: Accelerating Diffusion Models with Stochastic Consistency Distillation (Hongjian Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongjian Liu, Qingsong Xie, Zhijie Deng, Chen Chen, Shixiang Tang, Fueyang Fu, Zheng-jun Zha, Haonan Lu. (2024)<br><strong>SCott: Accelerating Diffusion Models with Stochastic Consistency Distillation</strong><br><button class=copy-to-clipboard title="SCott: Accelerating Diffusion Models with Stochastic Consistency Distillation" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Knowledge Distillation, Knowledge Distillation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01505v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01505v1.pdf filename=2403.01505v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The iterative sampling procedure employed by <b>diffusion</b> <b>models</b> (DMs) often leads to significant inference latency. To address this, we propose Stochastic Consistency <b>Distillation</b> (SCott) to enable accelerated <b>text-to-image</b> generation, where high-quality generations can be achieved with just 1-2 sampling steps, and further improvements can be obtained by adding additional steps. In contrast to vanilla consistency <b>distillation</b> (CD) which <b>distills</b> the ordinary differential equation solvers-based sampling process of a pretrained teacher model into a student, SCott explores the possibility and validates the efficacy of integrating stochastic differential equation (SDE) solvers into CD to fully unleash the potential of the teacher. SCott is augmented with elaborate strategies to control the noise strength and sampling process of the SDE solver. An adversarial loss is further incorporated to strengthen the sample quality with rare sampling steps. Empirically, on the MSCOCO-2017 5K dataset with a Stable <b>Diffusion-V1.5</b> <b>teacher,</b> SCott achieves an FID (Frechet Inceptio Distance) of 22.1, surpassing that (23.4) of the 1-step InstaFlow (Liu et al., 2023) and matching that of 4-step UFOGen (Xue et al., 2023b). Moreover, SCott can yield more diverse samples than other consistency models for high-resolution image generation (Luo et al., 2023a), with up to 16% improvement in a qualified metric. The code and checkpoints are coming soon.</p></p class="citation"></blockquote><h3 id=829--8128-is-in-domain-data-beneficial-in-transfer-learning-for-landmarks-detection-in-x-ray-images-roberto-di-via-et-al-2024>(8/29 | 8/128) Is in-domain data beneficial in transfer learning for landmarks detection in x-ray images? (Roberto Di Via et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roberto Di Via, Matteo Santacesaria, Francesca Odone, Vito Paolo Pastore. (2024)<br><strong>Is in-domain data beneficial in transfer learning for landmarks detection in x-ray images?</strong><br><button class=copy-to-clipboard title="Is in-domain data beneficial in transfer learning for landmarks detection in x-ray images?" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Out-of-domain, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01470v1.pdf filename=2403.01470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, deep learning has emerged as a promising technique for medical image analysis. However, this application domain is likely to suffer from a limited availability of large public datasets and annotations. A common solution to these challenges in deep learning is the usage of a <b>transfer</b> <b>learning</b> framework, typically with a <b>fine-tuning</b> protocol, where a large-scale source dataset is used to pre-train a model, further <b>fine-tuned</b> on the target dataset. In this paper, we present a systematic study analyzing whether the usage of small-scale in-domain x-ray image datasets may provide any improvement for landmark detection over models pre-trained on large natural image datasets only. We focus on the multi-landmark localization task for three datasets, including chest, head, and hand x-ray images. Our results show that using in-domain source datasets brings marginal or no benefit with respect to an ImageNet <b>out-of-domain</b> pre-training. Our findings can provide an indication for the development of robust landmark detection systems in medical images when no large annotated dataset is available.</p></p class="citation"></blockquote><h3 id=929--9128-image2sentence-based-asymmetrical-zero-shot-composed-image-retrieval-yongchao-du-et-al-2024>(9/29 | 9/128) Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval (Yongchao Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongchao Du, Min Wang, Wengang Zhou, Shuping Hui, Houqiang Li. (2024)<br><strong>Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval</strong><br><button class=copy-to-clipboard title="Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Zero-shot, Vision-and-Language, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01431v1.pdf filename=2403.01431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of composed image retrieval (CIR) aims to retrieve images based on the query image and the text describing the users&rsquo; intent. Existing methods have made great progress with the advanced large <b>vision-language</b> (VL) model in CIR task, however, they generally suffer from two main issues: lack of labeled triplets for model training and difficulty of deployment on resource-restricted environments when deploying the large <b>vision-language</b> model. To tackle the above problems, we propose Image2Sentence based Asymmetric <b>zero-shot</b> composed image retrieval (ISA), which takes advantage of the VL model and only relies on unlabeled images for composition learning. In the framework, we propose a new adaptive token learner that maps an image to a sentence in the <b>word</b> <b>embedding</b> space of VL model. The sentence adaptively captures discriminative visual information and is further integrated with the text modifier. An asymmetric structure is devised for flexible deployment, in which the lightweight model is adopted for the query side while the large VL model is deployed on the gallery side. The global contrastive <b>distillation</b> and the local alignment regularization are adopted for the alignment between the light model and the VL model for CIR task. Our experiments demonstrate that the proposed ISA could better cope with the real retrieval scenarios and further improve retrieval accuracy and efficiency.</p></p class="citation"></blockquote><h3 id=1029--10128-lum-vit-learnable-under-sampling-mask-vision-transformer-for-bandwidth-limited-optical-signal-acquisition-lingfeng-liu-et-al-2024>(10/29 | 10/128) LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition (Lingfeng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingfeng Liu, Dong Ni, Hangjie Yuan. (2024)<br><strong>LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition</strong><br><button class=copy-to-clipboard title="LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV, eess-SP<br>Keyword Score: 40<br>Keywords: Vision Transformer, Fine-tuning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01412v1.pdf filename=2403.01412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bandwidth constraints during signal acquisition frequently impede real-time detection applications. Hyperspectral data is a notable example, whose vast volume compromises real-time hyperspectral detection. To tackle this hurdle, we introduce a novel approach leveraging pre-acquisition modulation to reduce the acquisition volume. This modulation process is governed by a deep learning model, utilizing prior information. Central to our approach is LUM-ViT, a <b>Vision</b> <b>Transformer</b> variant. Uniquely, LUM-ViT incorporates a learnable under-sampling mask tailored for pre-acquisition modulation. To further optimize for optical calculations, we propose a kernel-level weight binarization technique and a three-stage <b>fine-tuning</b> strategy. Our evaluations reveal that, by sampling a mere 10% of the original image pixels, LUM-ViT maintains the accuracy loss within 1.8% on the ImageNet classification task. The method sustains near-original accuracy when implemented on real-world optical hardware, demonstrating its practicality. Code will be available at <a href=https://github.com/MaxLLF/LUM-ViT>https://github.com/MaxLLF/LUM-ViT</a>.</p></p class="citation"></blockquote><h3 id=1129--11128-multiview-subspace-clustering-of-hyperspectral-images-based-on-graph-convolutional-networks-xianju-li-et-al-2024>(11/29 | 11/128) Multiview Subspace Clustering of Hyperspectral Images based on Graph Convolutional Networks (Xianju Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianju Li, Renxiang Guan, Zihao Li, Hao Liu, Jing Yang. (2024)<br><strong>Multiview Subspace Clustering of Hyperspectral Images based on Graph Convolutional Networks</strong><br><button class=copy-to-clipboard title="Multiview Subspace Clustering of Hyperspectral Images based on Graph Convolutional Networks" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Graph Convolutional Network, Graph, Clustering, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01465v1.pdf filename=2403.01465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-dimensional and complex spectral structures make <b>clustering</b> of hy-perspectral images (HSI) a challenging task. Subspace <b>clustering</b> has been shown to be an effective approach for addressing this problem. However, current subspace <b>clustering</b> algorithms are mainly designed for a single view and do not fully exploit spatial or texture feature information in HSI. This study proposed a multiview subspace <b>clustering</b> of HSI based on <b>graph</b> <b>convolutional</b> <b>networks.</b> (1) This paper uses the powerful classification ability of <b>graph</b> <b>convolutional</b> <b>network</b> and the learning ability of topologi-cal relationships between nodes to analyze and express the spatial relation-ship of HSI. (2) Pixel texture and pixel neighbor spatial-spectral infor-mation were sent to construct two <b>graph</b> <b>convolutional</b> <b>subspaces.</b> (3) An attention-based fusion module was used to adaptively construct a more discriminative feature map. The model was evaluated on three popular HSI datasets, including Indian Pines, Pavia University, and Houston. It achieved overall accuracies of 92.38%, 93.43%, and 83.82%, respectively and significantly outperformed the state-of-the-art <b>clustering</b> methods. In conclusion, the proposed model can effectively improve the <b>clustering</b> ac-curacy of HSI.</p></p class="citation"></blockquote><h3 id=1229--12128-moviellm-enhancing-long-video-understanding-with-ai-generated-movies-zhende-song-et-al-2024>(12/29 | 12/128) MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies (Zhende Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhende Song, Chenchen Wang, Jiamu Sheng, Chi Zhang, Gang Yu, Jiayuan Fan, Tao Chen. (2024)<br><strong>MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies</strong><br><button class=copy-to-clipboard title="MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, GPT, GPT-4, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01422v1.pdf filename=2403.01422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of <b>multimodal</b> models has marked a significant step forward in how machines understand videos. These models have shown promise in analyzing short video clips. However, when it comes to longer formats like movies, they often fall short. The main hurdles are the lack of high-quality, diverse video data and the intensive work required to collect or annotate such data. In the face of these challenges, we propose MovieLLM, a novel framework designed to create synthetic, high-quality data for long videos. This framework leverages the power of <b>GPT-4</b> and <b>text-to-image</b> models to generate detailed scripts and corresponding visuals. Our approach stands out for its flexibility and scalability, making it a superior alternative to traditional data collection methods. Our extensive experiments validate that the data produced by MovieLLM significantly improves the performance of <b>multimodal</b> models in understanding complex video narratives, overcoming the limitations of existing datasets regarding scarcity and bias.</p></p class="citation"></blockquote><h3 id=1329--13128-dynamic-adapter-meets-prompt-tuning-parameter-efficient-transfer-learning-for-point-cloud-analysis-xin-zhou-et-al-2024>(13/29 | 13/128) Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis (Xin Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Zhou, Dingkang Liang, Wei Xu, Xingkui Zhu, Yihan Xu, Zhikang Zou, Xiang Bai. (2024)<br><strong>Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis</strong><br><button class=copy-to-clipboard title="Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Transfer Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01439v1.pdf filename=2403.01439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point cloud analysis has achieved outstanding performance by transferring point cloud pre-trained models. However, existing methods for model adaptation usually update all model parameters, i.e., full <b>fine-tuning</b> paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient <b>transfer</b> <b>learning</b> for point cloud analysis with an ideal trade-off between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pre-trained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with <b>Prompt</b> Tuning (DAPT) by constructing Internal <b>Prompts,</b> capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full <b>fine-tuning</b> counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at <a href=https://github.com/LMD0311/DAPT>https://github.com/LMD0311/DAPT</a>.</p></p class="citation"></blockquote><h3 id=1429--14128-gptsee-enhancing-moment-retrieval-and-highlight-detection-via-description-based-similarity-features-yunzhuo-sun-et-al-2024>(14/29 | 14/128) GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features (Yunzhuo Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunzhuo Sun, Yifang Xu, Zien Xie, Yukun Shu, Sidan Du. (2024)<br><strong>GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features</strong><br><button class=copy-to-clipboard title="GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01437v1.pdf filename=2403.01437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Moment retrieval (MR) and highlight detection (HD) aim to identify relevant moments and highlights in video from corresponding natural language query. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated proficiency in various computer vision tasks. However, existing methods for MR&amp;HD have not yet been integrated with <b>LLMs.</b> In this letter, we propose a novel two-stage model that takes the output of <b>LLMs</b> as the input to the second-stage <b>transformer</b> encoder-decoder. First, MiniGPT-4 is employed to generate the detailed description of the video frame and rewrite the query statement, fed into the encoder as new features. Then, semantic similarity is computed between the generated description and the rewritten queries. Finally, continuous high-similarity video frames are converted into span anchors, serving as prior position information for the decoder. Experiments demonstrate that our approach achieves a state-of-the-art result, and by using only span anchors and similarity scores as outputs, positioning accuracy outperforms traditional methods, like Moment-DETR.</p></p class="citation"></blockquote><h3 id=1529--15128-region-transformer-self-attention-region-based-class-agnostic-point-cloud-segmentation-dipesh-gyawali-et-al-2024>(15/29 | 15/128) Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation (Dipesh Gyawali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dipesh Gyawali, Jian Zhang, BB Karki. (2024)<br><strong>Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation</strong><br><button class=copy-to-clipboard title="Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 23<br>Keywords: Clustering, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01407v1.pdf filename=2403.01407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point cloud segmentation, which helps us understand the environment of specific structures and objects, can be performed in class-specific and class-agnostic ways. We propose a novel region-based <b>transformer</b> model called Region-Transformer for performing class-agnostic point cloud segmentation. The model utilizes a region-growth approach and <b>self-attention</b> mechanism to iteratively expand or contract a region by adding or removing points. It is trained on simulated point clouds with instance labels only, avoiding semantic labels. Attention-based networks have succeeded in many previous methods of performing point cloud segmentation. However, a region-growth approach with attention-based networks has yet to be used to explore its performance gain. To our knowledge, we are the first to use a <b>self-attention</b> mechanism in a region-growth approach. With the introduction of <b>self-attention</b> to region-growth that can utilize local contextual information of neighborhood points, our experiments demonstrate that the Region-Transformer model outperforms previous class-agnostic and class-specific methods on indoor datasets regarding <b>clustering</b> metrics. The model generalizes well to large-scale scenes. Key advantages include capturing long-range dependencies through <b>self-attention,</b> avoiding the need for semantic labels during training, and applicability to a variable number of objects. The Region-Transformer model represents a promising approach for flexible point cloud segmentation with applications in robotics, digital twinning, and autonomous vehicles.</p></p class="citation"></blockquote><h3 id=1629--16128-learning-a-physical-aware-diffusion-model-based-on-transformer-for-underwater-image-enhancement-chen-zhao-et-al-2024>(16/29 | 16/128) Learning A Physical-aware Diffusion Model Based on Transformer for Underwater Image Enhancement (Chen Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Zhao, Chenyu Dong, Weiling Cai. (2024)<br><strong>Learning A Physical-aware Diffusion Model Based on Transformer for Underwater Image Enhancement</strong><br><button class=copy-to-clipboard title="Learning A Physical-aware Diffusion Model Based on Transformer for Underwater Image Enhancement" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01497v1.pdf filename=2403.01497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Underwater visuals undergo various complex degradations, inevitably influencing the efficiency of underwater vision tasks. Recently, <b>diffusion</b> <b>models</b> were employed to underwater image enhancement (UIE) tasks, and gained SOTA performance. However, these methods fail to consider the physical properties and underwater imaging mechanisms in the <b>diffusion</b> <b>process,</b> limiting information completion capacity of <b>diffusion</b> <b>models.</b> In this paper, we introduce a novel UIE framework, named PA-Diff, designed to exploiting the knowledge of physics to guide the <b>diffusion</b> <b>process.</b> PA-Diff consists of Physics Prior Generation (PPG) Branch and Physics-aware <b>Diffusion</b> <b>Transformer</b> (PDT) Branch. Our designed PPG branch is a plug-and-play network to produce the physics prior, which can be integrated into any deep framework. With utilizing the physics prior knowledge to guide the <b>diffusion</b> <b>process,</b> PDT branch can obtain underwater-aware ability and model the complex distribution in real-world underwater scenes. Extensive experiments prove that our method achieves best performance on UIE tasks.</p></p class="citation"></blockquote><h3 id=1729--17128-regeneration-based-training-free-attribution-of-fake-images-generated-by-text-to-image-generative-models-meiling-li-et-al-2024>(17/29 | 17/128) Regeneration Based Training-free Attribution of Fake Images Generated by Text-to-Image Generative Models (Meiling Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meiling Li, Zhenxing Qian, Xinpeng Zhang. (2024)<br><strong>Regeneration Based Training-free Attribution of Fake Images Generated by Text-to-Image Generative Models</strong><br><button class=copy-to-clipboard title="Regeneration Based Training-free Attribution of Fake Images Generated by Text-to-Image Generative Models" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01489v1.pdf filename=2403.01489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> generative models have recently garnered significant attention due to their ability to generate images based on <b>prompt</b> descriptions. While these models have shown promising performance, concerns have been raised regarding the potential misuse of the generated fake images. In response to this, we have presented a simple yet effective training-free method to attribute fake images generated by <b>text-to-image</b> models to their source models. Given a test image to be attributed, we first inverse the textual <b>prompt</b> of the image, and then put the reconstructed <b>prompt</b> into different candidate models to regenerate candidate fake images. By calculating and ranking the similarity of the test image and the candidate images, we can determine the source of the image. This attribution allows model owners to be held accountable for any misuse of their models. Note that our approach does not limit the number of candidate <b>text-to-image</b> generative models. Comprehensive experiments reveal that (1) Our method can effectively attribute fake images to their source models, achieving comparable attribution performance with the state-of-the-art method; (2) Our method has high scalability ability, which is well adapted to real-world attribution scenarios. (3) The proposed method yields satisfactory robustness to common attacks, such as Gaussian blurring, JPEG compression, and Resizing. We also analyze the factors that influence the attribution performance, and explore the boost brought by the proposed method as a plug-in to improve the performance of existing SOTA. We hope our work can shed some light on the solutions to addressing the source of AI-generated images, as well as to prevent the misuse of <b>text-to-image</b> generative models.</p></p class="citation"></blockquote><h3 id=1829--18128-pyramid-feature-attention-network-for-monocular-depth-prediction-yifang-xu-et-al-2024>(18/29 | 18/128) Pyramid Feature Attention Network for Monocular Depth Prediction (Yifang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifang Xu, Chenglei Peng, Ming Li, Yang Li, Sidan Du. (2024)<br><strong>Pyramid Feature Attention Network for Monocular Depth Prediction</strong><br><button class=copy-to-clipboard title="Pyramid Feature Attention Network for Monocular Depth Prediction" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01440v1.pdf filename=2403.01440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>convolutional</b> <b>neural</b> <b>networks</b> (DCNNs) have achieved great success in monocular depth estimation (MDE). However, few existing works take the contributions for MDE of different levels feature maps into account, leading to inaccurate spatial layout, ambiguous boundaries and discontinuous object surface in the prediction. To better tackle these problems, we propose a Pyramid Feature Attention Network (PFANet) to improve the high-level context features and low-level spatial features. In the proposed PFANet, we design a Dual-scale Channel Attention Module (DCAM) to employ channel attention in different scales, which aggregate global context and local information from the high-level feature maps. To exploit the spatial relationship of visual features, we design a Spatial Pyramid Attention Module (SPAM) which can guide the network attention to multi-scale detailed information in the low-level feature maps. Finally, we introduce scale-invariant gradient loss to increase the penalty on errors in depth-wise discontinuous regions. Experimental results show that our method outperforms state-of-the-art methods on the KITTI dataset.</p></p class="citation"></blockquote><h3 id=1929--19128-logit-standardization-in-knowledge-distillation-shangquan-sun-et-al-2024>(19/29 | 19/128) Logit Standardization in Knowledge Distillation (Shangquan Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shangquan Sun, Wenqi Ren, Jingzhi Li, Rui Wang, Xiaochun Cao. (2024)<br><strong>Logit Standardization in Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Logit Standardization in Knowledge Distillation" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01427v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01427v1.pdf filename=2403.01427v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>distillation</b> involves transferring soft labels from a teacher to a student using a shared temperature-based softmax function. However, the assumption of a shared temperature between teacher and student implies a mandatory exact match between their logits in terms of logit range and variance. This side-effect limits the performance of student, considering the capacity discrepancy between them and the finding that the innate logit relations of teacher are sufficient for student to learn. To address this issue, we propose setting the temperature as the weighted standard deviation of logit and performing a plug-and-play Z-score pre-process of logit standardization before applying softmax and Kullback-Leibler divergence. Our pre-process enables student to focus on essential logit relations from teacher rather than requiring a magnitude match, and can improve the performance of existing logit-based <b>distillation</b> methods. We also show a typical case where the conventional setting of sharing temperature between teacher and student cannot reliably yield the authentic <b>distillation</b> evaluation; nonetheless, this challenge is successfully alleviated by our Z-score. We extensively evaluate our method for various student and teacher models on CIFAR-100 and ImageNet, showing its significant superiority. The vanilla <b>knowledge</b> <b>distillation</b> powered by our pre-process can achieve favorable performance against state-of-the-art methods, and other <b>distillation</b> variants can obtain considerable gain with the assistance of our pre-process.</p></p class="citation"></blockquote><h3 id=2029--20128-a-simple-but-effective-baseline-for-training-free-class-agnostic-counting-yuhao-lin-et-al-2024>(20/29 | 20/128) A Simple-but-effective Baseline for Training-free Class-Agnostic Counting (Yuhao Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhao Lin, Haiming Xu, Lingqiao Liu, Javen Qinfeng Shi. (2024)<br><strong>A Simple-but-effective Baseline for Training-free Class-Agnostic Counting</strong><br><button class=copy-to-clipboard title="A Simple-but-effective Baseline for Training-free Class-Agnostic Counting" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Foundation Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01418v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01418v1.pdf filename=2403.01418v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Class-Agnostic Counting (CAC) seeks to accurately count objects in a given image with only a few reference examples. While previous methods achieving this relied on additional training, recent efforts have shown that it&rsquo;s possible to accomplish this without training by utilizing pre-existing <b>foundation</b> <b>models,</b> particularly the Segment Anything Model (SAM), for counting via instance-level segmentation. Although promising, current training-free methods still lag behind their training-based counterparts in terms of performance. In this research, we present a straightforward training-free solution that effectively bridges this performance gap, serving as a strong baseline. The primary contribution of our work lies in the discovery of four key technologies that can enhance performance. Specifically, we suggest employing a superpixel algorithm to generate more precise initial point <b>prompts,</b> utilizing an image encoder with richer semantic knowledge to replace the SAM encoder for representing candidate objects, and adopting a multiscale mechanism and a transductive prototype scheme to update the representation of reference examples. By combining these four technologies, our approach achieves significant improvements over existing training-free methods and delivers performance on par with training-based ones.</p></p class="citation"></blockquote><h3 id=2129--21128-sa-mixnet-structure-aware-mixup-and-invariance-learning-for-scribble-supervised-road-extraction-in-remote-sensing-images-jie-feng-et-al-2024>(21/29 | 21/128) SA-MixNet: Structure-aware Mixup and Invariance Learning for Scribble-supervised Road Extraction in Remote Sensing Images (Jie Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Feng, Hao Huang, Junpeng Zhang, Weisheng Dong, Dingwen Zhang, Licheng Jiao. (2024)<br><strong>SA-MixNet: Structure-aware Mixup and Invariance Learning for Scribble-supervised Road Extraction in Remote Sensing Images</strong><br><button class=copy-to-clipboard title="SA-MixNet: Structure-aware Mixup and Invariance Learning for Scribble-supervised Road Extraction in Remote Sensing Images" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01381v1.pdf filename=2403.01381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mainstreamed weakly <b>supervised</b> road extractors rely on highly confident pseudo-labels propagated from scribbles, and their performance often degrades gradually as the image scenes tend various. We argue that such degradation is due to the poor model&rsquo;s invariance to scenes with different complexities, whereas existing solutions to this problem are commonly based on crafted priors that cannot be derived from scribbles. To eliminate the reliance on such priors, we propose a novel Structure-aware Mixup and Invariance Learning framework (SA-MixNet) for weakly <b>supervised</b> road extraction that improves the model invariance in a data-driven manner. Specifically, we design a structure-aware Mixup scheme to paste road regions from one image onto another for creating an image scene with increased complexity while preserving the road&rsquo;s structural integrity. Then an invariance regularization is imposed on the predictions of constructed and origin images to minimize their conflicts, which thus forces the model to behave consistently on various scenes. Moreover, a discriminator-based regularization is designed for enhancing the connectivity meanwhile preserving the structure of roads. Combining these designs, our framework demonstrates superior performance on the DeepGlobe, Wuhan, and Massachusetts datasets outperforming the state-of-the-art techniques by 1.47%, 2.12%, 4.09% respectively in IoU metrics, and showing its potential of plug-and-play. The code will be made publicly available.</p></p class="citation"></blockquote><h3 id=2229--22128-infimm-hd-a-leap-forward-in-high-resolution-multimodal-understanding-haogeng-liu-et-al-2024>(22/29 | 22/128) InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding (Haogeng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haogeng Liu, Quanzeng You, Xiaotian Han, Yiqi Wang, Bohan Zhai, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang. (2024)<br><strong>InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding</strong><br><button class=copy-to-clipboard title="InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01487v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01487v1.pdf filename=2403.01487v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have experienced significant advancements recently. Nevertheless, challenges persist in the accurate recognition and comprehension of intricate details within high-resolution images. Despite being indispensable for the development of robust MLLMs, this area remains underinvestigated. To tackle this challenge, our work introduces InfiMM-HD, a novel architecture specifically designed for processing images of different resolutions with low computational overhead. This innovation facilitates the enlargement of MLLMs to higher-resolution capabilities. InfiMM-HD incorporates a cross-attention module and visual windows to reduce computation costs. By integrating this architectural design with a four-stage training pipeline, our model attains improved visual perception efficiently and cost-effectively. Empirical study underscores the robustness and effectiveness of InfiMM-HD, opening new avenues for exploration in related areas. Codes and models can be found at <a href=https://huggingface.co/Infi-MM/infimm-hd>https://huggingface.co/Infi-MM/infimm-hd</a></p></p class="citation"></blockquote><h3 id=2329--23128-efficient-action-counting-with-dynamic-queries-zishi-li-et-al-2024>(23/29 | 23/128) Efficient Action Counting with Dynamic Queries (Zishi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zishi Li, Xiaoxuan Ma, Qiuyan Shang, Wentao Zhu, Hai Ci, Yu Qiao, Yizhou Wang. (2024)<br><strong>Efficient Action Counting with Dynamic Queries</strong><br><button class=copy-to-clipboard title="Efficient Action Counting with Dynamic Queries" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01543v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01543v2.pdf filename=2403.01543v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal repetition counting aims to quantify the repeated action cycles within a video. The majority of existing methods rely on the similarity correlation matrix to characterize the repetitiveness of actions, but their scalability is hindered due to the quadratic computational complexity. In this work, we introduce a novel approach that employs an action query representation to localize repeated action cycles with linear computational complexity. Based on this representation, we further develop two key components to tackle the essential challenges of temporal repetition counting. Firstly, to facilitate open-set action counting, we propose the dynamic update scheme on action queries. Unlike static action queries, this approach dynamically embeds video features into action queries, offering a more flexible and generalizable representation. Secondly, to distinguish between actions of interest and background noise actions, we incorporate inter-query <b>contrastive</b> <b>learning</b> to regularize the video representations corresponding to different action queries. As a result, our method significantly outperforms previous works, particularly in terms of long video sequences, unseen actions, and actions at various speeds. On the challenging RepCountA <b>benchmark,</b> we outperform the state-of-the-art method TransRAC by 26.5% in OBO accuracy, with a 22.7% mean error decrease and 94.1% computational burden reduction. Code is available at <a href=https://github.com/lizishi/DeTRC>https://github.com/lizishi/DeTRC</a>.</p></p class="citation"></blockquote><h3 id=2429--24128-aio2-online-correction-of-object-labels-for-deep-learning-with-incomplete-annotation-in-remote-sensing-image-segmentation-chenying-liu-et-al-2024>(24/29 | 24/128) AIO2: Online Correction of Object Labels for Deep Learning with Incomplete Annotation in Remote Sensing Image Segmentation (Chenying Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenying Liu, Conrad M Albrecht, Yi Wang, Qingyu Li, Xiao Xiang Zhu. (2024)<br><strong>AIO2: Online Correction of Object Labels for Deep Learning with Incomplete Annotation in Remote Sensing Image Segmentation</strong><br><button class=copy-to-clipboard title="AIO2: Online Correction of Object Labels for Deep Learning with Incomplete Annotation in Remote Sensing Image Segmentation" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01641v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01641v1.pdf filename=2403.01641v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While the volume of remote sensing data is increasing daily, deep learning in Earth Observation faces lack of accurate annotations for <b>supervised</b> optimization. Crowdsourcing projects such as OpenStreetMap distribute the annotation load to their community. However, such annotation inevitably generates noise due to insufficient control of the label quality, lack of annotators, frequent changes of the Earth&rsquo;s surface as a result of natural disasters and urban development, among many other factors. We present Adaptively trIggered Online Object-wise correction (AIO2) to address annotation noise induced by incomplete label sets. AIO2 features an Adaptive Correction Trigger (ACT) module that avoids label correction when the model training under- or overfits, and an Online Object-wise Correction (O2C) methodology that employs spatial information for automated label modification. AIO2 utilizes a mean teacher model to enhance training robustness with noisy labels to both stabilize the training accuracy curve for fitting in ACT and provide pseudo labels for correction in O2C. Moreover, O2C is implemented online without the need to store updated labels every training epoch. We validate our approach on two building footprint segmentation datasets with different spatial resolutions. Experimental results with varying degrees of building label noise demonstrate the robustness of AIO2. Source code will be available at <a href=https://github.com/zhu-xlab/AIO2.git>https://github.com/zhu-xlab/AIO2.git</a>.</p></p class="citation"></blockquote><h3 id=2529--25128-depth-estimation-algorithm-based-on-transformer-encoder-and-feature-fusion-linhan-xia-et-al-2024>(25/29 | 25/128) Depth Estimation Algorithm Based on Transformer-Encoder and Feature Fusion (Linhan Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linhan Xia, Junbang Liu, Tong Wu. (2024)<br><strong>Depth Estimation Algorithm Based on Transformer-Encoder and Feature Fusion</strong><br><button class=copy-to-clipboard title="Depth Estimation Algorithm Based on Transformer-Encoder and Feature Fusion" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01370v1.pdf filename=2403.01370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research presents a novel depth estimation algorithm based on a <b>Transformer-encoder</b> architecture, tailored for the NYU and KITTI Depth Dataset. This research adopts a <b>transformer</b> model, initially renowned for its success in natural language processing, to capture intricate spatial relationships in visual data for depth estimation tasks. A significant innovation of the research is the integration of a composite loss function that combines Structural Similarity Index Measure (SSIM) with Mean Squared Error (MSE). This combined loss function is designed to ensure the structural integrity of the predicted depth maps relative to the original images (via SSIM) while minimizing pixel-wise estimation errors (via MSE). This research approach addresses the challenges of over-smoothing often seen in MSE-based losses and enhances the model&rsquo;s ability to predict depth maps that are not only accurate but also maintain structural coherence with the input images. Through rigorous training and evaluation using the NYU Depth Dataset, the model demonstrates superior performance, marking a significant advancement in single-image depth estimation, particularly in complex indoor and traffic environments.</p></p class="citation"></blockquote><h3 id=2629--26128-matchu-matching-unseen-objects-for-6d-pose-estimation-from-rgb-d-images-junwen-huang-et-al-2024>(26/29 | 26/128) MatchU: Matching Unseen Objects for 6D Pose Estimation from RGB-D Images (Junwen Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junwen Huang, Hao Yu, Kuan-Ting Yu, Nassir Navab, Slobodan Ilic, Benjamin Busam. (2024)<br><strong>MatchU: Matching Unseen Objects for 6D Pose Estimation from RGB-D Images</strong><br><button class=copy-to-clipboard title="MatchU: Matching Unseen Objects for 6D Pose Estimation from RGB-D Images" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01517v1.pdf filename=2403.01517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent learning methods for object pose estimation require resource-intensive training for each individual object instance or category, hampering their scalability in real applications when confronted with previously unseen objects. In this paper, we propose MatchU, a Fuse-Describe-Match strategy for 6D pose estimation from RGB-D images. MatchU is a generic approach that fuses 2D texture and 3D geometric cues for 6D pose prediction of unseen objects. We rely on learning geometric 3D descriptors that are rotation-invariant by design. By encoding pose-agnostic <b>geometry,</b> the learned descriptors naturally generalize to unseen objects and capture symmetries. To tackle ambiguous associations using 3D <b>geometry</b> only, we fuse additional RGB information into our descriptor. This is achieved through a novel attention-based mechanism that fuses cross-modal information, together with a matching loss that leverages the latent space learned from RGB data to guide the descriptor learning process. Extensive experiments reveal the generalizability of both the RGB-D fusion strategy as well as the descriptor efficacy. Benefiting from the novel designs, MatchU surpasses all existing methods by a significant margin in terms of both accuracy and speed, even without the requirement of expensive re-training or rendering.</p></p class="citation"></blockquote><h3 id=2729--27128-occfusion-a-straightforward-and-effective-multi-sensor-fusion-framework-for-3d-occupancy-prediction-zhenxing-ming-et-al-2024>(27/29 | 27/128) OccFusion: A Straightforward and Effective Multi-Sensor Fusion Framework for 3D Occupancy Prediction (Zhenxing Ming et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenxing Ming, Julie Stephany Berrio, Mao Shan, Stewart Worrall. (2024)<br><strong>OccFusion: A Straightforward and Effective Multi-Sensor Fusion Framework for 3D Occupancy Prediction</strong><br><button class=copy-to-clipboard title="OccFusion: A Straightforward and Effective Multi-Sensor Fusion Framework for 3D Occupancy Prediction" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01644v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01644v1.pdf filename=2403.01644v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces OccFusion, a straightforward and efficient sensor fusion framework for predicting 3D occupancy. A comprehensive understanding of 3D scenes is crucial in autonomous driving, and recent models for 3D semantic occupancy prediction have successfully addressed the challenge of describing real-world objects with varied shapes and classes. However, existing methods for 3D occupancy prediction heavily rely on surround-view camera images, making them susceptible to changes in lighting and weather conditions. By integrating features from additional sensors, such as lidar and surround view radars, our framework enhances the accuracy and robustness of occupancy prediction, resulting in top-tier performance on the nuScenes <b>benchmark.</b> Furthermore, extensive experiments conducted on the nuScenes dataset, including challenging night and rainy scenarios, confirm the superior performance of our sensor fusion strategy across various perception ranges. The code for this framework will be made available at <a href=https://github.com/DanielMing123/OCCFusion>https://github.com/DanielMing123/OCCFusion</a>.</p></p class="citation"></blockquote><h3 id=2829--28128-a-unified-model-selection-technique-for-spectral-clustering-based-motion-segmentation-yuxiang-huang-et-al-2024>(28/29 | 28/128) A Unified Model Selection Technique for Spectral Clustering Based Motion Segmentation (Yuxiang Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxiang Huang, John Zelek. (2024)<br><strong>A Unified Model Selection Technique for Spectral Clustering Based Motion Segmentation</strong><br><button class=copy-to-clipboard title="A Unified Model Selection Technique for Spectral Clustering Based Motion Segmentation" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01606v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01606v1.pdf filename=2403.01606v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motion segmentation is a fundamental problem in computer vision and is crucial in various applications such as robotics, autonomous driving and action recognition. Recently, spectral <b>clustering</b> based methods have shown impressive results on motion segmentation in dynamic environments. These methods perform spectral <b>clustering</b> on motion affinity matrices to cluster objects or point trajectories in the scene into different motion groups. However, existing methods often need the number of motions present in the scene to be known, which significantly reduces their practicality. In this paper, we propose a unified model selection technique to automatically infer the number of motion groups for spectral <b>clustering</b> based motion segmentation methods by combining different existing model selection techniques together. We evaluate our method on the KT3DMoSeg dataset and achieve competitve results comparing to the baseline where the number of clusters is given as ground truth information.</p></p class="citation"></blockquote><h3 id=2929--29128-rethinking-clip-based-video-learners-in-cross-domain-open-vocabulary-action-recognition-kun-yu-lin-et-al-2024>(29/29 | 29/128) Rethinking CLIP-based Video Learners in Cross-Domain Open-Vocabulary Action Recognition (Kun-Yu Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun-Yu Lin, Henghui Ding, Jiaming Zhou, Yi-Xing Peng, Zhilin Zhao, Chen Change Loy, Wei-Shi Zheng. (2024)<br><strong>Rethinking CLIP-based Video Learners in Cross-Domain Open-Vocabulary Action Recognition</strong><br><button class=copy-to-clipboard title="Rethinking CLIP-based Video Learners in Cross-Domain Open-Vocabulary Action Recognition" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01560v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01560v1.pdf filename=2403.01560v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contrastive Language-Image Pretraining (CLIP) has shown remarkable open-vocabulary abilities across various image understanding tasks. Building upon this impressive success, recent pioneer works have proposed to adapt the powerful CLIP to video data, leading to efficient and effective video learners for open-vocabulary action recognition. Inspired by the fact that humans perform actions in diverse environments, our work delves into an intriguing question: Can CLIP-based video learners effectively generalize to video domains they have not encountered during training? To answer this, we establish a CROSS-domain Open-Vocabulary Action recognition <b>benchmark</b> named XOV-Action, and conduct a comprehensive evaluation of five state-of-the-art CLIP-based video learners under various types of domain gaps. Our evaluation demonstrates that previous methods exhibit limited action recognition performance in unseen video domains, revealing potential challenges of the cross-domain open-vocabulary action recognition task. To address this task, our work focuses on a critical challenge, namely scene bias, and we accordingly contribute a novel scene-aware video-text alignment method. Our key idea is to distinguish video representations apart from scene-encoded text representations, aiming to learn scene-agnostic video representations for recognizing actions across domains. Extensive experimental results demonstrate the effectiveness of our method. The <b>benchmark</b> and code will be available at <a href=https://github.com/KunyuLin/XOV-Action/>https://github.com/KunyuLin/XOV-Action/</a>.</p></p class="citation"></blockquote><h2 id=cscl-21>cs.CL (21)</h2><h3 id=121--30128-enhancing-neural-machine-translation-of-low-resource-languages-corpus-development-human-evaluation-and-explainable-ai-architectures-séamus-lankford-2024>(1/21 | 30/128) Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures (Séamus Lankford, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Séamus Lankford. (2024)<br><strong>Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures</strong><br><button class=copy-to-clipboard title="Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Explainable AI, Fine-tuning, High-Resource, Low-Resource, Recurrent Neural Network, Transformer, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01580v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01580v1.pdf filename=2403.01580v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the current <b>machine</b> <b>translation</b> <b>(MT)</b> landscape, the <b>Transformer</b> architecture stands out as the gold standard, especially for <b>high-resource</b> language pairs. This research delves into its efficacy for <b>low-resource</b> language pairs including both the English$\leftrightarrow$Irish and English$\leftrightarrow$Marathi language pairs. Notably, the study identifies the optimal hyperparameters and subword model type to significantly improve the translation quality of <b>Transformer</b> models for <b>low-resource</b> language pairs. The scarcity of parallel datasets for <b>low-resource</b> languages can hinder <b>MT</b> development. To address this, gaHealth was developed, the first bilingual corpus of health data for the Irish language. Focusing on the health domain, models developed using this in-domain dataset exhibited very significant improvements in <b>BLEU</b> score when compared with models from the LoResMT2021 Shared Task. A subsequent human evaluation using the multidimensional quality metrics error taxonomy showcased the superior performance of the <b>Transformer</b> system in reducing both accuracy and fluency errors compared to an <b>RNN-based</b> counterpart. Furthermore, this thesis introduces adaptNMT and adaptMLLM, two open-source applications streamlined for the development, <b>fine-tuning,</b> and deployment of <b>neural</b> <b>machine</b> <b>translation</b> models. These tools considerably simplify the setup and evaluation process, making <b>MT</b> more accessible to both developers and translators. Notably, adaptNMT, grounded in the OpenNMT ecosystem, promotes eco-friendly natural language processing research by highlighting the environmental footprint of model development. <b>Fine-tuning</b> of MLLMs by adaptMLLM demonstrated advancements in translation performance for two <b>low-resource</b> language pairs: English$\leftrightarrow$Irish and English$\leftrightarrow$Marathi, compared to baselines from the LoResMT2021 Shared Task.</p></p class="citation"></blockquote><h3 id=221--31128-align-to-distill-trainable-attention-alignment-for-knowledge-distillation-in-neural-machine-translation-heegon-jin-et-al-2024>(2/21 | 31/128) Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation (Heegon Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heegon Jin, Seonil Son, Jemin Park, Youngseok Kim, Hyungjong Noh, Yeonsoo Lee. (2024)<br><strong>Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation</strong><br><button class=copy-to-clipboard title="Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Transformer, Attention Alignment, Neural Machine Translation, Neural Machine Translation, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01479v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01479v1.pdf filename=2403.01479v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of scalable deep models and large datasets has improved the performance of <b>Neural</b> <b>Machine</b> <b>Translation.</b> <b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> enhances efficiency by transferring <b>knowledge</b> <b>from</b> a teacher model to a more compact student model. However, <b>KD</b> approaches to <b>Transformer</b> architecture often rely on heuristics, particularly when deciding which teacher layers to <b>distill</b> from. In this paper, we introduce the &lsquo;Align-to-Distill&rsquo; (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student <b>attention</b> <b>heads</b> with their teacher counterparts during training. The <b>Attention</b> <b>Alignment</b> Module in A2D performs a dense head-by-head comparison between student and teacher <b>attention</b> <b>heads</b> across layers, turning the combinatorial mapping heuristics into a learning problem. Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 <b>BLEU</b> points for WMT-2022 De->Dsb and WMT-2014 En->De, respectively, compared to <b>Transformer</b> baselines.</p></p class="citation"></blockquote><h3 id=321--32128-serval-synergy-learning-between-vertical-models-and-llms-towards-oracle-level-zero-shot-medical-prediction-jiahuan-yan-et-al-2024>(3/21 | 32/128) SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction (Jiahuan Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahuan Yan, Jintai Chen, Chaowen Hu, Bo Zheng, Yaojun Hu, Jimeng Sun, Jian Wu. (2024)<br><strong>SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction</strong><br><button class=copy-to-clipboard title="SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Unsupervised Learning, Zero-shot, GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01570v1.pdf filename=2403.01570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has exhibited impressive <b>zero-shot</b> proficiency on generic and common sense questions. However, <b>LLMs&rsquo;</b> application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge. Furthermore, the vertical data annotation process often requires labor-intensive expert involvement, thereby presenting an additional challenge in enhancing the model&rsquo;s vertical capabilities. In this paper, we propose SERVAL, a synergy learning pipeline designed for <b>unsupervised</b> development of vertical capabilities in both <b>LLMs</b> and small models by mutual enhancement. Specifically, SERVAL utilizes the <b>LLM&rsquo;s</b> <b>zero-shot</b> outputs as annotations, leveraging its confidence to teach a robust vertical model from scratch. Reversely, the trained vertical model guides the <b>LLM</b> <b>fine-tuning</b> to enhance its <b>zero-shot</b> capability, progressively improving both models through an iterative process. In medical domain, known for complex vertical knowledge and costly annotations, comprehensive experiments show that, without access to any gold labels, SERVAL with the synergy learning of OpenAI <b>GPT-3.5</b> and a simple model attains fully-supervised competitive performance across ten widely used medical datasets. These datasets represent vertically specialized medical diagnostic scenarios (e.g., diabetes, heart diseases, COVID-19), highlighting the potential of SERVAL in refining the vertical capabilities of <b>LLMs</b> and training vertical models from scratch, all achieved without the need for annotations.</p></p class="citation"></blockquote><h3 id=421--33128-fine-tuning-vs-retrieval-augmented-generation-for-less-popular-knowledge-heydar-soudani-et-al-2024>(4/21 | 33/128) Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge (Heydar Soudani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi. (2024)<br><strong>Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge</strong><br><button class=copy-to-clipboard title="Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Data Augmentation, Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01432v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01432v2.pdf filename=2403.01432v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of <b>LLMs</b> on low-frequent topics are: <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)</b> and <b>fine-tuning</b> (FT) over synthetic <b>data.</b> <b>This</b> paper explores and evaluates the impact of <b>RAG</b> and FT on customizing <b>LLMs</b> in handling low-frequency entities on <b>question</b> <b>answering</b> task. Our findings indicate that FT significantly boosts the performance across entities of varying popularity, especially in the most and least popular groups, while <b>RAG</b> surpasses other methods. Additionally, the success of both <b>RAG</b> and FT approaches is amplified by advancements in <b>retrieval</b> <b>and</b> <b>data</b> <b>augmentation</b> techniques. We release our <b>data</b> <b>and</b> code at <a href=https://github.com/informagi/RAGvsFT>https://github.com/informagi/RAGvsFT</a>.</p></p class="citation"></blockquote><h3 id=521--34128-right-for-right-reasons-large-language-models-for-verifiable-commonsense-knowledge-graph-question-answering-armin-toroghi-et-al-2024>(5/21 | 34/128) Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering (Armin Toroghi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Armin Toroghi, Willis Guo, Mohammad Mahdi Abdollah Pour, Scott Sanner. (2024)<br><strong>Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering</strong><br><button class=copy-to-clipboard title="Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Common-sense Reasoning, Grounding, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01390v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01390v1.pdf filename=2403.01390v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>Graph</b> <b>Question</b> <b>Answering</b> (KGQA) methods seek to answer Natural Language <b>questions</b> <b>using</b> the relational information stored in <b>Knowledge</b> <b>Graphs</b> <b>(KGs).</b> With the recent advancements of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and their remarkable <b>reasoning</b> abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual <b>questions,</b> <b>e.g.,</b> &ldquo;In which city was Silvio Berlusconi&rsquo;s first wife born?&rdquo;, leaving <b>questions</b> <b>involving</b> <b>commonsense</b> <b>reasoning</b> that real-world users may pose more often, e.g., &ldquo;Do I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?&rdquo; unaddressed. In this work, we first observe that existing <b>LLM-based</b> methods for KGQA struggle with hallucination on such <b>questions,</b> <b>especially</b> on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their <b>reasoning</b> processes are not easily verifiable. In response, we propose Right for Right Reasons (R3), a <b>commonsense</b> <b>KGQA</b> methodology that allows for a verifiable <b>reasoning</b> procedure by axiomatically surfacing intrinsic <b>commonsense</b> <b>knowledge</b> <b>of</b> <b>LLMs</b> and <b>grounding</b> every factual <b>reasoning</b> step on <b>KG</b> triples. Through experimental evaluations across three different tasks&ndash;question answering, claim verification, and preference matching&ndash;our findings showcase R3 as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and <b>reasoning</b> errors.</p></p class="citation"></blockquote><h3 id=621--35128-cr-lt-kgqa-a-knowledge-graph-question-answering-dataset-requiring-commonsense-reasoning-and-long-tail-knowledge-willis-guo-et-al-2024>(6/21 | 35/128) CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring Commonsense Reasoning and Long-Tail Knowledge (Willis Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Willis Guo, Armin Toroghi, Scott Sanner. (2024)<br><strong>CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring Commonsense Reasoning and Long-Tail Knowledge</strong><br><button class=copy-to-clipboard title="CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring Commonsense Reasoning and Long-Tail Knowledge" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-4, I-2-7, cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Common-sense Reasoning, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01395v1.pdf filename=2403.01395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>graph</b> <b>question</b> <b>answering</b> (KGQA) is a well-established field that seeks to provide factual answers to natural language (NL) <b>questions</b> <b>by</b> leveraging <b>knowledge</b> <b>graphs</b> <b>(KGs).</b> However, existing KGQA datasets suffer from two significant limitations: (1) no existing KGQA dataset requires <b>commonsense</b> <b>reasoning</b> to arrive at an answer and (2) existing KGQA datasets focus on popular entities for which <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can directly answer without hallucinating and without leveraging the <b>KG.</b> In this work, we seek a novel KGQA dataset that supports <b>commonsense</b> <b>reasoning</b> and focuses on long-tail entities (e.g., non-mainstream and recent entities) where <b>LLMs</b> frequently hallucinate, and thus create the need for novel methodologies that leverage the <b>KG</b> for factual and attributable <b>commonsense</b> <b>inference.</b> We create a novel <b>Commonsense</b> <b>Reasoning</b> (CR) and Long-Tail (LT) KGQA dataset with two subtasks &ndash; <b>question</b> <b>answering</b> and claim verification &ndash; that address both limitations (1) and (2). We construct CR-LT-KGQA by building extensions to existing <b>reasoning</b> datasets StrategyQA and CREAK over Wikidata. While existing KGQA methods are not applicable due to their lack of <b>commonsense</b> <b>inference</b> support, baseline evaluation of <b>LLMs</b> on CR-LT KGQA demonstrate a high rate of hallucination. Thus, CR-LT KGQA poses significant challenges for hallucination-prone <b>LLMs,</b> hence paving the way for future <b>commonsense</b> <b>KGQA</b> research to provide accurate and factual answers for long-tail entities in the era of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=721--36128-automatic-question-answer-generation-for-long-tail-knowledge-rohan-kumar-et-al-2024>(7/21 | 36/128) Automatic Question-Answer Generation for Long-Tail Knowledge (Rohan Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohan Kumar, Youngmin Kim, Sunitha Ravi, Haitian Sun, Christos Faloutsos, Ruslan Salakhutdinov, Minji Yoon. (2024)<br><strong>Automatic Question-Answer Generation for Long-Tail Knowledge</strong><br><button class=copy-to-clipboard title="Automatic Question-Answer Generation for Long-Tail Knowledge" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 58<br>Keywords: Graph, Knowledge Graph, Open-Domain Question Answering, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01382v1.pdf filename=2403.01382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretrained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have gained significant attention for addressing <b>open-domain</b> <b>Question</b> <b>Answering</b> <b>(QA).</b> While they exhibit high accuracy in answering <b>questions</b> <b>related</b> to common <b>knowledge,</b> <b>LLMs</b> encounter difficulties in learning about uncommon long-tail <b>knowledge</b> <b>(tail</b> entities). Since manually constructing <b>QA</b> datasets demands substantial human resources, the types of existing <b>QA</b> datasets are limited, leaving us with a scarcity of datasets to study the performance of <b>LLMs</b> on tail entities. In this paper, we propose an automatic approach to generate specialized <b>QA</b> datasets for tail entities and present the associated research challenges. We conduct extensive experiments by employing pretrained <b>LLMs</b> on our newly generated long-tail <b>QA</b> datasets, comparing their performance with and without external resources including Wikipedia and Wikidata <b>knowledge</b> <b>graphs.</b></p></p class="citation"></blockquote><h3 id=821--37128-multi-level-product-category-prediction-through-text-classification-wesley-ferreira-maia-et-al-2024>(8/21 | 37/128) Multi-level Product Category Prediction through Text Classification (Wesley Ferreira Maia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wesley Ferreira Maia, Angelo Carmignani, Gabriel Bortoli, Lucas Maretti, David Luz, Daniel Camilo Fuentes Guzman, Marcos Jardel Henriques, Francisco Louzada Neto. (2024)<br><strong>Multi-level Product Category Prediction through Text Classification</strong><br><button class=copy-to-clipboard title="Multi-level Product Category Prediction through Text Classification" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Data Augmentation, BERT, LSTM, Text Classification, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01638v1.pdf filename=2403.01638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article investigates applying advanced machine learning models, specifically <b>LSTM</b> and <b>BERT,</b> for <b>text</b> <b>classification</b> to predict multiple categories in the retail sector. The study demonstrates how applying <b>data</b> <b>augmentation</b> techniques and the focal loss function can significantly enhance accuracy in classifying products into multiple categories using a robust Brazilian retail dataset. The <b>LSTM</b> model, enriched with Brazilian <b>word</b> <b>embedding,</b> and <b>BERT,</b> known for its effectiveness in understanding complex contexts, were adapted and optimized for this specific task. The results showed that the <b>BERT</b> model, with an F1 Macro Score of up to $99%$ for segments, $96%$ for categories and subcategories and $93%$ for name products, outperformed <b>LSTM</b> in more detailed categories. However, <b>LSTM</b> also achieved high performance, especially after applying <b>data</b> <b>augmentation</b> and focal loss techniques. These results underscore the effectiveness of NLP techniques in retail and highlight the importance of the careful selection of modelling and preprocessing strategies. This work contributes significantly to the field of NLP in retail, providing valuable insights for future research and practical applications.</p></p class="citation"></blockquote><h3 id=921--38128-towards-comprehensive-vietnamese-retrieval-augmented-generation-and-large-language-models-nguyen-quang-duc-et-al-2024>(9/21 | 38/128) Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models (Nguyen Quang Duc et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nguyen Quang Duc, Le Hai Son, Nguyen Duc Nhan, Nguyen Dich Nhat Minh, Le Thanh Huong, Dinh Viet Sang. (2024)<br><strong>Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models</strong><br><button class=copy-to-clipboard title="Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01616v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01616v2.pdf filename=2403.01616v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents our contributions towards advancing the state of Vietnamese language understanding and generation through the development and dissemination of open datasets and pre-trained models for Vietnamese <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> and <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b></p></p class="citation"></blockquote><h3 id=1021--39128-infusing-knowledge-into-large-language-models-with-contextual-prompts-kinshuk-vasisht-et-al-2024>(10/21 | 39/128) Infusing Knowledge into Large Language Models with Contextual Prompts (Kinshuk Vasisht et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kinshuk Vasisht, Balaji Ganesan, Vikas Kumar, Vasudha Bhatnagar. (2024)<br><strong>Infusing Knowledge into Large Language Models with Contextual Prompts</strong><br><button class=copy-to-clipboard title="Infusing Knowledge into Large Language Models with Contextual Prompts" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 48<br>Keywords: Graph, Fine-tuning, Knowledge Graph, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01481v1.pdf filename=2403.01481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>infusion</b> is a promising method for enhancing <b>Large</b> <b>Language</b> <b>Models</b> for domain-specific NLP tasks rather than pre-training models over <b>large</b> <b>data</b> <b>from</b> scratch. These augmented <b>LLMs</b> typically depend on additional pre-training or <b>knowledge</b> <b>prompts</b> from an existing <b>knowledge</b> <b>graph,</b> which is impractical in many applications. In contrast, <b>knowledge</b> <b>infusion</b> directly from relevant documents is more generalisable and alleviates the need for structured <b>knowledge</b> <b>graphs</b> while also being useful for entities that are usually not found in any <b>knowledge</b> <b>graph.</b> With this motivation, we propose a simple yet generalisable approach for <b>knowledge</b> <b>infusion</b> by generating <b>prompts</b> from the context in the input text. Our experiments show the effectiveness of our approach which we evaluate by probing the <b>fine-tuned</b> <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1121--40128-revisiting-dynamic-evaluation-online-adaptation-for-large-language-models-amal-rannen-triki-et-al-2024>(11/21 | 40/128) Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models (Amal Rannen-Triki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amal Rannen-Triki, Jorg Bornschein, Razvan Pascanu, Marcus Hutter, Andras György, Alexandre Galashov, Yee Whye Teh, Michalis K. Titsias. (2024)<br><strong>Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models</strong><br><button class=copy-to-clipboard title="Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01518v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01518v1.pdf filename=2403.01518v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of online fine tuning the parameters of a language model at test time, also known as dynamic evaluation. While it is generally known that this approach improves the overall predictive performance, especially when considering distributional shift between training and evaluation data, we here emphasize the perspective that online adaptation turns parameters into temporally changing states and provides a form of context-length extension with memory in weights, more in line with the concept of memory in neuroscience. We pay particular attention to the speed of adaptation (in terms of sample efficiency),sensitivity to the overall distributional drift, and the computational overhead for performing gradient computations and parameter updates. Our empirical study provides insights on when online adaptation is particularly interesting. We highlight that with online adaptation the conceptual distinction between <b>in-context</b> <b>learning</b> and fine tuning blurs: both are methods to condition the model on previously observed tokens.</p></p class="citation"></blockquote><h3 id=1221--41128-fantastic-semantics-and-where-to-find-them-investigating-which-layers-of-generative-llms-reflect-lexical-semantics-zhu-liu-et-al-2024>(12/21 | 41/128) Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics (Zhu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhu Liu, Cunliang Kong, Ying Liu, Maosong Sun. (2024)<br><strong>Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics</strong><br><button class=copy-to-clipboard title="Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: BERT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01509v1.pdf filename=2403.01509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> have achieved remarkable success in general language understanding tasks. However, as a family of generative methods with the objective of next token prediction, the semantic evolution with the depth of these models are not fully explored, unlike their predecessors, such as <b>BERT-like</b> architectures. In this paper, we specifically investigate the bottom-up evolution of lexical semantics for a popular <b>LLM,</b> namely Llama2, by probing its hidden states at the end of each layer using a contextualized word identification task. Our experiments show that the representations in lower layers encode lexical semantics, while the higher layers, with weaker semantic induction, are responsible for prediction. This is in contrast to models with discriminative objectives, such as mask language modeling, where the higher layers obtain better lexical semantics. The conclusion is further supported by the monotonic increase in performance via the hidden states for the last meaningless symbols, such as punctuation, in the <b>prompting</b> strategy.</p></p class="citation"></blockquote><h3 id=1321--42128-what-is-missing-in-multilingual-visual-reasoning-and-how-to-fix-it-yueqi-song-et-al-2024>(13/21 | 42/128) What Is Missing in Multilingual Visual Reasoning and How to Fix It (Yueqi Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yueqi Song, Simran Khanuja, Graham Neubig. (2024)<br><strong>What Is Missing in Multilingual Visual Reasoning and How to Fix It</strong><br><button class=copy-to-clipboard title="What Is Missing in Multilingual Visual Reasoning and How to Fix It" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Zero-shot, GPT, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01404v1.pdf filename=2403.01404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>NLP models today strive for supporting multiple languages and modalities, improving accessibility for diverse users. In this paper, we evaluate their multilingual, <b>multimodal</b> capabilities by testing on a visual <b>reasoning</b> task. We observe that proprietary systems like <b>GPT-4V</b> obtain the best performance on this task now, but open models lag in comparison. Surprisingly, <b>GPT-4V</b> exhibits similar performance between English and other languages, indicating the potential for equitable system development across languages. Our analysis on model failures reveals three key aspects that make this task challenging: multilinguality, complex <b>reasoning,</b> and multimodality. To address these challenges, we propose three targeted interventions including a translate-test approach to tackle multilinguality, a visual programming approach to break down complex <b>reasoning,</b> and a novel method that leverages image captioning to address multimodality. Our interventions achieve the best open performance on this task in a <b>zero-shot</b> setting, boosting open model LLaVA by 13.4%, while also minorly improving <b>GPT-4V&rsquo;s</b> performance.</p></p class="citation"></blockquote><h3 id=1421--43128-in-context-sharpness-as-alerts-an-inner-representation-perspective-for-hallucination-mitigation-shiqi-chen-et-al-2024>(14/21 | 43/128) In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation (Shiqi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, Junxian He. (2024)<br><strong>In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation</strong><br><button class=copy-to-clipboard title="In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01548v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01548v2.pdf filename=2403.01548v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of <b>LLM</b> hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the <b>in-context</b> tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness&rsquo;&rsquo; among the <b>in-context</b> hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination <b>benchmarks</b> demonstrate our approach&rsquo;s consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinations and serve as a practical solution for hallucination mitigation.</p></p class="citation"></blockquote><h3 id=1521--44128-answerability-in-retrieval-augmented-open-domain-question-answering-rustam-abdumalikov-et-al-2024>(15/21 | 44/128) Answerability in Retrieval-Augmented Open-Domain Question Answering (Rustam Abdumalikov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rustam Abdumalikov, Pasquale Minervini, Yova Kementchedjhieva. (2024)<br><strong>Answerability in Retrieval-Augmented Open-Domain Question Answering</strong><br><button class=copy-to-clipboard title="Answerability in Retrieval-Augmented Open-Domain Question Answering" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Open-Domain Question Answering, Open-Domain Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01461v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01461v1.pdf filename=2403.01461v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The performance of <b>Open-Domain</b> <b>Question</b> <b>Answering</b> <b>(ODQA)</b> retrieval systems can exhibit sub-optimal behavior, providing text excerpts with varying degrees of irrelevance. Unfortunately, many existing <b>ODQA</b> datasets lack examples specifically targeting the identification of irrelevant text excerpts. Previous attempts to address this gap have relied on a simplistic approach of pairing <b>questions</b> <b>with</b> random text excerpts. This paper aims to investigate the effectiveness of models trained using this randomized strategy, uncovering an important limitation in their ability to generalize to irrelevant text excerpts with high semantic overlap. As a result, we observed a substantial decrease in predictive accuracy, from 98% to 1%. To address this limitation, we discovered an efficient approach for training models to recognize such excerpts. By leveraging unanswerable pairs from the SQuAD 2.0 dataset, our models achieve a nearly perfect (~100%) accuracy when confronted with these challenging text excerpts.</p></p class="citation"></blockquote><h3 id=1621--45128-improving-cross-lingual-representation-for-semantic-retrieval-with-code-switching-mieradilijiang-maimaiti-et-al-2024>(16/21 | 45/128) Improving Cross-lingual Representation for Semantic Retrieval with Code-switching (Mieradilijiang Maimaiti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mieradilijiang Maimaiti, Yuanhang Zheng, Ji Zhang, Fei Huang, Yue Zhang, Wenpei Luo, Kaiyu Huang. (2024)<br><strong>Improving Cross-lingual Representation for Semantic Retrieval with Code-switching</strong><br><button class=copy-to-clipboard title="Improving Cross-lingual Representation for Semantic Retrieval with Code-switching" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01364v1.pdf filename=2403.01364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic Retrieval (SR) has become an indispensable part of the FAQ system in the task-oriented <b>question-answering</b> <b>(QA)</b> dialogue scenario. The demands for a cross-lingual smart-customer-service system for an e-commerce platform or some particular business conditions have been increasing recently. Most previous studies exploit cross-lingual pre-trained models (PTMs) for multi-lingual knowledge retrieval directly, while some others also leverage the continual pre-training before <b>fine-tuning</b> PTMs on the downstream tasks. However, no matter which schema is used, the previous work ignores to inform PTMs of some features of the downstream task, i.e. train their PTMs without providing any signals related to SR. To this end, in this work, we propose an Alternative Cross-lingual PTM for SR via code-switching. We are the first to utilize the code-switching approach for cross-lingual SR. Besides, we introduce the novel code-switched continual pre-training instead of directly using the PTMs on the SR tasks. The experimental results show that our proposed approach consistently outperforms the previous SOTA methods on SR and semantic textual similarity (STS) tasks with three business corpora and four open datasets in 20+ languages.</p></p class="citation"></blockquote><h3 id=1721--46128-kormedmcqa-multi-choice-question-answering-benchmark-for-korean-healthcare-professional-licensing-examinations-sunjun-kweon-et-al-2024>(17/21 | 46/128) KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations (Sunjun Kweon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sunjun Kweon, Byungjin Choi, Minkyu Kim, Rae Woong Park, Edward Choi. (2024)<br><strong>KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations</strong><br><button class=copy-to-clipboard title="KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01469v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01469v2.pdf filename=2403.01469v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce KorMedMCQA, the first Korean multiple-choice <b>question</b> <b>answering</b> (MCQA) <b>benchmark</b> derived from Korean healthcare professional licensing examinations, covering from the year 2012 to year 2023. This dataset consists of a selection of <b>questions</b> <b>from</b> the license examinations for doctors, nurses, and pharmacists, featuring a diverse array of subjects. We conduct baseline experiments on various <b>large</b> <b>language</b> <b>models,</b> including proprietary/open-source, multilingual/Korean-additional pretrained, and clinical context pretrained models, highlighting the potential for further enhancements. We make our data publicly available on HuggingFace (<a href=https://huggingface.co/datasets/sean0042/KorMedMCQA>https://huggingface.co/datasets/sean0042/KorMedMCQA</a>) and provide a evaluation script via LM-Harness, inviting further exploration and advancement in Korean healthcare environments.</p></p class="citation"></blockquote><h3 id=1821--47128-controlling-cloze-test-question-item-difficulty-with-plm-based-surrogate-models-for-irt-assessment-jingshen-zhang-et-al-2024>(18/21 | 47/128) Controlling Cloze-test Question Item Difficulty with PLM-based Surrogate Models for IRT Assessment (Jingshen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingshen Zhang, Jiajun Xie, Xinying Qiu. (2024)<br><strong>Controlling Cloze-test Question Item Difficulty with PLM-based Surrogate Models for IRT Assessment</strong><br><button class=copy-to-clipboard title="Controlling Cloze-test Question Item Difficulty with PLM-based Surrogate Models for IRT Assessment" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01456v1.pdf filename=2403.01456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Item difficulty plays a crucial role in adaptive testing. However, few works have focused on generating questions of varying difficulty levels, especially for multiple-choice (MC) cloze tests. We propose training <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> as surrogate models to enable item response theory (IRT) assessment, avoiding the need for human test subjects. We also propose two strategies to control the difficulty levels of both the gaps and the distractors using ranking rules to reduce invalid distractors. Experimentation on a <b>benchmark</b> dataset demonstrates that our proposed framework and methods can effectively control and evaluate the difficulty levels of MC cloze tests.</p></p class="citation"></blockquote><h3 id=1921--48128-ovel-large-language-model-as-memory-manager-for-online-video-entity-linking-haiquan-zhao-et-al-2024>(19/21 | 48/128) OVEL: Large Language Model as Memory Manager for Online Video Entity Linking (Haiquan Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiquan Zhao, Xuwu Wang, Shisong Chen, Zhixu Li, Xin Zheng, Yanghua Xiao. (2024)<br><strong>OVEL: Large Language Model as Memory Manager for Online Video Entity Linking</strong><br><button class=copy-to-clipboard title="OVEL: Large Language Model as Memory Manager for Online Video Entity Linking" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01411v1.pdf filename=2403.01411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>multi-modal</b> entity linking (MEL) has garnered increasing attention in the research community due to its significance in numerous <b>multi-modal</b> applications. Video, as a popular means of information transmission, has become prevalent in people&rsquo;s daily lives. However, most existing MEL methods primarily focus on linking textual and visual mentions or offline videos&rsquo;s mentions to entities in <b>multi-modal</b> knowledge bases, with limited efforts devoted to linking mentions within online video content. In this paper, we propose a task called Online Video Entity Linking OVEL, aiming to establish connections between mentions in online videos and a knowledge base with high accuracy and timeliness. To facilitate the research works of OVEL, we specifically concentrate on live delivery scenarios and construct a live delivery entity linking dataset called LIVE. Besides, we propose an evaluation metric that considers timelessness, robustness, and accuracy. Furthermore, to effectively handle OVEL task, we leverage a memory block managed by a <b>Large</b> <b>Language</b> <b>Model</b> and retrieve entity candidates from the knowledge base to augment <b>LLM</b> performance on memory management. The experimental results prove the effectiveness and efficiency of our method.</p></p class="citation"></blockquote><h3 id=2021--49128-evaluating-and-mitigating-number-hallucinations-in-large-vision-language-models-a-consistency-perspective-huixuan-zhang-et-al-2024>(20/21 | 49/128) Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective (Huixuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huixuan Zhang, Junzhe Zhang, Xiaojun Wan. (2024)<br><strong>Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective</strong><br><button class=copy-to-clipboard title="Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01373v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01373v1.pdf filename=2403.01373v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large vision language models have demonstrated remarkable efficacy in addressing challenges related to both textual and visual content. Nevertheless, these models are susceptible to various hallucinations. In this paper, we focus on a new form of hallucination, specifically termed as number hallucination, which denotes instances where models fail to accurately identify the quantity of objects in an image. We establish a dataset and employ evaluation metrics to assess number hallucination, revealing a pronounced prevalence of this issue across mainstream large vision language models (LVLMs). Additionally, we delve into a thorough analysis of number hallucination, examining inner and outer inconsistency problem from two related perspectives. We assert that this inconsistency is one cause of number hallucination and propose a consistency training method as a means to alleviate such hallucination, which achieves an average improvement of 8% compared with direct <b>finetuning</b> method.</p></p class="citation"></blockquote><h3 id=2121--50128-leveraging-biomolecule-and-natural-language-through-multi-modal-learning-a-survey-qizhi-pei-et-al-2024>(21/21 | 50/128) Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey (Qizhi Pei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao Qin, Rui Yan. (2024)<br><strong>Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey</strong><br><button class=copy-to-clipboard title="Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL, q-bio-BM<br>Keyword Score: 16<br>Keywords: Graph, Multi-modal, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01528v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01528v2.pdf filename=2403.01528v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this review, we provide an extensive analysis of recent advancements achieved through cross modeling of biomolecules and natural language. (1) We begin by outlining the technical representations of biomolecules employed, including sequences, 2D <b>graphs,</b> and 3D structures. (2) We then examine in depth the rationale and key objectives underlying effective <b>multi-modal</b> integration of language and molecular data sources. (3) We subsequently survey the practical applications enabled to date in this developing research area. (4) We also compile and <b>summarize</b> the available resources and datasets to facilitate future work. (5) Looking ahead, we identify several promising research directions worthy of further exploration and investment to continue advancing the field. The related resources and contents are updating in \url{https://github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling}.</p></p class="citation"></blockquote><h2 id=cslg-30>cs.LG (30)</h2><h3 id=130--51128-transformers-for-supervised-online-continual-learning-jorg-bornschein-et-al-2024>(1/30 | 51/128) Transformers for Supervised Online Continual Learning (Jorg Bornschein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jorg Bornschein, Yazhe Li, Amal Rannen-Triki. (2024)<br><strong>Transformers for Supervised Online Continual Learning</strong><br><button class=copy-to-clipboard title="Transformers for Supervised Online Continual Learning" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 93<br>Keywords: Benchmarking, Continual Learning, Few-shot, Few-shot Learning, Meta Learning, Stochastic Gradient Descent, Supervised Learning, Transformer, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01554v1.pdf filename=2403.01554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have become the dominant architecture for sequence modeling tasks such as natural language processing or audio processing, and they are now even considered for tasks that are not naturally sequential such as image classification. Their ability to attend to and to process a set of tokens as context enables them to develop <b>in-context</b> <b>few-shot</b> <b>learning</b> abilities. However, their potential for online <b>continual</b> <b>learning</b> remains relatively unexplored. In online <b>continual</b> <b>learning,</b> a model must adapt to a non-stationary stream of data, minimizing the cumulative nextstep prediction loss. We focus on the <b>supervised</b> online <b>continual</b> <b>learning</b> setting, where we learn a predictor $x_t \rightarrow y_t$ for a sequence of examples $(x_t, y_t)$. Inspired by the <b>in-context</b> <b>learning</b> capabilities of <b>transformers</b> and their connection to <b>meta-learning,</b> <b>we</b> propose a method that leverages these strengths for online <b>continual</b> <b>learning.</b> Our approach explicitly conditions a <b>transformer</b> on recent observations, while at the same time online training it with <b>stochastic</b> <b>gradient</b> <b>descent,</b> following the procedure introduced with <b>Transformer-XL.</b> We incorporate replay to maintain the benefits of multi-epoch training while adhering to the sequential protocol. We hypothesize that this combination enables fast adaptation through <b>in-context</b> <b>learning</b> and sustained longterm improvement via parametric learning. Our method demonstrates significant improvements over previous state-of-the-art results on CLOC, a challenging large-scale real-world <b>benchmark</b> for image geo-localization.</p></p class="citation"></blockquote><h3 id=230--52128-applying-self-supervised-learning-to-network-intrusion-detection-for-network-flows-with-graph-neural-network-renjie-xu-et-al-2024>(2/30 | 52/128) Applying Self-supervised Learning to Network Intrusion Detection for Network Flows with Graph Neural Network (Renjie Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renjie Xu, Guangwei Wu, Weiping Wang, Xing Gao, An He, Zhengpeng Zhang. (2024)<br><strong>Applying Self-supervised Learning to Network Intrusion Detection for Network Flows with Graph Neural Network</strong><br><button class=copy-to-clipboard title="Applying Self-supervised Learning to Network Intrusion Detection for Network Flows with Graph Neural Network" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 93<br>Keywords: Graph, Graph Contrastive Learning, Graph Embedding, Graph Neural Network, Graph Neural Network, Contrastive Learning, Self-supervised Learning, Self-supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01501v1.pdf filename=2403.01501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have garnered intensive attention for Network Intrusion Detection System (NIDS) due to their suitability for representing the network traffic flows. However, most present <b>GNN-based</b> methods for NIDS are <b>supervised</b> or semi-supervised. Network flows need to be manually annotated as supervisory labels, a process that is time-consuming or even impossible, making NIDS difficult to adapt to potentially complex attacks, especially in large-scale real-world scenarios. The existing <b>GNN-based</b> <b>self-supervised</b> <b>methods</b> focus on the binary classification of network flow as benign or not, and thus fail to reveal the types of attack in practice. This paper studies the application of <b>GNNs</b> to identify the specific types of network flows in an <b>unsupervised</b> manner. We first design an encoder to obtain <b>graph</b> <b>embedding,</b> <b>that</b> introduces the <b>graph</b> <b>attention</b> <b>mechanism</b> and considers the edge information as the only essential factor. Then, a <b>self-supervised</b> <b>method</b> based on <b>graph</b> <b>contrastive</b> <b>learning</b> is proposed. The method samples center nodes, and for each center node, generates subgraph by it and its direct neighbor nodes, and corresponding <b>contrastive</b> <b>subgraph</b> from the interpolated <b>graph,</b> <b>and</b> <b>finally</b> constructs positive and negative samples from subgraphs. Furthermore, a structured <b>contrastive</b> <b>loss</b> function based on edge features and <b>graph</b> <b>local</b> <b>topology</b> is introduced. To the best of our knowledge, it is the first <b>GNN-based</b> <b>self-supervised</b> <b>method</b> for the multiclass classification of network flows in NIDS. Detailed experiments conducted on four real-world databases (NF-Bot-IoT, NF-Bot-IoT-v2, NF-CSE-CIC-IDS2018, and NF-CSE-CIC-IDS2018-v2) systematically compare our model with the state-of-the-art <b>supervised</b> and <b>self-supervised</b> <b>models,</b> illustrating the considerable potential of our method. Our code is accessible through <a href=https://github.com/renj-xu/NEGSC>https://github.com/renj-xu/NEGSC</a>.</p></p class="citation"></blockquote><h3 id=330--53128-representation-learning-on-heterophilic-graph-with-directional-neighborhood-attention-qincheng-lu-et-al-2024>(3/30 | 53/128) Representation Learning on Heterophilic Graph with Directional Neighborhood Attention (Qincheng Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qincheng Lu, Jiaqi Zhu, Sitao Luan, Xiao-Wen Chang. (2024)<br><strong>Representation Learning on Heterophilic Graph with Directional Neighborhood Attention</strong><br><button class=copy-to-clipboard title="Representation Learning on Heterophilic Graph with Directional Neighborhood Attention" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SI, cs.LG<br>Keyword Score: 51<br>Keywords: Graph Attention Networks, Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Pruning, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01475v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01475v1.pdf filename=2403.01475v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Attention</b> <b>Network</b> <b>(GAT)</b> is one of the most popular <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)</b> architecture, which employs the attention mechanism to learn edge weights and has demonstrated promising performance in various applications. However, since it only incorporates information from immediate neighborhood, it lacks the ability to capture long-range and global <b>graph</b> <b>information,</b> <b>leading</b> to unsatisfactory performance on some datasets, particularly on heterophilic <b>graphs.</b> <b>To</b> <b>address</b> this limitation, we propose the Directional <b>Graph</b> <b>Attention</b> <b>Network</b> (DGAT) in this paper. DGAT is able to combine the feature-based attention with the global directional information extracted from the <b>graph</b> <b>topology.</b> <b>To</b> this end, a new class of Laplacian matrices is proposed which can provably reduce the diffusion distance between nodes. Based on the new Laplacian, topology-guided neighbour <b>pruning</b> and edge adding mechanisms are proposed to remove the noisy and capture the helpful long-range neighborhood information. Besides, a global directional attention is designed to enable a topological-aware information propagation. The superiority of the proposed DGAT over the baseline <b>GAT</b> has also been verified through experiments on real-world <b>benchmarks</b> and synthetic data sets. It also outperforms the state-of-the-art (SOTA) models on 6 out of 7 real-world <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=430--54128-improving-uncertainty-sampling-with-bell-curve-weight-function-zan-kai-chong-et-al-2024>(4/30 | 54/128) Improving Uncertainty Sampling with Bell Curve Weight Function (Zan-Kai Chong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zan-Kai Chong, Hiroyuki Ohsaki, Bok-Min Goi. (2024)<br><strong>Improving Uncertainty Sampling with Bell Curve Weight Function</strong><br><button class=copy-to-clipboard title="Improving Uncertainty Sampling with Bell Curve Weight Function" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Active Learning, Simulation, Simulator, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01352v1.pdf filename=2403.01352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Typically, a <b>supervised</b> <b>learning</b> model is trained using passive learning by randomly selecting unlabelled instances to annotate. This approach is effective for learning a model, but can be costly in cases where acquiring labelled instances is expensive. For example, it can be time-consuming to manually identify spam mails (labelled instances) from thousands of emails (unlabelled instances) flooding an inbox during initial data collection. Generally, we answer the above scenario with uncertainty sampling, an <b>active</b> <b>learning</b> method that improves the efficiency of <b>supervised</b> <b>learning</b> by using fewer labelled instances than passive learning. Given an unlabelled data pool, uncertainty sampling queries the labels of instances where the predicted probabilities, p, fall into the uncertainty region, i.e., $p \approx 0.5$. The newly acquired labels are then added to the existing labelled data pool to learn a new model. Nonetheless, the performance of uncertainty sampling is susceptible to the area of unpredictable responses (AUR) and the nature of the dataset. It is difficult to determine whether to use passive learning or uncertainty sampling without prior knowledge of a new dataset. To address this issue, we propose bell curve sampling, which employs a bell curve weight function to acquire new labels. With the bell curve centred at p=0.5, bell curve sampling selects instances whose predicted values are in the uncertainty area most of the time without neglecting the rest. <b>Simulation</b> results show that, most of the time bell curve sampling outperforms uncertainty sampling and passive learning in datasets of different natures and with AUR.</p></p class="citation"></blockquote><h3 id=530--55128-on-the-compressibility-of-quantized-large-language-models-yu-mao-et-al-2024>(5/30 | 55/128) On the Compressibility of Quantized Large Language Models (Yu Mao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Mao, Weilan Wang, Hongchao Du, Nan Guan, Chun Jason Xue. (2024)<br><strong>On the Compressibility of Quantized Large Language Models</strong><br><button class=copy-to-clipboard title="On the Compressibility of Quantized Large Language Models" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Quantization, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01384v1.pdf filename=2403.01384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deploying <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> on edge or mobile devices offers significant benefits, such as enhanced data privacy and real-time processing capabilities. However, it also faces critical challenges due to the substantial memory requirement of <b>LLMs.</b> <b>Quantization</b> is an effective way of reducing the model size while maintaining good performance. However, even after <b>quantization,</b> <b>LLMs</b> may still be too big to fit entirely into the limited memory of edge or mobile devices and have to be partially loaded from the storage to complete the inference. In this case, the I/O latency of model loading becomes the bottleneck of the <b>LLM</b> inference latency. In this work, we take a preliminary step of studying applying data compression techniques to reduce data movement and thus speed up the inference of <b>quantized</b> <b>LLM</b> on memory-constrained devices. In particular, we discussed the compressibility of <b>quantized</b> <b>LLMs,</b> the trade-off between the compressibility and performance of <b>quantized</b> <b>LLMs,</b> and opportunities to optimize both of them jointly.</p></p class="citation"></blockquote><h3 id=630--56128-partial-federated-learning-tiantian-feng-et-al-2024>(6/30 | 56/128) Partial Federated Learning (Tiantian Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiantian Feng, Anil Ramakrishna, Jimit Majmudar, Charith Peris, Jixuan Wang, Clement Chung, Richard Zemel, Morteza Ziyadi, Rahul Gupta. (2024)<br><strong>Partial Federated Learning</strong><br><button class=copy-to-clipboard title="Partial Federated Learning" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Contrastive Learning, Federated Learning, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01615v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01615v1.pdf filename=2403.01615v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) is a popular algorithm to train machine learning models on user data constrained to edge devices (for example, mobile phones) due to privacy concerns. Typically, FL is trained with the assumption that no part of the user data can be egressed from the edge. However, in many production settings, specific data-modalities/meta-data are limited to be on device while others are not. For example, in commercial SLU systems, it is typically desired to prevent transmission of biometric signals (such as audio recordings of the input <b>prompt)</b> to the cloud, but egress of locally (i.e. on the edge device) transcribed text to the cloud may be possible. In this work, we propose a new algorithm called Partial <b>Federated</b> <b>Learning</b> (PartialFL), where a machine learning model is trained using data where a subset of data modalities or their intermediate representations can be made available to the server. We further restrict our model training by preventing the egress of data labels to the cloud for better privacy, and instead use a <b>contrastive</b> <b>learning</b> based model objective. We evaluate our approach on two different <b>multi-modal</b> datasets and show promising results with our proposed approach.</p></p class="citation"></blockquote><h3 id=730--57128-collaborate-to-adapt-source-free-graph-domain-adaptation-via-bi-directional-adaptation-zhen-zhang-et-al-2024>(7/30 | 57/128) Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation (Zhen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Zhang, Meihan Liu, Anhui Wang, Hongyang Chen, Zhao Li, Jiajun Bu, Bingsheng He. (2024)<br><strong>Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation</strong><br><button class=copy-to-clipboard title="Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Contrastive Learning, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01467v1.pdf filename=2403.01467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> <b>Graph</b> <b>Domain</b> <b>Adaptation</b> (UGDA) has emerged as a practical solution to transfer knowledge from a label-rich source <b>graph</b> to a completely unlabelled target <b>graph.</b> However, most methods require a labelled source <b>graph</b> to provide supervision signals, which might not be accessible in the real-world settings due to regulations and privacy concerns. In this paper, we explore the scenario of source-free <b>unsupervised</b> <b>graph</b> <b>domain</b> <b>adaptation,</b> which tries to address the <b>domain</b> <b>adaptation</b> problem without accessing the labelled source <b>graph.</b> Specifically, we present a novel paradigm called GraphCTA, which performs model adaptation and <b>graph</b> adaptation collaboratively through a series of procedures: (1) conduct model adaptation based on node&rsquo;s neighborhood predictions in target <b>graph</b> considering both local and global information; (2) perform <b>graph</b> adaptation by updating <b>graph</b> structure and node attributes via neighborhood <b>contrastive</b> <b>learning;</b> and (3) the updated <b>graph</b> serves as an input to facilitate the subsequent iteration of model adaptation, thereby establishing a collaborative loop between model adaptation and <b>graph</b> adaptation. Comprehensive experiments are conducted on various public datasets. The experimental results demonstrate that our proposed model outperforms recent source-free baselines by large margins.</p></p class="citation"></blockquote><h3 id=830--58128-improving-llm-code-generation-with-grammar-augmentation-shubham-ugare-et-al-2024>(8/30 | 58/128) Improving LLM Code Generation with Grammar Augmentation (Shubham Ugare et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, Gagandeep Singh. (2024)<br><strong>Improving LLM Code Generation with Grammar Augmentation</strong><br><button class=copy-to-clipboard title="Improving LLM Code Generation with Grammar Augmentation" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-FL, cs-LG, cs-PL, cs-SE, cs.LG<br>Keyword Score: 30<br>Keywords: Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01632v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01632v1.pdf filename=2403.01632v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present SynCode a novel framework for efficient and general syntactical decoding of <b>code</b> <b>with</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> SynCode leverages the grammar of a programming language, utilizing an offline-constructed efficient lookup table called DFA mask store based on language grammar terminals. We demonstrate SynCode&rsquo;s soundness and completeness given the context-free grammar (CFG) of the programming language, presenting its ability to retain syntactically valid tokens while rejecting invalid ones. The framework seamlessly integrates with any language defined by CFG, as evidenced by experiments on CFGs for Python and Go. The results underscore the significant reduction of 96.07% of syntax errors achieved when SynCode is combined with state-of-the-art <b>LLMs,</b> showcasing its substantial impact on enhancing syntactical precision in <b>code</b> <b>generation.</b> Our <b>code</b> <b>is</b> available at <a href=https://github.com/uiuc-focal-lab/syncode>https://github.com/uiuc-focal-lab/syncode</a>.</p></p class="citation"></blockquote><h3 id=930--59128-ml4physim--machine-learning-for-physical-simulations-challenge-the-airfoil-design-mouadh-yagoubi-et-al-2024>(9/30 | 59/128) ML4PhySim : Machine Learning for Physical Simulations Challenge (The airfoil design) (Mouadh Yagoubi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mouadh Yagoubi, Milad Leyli-Abadi, David Danan, Jean-Patrick Brunet, Jocelyn Ahmed Mazari, Florent Bonnet, Asma Farjallah, Marc Schoenauer, Patrick Gallinari. (2024)<br><strong>ML4PhySim : Machine Learning for Physical Simulations Challenge (The airfoil design)</strong><br><button class=copy-to-clipboard title="ML4PhySim : Machine Learning for Physical Simulations Challenge (The airfoil design)" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CE, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01623v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01623v1.pdf filename=2403.01623v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The use of machine learning (ML) techniques to solve complex physical problems has been considered recently as a promising approach. However, the evaluation of such learned physical models remains an important issue for industrial use. The aim of this competition is to encourage the development of new ML techniques to solve physical problems using a unified evaluation framework proposed recently, called Learning Industrial Physical <b>Simulations</b> (LIPS). We propose learning a task representing a well-known physical use case: the airfoil design <b>simulation,</b> using a dataset called AirfRANS. The global score calculated for each submitted solution is based on three main categories of criteria covering different aspects, namely: ML-related, <b>Out-Of-Distribution,</b> and physical compliance criteria. To the best of our knowledge, this is the first competition addressing the use of ML-based surrogate approaches to improve the trade-off computational cost/accuracy of physical <b>simulation.The</b> competition is hosted by the Codabench platform with online training and evaluation of all submitted solutions.</p></p class="citation"></blockquote><h3 id=1030--60128-quantized-hierarchical-federated-learning-a-robust-approach-to-statistical-heterogeneity-seyed-mohammad-azimi-abarghouyi-et-al-2024>(10/30 | 60/128) Quantized Hierarchical Federated Learning: A Robust Approach to Statistical Heterogeneity (Seyed Mohammad Azimi-Abarghouyi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seyed Mohammad Azimi-Abarghouyi, Viktoria Fodor. (2024)<br><strong>Quantized Hierarchical Federated Learning: A Robust Approach to Statistical Heterogeneity</strong><br><button class=copy-to-clipboard title="Quantized Hierarchical Federated Learning: A Robust Approach to Statistical Heterogeneity" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 30<br>Keywords: Federated Learning, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01540v1.pdf filename=2403.01540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel hierarchical <b>federated</b> <b>learning</b> algorithm within multiple sets that incorporates <b>quantization</b> for communication-efficiency and demonstrates resilience to statistical heterogeneity. Unlike conventional hierarchical <b>federated</b> <b>learning</b> algorithms, our approach combines gradient aggregation in intra-set iterations with model aggregation in inter-set iterations. We offer a comprehensive analytical framework to evaluate its optimality gap and convergence rate, comparing these aspects with those of conventional algorithms. Additionally, we develop a problem formulation to derive optimal system parameters in a closed-form solution. Our findings reveal that our algorithm consistently achieves high learning accuracy over a range of parameters and significantly outperforms other hierarchical algorithms, particularly in scenarios with heterogeneous data distributions.</p></p class="citation"></blockquote><h3 id=1130--61128-convtimenet-a-deep-hierarchical-fully-convolutional-model-for-multivariate-time-series-analysis-mingyue-cheng-et-al-2024>(11/30 | 61/128) ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for Multivariate Time Series Analysis (Mingyue Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyue Cheng, Jiqian Yang, Tingyue Pan, Qi Liu, Zhi Li. (2024)<br><strong>ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for Multivariate Time Series Analysis</strong><br><button class=copy-to-clipboard title="ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for Multivariate Time Series Analysis" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01493v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01493v1.pdf filename=2403.01493v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces ConvTimeNet, a novel deep hierarchical fully <b>convolutional</b> <b>network</b> designed to serve as a general-purpose model for time series analysis. The key design of this network is twofold, designed to overcome the limitations of traditional <b>convolutional</b> <b>networks.</b> Firstly, we propose an adaptive segmentation of time series into sub-series level patches, treating these as fundamental modeling units. This setting avoids the sparsity semantics associated with raw point-level time steps. Secondly, we design a fully <b>convolutional</b> <b>block</b> by skillfully integrating deepwise and pointwise <b>convolution</b> operations, following the advanced building block style employed in <b>Transformer</b> encoders. This backbone network allows for the effective capture of both global sequence and cross-variable dependence, as it not only incorporates the advancements of <b>Transformer</b> architecture but also inherits the inherent properties of <b>convolution.</b> Furthermore, multi-scale representations of given time series instances can be learned by controlling the kernel size flexibly. Extensive experiments are conducted on both time series forecasting and classification tasks. The results consistently outperformed strong baselines in most situations in terms of effectiveness.The code is publicly available.</p></p class="citation"></blockquote><h3 id=1230--62128-the-implicit-bias-of-heterogeneity-towards-invariance-and-causality-yang-xu-et-al-2024>(12/30 | 62/128) The Implicit Bias of Heterogeneity towards Invariance and Causality (Yang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Xu, Yihong Gu, Cong Fang. (2024)<br><strong>The Implicit Bias of Heterogeneity towards Invariance and Causality</strong><br><button class=copy-to-clipboard title="The Implicit Bias of Heterogeneity towards Invariance and Causality" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 30<br>Keywords: Stochastic Gradient Descent, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01420v1.pdf filename=2403.01420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is observed empirically that the <b>large</b> <b>language</b> <b>models</b> <b>(LLM),</b> trained with a variant of regression loss using numerous corpus from the Internet, can unveil causal associations to some extent. This is contrary to the traditional wisdom that ``association is not causation&rsquo;&rsquo; and the paradigm of traditional causal inference in which prior causal knowledge should be carefully incorporated into the design of methods. It is a mystery why causality, in a higher layer of understanding, can emerge from the regression task that pursues associations. In this paper, we claim the emergence of causality from association-oriented training can be attributed to the coupling effects from the heterogeneity of the source data, stochasticity of training algorithms, and over-parameterization of the learning models. We illustrate such an intuition using a simple but insightful model that learns invariance, a quasi-causality, using regression loss. To be specific, we consider multi-environment low-rank matrix sensing problems where the unknown r-rank ground-truth d*d matrices diverge across the environments but contain a lower-rank invariant, causal part. In this case, running pooled gradient descent will result in biased solutions that only learn associations in general. We show that running <b>large-batch</b> <b>Stochastic</b> <b>Gradient</b> <b>Descent,</b> whose each batch being linear measurement samples randomly selected from a certain environment, can successfully drive the solution towards the invariant, causal solution under certain conditions. This step is related to the relatively strong heterogeneity of the environments, the <b>large</b> <b>step</b> <b>size</b> and noises in the optimization algorithm, and the over-parameterization of the model. In summary, we unveil another implicit bias that is a result of the symbiosis between the heterogeneity of data and modern algorithms, which is, to the best of our knowledge, first in the literature.</p></p class="citation"></blockquote><h3 id=1330--63128-a-comprehensive-survey-of-federated-transfer-learning-challenges-methods-and-applications-wei-guo-et-al-2024>(13/30 | 63/128) A Comprehensive Survey of Federated Transfer Learning: Challenges, Methods and Applications (Wei Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Guo, Fuzhen Zhuang, Xiao Zhang, Yiqi Tong, Jin Dong. (2024)<br><strong>A Comprehensive Survey of Federated Transfer Learning: Challenges, Methods and Applications</strong><br><button class=copy-to-clipboard title="A Comprehensive Survey of Federated Transfer Learning: Challenges, Methods and Applications" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Transfer Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01387v1.pdf filename=2403.01387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is a novel distributed machine learning paradigm that enables participants to collaboratively train a centralized model with privacy preservation by eliminating the requirement of data sharing. In practice, FL often involves multiple participants and requires the third party to aggregate global information to guide the update of the target participant. Therefore, many FL methods do not work well due to the training and test data of each participant may not be sampled from the same feature space and the same underlying distribution. Meanwhile, the differences in their local devices (system heterogeneity), the continuous influx of online data (incremental data), and labeled data scarcity may further influence the performance of these methods. To solve this problem, <b>federated</b> <b>transfer</b> <b>learning</b> (FTL), which integrates <b>transfer</b> <b>learning</b> (TL) into FL, has attracted the attention of numerous researchers. However, since FL enables a continuous share of knowledge among participants with each communication round while not allowing local data to be accessed by other participants, FTL faces many unique challenges that are not present in TL. In this survey, we focus on categorizing and reviewing the current progress on <b>federated</b> <b>transfer</b> <b>learning,</b> and outlining corresponding solutions and applications. Furthermore, the common setting of FTL scenarios, available datasets, and significant related research are <b>summarized</b> in this survey.</p></p class="citation"></blockquote><h3 id=1430--64128-neural-graph-generator-feature-conditioned-graph-generation-using-latent-diffusion-models-iakovos-evdaimon-et-al-2024>(14/30 | 64/128) Neural Graph Generator: Feature-Conditioned Graph Generation using Latent Diffusion Models (Iakovos Evdaimon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Iakovos Evdaimon, Giannis Nikolentzos, Michail Chatzianastasis, Hadi Abdine, Michalis Vazirgiannis. (2024)<br><strong>Neural Graph Generator: Feature-Conditioned Graph Generation using Latent Diffusion Models</strong><br><button class=copy-to-clipboard title="Neural Graph Generator: Feature-Conditioned Graph Generation using Latent Diffusion Models" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 23<br>Keywords: Diffusion Model, Graph, Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01535v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01535v1.pdf filename=2403.01535v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> generation has emerged as a crucial task in machine learning, with significant challenges in generating <b>graphs</b> that accurately reflect specific properties. Existing methods often fall short in efficiently addressing this need as they struggle with the high-dimensional complexity and varied nature of <b>graph</b> properties. In this paper, we introduce the Neural <b>Graph</b> Generator (NGG), a novel approach which utilizes conditioned latent <b>diffusion</b> <b>models</b> for <b>graph</b> generation. NGG demonstrates a remarkable capacity to model complex <b>graph</b> patterns, offering control over the <b>graph</b> generation process. NGG employs a variational <b>graph</b> <b>autoencoder</b> for <b>graph</b> compression and a <b>diffusion</b> <b>process</b> in the latent vector space, guided by vectors summarizing <b>graph</b> statistics. We demonstrate NGG&rsquo;s versatility across various <b>graph</b> generation tasks, showing its capability to capture desired <b>graph</b> properties and generalize to unseen <b>graphs.</b> This work signifies a significant shift in <b>graph</b> generation methodologies, offering a more practical and efficient solution for generating diverse types of <b>graphs</b> with specific characteristics.</p></p class="citation"></blockquote><h3 id=1530--65128-you-need-to-pay-better-attention-mehran-hosseini-et-al-2024>(15/30 | 65/128) You Need to Pay Better Attention (Mehran Hosseini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mehran Hosseini, Peyman Hosseini. (2024)<br><strong>You Need to Pay Better Attention</strong><br><button class=copy-to-clipboard title="You Need to Pay Better Attention" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T07 (Primary) 68T45, 68T50, 68T10, 15A03, 15A04 (Secondary), I-2-6; I-2-7; I-2-10; I-4-0; I-5-0; I-7-0, cs-AI, cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: MNIST, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01643v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01643v1.pdf filename=2403.01643v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of <b>Transformer</b> models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on <b>MNIST,</b> CIFAR100, IMDB Movie Reviews, and Amazon Reviews datasets.</p></p class="citation"></blockquote><h3 id=1630--66128-blue-and-green-mode-energy-efficient-chemiresistive-sensor-array-realized-by-rapid-ensemble-learning-zeheng-wang-et-al-2024>(16/30 | 66/128) Blue and Green-Mode Energy-Efficient Chemiresistive Sensor Array Realized by Rapid Ensemble Learning (Zeheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeheng Wang, James Cooper, Muhammad Usman, Timothy van der Laan. (2024)<br><strong>Blue and Green-Mode Energy-Efficient Chemiresistive Sensor Array Realized by Rapid Ensemble Learning</strong><br><button class=copy-to-clipboard title="Blue and Green-Mode Energy-Efficient Chemiresistive Sensor Array Realized by Rapid Ensemble Learning" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CE, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01642v1.pdf filename=2403.01642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of Internet of Things (IoT) necessitates the development of optimized Chemiresistive Sensor (CRS) arrays that are both energy-efficient and capable. This study introduces a novel optimization strategy that employs a rapid ensemble learning-based model committee approach to achieve these goals. Utilizing machine learning models such as Elastic Net Regression, Random Forests, and XGBoost, among others, the strategy identifies the most impactful sensors in a CRS array for accurate classification: A weighted voting mechanism is introduced to aggregate the models&rsquo; opinions in sensor selection, thereby setting up wo distinct working modes, termed &ldquo;Blue&rdquo; and &ldquo;Green&rdquo;. The Blue mode operates with all sensors for maximum detection capability, while the Green mode selectively activates only key sensors, significantly reducing energy consumption without compromising detection accuracy. The strategy is validated through theoretical calculations and Monte Carlo <b>simulations,</b> demonstrating its effectiveness and accuracy. The proposed optimization strategy not only elevates the detection capability of CRS arrays but also brings it closer to theoretical limits, promising significant implications for the development of low-cost, easily fabricable next-generation IoT sensor terminals.</p></p class="citation"></blockquote><h3 id=1730--67128-theoretical-insights-for-diffusion-guidance-a-case-study-for-gaussian-mixture-models-yuchen-wu-et-al-2024>(17/30 | 67/128) Theoretical Insights for Diffusion Guidance: A Case Study for Gaussian Mixture Models (Yuchen Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchen Wu, Minshuo Chen, Zihao Li, Mengdi Wang, Yuting Wei. (2024)<br><strong>Theoretical Insights for Diffusion Guidance: A Case Study for Gaussian Mixture Models</strong><br><button class=copy-to-clipboard title="Theoretical Insights for Diffusion Guidance: A Case Study for Gaussian Mixture Models" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01639v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01639v1.pdf filename=2403.01639v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> benefit from instillation of task-specific information into the score function to steer the sample generation towards desired properties. Such information is coined as guidance. For example, in <b>text-to-image</b> synthesis, text input is encoded as guidance to generate semantically aligned images. Proper guidance inputs are closely tied to the performance of <b>diffusion</b> <b>models.</b> A common observation is that strong guidance promotes a tight alignment to the task-specific information, while reducing the diversity of the generated samples. In this paper, we provide the first theoretical study towards understanding the influence of guidance on <b>diffusion</b> <b>models</b> in the context of Gaussian mixture models. Under mild conditions, we prove that incorporating <b>diffusion</b> <b>guidance</b> not only boosts classification confidence but also diminishes distribution diversity, leading to a reduction in the differential entropy of the output distribution. Our analysis covers the widely adopted sampling schemes including DDPM and DDIM, and leverages comparison inequalities for differential equations as well as the Fokker-Planck equation that characterizes the evolution of probability density function, which may be of independent theoretical interest.</p></p class="citation"></blockquote><h3 id=1830--68128-critical-windows-non-asymptotic-theory-for-feature-emergence-in-diffusion-models-marvin-li-et-al-2024>(18/30 | 68/128) Critical windows: non-asymptotic theory for feature emergence in diffusion models (Marvin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marvin Li, Sitan Chen. (2024)<br><strong>Critical windows: non-asymptotic theory for feature emergence in diffusion models</strong><br><button class=copy-to-clipboard title="Critical windows: non-asymptotic theory for feature emergence in diffusion models" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Diffusion Model, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01633v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01633v1.pdf filename=2403.01633v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop theory to understand an intriguing property of <b>diffusion</b> <b>models</b> for image generation that we term critical windows. Empirically, it has been observed that there are narrow time intervals in sampling during which particular features of the final image emerge, e.g. the image class or background color (Ho et al., 2020b; Georgiev et al., 2023; Raya & Ambrogioni, 2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous for interpretability as it implies one can localize properties of the generation to a small segment of the trajectory, it seems at odds with the continuous nature of the <b>diffusion.</b> <b>We</b> propose a formal framework for studying these windows and show that for data coming from a mixture of strongly log-concave densities, these windows can be provably bounded in terms of certain measures of inter- and intra-group separation. We also instantiate these bounds for concrete examples like well-conditioned Gaussian mixtures. Finally, we use our bounds to give a rigorous interpretation of <b>diffusion</b> <b>models</b> as hierarchical samplers that progressively &ldquo;decide&rdquo; output features over a discrete sequence of times. We validate our bounds with synthetic experiments. Additionally, preliminary experiments on Stable <b>Diffusion</b> <b>suggest</b> critical windows may serve as a useful tool for diagnosing <b>fairness</b> and privacy violations in real-world <b>diffusion</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=1930--69128-respiratory-motion-forecasting-with-online-learning-of-recurrent-neural-networks-for-safety-enhancement-in-externally-guided-radiotherapy-michel-pohl-et-al-2024>(19/30 | 69/128) Respiratory motion forecasting with online learning of recurrent neural networks for safety enhancement in externally guided radiotherapy (Michel Pohl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michel Pohl, Mitsuru Uesaka, Hiroyuki Takahashi, Kazuyuki Demachi, Ritu Bhusal Chhatkuli. (2024)<br><strong>Respiratory motion forecasting with online learning of recurrent neural networks for safety enhancement in externally guided radiotherapy</strong><br><button class=copy-to-clipboard title="Respiratory motion forecasting with online learning of recurrent neural networks for safety enhancement in externally guided radiotherapy" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG, eess-IV, eess-SP<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01607v1.pdf filename=2403.01607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In lung radiotherapy, infrared cameras can record the location of reflective objects on the chest to infer the position of the tumor moving due to breathing, but treatment system latencies hinder radiation beam precision. Real-time <b>recurrent</b> <b>learning</b> <b>(RTRL),</b> is a potential solution as it can learn patterns within non-stationary respiratory data but has high complexity. This study assesses the capabilities of resource-efficient online <b>RNN</b> algorithms, namely unbiased online <b>recurrent</b> <b>optimization</b> <b>(UORO),</b> sparse-1 step approximation (SnAp-1), and decoupled neural interfaces (DNI) to forecast respiratory motion during radiotherapy treatment accurately. We use time series containing the 3D position of external markers on the chest of healthy subjects. We propose efficient implementations for SnAp-1 and DNI based on compression of the influence and immediate Jacobian matrices and an accurate update of the linear coefficients used in credit assignment estimation, respectively. The original sampling frequency was 10Hz; we performed resampling at 3.33Hz and 30Hz. We use UORO, SnAp-1, and DNI to forecast each marker&rsquo;s 3D position with horizons (the time interval in advance for which the prediction is made) h&lt;=2.1s and compare them with RTRL, least mean squares, and linear regression. <b>RNNs</b> trained online achieved similar or better accuracy than most previous works using larger training databases and deep learning, even though we used only the first minute of each sequence to predict motion within that exact sequence. SnAp-1 had the lowest normalized root mean square errors (nRMSE) averaged over the horizon values considered, equal to 0.335 and 0.157, at 3.33Hz and 10.0Hz, respectively. Similarly, UORO had the highest accuracy at 30Hz, with an nRMSE of 0.0897. DNI&rsquo;s inference time, equal to 6.8ms per time step at 30Hz (Intel Core i7-13700 CPU), was the lowest among the <b>RNN</b> methods examined.</p></p class="citation"></blockquote><h3 id=2030--70128-towards-provable-log-density-policy-gradient-pulkit-katdare-et-al-2024>(20/30 | 70/128) Towards Provable Log Density Policy Gradient (Pulkit Katdare et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pulkit Katdare, Anant Joshi, Katherine Driggs-Campbell. (2024)<br><strong>Towards Provable Log Density Policy Gradient</strong><br><button class=copy-to-clipboard title="Towards Provable Log Density Policy Gradient" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01605v1.pdf filename=2403.01605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Policy gradient methods are a vital ingredient behind the success of modern <b>reinforcement</b> <b>learning.</b> Modern policy gradient methods, although successful, introduce a residual error in gradient estimation. In this work, we argue that this residual term is significant and correcting for it could potentially improve sample-complexity of <b>reinforcement</b> <b>learning</b> methods. To that end, we propose log density gradient to estimate the policy gradient, which corrects for this residual error term. Log density gradient method computes policy gradient by utilising the state-action discounted distributional formulation. We first present the equations needed to exactly find the log density gradient for a tabular Markov Decision Processes <b>(MDPs).</b> For more complex environments, we propose a temporal difference (TD) method that approximates log density gradient by utilizing backward on-policy samples. Since backward sampling from a Markov chain is highly restrictive we also propose a min-max optimization that can approximate log density gradient using just on-policy samples. We also prove uniqueness, and convergence under linear function approximation, for this min-max optimization. Finally, we show that the sample complexity of our min-max optimization to be of the order of $m^{-1/2}$, where $m$ is the number of on-policy samples. We also demonstrate a proof-of-concept for our log density gradient method on gridworld environment, and observe that our method is able to improve upon the classical policy gradient method by a clear margin, thus indicating a promising novel direction to develop <b>reinforcement</b> <b>learning</b> algorithms that require fewer samples.</p></p class="citation"></blockquote><h3 id=2130--71128-the-hidden-attention-of-mamba-models-ameen-ali-et-al-2024>(21/30 | 71/128) The Hidden Attention of Mamba Models (Ameen Ali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ameen Ali, Itamar Zimerman, Lior Wolf. (2024)<br><strong>The Hidden Attention of Mamba Models</strong><br><button class=copy-to-clipboard title="The Hidden Attention of Mamba Models" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: F-2-2, I-2-7, F-2-2; I-2-7, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01590v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01590v1.pdf filename=2403.01590v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Mamba layer offers an efficient selective state space model (SSM) that is highly effective in modeling multiple domains including NLP, long-range sequences processing, and computer vision. Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via IO-aware parallel scan, and deploys in an autoregressive manner. We add a third view and show that such models can be viewed as attention-driven models. This new perspective enables us to compare the underlying mechanisms to that of the <b>self-attention</b> layers in <b>transformers</b> and allows us to peer inside the inner workings of the Mamba model with explainability methods. Our code is publicly available.</p></p class="citation"></blockquote><h3 id=2230--72128-on-the-model-agnostic-multi-source-free-unsupervised-domain-adaptation-jiangbo-pei-et-al-2024>(22/30 | 72/128) On the Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation (Jiangbo Pei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiangbo Pei, Ruizhe Li, Qingchao Chen. (2024)<br><strong>On the Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation</strong><br><button class=copy-to-clipboard title="On the Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01582v1.pdf filename=2403.01582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-Source-Free <b>Unsupervised</b> <b>Domain</b> <b>Adaptation</b> (MSFDA) aims to transfer knowledge from multiple well-labeled source <b>domains</b> <b>to</b> an unlabeled target <b>domain,</b> <b>using</b> source models instead of source data. Existing MSFDA methods limited that each source <b>domain</b> <b>provides</b> only a single model, with a uniform structure. This paper introduces a new MSFDA setting: Model-Agnostic Multi-Source-Free <b>Unsupervised</b> <b>Domain</b> <b>Adaptation</b> (MMDA), allowing diverse source models with varying architectures, without quantitative restrictions. While MMDA holds promising potential, incorporating numerous source models poses a high risk of including undesired models, which highlights the source model selection problem. To address it, we first provide a theoretical analysis of this problem. We reveal two fundamental selection principles: transferability principle and diversity principle, and introduce a selection algorithm to integrate them. Then, considering the measure of transferability is challenging, we propose a novel Source-Free <b>Unsupervised</b> Transferability Estimation (SUTE). This novel formulation enables the assessment and comparison of transferability across multiple source models with different architectures in the context of <b>domain</b> <b>shift,</b> without requiring access to any target labels or source data. Based on the above, we introduce a new framework to address MMDA. Specifically, we first conduct source model selection based on the proposed selection principles. Subsequently, we design two modules to aggregate knowledge from included models and recycle useful knowledge from excluded models. These modules enable us to leverage source knowledge efficiently and effectively, thereby supporting us in learning a discriminative target model via adaptation. We validate the effectiveness of our method through numerous experimental results, and demonstrate that our approach achieves state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=2330--73128-privacy-preserving-collaborative-split-learning-framework-for-smart-grid-load-forecasting-asif-iqbal-et-al-2024>(23/30 | 73/128) Privacy-Preserving Collaborative Split Learning Framework for Smart Grid Load Forecasting (Asif Iqbal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Asif Iqbal, Prosanta Gope, Biplab Sikdar. (2024)<br><strong>Privacy-Preserving Collaborative Split Learning Framework for Smart Grid Load Forecasting</strong><br><button class=copy-to-clipboard title="Privacy-Preserving Collaborative Split Learning Framework for Smart Grid Load Forecasting" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Transformer, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01438v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01438v1.pdf filename=2403.01438v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate load forecasting is crucial for energy management, infrastructure planning, and demand-supply balancing. Smart meter data availability has led to the demand for sensor-based load forecasting. Conventional ML allows training a single global model using data from multiple smart meters requiring data transfer to a central server, raising concerns for network requirements, privacy, and security. We propose a split learning-based framework for load forecasting to alleviate this issue. We split a deep neural network model into two parts, one for each Grid Station (GS) responsible for an entire neighbourhood&rsquo;s smart meters and the other for the Service Provider (SP). Instead of sharing their data, client smart meters use their respective GSs&rsquo; model split for forward pass and only share their activations with the GS. Under this framework, each GS is responsible for training a personalized model split for their respective neighbourhoods, whereas the SP can train a single global or personalized model for each GS. Experiments show that the proposed models match or exceed a centrally trained model&rsquo;s performance and generalize well. Privacy is analyzed by assessing information leakage between data and shared activations of the GS model split. Additionally, <b>differential</b> <b>privacy</b> enhances local data privacy while examining its impact on performance. A <b>transformer</b> model is used as our base learner.</p></p class="citation"></blockquote><h3 id=2430--74128-introduction-to-algogens-amir-shachar-2024>(24/30 | 74/128) Introduction to Algogens (Amir Shachar, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Shachar. (2024)<br><strong>Introduction to Algogens</strong><br><button class=copy-to-clipboard title="Introduction to Algogens" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01426v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01426v1.pdf filename=2403.01426v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This book introduces the concept of Algogens, a promising integration of <b>generative</b> <b>AI</b> with traditional algorithms aimed at improving problem-solving techniques across various fields. It provides an accessible overview of how Algogens combine AI&rsquo;s innovative potential with algorithms&rsquo; reliability to tackle complex challenges more effectively than either could alone. The text explores the basics of Algogens, their development, applications, and advantages, such as better adaptability and efficiency. Through examples and case studies, readers will learn about Algogens&rsquo; practical uses today and their potential for future cybersecurity, healthcare, and environmental science innovation. Acknowledging new technologies&rsquo; challenges and ethical considerations, the book offers a balanced look at the prospects and obstacles facing Algogens. It invites a broad audience, including experts and newcomers, to engage with the topic and consider Algogens&rsquo; role in advancing our problem-solving capabilities. This work is presented as a starting point for anyone interested in the intersection of AI and algorithms, encouraging further exploration and discussion on this emerging field. It aims to spark curiosity and contribute to the ongoing conversation about how technology can evolve to meet the complex demands of the AI era.</p></p class="citation"></blockquote><h3 id=2530--75128-asyn2f-an-asynchronous-federated-learning-framework-with-bidirectional-model-aggregation-tien-dung-cao-et-al-2024>(25/30 | 75/128) Asyn2F: An Asynchronous Federated Learning Framework with Bidirectional Model Aggregation (Tien-Dung Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tien-Dung Cao, Nguyen T. Vuong, Thai Q. Le, Hoang V. N. Dao, Tram Truong-Huu. (2024)<br><strong>Asyn2F: An Asynchronous Federated Learning Framework with Bidirectional Model Aggregation</strong><br><button class=copy-to-clipboard title="Asyn2F: An Asynchronous Federated Learning Framework with Bidirectional Model Aggregation" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01417v1.pdf filename=2403.01417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>federated</b> <b>learning,</b> the models can be trained synchronously or asynchronously. Many research works have focused on developing an aggregation method for the server to aggregate multiple local models into the global model with improved performance. They ignore the heterogeneity of the training workers, which causes the delay in the training of the local models, leading to the obsolete information issue. In this paper, we design and develop Asyn2F, an Asynchronous <b>Federated</b> <b>learning</b> Framework with bidirectional model aggregation. By bidirectional model aggregation, Asyn2F, on one hand, allows the server to asynchronously aggregate multiple local models and results in a new global model. On the other hand, it allows the training workers to aggregate the new version of the global model into the local model, which is being trained even in the middle of a training epoch. We develop Asyn2F considering the practical implementation requirements such as using cloud services for model storage and message queuing protocols for communications. Extensive experiments with different datasets show that the models trained by Asyn2F achieve higher performance compared to the state-of-the-art techniques. The experiments also demonstrate the effectiveness, practicality, and scalability of Asyn2F, making it ready for deployment in real scenarios.</p></p class="citation"></blockquote><h3 id=2630--76128-bandit-profit-maximization-for-targeted-marketing-joon-suk-huh-et-al-2024>(26/30 | 76/128) Bandit Profit-maximization for Targeted Marketing (Joon Suk Huh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joon Suk Huh, Ellen Vitercik, Kirthevasan Kandasamy. (2024)<br><strong>Bandit Profit-maximization for Targeted Marketing</strong><br><button class=copy-to-clipboard title="Bandit Profit-maximization for Targeted Marketing" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-GT, cs-LG, cs.LG, econ-GN, q-fin-EC, q-fin-GN<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01361v1.pdf filename=2403.01361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a sequential profit-maximization problem, optimizing for both price and ancillary variables like marketing expenditures. Specifically, we aim to maximize profit over an arbitrary sequence of multiple demand curves, each dependent on a distinct ancillary variable, but sharing the same price. A prototypical example is targeted marketing, where a firm (seller) wishes to sell a product over multiple markets. The firm may invest different marketing expenditures for different markets to optimize customer acquisition, but must maintain the same price across all markets. Moreover, markets may have heterogeneous demand curves, each responding to prices and marketing expenditures differently. The firm&rsquo;s objective is to maximize its gross profit, the total revenue minus marketing costs. Our results are near-optimal algorithms for this class of problems in an adversarial <b>bandit</b> setting, where demand curves are arbitrary non-adaptive sequences, and the firm observes only noisy evaluations of chosen points on the demand curves. We prove a regret upper bound of $\widetilde{\mathcal{O}}\big(nT^{3/4}\big)$ and a lower bound of $\Omega\big((nT)^{3/4}\big)$ for monotonic demand curves, and a regret bound of $\widetilde{\Theta}\big(nT^{2/3}\big)$ for demands curves that are monotonic in price and concave in the ancillary variables.</p></p class="citation"></blockquote><h3 id=2730--77128-sangria-stacked-autoencoder-neural-networks-with-gradient-boosting-for-indoor-localization-danish-gufran-et-al-2024>(27/30 | 77/128) SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for Indoor Localization (Danish Gufran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danish Gufran, Saideep Tiku, Sudeep Pasricha. (2024)<br><strong>SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for Indoor Localization</strong><br><button class=copy-to-clipboard title="SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for Indoor Localization" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01348v1.pdf filename=2403.01348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Indoor localization is a critical task in many embedded applications, such as asset tracking, emergency response, and realtime navigation. In this article, we propose a novel fingerprintingbased framework for indoor localization called SANGRIA that uses stacked <b>autoencoder</b> neural networks with gradient boosted trees. Our approach is designed to overcome the device heterogeneity challenge that can create uncertainty in wireless signal measurements across embedded devices used for localization. We compare SANGRIA to several state-of-the-art frameworks and demonstrate 42.96% lower average localization error across diverse indoor locales and heterogeneous devices.</p></p class="citation"></blockquote><h3 id=2830--78128-decoupling-weighing-and-selecting-for-integrating-multiple-graph-pre-training-tasks-tianyu-fan-et-al-2024>(28/30 | 78/128) Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks (Tianyu Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyu Fan, Lirong Wu, Yufei Huang, Haitao Lin, Cheng Tan, Zhangyang Gao, Stan Z. Li. (2024)<br><strong>Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks</strong><br><button class=copy-to-clipboard title="Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 8<br>Keywords: Graph, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01400v1.pdf filename=2403.01400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have witnessed the great success of <b>graph</b> pre-training for <b>graph</b> <b>representation</b> <b>learning.</b> With hundreds of <b>graph</b> pre-training tasks proposed, integrating knowledge acquired from multiple pre-training tasks has become a popular research topic. In this paper, we identify two important collaborative processes for this topic: (1) select: how to select an optimal task combination from a given task pool based on their compatibility, and (2) weigh: how to weigh the selected tasks based on their importance. While there currently has been a lot of work focused on weighing, comparatively little effort has been devoted to selecting. This paper proposes a novel instance-level framework for integrating multiple <b>graph</b> pre-training tasks, Weigh And Select (WAS), where the two collaborative processes, weighing and selecting, are combined by decoupled siamese networks. Specifically, it first adaptively learns an optimal combination of tasks for each instance from a given task pool, based on which a customized instance-level task weighing strategy is learned. Extensive experiments on 16 <b>graph</b> datasets across node-level and <b>graph-level</b> downstream tasks have demonstrated that by combining a few simple but classical tasks, WAS can achieve comparable performance to other leading counterparts. The code is available at <a href=https://github.com/TianyuFan0504/WAS>https://github.com/TianyuFan0504/WAS</a>.</p></p class="citation"></blockquote><h3 id=2930--79128-one-step-multi-view-clustering-based-on-transition-probability-wenhui-zhao-et-al-2024>(29/30 | 79/128) One-Step Multi-View Clustering Based on Transition Probability (Wenhui Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhui Zhao, Quanxue Gao, Guangfei Li, Cheng Deng, Ming Yang. (2024)<br><strong>One-Step Multi-View Clustering Based on Transition Probability</strong><br><button class=copy-to-clipboard title="One-Step Multi-View Clustering Based on Transition Probability" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01460v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01460v1.pdf filename=2403.01460v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The large-scale multi-view <b>clustering</b> algorithms, based on the anchor <b>graph,</b> have shown promising performance and efficiency and have been extensively explored in recent years. Despite their successes, current methods lack interpretability in the <b>clustering</b> process and do not sufficiently consider the complementary information across different views. To address these shortcomings, we introduce the One-Step Multi-View <b>Clustering</b> Based on Transition Probability (OSMVC-TP). This method adopts a probabilistic approach, which leverages the anchor <b>graph,</b> representing the transition probabilities from samples to anchor points. Our method directly learns the transition probabilities from anchor points to categories, and calculates the transition probabilities from samples to categories, thus obtaining soft label matrices for samples and anchor points, enhancing the interpretability of <b>clustering.</b> Furthermore, to maintain consistency in labels across different views, we apply a Schatten p-norm constraint on the tensor composed of the soft labels. This approach effectively harnesses the complementary information among the views. Extensive experiments have confirmed the effectiveness and robustness of OSMVC-TP.</p></p class="citation"></blockquote><h3 id=3030--80128-on-diffusion-process-in-se3-invariant-space-zihan-zhou-et-al-2024>(30/30 | 80/128) On Diffusion Process in SE(3)-invariant Space (Zihan Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Zhou, Ruiying Liu, Jiachen Zheng, Xiaoxue Wang, Tianshu Yu. (2024)<br><strong>On Diffusion Process in SE(3)-invariant Space</strong><br><button class=copy-to-clipboard title="On Diffusion Process in SE(3)-invariant Space" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01430v1.pdf filename=2403.01430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sampling viable 3D structures (e.g., molecules and point clouds) with SE(3)-invariance using diffusion-based models proved promising in a variety of real-world applications, wherein SE(3)-invariant properties can be naturally characterized by the inter-point distance manifold. However, due to the non-trivial <b>geometry,</b> we still lack a comprehensive understanding of the diffusion mechanism within such SE(3)-invariant space. This study addresses this gap by mathematically delineating the diffusion mechanism under SE(3)-invariance, via zooming into the interaction behavior between coordinates and the inter-point distance manifold through the lens of differential <b>geometry.</b> Upon this analysis, we propose accurate and projection-free diffusion SDE and ODE accordingly. Such formulations enable enhancing the performance and the speed of generation pathways; meanwhile offering valuable insights into other systems incorporating SE(3)-invariance.</p></p class="citation"></blockquote><h2 id=cscr-6>cs.CR (6)</h2><h3 id=16--81128-using-llms-for-tabletop-exercises-within-the-security-domain-sam-hays-et-al-2024>(1/6 | 81/128) Using LLMs for Tabletop Exercises within the Security Domain (Sam Hays et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sam Hays, Dr. Jules White. (2024)<br><strong>Using LLMs for Tabletop Exercises within the Security Domain</strong><br><button class=copy-to-clipboard title="Using LLMs for Tabletop Exercises within the Security Domain" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 60<br>Keywords: Recommendation, Simulation, Simulator, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01626v1.pdf filename=2403.01626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tabletop exercises are a crucial component of many company&rsquo;s strategy to test and evaluate its preparedness for security incidents in a realistic way. Traditionally led by external firms specializing in cybersecurity, these exercises can be costly, time-consuming, and may not always align precisely with the client&rsquo;s specific needs. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>ChatGPT</b> offer a compelling alternative. They enable faster iteration, provide rich and adaptable <b>simulations,</b> and offer infinite patience in handling feedback and <b>recommendations.</b> This approach can enhances the efficiency and relevance of security preparedness exercises.</p></p class="citation"></blockquote><h3 id=26--82128-iot-device-labeling-using-large-language-models-bar-meyuhas-et-al-2024>(2/6 | 82/128) IoT Device Labeling Using Large Language Models (Bar Meyuhas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bar Meyuhas, Anat Bremler-Barr, Tal Shapira. (2024)<br><strong>IoT Device Labeling Using Large Language Models</strong><br><button class=copy-to-clipboard title="IoT Device Labeling Using Large Language Models" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-NI, cs.CR<br>Keyword Score: 30<br>Keywords: Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01586v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01586v1.pdf filename=2403.01586v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The IoT market is diverse and characterized by a multitude of vendors that support different device functions (e.g., speaker, camera, vacuum cleaner, etc.). Within this market, IoT security and observability systems use real-time identification techniques to manage these devices effectively. Most existing IoT identification solutions employ machine learning techniques that assume the IoT device, labeled by both its vendor and function, was observed during their training phase. We tackle a key challenge in IoT labeling: how can an AI solution label an IoT device that has never been seen before and whose label is unknown? Our solution extracts textual features such as domain names and hostnames from network traffic, and then enriches these features using Google search data alongside catalog of vendors and device functions. The solution also integrates an auto-update mechanism that uses <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to update these catalogs with emerging device types. Based on the information gathered, the device&rsquo;s vendor is identified through string matching with the enriched features. The function is then deduced by <b>LLMs</b> and <b>zero-shot</b> classification from a predefined catalog of IoT functions. In an evaluation of our solution on 97 unique IoT devices, our function labeling approach achieved HIT1 and HIT2 scores of 0.7 and 0.77, respectively. As far as we know, this is the first research to tackle AI-automated IoT labeling.</p></p class="citation"></blockquote><h3 id=36--83128-warden-multi-directional-backdoor-watermarks-for-embedding-as-a-service-copyright-protection-anudeex-shetty-et-al-2024>(3/6 | 83/128) WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection (Anudeex Shetty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anudeex Shetty, Yue Teng, Ke He, Qiongkai Xu. (2024)<br><strong>WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection</strong><br><button class=copy-to-clipboard title="WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 23<br>Keywords: Clustering, Text Embedding, Model Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01472v1.pdf filename=2403.01472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to <b>model</b> <b>extraction</b> attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the <b>text</b> <b>embeddings</b> and subsequently verifying the attack <b>models</b> <b>post-publication.</b> Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE <b>(Clustering,</b> Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the stealthiness of watermarks and empirically has been shown effective against CSE attack.</p></p class="citation"></blockquote><h3 id=46--84128-issf-the-intelligent-security-service-framework-for-cloud-native-operation-yikuan-yan-et-al-2024>(4/6 | 84/128) ISSF: The Intelligent Security Service Framework for Cloud-Native Operation (Yikuan Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yikuan Yan, Keman Huang, Michael Siegel. (2024)<br><strong>ISSF: The Intelligent Security Service Framework for Cloud-Native Operation</strong><br><button class=copy-to-clipboard title="ISSF: The Intelligent Security Service Framework for Cloud-Native Operation" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 16<br>Keywords: Graph, Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01507v1.pdf filename=2403.01507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing system complexity from microservice architectures and the bilateral enhancement of artificial intelligence (AI) for both attackers and defenders presents increasing security challenges for cloud-native operations. In particular, cloud-native operators require a holistic view of the dynamic security posture for the cloud-native environment from a defense aspect. Additionally, both attackers and defenders can adopt advanced AI technologies. This makes the dynamic interaction and <b>benchmark</b> among different intelligent offense and defense strategies more crucial. Hence, following the multi-agent deep <b>reinforcement</b> <b>learning</b> (RL) paradigm, this research develops an agent-based intelligent security service framework (ISSF) for cloud-native operation. It includes a dynamic access <b>graph</b> model to represent the cloud-native environment and an action model to represent offense and defense actions. Then we develop an approach to enable the training, publishing, and evaluating of intelligent security services using diverse deep RL algorithms and training strategies, facilitating their systematic development and <b>benchmark.</b> The experiments demonstrate that our framework can sufficiently model the security posture of a cloud-native system for defenders, effectively develop and quantitatively <b>benchmark</b> different services for both attackers and defenders and guide further service optimization.</p></p class="citation"></blockquote><h3 id=56--85128-collective-certified-robustness-against-graph-injection-attacks-yuni-lai-et-al-2024>(5/6 | 85/128) Collective Certified Robustness against Graph Injection Attacks (Yuni Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuni Lai, Bailin Pan, Kaihuang Chen, Yancheng Yuan, Kai Zhou. (2024)<br><strong>Collective Certified Robustness against Graph Injection Attacks</strong><br><button class=copy-to-clipboard title="Collective Certified Robustness against Graph Injection Attacks" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01423v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01423v1.pdf filename=2403.01423v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate certified robustness for <b>GNNs</b> under <b>graph</b> injection attacks. Existing research only provides sample-wise certificates by verifying each node independently, leading to very limited certifying performance. In this paper, we present the first collective certificate, which certifies a set of target nodes simultaneously. To achieve it, we formulate the problem as a binary integer quadratic constrained linear programming (BQCLP). We further develop a customized linearization technique that allows us to relax the BQCLP into linear programming (LP) that can be efficiently solved. Through comprehensive experiments, we demonstrate that our collective certification scheme significantly improves certification performance with minimal computational overhead. For instance, by solving the LP within 1 minute on the Citeseer dataset, we achieve a significant increase in the certified ratio from 0.0% to 81.2% when the injected node number is 5% of the <b>graph</b> size. Our step marks a crucial step towards making provable defense more practical.</p></p class="citation"></blockquote><h3 id=66--86128-enhancing-data-provenance-and-model-transparency-in-federated-learning-systems----a-database-approach-michael-gu-et-al-2024>(6/6 | 86/128) Enhancing Data Provenance and Model Transparency in Federated Learning Systems &ndash; A Database Approach (Michael Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Gu, Ramasoumya Naraparaju, Dongfang Zhao. (2024)<br><strong>Enhancing Data Provenance and Model Transparency in Federated Learning Systems &ndash; A Database Approach</strong><br><button class=copy-to-clipboard title="Enhancing Data Provenance and Model Transparency in Federated Learning Systems -- A Database Approach" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DB, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01451v1.pdf filename=2403.01451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) presents a promising paradigm for training machine learning models across decentralized edge devices while preserving data privacy. Ensuring the integrity and traceability of data across these distributed environments, however, remains a critical challenge. The ability to create transparent artificial intelligence, such as detailing the training process of a machine learning model, has become an increasingly prominent concern due to the large number of sensitive (hyper)parameters it utilizes; thus, it is imperative to strike a reasonable balance between openness and the need to protect sensitive information. In this paper, we propose one of the first approaches to enhance data provenance and model transparency in <b>federated</b> <b>learning</b> systems. Our methodology leverages a combination of cryptographic techniques and efficient model management to track the transformation of data throughout the FL process, and seeks to increase the reproducibility and trustworthiness of a trained FL model. We demonstrate the effectiveness of our approach through experimental evaluations on diverse FL scenarios, showcasing its ability to tackle accountability and explainability across the board. Our findings show that our system can greatly enhance data transparency in various FL environments by storing chained cryptographic hashes and client model snapshots in our proposed design for data decoupled FL. This is made possible by also employing multiple optimization techniques which enables comprehensive data provenance without imposing substantial computational loads. Extensive experimental results suggest that integrating a database subsystem into <b>federated</b> <b>learning</b> systems can improve data provenance in an efficient manner, encouraging secure FL adoption in privacy-sensitive applications and paving the way for future advancements in FL transparency and security features.</p></p class="citation"></blockquote><h2 id=csro-9>cs.RO (9)</h2><h3 id=19--87128-bronchocopilot-towards-autonomous-robotic-bronchoscopy-via-multimodal-reinforcement-learning-jianbo-zhao-et-al-2024>(1/9 | 87/128) BronchoCopilot: Towards Autonomous Robotic Bronchoscopy via Multimodal Reinforcement Learning (Jianbo Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianbo Zhao, Hao Chen, Qingyao Tian, Jian Chen, Bingyu Yang, Hongbin Liu. (2024)<br><strong>BronchoCopilot: Towards Autonomous Robotic Bronchoscopy via Multimodal Reinforcement Learning</strong><br><button class=copy-to-clipboard title="BronchoCopilot: Towards Autonomous Robotic Bronchoscopy via Multimodal Reinforcement Learning" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 46<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01483v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01483v1.pdf filename=2403.01483v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bronchoscopy plays a significant role in the early diagnosis and treatment of lung diseases. This process demands physicians to maneuver the flexible endoscope for reaching distal lesions, particularly requiring substantial expertise when examining the airways of the upper lung lobe. With the development of artificial intelligence and robotics, <b>reinforcement</b> <b>learning</b> (RL) method has been applied to the manipulation of interventional surgical robots. However, unlike human physicians who utilize <b>multimodal</b> information, most of the current RL methods rely on a single modality, limiting their performance. In this paper, we propose BronchoCopilot, a <b>multimodal</b> RL agent designed to acquire manipulation skills for autonomous bronchoscopy. BronchoCopilot specifically integrates images from the bronchoscope camera and estimated robot poses, aiming for a higher success rate within challenging airway environment. We employ auxiliary reconstruction tasks to compress <b>multimodal</b> data and utilize attention mechanisms to achieve an efficient latent representation of this data, serving as input for the RL module. This framework adopts a stepwise training and <b>fine-tuning</b> approach to mitigate the challenges of training difficulty. Our evaluation in the realistic <b>simulation</b> environment reveals that BronchoCopilot, by effectively harnessing <b>multimodal</b> information, attains a success rate of approximately 90% in fifth generation airways with consistent movements. Additionally, it demonstrates a robust capacity to adapt to diverse cases.</p></p class="citation"></blockquote><h3 id=29--88128-deep-incremental-model-based-reinforcement-learning-a-one-step-lookback-approach-for-continuous-robotics-control-cong-li-2024>(2/9 | 88/128) Deep Incremental Model Based Reinforcement Learning: A One-Step Lookback Approach for Continuous Robotics Control (Cong Li, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cong Li. (2024)<br><strong>Deep Incremental Model Based Reinforcement Learning: A One-Step Lookback Approach for Continuous Robotics Control</strong><br><button class=copy-to-clipboard title="Deep Incremental Model Based Reinforcement Learning: A One-Step Lookback Approach for Continuous Robotics Control" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 33<br>Keywords: Benchmarking, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01529v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01529v1.pdf filename=2403.01529v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model-based <b>reinforcement</b> <b>learning</b> (MBRL) attempts to use an available or a learned model to improve the data efficiency of <b>reinforcement</b> <b>learning.</b> This work proposes a one-step lookback approach that jointly learns the latent-space model and the policy to realize the sample-efficient continuous robotic control, wherein the control-theoretical knowledge is utilized to decrease the model learning difficulty. Specifically, the so-called one-step backward data is utilized to facilitate the incremental evolution model, an alternative structured representation of the robotics evolution model in the MBRL field. The incremental evolution model accurately predicts the robotics movement but with low sample complexity. This is because the formulated incremental evolution model degrades the model learning difficulty into a parametric matrix learning problem, which is especially favourable to high-dimensional robotics applications. The imagined data from the learned incremental evolution model is used to supplement training data to enhance the sample efficiency. Comparative numerical <b>simulations</b> on <b>benchmark</b> continuous robotics control problems are conducted to validate the efficiency of our proposed one-step lookback approach.</p></p class="citation"></blockquote><h3 id=39--89128-barrier-functions-inspired-reward-shaping-for-reinforcement-learning-nilaksh-et-al-2024>(3/9 | 89/128) Barrier Functions Inspired Reward Shaping for Reinforcement Learning (Nilaksh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nilaksh, Abhishek Ranjan, Shreenabh Agrawal, Aayush Jain, Pushpak Jagtap, Shishir Kolathaya. (2024)<br><strong>Barrier Functions Inspired Reward Shaping for Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Barrier Functions Inspired Reward Shaping for Reinforcement Learning" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: I-2-9, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01410v1.pdf filename=2403.01410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> (RL) has progressed from simple control tasks to complex real-world challenges with large state spaces. While RL excels in these tasks, training time remains a limitation. Reward shaping is a popular solution, but existing methods often rely on value functions, which face scalability issues. This paper presents a novel safety-oriented reward-shaping framework inspired by barrier functions, offering simplicity and ease of implementation across various environments and tasks. To evaluate the effectiveness of the proposed reward formulations, we conduct <b>simulation</b> experiments on CartPole, Ant, and Humanoid environments, along with real-world deployment on the Unitree Go1 quadruped robot. Our results demonstrate that our method leads to 1.4-2.8 times faster convergence and as low as 50-60% actuation effort compared to the vanilla reward. In a sim-to-real experiment with the Go1 robot, we demonstrated better control and dynamics of the bot with our reward framework.</p></p class="citation"></blockquote><h3 id=49--90128-the-grasp-loop-signature-a-topological-representation-for-manipulation-planning-with-ropes-and-cables-peter-mitrano-et-al-2024>(4/9 | 90/128) The Grasp Loop Signature: A Topological Representation for Manipulation Planning with Ropes and Cables (Peter Mitrano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Mitrano, Dmitry Berenson. (2024)<br><strong>The Grasp Loop Signature: A Topological Representation for Manipulation Planning with Ropes and Cables</strong><br><button class=copy-to-clipboard title="The Grasp Loop Signature: A Topological Representation for Manipulation Planning with Ropes and Cables" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01611v1.pdf filename=2403.01611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic manipulation of deformable, one-dimensional objects (DOOs) like ropes or cables has important potential applications in manufacturing, agriculture, and surgery. In such environments, the task may involve threading through or avoiding becoming tangled with objects like racks or frames. Grasping with multiple grippers can create closed loops between the robot and DOO, and If an obstacle lies within this loop, it may be impossible to reach the goal. However, prior work has only considered the topology of the DOO in isolation, ignoring the arms that are manipulating it. Searching over possible grasps to accomplish the task without considering such topological information is very inefficient, as many grasps will not lead to progress on the task due to topological constraints. Therefore, we propose a grasp loop signature which categorizes the topology of these grasp loops and show how it can be used to guide planning. We perform experiments in <b>simulation</b> on two DOO manipulation tasks to show that using the signature is faster and succeeds more often than methods that rely on local <b>geometry</b> or finite-horizon planning. Finally, we demonstrate using the signature in the real world to manipulate a cable in a scene with obstacles using a dual-arm robot.</p></p class="citation"></blockquote><h3 id=59--91128-localization-matters-too-how-localization-error-affects-uav-flight-suquan-zhang-et-al-2024>(5/9 | 91/128) Localization matters too: How localization error affects UAV flight (Suquan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suquan Zhang, Yuanfan Xu, Shu&rsquo;ang Yu, Qingmin Liao, Jincheng Yu, Yu Wang. (2024)<br><strong>Localization matters too: How localization error affects UAV flight</strong><br><button class=copy-to-clipboard title="Localization matters too: How localization error affects UAV flight" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01428v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01428v2.pdf filename=2403.01428v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The maximum safe flight speed of a Unmanned Aerial Vehicle (UAV) is an important indicator for measuring its efficiency in completing various tasks. This indicator is influenced by numerous parameters such as UAV localization error, perception range, and system latency. However, in terms of localization errors, although there have been many studies dedicated to improving the localization capability of UAVs, there is a lack of quantitative research on their impact on speed. In this work, we model the relationship between various parameters of the UAV and its maximum flight speed. We consider a scenario similar to navigating through dense forests, where the UAV needs to quickly avoid obstacles directly ahead and swiftly reorient after avoidance. Based on this scenario, we studied how parameters such as localization error affect the maximum safe speed during UAV flight, as well as the coupling relationships between these parameters. Furthermore, we validated our model in a <b>simulation</b> environment, and the results showed that the predicted maximum safe speed had an error of less than 20% compared to the test speed. In high-density situations, localization error has a significant impact on the UAV&rsquo;s maximum safe flight speed. This model can help designers utilize more suitable software and hardware to construct a UAV system.</p></p class="citation"></blockquote><h3 id=69--92128-a-human-centered-approach-for-bootstrapping-causal-graph-creation-minh-q-tram-et-al-2024>(6/9 | 92/128) A Human-Centered Approach for Bootstrapping Causal Graph Creation (Minh Q. Tram et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minh Q. Tram, Nolan B. Gutierrez, William J. Beksi. (2024)<br><strong>A Human-Centered Approach for Bootstrapping Causal Graph Creation</strong><br><button class=copy-to-clipboard title="A Human-Centered Approach for Bootstrapping Causal Graph Creation" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Graph, Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01622v1.pdf filename=2403.01622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causal inference, a cornerstone in disciplines such as economics, genomics, and medicine, is increasingly being recognized as fundamental to advancing the field of robotics. In particular, the ability to reason about cause and effect from observational data is crucial for robust generalization in robotic systems. However, the construction of a causal graphical model, a mechanism for representing causal relations, presents an immense challenge. Currently, a nuanced grasp of causal inference, coupled with an understanding of causal relationships, must be manually programmed into a causal graphical model. To address this difficulty, we present initial results towards a human-centered augmented reality framework for creating causal graphical models. Concretely, our system bootstraps the causal discovery process by involving humans in selecting variables, establishing relationships, performing interventions, generating <b>counterfactual</b> explanations, and evaluating the resulting causal <b>graph</b> at every step. We highlight the potential of our framework via a physical robot manipulator on a pick-and-place task.</p></p class="citation"></blockquote><h3 id=79--93128-collision-free-robot-navigation-in-crowded-environments-using-learning-based-convex-model-predictive-control-zhuanglei-wen-et-al-2024>(7/9 | 93/128) Collision-Free Robot Navigation in Crowded Environments using Learning based Convex Model Predictive Control (Zhuanglei Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuanglei Wen, Mingze Dong, Xiai Chen. (2024)<br><strong>Collision-Free Robot Navigation in Crowded Environments using Learning based Convex Model Predictive Control</strong><br><button class=copy-to-clipboard title="Collision-Free Robot Navigation in Crowded Environments using Learning based Convex Model Predictive Control" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01450v1.pdf filename=2403.01450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of deep <b>reinforcement</b> <b>learning</b> (DRL) has significantly expanded the application range for autonomous robots. However, safe navigation in crowded and complex environments remains a persistent challenge. This study proposes a robot navigation strategy that utilizes DRL, conceptualizing the observation as the convex static obstacle-free region, a departure from traditional reliance on raw sensor inputs. The novelty of this work is threefold: (1) Formulating an action space that includes both short-term and long-term reference points, based on the robot&rsquo;s kinematic limits and the convex region computed from 2D LiDAR sensor data. (2) Exploring a hybrid solution that combines DRL with Model Predictive Control (MPC). (3) Designing a customized state space and reward function based on the static obstacle-free region, reference points, and the trajectory optimized by MPC. The effectiveness of these improvements has been confirmed through experimental results, demonstrating improved navigation performance in crowded and complex environments.</p></p class="citation"></blockquote><h3 id=89--94128-fast-ergodic-search-with-kernel-functions-muchen-sun-et-al-2024>(8/9 | 94/128) Fast Ergodic Search with Kernel Functions (Muchen Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muchen Sun, Ayush Gaggar, Peter Trautman, Todd Murphey. (2024)<br><strong>Fast Ergodic Search with Kernel Functions</strong><br><button class=copy-to-clipboard title="Fast Ergodic Search with Kernel Functions" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01536v1.pdf filename=2403.01536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ergodic search enables optimal exploration of an information distribution while guaranteeing the asymptotic coverage of the search space. However, current methods typically have exponential computation complexity in the search space dimension and are restricted to Euclidean space. We introduce a computationally efficient ergodic search method. Our contributions are two-fold. First, we develop a kernel-based ergodic metric and generalize it from Euclidean space to Lie groups. We formally prove the proposed metric is consistent with the standard ergodic metric while guaranteeing linear complexity in the search space dimension. Secondly, we derive the first-order optimality condition of the kernel ergodic metric for nonlinear systems, which enables efficient trajectory optimization. Comprehensive numerical <b>benchmarks</b> show that the proposed method is at least two orders of magnitude faster than the state-of-the-art algorithm. Finally, we demonstrate the proposed algorithm with a peg-in-hole insertion task. We formulate the problem as a coverage task in the space of SE(3) and use a 30-second-long human demonstration as the prior distribution for ergodic coverage. Ergodicity guarantees the asymptotic solution of the peg-in-hole problem so long as the solution resides within the prior information distribution, which is seen in the 100% success rate.</p></p class="citation"></blockquote><h3 id=99--95128-dufomap-efficient-dynamic-awareness-mapping-daniel-duberg-et-al-2024>(9/9 | 95/128) DUFOMap: Efficient Dynamic Awareness Mapping (Daniel Duberg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Duberg, Qingwen Zhang, MingKai Jia, Patric Jensfelt. (2024)<br><strong>DUFOMap: Efficient Dynamic Awareness Mapping</strong><br><button class=copy-to-clipboard title="DUFOMap: Efficient Dynamic Awareness Mapping" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01449v1.pdf filename=2403.01449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The dynamic nature of the real world is one of the main challenges in robotics. The first step in dealing with it is to detect which parts of the world are dynamic. A typical <b>benchmark</b> task is to create a map that contains only the static part of the world to support, for example, localization and planning. Current solutions are often applied in post-processing, where parameter tuning allows the user to adjust the setting for a specific dataset. In this paper, we propose DUFOMap, a novel dynamic awareness mapping framework designed for efficient online processing. Despite having the same parameter settings for all scenarios, it performs better or is on par with state-of-the-art methods. Ray casting is utilized to identify and classify fully observed empty regions. Since these regions have been observed empty, it follows that anything inside them at another time must be dynamic. Evaluation is carried out in various scenarios, including outdoor environments in KITTI and Argoverse 2, open areas on the KTH campus, and with different sensor types. DUFOMap outperforms the state of the art in terms of accuracy and computational efficiency. The source code, <b>benchmarks,</b> and links to the datasets utilized are provided. See <a href=https://kin-zhang.github.io/dufomap>https://kin-zhang.github.io/dufomap</a> for more details.</p></p class="citation"></blockquote><h2 id=statml-2>stat.ML (2)</h2><h3 id=12--96128-approximations-to-the-fisher-information-metric-of-deep-generative-models-for-out-of-distribution-detection-sam-dauncey-et-al-2024>(1/2 | 96/128) Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection (Sam Dauncey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sam Dauncey, Chris Holmes, Christopher Williams, Fabian Falck. (2024)<br><strong>Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection</strong><br><button class=copy-to-clipboard title="Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CV, cs-LG, stat-ML, stat.ML<br>Keyword Score: 40<br>Keywords: Diffusion Model, Autoencoder, Out-of-distribution, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01485v1.pdf filename=2403.01485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Likelihood-based deep generative models such as score-based <b>diffusion</b> <b>models</b> and <b>variational</b> <b>autoencoders</b> are state-of-the-art machine learning models approximating high-dimensional distributions of data such as images, text, or audio. One of many downstream tasks they can be naturally applied to is <b>out-of-distribution</b> (OOD) detection. However, seminal work by Nalisnick et al. which we reproduce showed that deep generative models consistently infer higher log-likelihoods for OOD data than data they were trained on, marking an open problem. In this work, we analyse using the gradient of a data point with respect to the parameters of the deep generative model for OOD detection, based on the simple intuition that OOD data should have larger gradient norms than training data. We formalise measuring the size of the gradient as approximating the Fisher information metric. We show that the Fisher information matrix (FIM) has large absolute diagonal values, motivating the use of chi-square distributed, layer-wise gradient norms as features. We combine these features to make a simple, model-agnostic and hyperparameter-free method for OOD detection which estimates the joint density of the layer-wise gradient norms for a given data point. We find that these layer-wise gradient norms are weakly correlated, rendering their combined usage informative, and prove that the layer-wise gradient norms satisfy the principle of (data representation) invariance. Our empirical results indicate that this method outperforms the Typicality test for most deep generative models and image dataset pairings.</p></p class="citation"></blockquote><h3 id=22--97128-sample-efficient-myopic-exploration-through-multitask-reinforcement-learning-with-diverse-tasks-ziping-xu-et-al-2024>(2/2 | 97/128) Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks (Ziping Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziping Xu, Zifan Xu, Runxuan Jiang, Peter Stone, Ambuj Tewari. (2024)<br><strong>Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks</strong><br><button class=copy-to-clipboard title="Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Curriculum Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01636v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01636v2.pdf filename=2403.01636v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multitask <b>Reinforcement</b> <b>Learning</b> (MTRL) approaches have gained increasing attention for its wide applications in many important <b>Reinforcement</b> <b>Learning</b> (RL) tasks. However, while recent advancements in MTRL theory have focused on the improved statistical efficiency by assuming a shared structure across tasks, exploration&ndash;a crucial aspect of RL&ndash;has been largely overlooked. This paper addresses this gap by showing that when an agent is trained on a sufficiently diverse set of tasks, a generic policy-sharing algorithm with myopic exploration design like $\epsilon$-greedy that are inefficient in general can be sample-efficient for MTRL. To the best of our knowledge, this is the first theoretical demonstration of the &ldquo;exploration benefits&rdquo; of MTRL. It may also shed light on the enigmatic success of the wide applications of myopic exploration in practice. To validate the role of diversity, we conduct experiments on synthetic robotic control environments, where the diverse task set aligns with the task selection by automatic <b>curriculum</b> <b>learning,</b> which is empirically shown to improve sample-efficiency.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=12--98128-a-closer-look-at-wav2vec2-embeddings-for-on-device-single-channel-speech-enhancement-ravi-shankar-et-al-2024>(1/2 | 98/128) A Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel Speech Enhancement (Ravi Shankar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ravi Shankar, Ke Tan, Buye Xu, Anurag Kumar. (2024)<br><strong>A Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel Speech Enhancement</strong><br><button class=copy-to-clipboard title="A Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel Speech Enhancement" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-LG, eess-AS, eess.AS<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Self-supervised Learning, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01369v1.pdf filename=2403.01369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> learned models have been found to be very effective for certain <b>speech</b> <b>tasks</b> such as <b>automatic</b> <b>speech</b> <b>recognition,</b> speaker identification, keyword spotting and others. While the features are undeniably useful in <b>speech</b> <b>recognition</b> and associated tasks, their utility in <b>speech</b> <b>enhancement</b> systems is yet to be firmly established, and perhaps not properly understood. In this paper, we investigate the uses of SSL representations for single-channel <b>speech</b> <b>enhancement</b> in challenging conditions and find that they add very little value for the enhancement task. Our constraints are designed around on-device real-time <b>speech</b> <b>enhancement</b> &ndash; model is causal, the compute footprint is small. Additionally, we focus on low SNR conditions where such models struggle to provide good enhancement. In order to systematically examine how SSL representations impact performance of such enhancement models, we propose a variety of techniques to utilize these embeddings which include different forms of <b>knowledge-distillation</b> <b>and</b> pre-training.</p></p class="citation"></blockquote><h3 id=22--99128-a-dcf-an-architecture-agnostic-metric-with-application-to-spoofing-robust-speaker-verification-hye-jin-shim-et-al-2024>(2/2 | 99/128) a-DCF: an architecture agnostic metric with application to spoofing-robust speaker verification (Hye-jin Shim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hye-jin Shim, Jee-weon Jung, Tomi Kinnunen, Nicholas Evans, Jean-Francois Bonastre, Itshak Lapidot. (2024)<br><strong>a-DCF: an architecture agnostic metric with application to spoofing-robust speaker verification</strong><br><button class=copy-to-clipboard title="a-DCF: an architecture agnostic metric with application to spoofing-robust speaker verification" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-LG, eess-AS, eess.AS<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01355v1.pdf filename=2403.01355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spoofing detection is today a mainstream research topic. Standard metrics can be applied to evaluate the performance of isolated spoofing detection solutions and others have been proposed to support their evaluation when they are combined with speaker detection. These either have well-known deficiencies or restrict the architectural approach to combine speaker and spoof detectors. In this paper, we propose an architecture-agnostic detection cost function (a-DCF). A generalisation of the original DCF used widely for the assessment of automatic speaker verification (ASV), the a-DCF is designed for the evaluation of spoofing-robust ASV. Like the DCF, the a-DCF reflects the cost of decisions in a Bayes risk sense, with explicitly defined class priors and detection cost model. We demonstrate the merit of the a-DCF through the <b>benchmarking</b> evaluation of architecturally-heterogeneous spoofing-robust ASV solutions.</p></p class="citation"></blockquote><h2 id=csir-1>cs.IR (1)</h2><h3 id=11--100128-logic-rules-as-explanations-for-legal-case-retrieval-zhongxiang-sun-et-al-2024>(1/1 | 100/128) Logic Rules as Explanations for Legal Case Retrieval (Zhongxiang Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongxiang Sun, Kepu Zhang, Weijie Yu, Haoyu Wang, Jun Xu. (2024)<br><strong>Logic Rules as Explanations for Legal Case Retrieval</strong><br><button class=copy-to-clipboard title="Logic Rules as Explanations for Legal Case Retrieval" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01457v1.pdf filename=2403.01457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the issue of using logic rules to explain the results from legal case retrieval. The task is critical to legal case retrieval because the users (e.g., lawyers or judges) are highly specialized and require the system to provide logical, faithful, and interpretable explanations before making legal decisions. Recently, research efforts have been made to learn explainable legal case retrieval models. However, these methods usually select rationales (key sentences) from the legal cases as explanations, failing to provide faithful and logically correct explanations. In this paper, we propose Neural-Symbolic enhanced Legal Case Retrieval (NS-LCR), a framework that explicitly conducts <b>reasoning</b> on the matching of legal cases through learning case-level and law-level logic rules. The learned rules are then integrated into the retrieval process in a neuro-symbolic manner. Benefiting from the logic and interpretable nature of the logic rules, NS-LCR is equipped with built-in faithful explainability. We also show that NS-LCR is a model-agnostic framework that can be plugged in for multiple legal retrieval models. To showcase NS-LCR&rsquo;s superiority, we enhance existing <b>benchmarks</b> by adding manually annotated logic rules and introducing a novel explainability metric using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Our comprehensive experiments reveal NS-LCR&rsquo;s effectiveness for ranking, alongside its proficiency in delivering reliable explanations for legal case retrieval.</p></p class="citation"></blockquote><h2 id=csdb-3>cs.DB (3)</h2><h3 id=13--101128-rematch-retrieval-enhanced-schema-matching-with-llms-eitam-sheetrit-et-al-2024>(1/3 | 101/128) ReMatch: Retrieval Enhanced Schema Matching with LLMs (Eitam Sheetrit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eitam Sheetrit, Menachem Brief, Moshik Mishaeli, Oren Elisha. (2024)<br><strong>ReMatch: Retrieval Enhanced Schema Matching with LLMs</strong><br><button class=copy-to-clipboard title="ReMatch: Retrieval Enhanced Schema Matching with LLMs" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-DB, cs.DB<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01567v1.pdf filename=2403.01567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Schema matching is a crucial task in data integration, involving the alignment of a source database schema with a target schema to establish correspondence between their elements. This task is challenging due to textual and semantic heterogeneity, as well as differences in schema sizes. Although machine-learning-based solutions have been explored in numerous studies, they often suffer from low accuracy, require manual mapping of the schemas for model training, or need access to source schema data which might be unavailable due to privacy concerns. In this paper we present a novel method, named ReMatch, for matching schemas using retrieval-enhanced <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Our method avoids the need for predefined mapping, any model training, or access to data in the source database. In the ReMatch method the tables of the target schema and the attributes of the source schema are first represented as structured passage-based documents. For each source attribute document, we retrieve $J$ documents, representing target schema tables, according to their semantic relevance. Subsequently, we create a <b>prompt</b> for every source table, comprising all its attributes and their descriptions, alongside all attributes from the set of top $J$ target tables retrieved previously. We employ <b>LLMs</b> using this <b>prompt</b> for the matching task, yielding a ranked list of $K$ potential matches for each source attribute. Our experimental results on <b>large</b> <b>real-world</b> <b>schemas</b> demonstrate that ReMatch significantly improves matching capabilities and outperforms other machine learning approaches. By eliminating the requirement for training data, ReMatch becomes a viable solution for real-world scenarios.</p></p class="citation"></blockquote><h3 id=23--102128-treetracker-join-turning-the-tide-when-a-tuple-fails-to-join-zeyuan-hu-et-al-2024>(2/3 | 102/128) TreeTracker Join: Turning the Tide When a Tuple Fails to Join (Zeyuan Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyuan Hu, Daniel P. Miranker. (2024)<br><strong>TreeTracker Join: Turning the Tide When a Tuple Fails to Join</strong><br><button class=copy-to-clipboard title="TreeTracker Join: Turning the Tide When a Tuple Fails to Join" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: H-2-4, cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01631v1.pdf filename=2403.01631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many important query processing methods proactively use semijoins or semijoin-like filters to delete dangling tuples, i.e., tuples that do not appear in the final query result. Semijoin methods can achieve formal optimality but have high upfront cost in practice. Filter methods reduce the cost but lose the optimality guarantee. We propose a new join algorithm, TreeTracker Join ($\mathsf{TTJ}$), that achieves the data complexity optimality for acyclic conjunctive queries (ACQs) without semijoins or semijoin-like filters. $\mathsf{TTJ}$ leverages join failure events, where a tuple from one of the relations of a binary join operator fails to match any tuples from the other relation. $\mathsf{TTJ}$ starts join evaluation immediately and when join fails, $\mathsf{TTJ}$ identifies the tuple as dangling and prevents it from further consideration in the execution of the query. The design of $\mathsf{TTJ}$ exploits the connection between query evaluation and constraint satisfaction problem (CSP) by treating a join tree of an ACQ as a constraint network and the query evaluation as a CSP search problem. $\mathsf{TTJ}$ is a direct extension of a CSP algorithm, TreeTracker, that embodies two search techniques backjumping and no-good. We establish that join tree and plan can be constructed from each other in order to incorporate the search techniques into physical operators in the iterator form. We compare $\mathsf{TTJ}$ with hash-join, a classic semijoin method: Yannakakis&rsquo;s algorithm, and two contemporary filter methods: Predicate Transfer and Lookahead Information Passing. Favorable empirical results are developed using standard query <b>benchmarks:</b> JOB, TPC-H, and SSB.</p></p class="citation"></blockquote><h3 id=33--103128-relational-to-rdf-data-migration-by-query-co-evaluation-ryan-wisnesky-et-al-2024>(3/3 | 103/128) Relational to RDF Data Migration by Query Co-Evaluation (Ryan Wisnesky et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan Wisnesky, Daniel Filonik. (2024)<br><strong>Relational to RDF Data Migration by Query Co-Evaluation</strong><br><button class=copy-to-clipboard title="Relational to RDF Data Migration by Query Co-Evaluation" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01630v1.pdf filename=2403.01630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we define a new algorithm to convert an input relational database to an output set of RDF triples. The algorithm can be used to e.g. load CSV data into a financial OWL ontology such as FIBO. The algorithm takes as input a set of relational conjunctive (select-from-where) queries, one for each input table, from the three column (subject, predicate, object) output RDF schema to the input table&rsquo;s relational schema. The algorithm&rsquo;s output is the only set of RDF triples for which a unique round-trip of the input data under the relational queries exists. The output may contain blank nodes, is unique up to unique isomorphism, and can be obtained using elementary formal methods (equational theorem proving and term model construction specifically). We also describe how (generalized) homomorphisms between <b>graphs</b> can be used to write such relational conjunctive (select-from-where) queries, which, due to the lack of structure in the three-column RDF schema, tend to be large in practice. We demonstrate examples of both the algorithm and mapping language on the FIBO financial ontology.</p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=12--104128-distributed-least-squares-optimization-solvers-with-differential-privacy-weijia-liu-et-al-2024>(1/2 | 104/128) Distributed Least-Squares Optimization Solvers with Differential Privacy (Weijia Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weijia Liu, Lei Wang, Fanghong Guo, Zhengguang Wu, Hongye Su. (2024)<br><strong>Distributed Least-Squares Optimization Solvers with Differential Privacy</strong><br><button class=copy-to-clipboard title="Distributed Least-Squares Optimization Solvers with Differential Privacy" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01435v1.pdf filename=2403.01435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies the distributed least-squares optimization problem with <b>differential</b> <b>privacy</b> requirement of local cost functions, for which two differentially private distributed solvers are proposed. The first is established on the distributed gradient tracking algorithm, by appropriately perturbing the initial values and parameters that contain the privacy-sensitive data with Gaussian and truncated Laplacian noises, respectively. Rigorous proofs are established to show the achievable trade-off between the ({\epsilon}, {\delta})-differential privacy and the computation accuracy. The second solver is established on the combination of the distributed shuffling mechanism and the average consensus algorithm, which enables each agent to obtain a noisy version of parameters characterizing the global gradient. As a result, the least-squares optimization problem can be eventually solved by each agent locally in such a way that any given ({\epsilon}, {\delta})-differential privacy requirement can be preserved while the solution may be computed with the accuracy independent of the network size, which makes the latter more suitable for large-scale distributed least-squares problems. Numerical <b>simulations</b> are presented to show the effectiveness of both solvers.</p></p class="citation"></blockquote><h3 id=22--105128-distributed-discrete-time-dynamic-outer-approximation-of-the-intersection-of-ellipsoids-eduardo-sebastián-et-al-2024>(2/2 | 105/128) Distributed Discrete-time Dynamic Outer Approximation of the Intersection of Ellipsoids (Eduardo Sebastián et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eduardo Sebastián, Rodrigo Aldana-López, Rosario Aragüés, Eduardo Montijano, Carlos Sagüés. (2024)<br><strong>Distributed Discrete-time Dynamic Outer Approximation of the Intersection of Ellipsoids</strong><br><button class=copy-to-clipboard title="Distributed Discrete-time Dynamic Outer Approximation of the Intersection of Ellipsoids" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01478v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01478v1.pdf filename=2403.01478v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the first <b>discrete-time</b> <b>distributed</b> algorithm to track the tightest ellipsoids that outer approximates the global dynamic intersection of ellipsoids. The ellipsoids are defined as time-varying positive definite matrices. On the other hand, given an undirected network, each node is equipped with one of these ellipsoids. The solution is based on a novel distributed reformulation of the original centralized semi-definite outer L"owner-John program, characterized by a non-separable objective function and global constraints. We prove finite-time convergence to the global minima of the centralized problem in the static case and finite-time bounded tracking error in the dynamic case. Moreover, we prove boundedness of estimation in the tracking of the global optimum and robustness in the estimation against time-varying inputs. As a by-product, the proposed algorithm extends min/max dynamic consensus algorithms to positive definite matrices. We illustrate the properties of the algorithm with different simulated examples, including a distributed estimation showcase where our proposal is integrated into a distributed Kalman filter to surpass the state-of-the-art in mean square error performance.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--106128-brainmass-advancing-brain-network-analysis-for-diagnosis-with-large-scale-self-supervised-learning-yanwu-yang-et-al-2024>(1/1 | 106/128) BrainMass: Advancing Brain Network Analysis for Diagnosis with Large-scale Self-Supervised Learning (Yanwu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanwu Yang, Chenfei Ye, Guinan Su, Ziyao Zhang, Zhikai Chang, Hairui Chen, Piu Chan, Yue Yu, Ting Ma. (2024)<br><strong>BrainMass: Advancing Brain Network Analysis for Diagnosis with Large-scale Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="BrainMass: Advancing Brain Network Analysis for Diagnosis with Large-scale Self-Supervised Learning" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE, q-bio-NC<br>Keyword Score: 30<br>Keywords: Foundation Model, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01433v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01433v1.pdf filename=2403.01433v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> pretrained on large-scale datasets via <b>self-supervised</b> <b>learning</b> demonstrate exceptional versatility across various tasks. Due to the heterogeneity and hard-to-collect medical data, this approach is especially beneficial for medical image analysis and neuroscience research, as it streamlines broad downstream tasks without the need for numerous costly annotations. However, there has been limited investigation into brain network <b>foundation</b> <b>models,</b> limiting their adaptability and generalizability for broad neuroscience studies. In this study, we aim to bridge this gap. In particular, (1) we curated a comprehensive dataset by collating images from 30 datasets, which comprises 70,781 samples of 46,686 participants. Moreover, we introduce pseudo-functional connectivity (pFC) to further generates millions of augmented brain networks by randomly dropping certain timepoints of the BOLD signal. (2) We propose the BrainMass framework for brain network <b>self-supervised</b> <b>learning</b> via mask modeling and feature alignment. BrainMass employs Mask-ROI Modeling (MRM) to bolster intra-network dependencies and regional specificity. Furthermore, Latent Representation Alignment (LRA) module is utilized to regularize augmented brain networks of the same participant with similar topological properties to yield similar latent representations by aligning their latent embeddings. Extensive experiments on eight internal tasks and seven external brain disorder diagnosis tasks show BrainMass&rsquo;s superior performance, highlighting its significant generalizability and adaptability. Nonetheless, BrainMass demonstrates powerful few/zero-shot learning abilities and exhibits meaningful interpretation to various diseases, showcasing its potential use for clinical applications.</p></p class="citation"></blockquote><h2 id=cspf-1>cs.PF (1)</h2><h3 id=11--107128-a-continuous-benchmarking-infrastructure-for-high-performance-computing-applications-christoph-alt-et-al-2024>(1/1 | 107/128) A Continuous Benchmarking Infrastructure for High-Performance Computing Applications (Christoph Alt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christoph Alt, Martin Lanser, Jonas Plewinski, Atin Janki, Axel Klawonn, Harald Köstler, Michael Selzer, Ulrich Rüde. (2024)<br><strong>A Continuous Benchmarking Infrastructure for High-Performance Computing Applications</strong><br><button class=copy-to-clipboard title="A Continuous Benchmarking Infrastructure for High-Performance Computing Applications" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PF<br>Categories: cs-PF, cs.PF<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01579v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01579v1.pdf filename=2403.01579v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For scientific software, especially those used for large-scale <b>simulations,</b> achieving good performance and efficiently using the available hardware resources is essential. It is important to regularly perform <b>benchmarks</b> to ensure the efficient use of hardware and software when systems are changing and the software evolves. However, this can become quickly very tedious when many options for parameters, solvers, and hardware architectures are available. We present a continuous <b>benchmarking</b> strategy that automates <b>benchmarking</b> new code changes on high-performance computing clusters. This makes it possible to track how each code change affects the performance and how it evolves.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--108128-an-rbf-partition-of-unity-method-for-geometry-reconstruction-and-pde-solution-in-thin-structures-elisabeth-larsson-et-al-2024>(1/2 | 108/128) An RBF partition of unity method for geometry reconstruction and PDE solution in thin structures (Elisabeth Larsson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elisabeth Larsson, Pierre-Frédéric Villard, Igor Tominec, Ulrika Sundin, Andreas Michael, Nicola Cacciani. (2024)<br><strong>An RBF partition of unity method for geometry reconstruction and PDE solution in thin structures</strong><br><button class=copy-to-clipboard title="An RBF partition of unity method for geometry reconstruction and PDE solution in thin structures" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N35 (Primary) 65N12 (Secondary), cs-NA, math-NA, math.NA<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01486v1.pdf filename=2403.01486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The main respiratory muscle, the diaphragm, is an example of a thin structure. We aim to perform detailed numerical <b>simulations</b> of the muscle mechanics based on individual patient data. This requires a representation of the diaphragm <b>geometry</b> extracted from medical image data. We design an adaptive reconstruction method based on a least-squares radial basis function partition of unity method. The method is adapted to thin structures by subdividing the structure rather than the surrounding space, and by introducing an anisotropic scaling of local subproblems. The resulting representation is an infinitely smooth level set function, which is stabilized such that there are no spurious zero level sets. We show reconstruction results for 2D cross sections of the diaphragm <b>geometry</b> as well as for the full 3D <b>geometry.</b> We also show solutions to basic PDE test problems in the reconstructed geometries.</p></p class="citation"></blockquote><h3 id=22--109128-fast-algorithm-for-quasi-2d-coulomb-systems-zecheng-gan-et-al-2024>(2/2 | 109/128) Fast Algorithm for Quasi-2D Coulomb Systems (Zecheng Gan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zecheng Gan, Xuanzhao Gao, Jiuyang Liang, Zhenli Xu. (2024)<br><strong>Fast Algorithm for Quasi-2D Coulomb Systems</strong><br><button class=copy-to-clipboard title="Fast Algorithm for Quasi-2D Coulomb Systems" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 82M37, 65D15, 65C35, cs-NA, math-NA, math.NA, physics-comp-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01521v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01521v1.pdf filename=2403.01521v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quasi-2D Coulomb systems are of fundamental importance and have attracted much attention in many areas nowadays. Their reduced symmetry gives rise to interesting collective behaviors, but also brings great challenges for particle-based <b>simulations.</b> Here, we propose a novel algorithm framework to address the $\mathcal O(N^2)$ <b>simulation</b> complexity associated with the long-range nature of Coulomb interactions. First, we introduce an efficient Sum-of-Exponentials (SOE) approximation for the long-range kernel associated with Ewald splitting, achieving uniform convergence in terms of inter-particle distance, which reduces the complexity to $\mathcal{O}(N^{7/5})$. We then introduce a random batch sampling method in the periodic dimensions, the stochastic approximation is proven to be both unbiased and with reduced variance via a tailored importance sampling strategy, further reducing the computational cost to $\mathcal{O}(N)$. The performance of our algorithm is demonstrated via varies numerical examples. Notably, it achieves a speedup of $2\sim 3$ orders of magnitude comparing with Ewald2D method, enabling molecular dynamics (MD) <b>simulations</b> with up to $10^6$ particles on a single core. The present approach is therefore well-suited for large-scale particle-based <b>simulations</b> of Coulomb systems under confinement, making it possible to investigate the role of Coulomb interaction in many practical situations.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--110128-a-face-centred-finite-volume-method-for-laminar-and-turbulent-incompressible-flows-luan-m-vieira-et-al-2024>(1/1 | 110/128) A face-centred finite volume method for laminar and turbulent incompressible flows (Luan M. Vieira et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luan M. Vieira, Matteo Giacomini, Ruben Sevilla, Antonio Huerta. (2024)<br><strong>A face-centred finite volume method for laminar and turbulent incompressible flows</strong><br><button class=copy-to-clipboard title="A face-centred finite volume method for laminar and turbulent incompressible flows" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: 76M12, 76D05, 65M12, 76F99, cs-CE, cs-NA, math-NA, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01496v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01496v1.pdf filename=2403.01496v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work develops, for the first time, a face-centred finite volume (FCFV) solver for the <b>simulation</b> of laminar and turbulent viscous incompressible flows. The formulation relies on the Reynolds-averaged Navier-Stokes (RANS) equations coupled with the negative Spalart-Allmaras (SA) model and three novel convective stabilisations, inspired by Riemann solvers, are derived and compared numerically. The resulting method achieves first-order convergence of the velocity, the velocity-gradient tensor and the pressure. FCFV accurately predicts engineering quantities of interest, such as drag and lift, on unstructured meshes and, by avoiding gradient reconstruction, the method is insensitive to mesh quality, even in the presence of highly distorted and stretched cells. A monolithic and a staggered solution strategies for the RANS-SA system are derived and compared numerically. Numerical <b>benchmarks,</b> involving laminar and turbulent, steady and transient cases are used to assess the performance, accuracy and robustness of the proposed FCFV method.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--111128-can-poverty-be-reduced-by-acting-on-discrimination-an-agent-based-model-for-policy-making-alba-aguilera-et-al-2024>(1/1 | 111/128) Can Poverty Be Reduced by Acting on Discrimination? An Agent-based Model for Policy Making (Alba Aguilera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alba Aguilera, Nieves Montes, Georgina Curto, Carles Sierra, Nardine Osman. (2024)<br><strong>Can Poverty Be Reduced by Acting on Discrimination? An Agent-based Model for Policy Making</strong><br><button class=copy-to-clipboard title="Can Poverty Be Reduced by Acting on Discrimination? An Agent-based Model for Policy Making" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs.MA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01600v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01600v1.pdf filename=2403.01600v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the last decades, there has been a deceleration in the rates of poverty reduction, suggesting that traditional redistributive approaches to poverty mitigation could be losing effectiveness, and alternative insights to advance the number one UN Sustainable Development Goal are required. The criminalization of poor people has been denounced by several NGOs, and an increasing number of voices suggest that discrimination against the poor (a phenomenon known as \emph{aporophobia}) could be an impediment to mitigating poverty. In this paper, we present the novel Aporophobia Agent-Based Model (AABM) to provide evidence of the correlation between aporophobia and poverty computationally. We present our use case built with real-world demographic data and poverty-mitigation public policies (either enforced or under parliamentary discussion) for the city of Barcelona. We classify policies as discriminatory or non-discriminatory against the poor, with the support of specialized NGOs, and we observe the results in the AABM in terms of the impact on wealth inequality. The <b>simulation</b> provides evidence of the relationship between aporophobia and the increase of wealth inequality levels, paving the way for a new generation of poverty reduction policies that act on discrimination and tackle poverty as a societal problem (not only a problem of the poor).</p></p class="citation"></blockquote><h2 id=csdl-1>cs.DL (1)</h2><h3 id=11--112128-it-takes-a-village-a-distributed-training-model-for-ai-based-chatbots-colleen-estes-et-al-2024>(1/1 | 112/128) It Takes a Village: A Distributed Training Model for AI-based Chatbots (Colleen Estes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Colleen Estes, Beth Twomey, Annie Johnson. (2024)<br><strong>It Takes a Village: A Distributed Training Model for AI-based Chatbots</strong><br><button class=copy-to-clipboard title="It Takes a Village: A Distributed Training Model for AI-based Chatbots" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-DL, cs.DL<br>Keyword Score: 20<br>Keywords: Chatbot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01545v1.pdf filename=2403.01545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Summer 2023, staff from the information technology and reference departments at the University of Delaware Library, Museums and Press came together in a unique partnership to pilot a low-cost AI-powered <b>chatbot.</b> The goal of the pilot is to learn more about student and faculty interest in engaging with this tool, and to better understand the labor required on the staff side. Reference librarians and other public facing staff, including student workers, were instrumental in helping to train the <b>chatbot.</b> This article discusses the development of <b>prompts,</b> leveraging of existing data sources for training materials, and workflows involved in the pilot. It argues that, when implementing AI-based tools in the academic library, involving staff from across the organization is essential to ensure buy-in and success. Although <b>chatbots</b> are designed to hide the effort of the people behind them, such labor can be substantial and needs to be recognized.</p></p class="citation"></blockquote><h2 id=cshc-3>cs.HC (3)</h2><h3 id=13--113128-a-preliminary-exploration-of-the-disruption-of-a-generative-ai-systems-facultystaff-and-student-perceptions-of-chatgpt-and-its-capability-of-completing-undergraduate-engineering-coursework-lance-white-et-al-2024>(1/3 | 113/128) A Preliminary Exploration of the Disruption of a Generative AI Systems: Faculty/Staff and Student Perceptions of ChatGPT and its Capability of Completing Undergraduate Engineering Coursework (Lance White et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lance White, Trini Balart, Sara Amani, Dr. Kristi J. Shryock, Dr. Karan L. Watson. (2024)<br><strong>A Preliminary Exploration of the Disruption of a Generative AI Systems: Faculty/Staff and Student Perceptions of ChatGPT and its Capability of Completing Undergraduate Engineering Coursework</strong><br><button class=copy-to-clipboard title="A Preliminary Exploration of the Disruption of a Generative AI Systems: Faculty/Staff and Student Perceptions of ChatGPT and its Capability of Completing Undergraduate Engineering Coursework" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Generative AI, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01538v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01538v1.pdf filename=2403.01538v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The authors of this study aim to assess the capabilities of the OpenAI <b>ChatGPT</b> tool to understand just how effective such a system might be for students to utilize in their studies as well as deepen understanding of faculty/staff and student perceptions about <b>ChatGPT</b> in general. The purpose of what is learned from the study is to continue the design of a model to facilitate the development of faculty for becoming adept at embracing change, the DANCE model (Designing Adaptations for the Next Changes in Education). This model is used in this study to help faculty with examining the impact that a disruptive new tool, such as <b>ChatGPT,</b> can pose for the learning environment. The authors analyzed the performance of <b>ChatGPT</b> used to complete course assignments at a variety of levels by novice engineering students working as research assistants. Those completed works have been assessed by the faculty who created those assignments to understand how these completed assignments might compare with the performance of a typical student. A set of surveys conducted by the authors of this work are discussed where students, faculty, and staff respondents in March of 2023 addressed their perceptions of <b>ChatGPT</b> (A follow-up survey is being administered now, February 2024). These survey instruments were analyzed, and the data visualized in this work to bring attention to relevant findings by the researchers. This work reports the findings of the researchers with the purpose of sharing the current state of this work at Texas A&amp;M University with the intention to provide insights to scholars both at our own institution and around the world. This work is not intended to be a finished work but reports these findings with full transparency that this work is currently continuing as the researchers gather new data and develop and validate various measurement instruments.</p></p class="citation"></blockquote><h3 id=23--114128-sard-a-human-ai-collaborative-story-generation-ahmed-y-radwan-et-al-2024>(2/3 | 114/128) SARD: A Human-AI Collaborative Story Generation (Ahmed Y. Radwan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmed Y. Radwan, Khaled M. Alasmari, Omar A. Abdulbagi, Emad A. Alghamdi. (2024)<br><strong>SARD: A Human-AI Collaborative Story Generation</strong><br><button class=copy-to-clipboard title="SARD: A Human-AI Collaborative Story Generation" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01575v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01575v1.pdf filename=2403.01575v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative artificial intelligence (GenAI) has ushered in a new era for storytellers, providing a powerful tool to ignite creativity and explore uncharted narrative territories. As technology continues to advance, the synergy between human creativity and AI-generated content holds the potential to redefine the landscape of storytelling. In this work, we propose SARD, a drag-and-drop visual interface for generating a multi-chapter story using <b>large</b> <b>language</b> <b>models.</b> Our evaluation of the usability of SARD and its creativity support shows that while node-based visualization of the narrative may help writers build a mental model, it exerts unnecessary mental overhead to the writer and becomes a source of distraction as the story becomes more elaborated. We also found that AI generates stories that are less lexically diverse, irrespective of the complexity of the story. We identified some patterns and limitations of our tool that can guide the development of future human-AI co-writing tools.</p></p class="citation"></blockquote><h3 id=33--115128-exploring-the-design-of-generative-ai-in-supporting-music-based-reminiscence-for-older-adults-yucheng-jin-et-al-2024>(3/3 | 115/128) Exploring the Design of Generative AI in Supporting Music-based Reminiscence for Older Adults (Yucheng Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yucheng Jin, Wanling Cai, Li Chen, Yizhe Zhang, Gavin Doherty, Tonglin Jiang. (2024)<br><strong>Exploring the Design of Generative AI in Supporting Music-based Reminiscence for Older Adults</strong><br><button class=copy-to-clipboard title="Exploring the Design of Generative AI in Supporting Music-based Reminiscence for Older Adults" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01413v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01413v1.pdf filename=2403.01413v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Music-based reminiscence has the potential to positively impact the psychological well-being of older adults. However, the aging process and physiological changes, such as memory decline and limited verbal communication, may impede the ability of older adults to recall their memories and life experiences. Given the advanced capabilities of <b>generative</b> <b>artificial</b> intelligence (AI) systems, such as generated conversations and images, and their potential to facilitate the reminiscing process, this study aims to explore the design of <b>generative</b> <b>AI</b> to support music-based reminiscence in older adults. This study follows a user-centered design approach incorporating various stages, including detailed interviews with two social workers and two design workshops (involving ten older adults). Our work contributes to an in-depth understanding of older adults&rsquo; attitudes toward utilizing <b>generative</b> <b>AI</b> for supporting music-based reminiscence and identifies concrete design considerations for the future design of <b>generative</b> <b>AI</b> to enhance the reminiscence experience of older adults.</p></p class="citation"></blockquote><h2 id=csit-4>cs.IT (4)</h2><h3 id=14--116128-deep-learning-based-design-of-uplink-integrated-sensing-and-communication-qiao-qi-et-al-2024>(1/4 | 116/128) Deep Learning-based Design of Uplink Integrated Sensing and Communication (Qiao Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiao Qi, Xiaoming Chen, Caijun Zhong, Chau Yuen, Zhaoyang Zhang. (2024)<br><strong>Deep Learning-based Design of Uplink Integrated Sensing and Communication</strong><br><button class=copy-to-clipboard title="Deep Learning-based Design of Uplink Integrated Sensing and Communication" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01480v1.pdf filename=2403.01480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the issue of uplink integrated sensing and communication (ISAC) in 6G wireless networks where the sensing echo signal and the communication signal are received simultaneously at the base station (BS). To effectively mitigate the mutual interference between sensing and communication caused by the sharing of spectrum and hardware resources, we provide a joint sensing transmit waveform and communication receive beamforming design with the objective of maximizing the weighted sum of normalized sensing rate and normalized communication rate. It is formulated as a computationally complicated non-convex optimization problem, which is quite difficult to be solved by conventional optimization methods. To this end, we first make a series of equivalent transformation on the optimization problem to reduce the design complexity, and then develop a deep learning (DL)-based scheme to enhance the overall performance of ISAC. Both theoretical analysis and <b>simulation</b> results confirm the effectiveness and robustness of the proposed DL-based scheme for ISAC in 6G wireless networks.</p></p class="citation"></blockquote><h3 id=24--117128-successful-transmission-probability-and-sir-meta-distribution-analysis-for-multi-antenna-cache-enabled-networks-with-interference-nulling-tianming-feng-et-al-2024>(2/4 | 117/128) Successful Transmission Probability and SIR Meta Distribution Analysis for Multi-Antenna Cache-Enabled Networks with Interference Nulling (Tianming Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianming Feng, Chenyu Wu, Xiaodong Zheng, Peilin Chen, Yilong Liu, Shuai Han. (2024)<br><strong>Successful Transmission Probability and SIR Meta Distribution Analysis for Multi-Antenna Cache-Enabled Networks with Interference Nulling</strong><br><button class=copy-to-clipboard title="Successful Transmission Probability and SIR Meta Distribution Analysis for Multi-Antenna Cache-Enabled Networks with Interference Nulling" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01394v1.pdf filename=2403.01394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates a multi-antenna cache-enabled network with interference nulling (IN) employed at base stations. Two IN schemes, namely, the fixed IN scheme and the flexible IN scheme are considered to improve the received signal-to-interference ratio (SIR) at users. To thoroughly explore the effects of the caching parameter and the IN parameters on the network performance, we focus on the analysis of not only the successful transmission probability (STP) but the SIR meta distribution. For each IN scheme, the expression for the STP is derived and an approximated expression for the SIR meta distribution is also obtained by deriving the first and second moments of an upper bound of the link reliability and utilizing the beta distribution. With this analytical framework, we compare the performance of these two IN schemes and gain some useful system design guidelines from the perspectives of the STP and the SIR meta distribution by numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=34--118128-ultimate-codes-ted-hurley-2024>(3/4 | 118/128) Ultimate codes (Ted Hurley, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ted Hurley. (2024)<br><strong>Ultimate codes</strong><br><button class=copy-to-clipboard title="Ultimate codes" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT, math-RA<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01491v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01491v2.pdf filename=2403.01491v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A linear block code over a field can be derived from a unit scheme. Looking at codes as structures within a unit scheme greatly extends the availability of linear block and <b>convolutional</b> codes and allows the construction of the codes to required length, rate, distance and type. Properties of a code emanate from properties of the unit from which it was derived. %% can thus be constructed and analysed by designating the units whose properties would give the required codes. Orthogonal units, units in group rings, Fourier/Vandermonde units and related units are used to construct and analyse linear block and <b>convolutional</b> codes and to construct these to predefined length, rate, distance and type. Self-dual, dual containing, quantum error-correcting and complementary dual linear block and <b>convolutional</b> codes are constructed. Low density parity check linear block and <b>convolutional</b> codes are constructed using group rings and are constructed with no short cycles in the control matrix. From a single unit, multiple codes of a required type are derivable.</p></p class="citation"></blockquote><h3 id=44--119128-maximum-length-rll-sequences-in-de-bruijn-graph-yeow-meng-chee-et-al-2024>(4/4 | 119/128) Maximum Length RLL Sequences in de Bruijn Graph (Yeow Meng Chee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeow Meng Chee, Tuvi Etzion, Tien Long Nguyen, Duy Hoang Ta, Vinh Duc Tran, Van Khu Vu. (2024)<br><strong>Maximum Length RLL Sequences in de Bruijn Graph</strong><br><button class=copy-to-clipboard title="Maximum Length RLL Sequences in de Bruijn Graph" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01454v1.pdf filename=2403.01454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A timing and synchronization system based on a de Bruijn sequence has been proposed and studied recently for a channel associated with quantum communication that requires reliable synchronization. To avoid a long period of no-pulse in such a system on-off pulses are used to simulate a zero and on-on pulses are used to simulate a one. However, these sequences have high redundancy. To reduce the redundancy, run-length limited sequences in the de Bruijn <b>graph</b> are proposed for the same purpose. The maximum length of such sequences in the de Bruijn <b>graph</b> is studied and an efficient algorithm to construct a large set of these sequences is presented. A maximum length sequence for which the position of each window can be computed efficiently is constructed. Finally, an enumeration of the number of such sequences is given and some generalizations are discussed.</p></p class="citation"></blockquote><h2 id=eessiv-3>eess.IV (3)</h2><h3 id=13--120128-enhancing-retinal-vascular-structure-segmentation-in-images-with-a-novel-design-two-path-interactive-fusion-module-model-rui-yang-et-al-2024>(1/3 | 120/128) Enhancing Retinal Vascular Structure Segmentation in Images With a Novel Design Two-Path Interactive Fusion Module Model (Rui Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Yang, Shunpu Zhang. (2024)<br><strong>Enhancing Retinal Vascular Structure Segmentation in Images With a Novel Design Two-Path Interactive Fusion Module Model</strong><br><button class=copy-to-clipboard title="Enhancing Retinal Vascular Structure Segmentation in Images With a Novel Design Two-Path Interactive Fusion Module Model" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01362v1.pdf filename=2403.01362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Precision in identifying and differentiating micro and macro blood vessels in the retina is crucial for the diagnosis of retinal diseases, although it poses a significant challenge. Current autoencoding-based segmentation approaches encounter limitations as they are constrained by the encoder and undergo a reduction in resolution during the encoding stage. The inability to recover lost information in the decoding phase further impedes these approaches. Consequently, their capacity to extract the retinal microvascular structure is restricted. To address this issue, we introduce Swin-Res-Net, a specialized module designed to enhance the precision of retinal vessel segmentation. Swin-Res-Net utilizes the Swin <b>transformer</b> which uses shifted windows with displacement for partitioning, to reduce network complexity and accelerate model convergence. Additionally, the model incorporates interactive fusion with a functional module in the Res2Net architecture. The Res2Net leverages multi-scale techniques to enlarge the receptive field of the <b>convolutional</b> kernel, enabling the extraction of additional semantic information from the image. This combination creates a new module that enhances the localization and separation of micro vessels in the retina. To improve the efficiency of processing vascular information, we&rsquo;ve added a module to eliminate redundant information between the encoding and decoding steps. Our proposed architecture produces outstanding results, either meeting or surpassing those of other published models. The AUC reflects significant enhancements, achieving values of 0.9956, 0.9931, and 0.9946 in pixel-wise segmentation of retinal vessels across three widely utilized datasets: CHASE-DB1, DRIVE, and STARE, respectively. Moreover, Swin-Res-Net outperforms alternative architectures, demonstrating superior performance in both IOU and F1 measure metrics.</p></p class="citation"></blockquote><h3 id=23--121128-cdse-unet-enhancing-covid-19-ct-image-segmentation-with-canny-edge-detection-and-dual-path-senet-feature-fusion-jiao-ding-et-al-2024>(2/3 | 121/128) CDSE-UNet: Enhancing COVID-19 CT Image Segmentation with Canny Edge Detection and Dual-Path SENet Feature Fusion (Jiao Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiao Ding, Jie Chang, Renrui Han, Li Yang. (2024)<br><strong>CDSE-UNet: Enhancing COVID-19 CT Image Segmentation with Canny Edge Detection and Dual-Path SENet Feature Fusion</strong><br><button class=copy-to-clipboard title="CDSE-UNet: Enhancing COVID-19 CT Image Segmentation with Canny Edge Detection and Dual-Path SENet Feature Fusion" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01513v1.pdf filename=2403.01513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate segmentation of COVID-19 CT images is crucial for reducing the severity and mortality rates associated with COVID-19 infections. In response to blurred boundaries and high variability characteristic of lesion areas in COVID-19 CT images, we introduce CDSE-UNet: a novel UNet-based segmentation model that integrates Canny operator edge detection and a dual-path SENet feature fusion mechanism. This model enhances the standard UNet architecture by employing the Canny operator for edge detection in sample images, paralleling this with a similar network structure for semantic feature extraction. A key innovation is the Double SENet Feature Fusion Block, applied across corresponding network layers to effectively combine features from both image paths. Moreover, we have developed a Multiscale <b>Convolution</b> approach, replacing the standard <b>Convolution</b> in UNet, to adapt to the varied lesion sizes and shapes. This addition not only aids in accurately classifying lesion edge pixels but also significantly improves channel differentiation and expands the capacity of the model. Our evaluations on public datasets demonstrate CDSE-UNet&rsquo;s superior performance over other leading models, particularly in segmenting large and small lesion areas, accurately delineating lesion edges, and effectively suppressing noise</p></p class="citation"></blockquote><h3 id=33--122128-apisr-anime-production-inspired-real-world-anime-super-resolution-boyang-wang-et-al-2024>(3/3 | 122/128) APISR: Anime Production Inspired Real-World Anime Super-Resolution (Boyang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyang Wang, Fengyu Yang, Xihang Yu, Chao Zhang, Hanbin Zhao. (2024)<br><strong>APISR: Anime Production Inspired Real-World Anime Super-Resolution</strong><br><button class=copy-to-clipboard title="APISR: Anime Production Inspired Real-World Anime Super-Resolution" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, eess-IV, eess.IV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01598v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01598v1.pdf filename=2403.01598v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While real-world anime super-resolution (SR) has gained increasing attention in the SR community, existing methods still adopt techniques from the photorealistic domain. In this paper, we analyze the anime production workflow and rethink how to use characteristics of it for the sake of the real-world anime SR. First, we argue that video networks and datasets are not necessary for anime SR due to the repetition use of hand-drawing frames. Instead, we propose an anime image collection pipeline by choosing the least compressed and the most informative frames from the video sources. Based on this pipeline, we introduce the Anime Production-oriented Image (API) dataset. In addition, we identify two anime-specific challenges of distorted and faint hand-drawn lines and unwanted color artifacts. We address the first issue by introducing a prediction-oriented compression module in the image degradation model and a pseudo-ground truth preparation with enhanced hand-drawn lines. In addition, we introduce the balanced twin perceptual loss combining both anime and photorealistic high-level features to mitigate unwanted color artifacts and increase visual clarity. We evaluate our method through extensive experiments on the public <b>benchmark,</b> showing our method outperforms state-of-the-art approaches by a large margin.</p></p class="citation"></blockquote><h2 id=csse-1>cs.SE (1)</h2><h3 id=11--123128-modelwriter-text--model-synchronized-document-engineering-platform-ferhat-erata-et-al-2024>(1/1 | 123/128) ModelWriter: Text & Model-Synchronized Document Engineering Platform (Ferhat Erata et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ferhat Erata, Claire Gardent, Bikash Gyawali, Anastasia Shimorina, Yvan Lussaud, Bedir Tekinerdogan, Geylani Kardas, Anne Monceaux. (2024)<br><strong>ModelWriter: Text & Model-Synchronized Document Engineering Platform</strong><br><button class=copy-to-clipboard title="ModelWriter: Text & Model-Synchronized Document Engineering Platform" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Reasoning, Semantic Parsing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01359v1.pdf filename=2403.01359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ModelWriter platform provides a generic framework for automated traceability analysis. In this paper, we demonstrate how this framework can be used to trace the consistency and completeness of technical documents that consist of a set of System Installation Design Principles used by Airbus to ensure the correctness of aircraft system installation. We show in particular, how the platform allows the integration of two types of <b>reasoning:</b> <b>reasoning</b> about the meaning of text using <b>semantic</b> <b>parsing</b> and description logic theorem proving; and <b>reasoning</b> about document structure using first-order relational logic and finite model finding for traceability analysis.</p></p class="citation"></blockquote><h2 id=csai-1>cs.AI (1)</h2><h3 id=11--124128-soft-reasoning-on-uncertain-knowledge-graphs-weizhi-fei-et-al-2024>(1/1 | 124/128) Soft Reasoning on Uncertain Knowledge Graphs (Weizhi Fei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weizhi Fei, Zihao Wang, Hang Yin, Yang Duan, Hanghang Tong, Yangqiu Song. (2024)<br><strong>Soft Reasoning on Uncertain Knowledge Graphs</strong><br><button class=copy-to-clipboard title="Soft Reasoning on Uncertain Knowledge Graphs" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 18<br>Keywords: Graph, Knowledge Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01508v1.pdf filename=2403.01508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study of machine learning-based logical query-answering enables <b>reasoning</b> with large-scale and incomplete <b>knowledge</b> <b>graphs.</b> This paper further advances this line of research by considering the uncertainty in the <b>knowledge.</b> <b>The</b> uncertain nature of <b>knowledge</b> <b>is</b> widely observed in the real world, but \textit{does not} align seamlessly with the first-order logic underpinning existing studies. To bridge this gap, we study the setting of soft queries on uncertain <b>knowledge,</b> <b>which</b> is motivated by the establishment of soft constraint programming. We further propose an ML-based approach with both forward inference and backward calibration to answer soft queries on large-scale, incomplete, and uncertain <b>knowledge</b> <b>graphs.</b> Theoretical discussions present that our methods share the same complexity as state-of-the-art inference algorithms for first-order queries. Empirical results justify the superior performance of our approach against previous ML-based methods with number embedding extensions.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--125128-measuring-technological-convergence-in-encryption-technologies-with-proximity-indices-a-text-mining-and-bibliometric-analysis-using-openalex-alessandro-tavazzi-et-al-2024>(1/1 | 125/128) Measuring Technological Convergence in Encryption Technologies with Proximity Indices: A Text Mining and Bibliometric Analysis using OpenAlex (Alessandro Tavazzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Tavazzi, Dimitri Percia David, Julian Jang-Jaccard, Alain Mermoud. (2024)<br><strong>Measuring Technological Convergence in Encryption Technologies with Proximity Indices: A Text Mining and Bibliometric Analysis using OpenAlex</strong><br><button class=copy-to-clipboard title="Measuring Technological Convergence in Encryption Technologies with Proximity Indices: A Text Mining and Bibliometric Analysis using OpenAlex" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CR, cs-CY, cs-IR, cs.CY<br>Keyword Score: 10<br>Keywords: Text Mining<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01601v1.pdf filename=2403.01601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying technological convergence among emerging technologies in cybersecurity is crucial for advancing science and fostering innovation. Unlike previous studies focusing on the binary relationship between a paper and the concept it attributes to technology, our approach utilizes attribution scores to enhance the relationships between research papers, combining keywords, citation rates, and collaboration status with specific technological concepts. The proposed method integrates <b>text</b> <b>mining</b> and bibliometric analyses to formulate and predict technological proximity indices for encryption technologies using the &ldquo;OpenAlex&rdquo; catalog. Our case study findings highlight a significant convergence between blockchain and public-key cryptography, evidenced by the increasing proximity indices. These results offer valuable strategic insights for those contemplating investments in these domains.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--126128-efficient-fir-filtering-with-bit-layer-multiply-accumulator-vincenzo-liguori-2024>(1/1 | 126/128) Efficient FIR filtering with Bit Layer Multiply Accumulator (Vincenzo Liguori, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincenzo Liguori. (2024)<br><strong>Efficient FIR filtering with Bit Layer Multiply Accumulator</strong><br><button class=copy-to-clipboard title="Efficient FIR filtering with Bit Layer Multiply Accumulator" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-ET, cs.AR<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01351v1.pdf filename=2403.01351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bit Layer Multiplier Accumulator (BLMAC) is an efficient method to perform dot products without multiplications that exploits the bit level sparsity of the weights. A total of 1,980,000 low, high, band pass and band stop type I FIR filters were generated by systematically sweeping through the cut off frequencies and by varying the number of taps from 55 to 255. After their coefficients were <b>quantized</b> to 16 bits, applying the filter using a BLMAC required, on average, from ~123.3 to ~513.6 additions, depending on the number of taps. A BLMAC dot product machine, specialised for 127 taps FIR filters, was designed for AMD FPGAs. The design footprint is ~110 LUTs, including coefficient and sample storage and is able to apply the filter in ~232 clock cycles on average. This implies a filtering rate of 1.4-3.4 Msamples/s, depending on the FPGA family.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=11--127128-approximations-and-hardness-of-packing-partially-ordered-items-ilan-doron-arad-et-al-2024>(1/1 | 127/128) Approximations and Hardness of Packing Partially Ordered Items (Ilan Doron-Arad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ilan Doron-Arad, Guy Kortsarz, Joseph Naor, Baruch Schieber, Hadas Shachnai. (2024)<br><strong>Approximations and Hardness of Packing Partially Ordered Items</strong><br><button class=copy-to-clipboard title="Approximations and Hardness of Packing Partially Ordered Items" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01568v1.pdf filename=2403.01568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by applications in production planning and storage allocation in hierarchical databases, we initiate the study of covering partially ordered items (CPO). Given a capacity $k \in \mathbb{Z}^+$, and a directed <b>graph</b> $G=(V,E)$ where each vertex has a size in ${0,1, \ldots,k}$, we seek a collection of subsets of vertices $S_1, \ldots, S_m$ that cover all the vertices, such that for any $1 \leq j \leq m$, the total size of vertices in $S_j$ is bounded by $k$, and there are no edges from $V \setminus S_j$ to $S_j$. The objective is to minimize the number of subsets $m$. CPO is closely related to the rule caching problem (RCP) that is of wide interest in the networking area. The input for RCP is a directed <b>graph</b> $G=(V,E)$, a profit function $p:V \rightarrow \mathbb{Z}_{0}^+$, and $k \in \mathbb{Z}^+$. The output is a subset $S \subseteq V$ of maximum profit such that $|S| \leq k$ and there are no edges from $V \setminus S$ to $S$. Our main result is a $2$-approximation algorithm for CPO on out-trees, complemented by an asymptotic $1.5$-hardness of approximation result. We also give a two-way reduction between RCP and the densest $k$-subhypergraph problem, surprisingly showing that the problems are equivalent w.r.t. polynomial-time approximation within any factor $\rho \geq 1$. This implies that RCP cannot be approximated within factor $|V|^{1-\eps}$ for any fixed $\eps>0$, under standard complexity assumptions. Prior to this work, RCP was just known to be strongly NP-hard. We further show that there is no EPTAS for the special case of RCP where the profits are uniform, assuming Gap-ETH. Since this variant admits a PTAS, we essentially resolve the complexity status of this problem.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--128128-spectral-antisymmetry-of-twisted-graph-adjacency-ye-luo-et-al-2024>(1/1 | 128/128) Spectral antisymmetry of twisted graph adjacency (Ye Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Luo, Arindam Roy. (2024)<br><strong>Spectral antisymmetry of twisted graph adjacency</strong><br><button class=copy-to-clipboard title="Spectral antisymmetry of twisted graph adjacency" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C50, 05C38, 11M41, cs-DM, math-CO, math-NT, math-SP, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01550v1.pdf filename=2403.01550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address a prime counting problem across the homology classes of a <b>graph,</b> presenting a <b>graph-theoretical</b> Dirichlet-type analogue of the prime number theorem. The main machinery we have developed and employed is a spectral antisymmetry theorem, revealing that the spectra of the twisted <b>graph</b> adjacency matrices have an antisymmetric distribution over the character group of the <b>graph.</b> Additionally, we derive some trace formulas based on the twisted adjacency matrices as part of our analysis.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.04</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.06</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscv-29>cs.CV (29)</a><ul><li><a href=#129--1128-hyperspectral-image-analysis-in-single-modal-and-multimodal-setting-using-deep-learning-techniques-shivam-pande-2024>(1/29 | 1/128) Hyperspectral Image Analysis in Single-Modal and Multimodal setting using Deep Learning Techniques (Shivam Pande, 2024)</a></li><li><a href=#229--2128-self-supervised-representation-learning-with-meta-comprehensive-regularization-huijie-guo-et-al-2024>(2/29 | 2/128) Self-Supervised Representation Learning with Meta Comprehensive Regularization (Huijie Guo et al., 2024)</a></li><li><a href=#329--3128-eagle-eigen-aggregation-learning-for-object-centric-unsupervised-semantic-segmentation-chanyoung-kim-et-al-2024>(3/29 | 3/128) EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation (Chanyoung Kim et al., 2024)</a></li><li><a href=#429--4128-schema-state-changes-matter-for-procedure-planning-in-instructional-videos-yulei-niu-et-al-2024>(4/29 | 4/128) SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos (Yulei Niu et al., 2024)</a></li><li><a href=#529--5128-kick-back--relax-scaling-beyond-ground-truth-depth-with-slowtv--cribstv-jaime-spencer-et-al-2024>(5/29 | 5/128) Kick Back & Relax++: Scaling Beyond Ground-Truth Depth with SlowTV & CribsTV (Jaime Spencer et al., 2024)</a></li><li><a href=#629--6128-guardt2i-defending-text-to-image-models-from-adversarial-prompts-yijun-yang-et-al-2024>(6/29 | 6/128) GuardT2I: Defending Text-to-Image Models from Adversarial Prompts (Yijun Yang et al., 2024)</a></li><li><a href=#729--7128-scott-accelerating-diffusion-models-with-stochastic-consistency-distillation-hongjian-liu-et-al-2024>(7/29 | 7/128) SCott: Accelerating Diffusion Models with Stochastic Consistency Distillation (Hongjian Liu et al., 2024)</a></li><li><a href=#829--8128-is-in-domain-data-beneficial-in-transfer-learning-for-landmarks-detection-in-x-ray-images-roberto-di-via-et-al-2024>(8/29 | 8/128) Is in-domain data beneficial in transfer learning for landmarks detection in x-ray images? (Roberto Di Via et al., 2024)</a></li><li><a href=#929--9128-image2sentence-based-asymmetrical-zero-shot-composed-image-retrieval-yongchao-du-et-al-2024>(9/29 | 9/128) Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval (Yongchao Du et al., 2024)</a></li><li><a href=#1029--10128-lum-vit-learnable-under-sampling-mask-vision-transformer-for-bandwidth-limited-optical-signal-acquisition-lingfeng-liu-et-al-2024>(10/29 | 10/128) LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition (Lingfeng Liu et al., 2024)</a></li><li><a href=#1129--11128-multiview-subspace-clustering-of-hyperspectral-images-based-on-graph-convolutional-networks-xianju-li-et-al-2024>(11/29 | 11/128) Multiview Subspace Clustering of Hyperspectral Images based on Graph Convolutional Networks (Xianju Li et al., 2024)</a></li><li><a href=#1229--12128-moviellm-enhancing-long-video-understanding-with-ai-generated-movies-zhende-song-et-al-2024>(12/29 | 12/128) MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies (Zhende Song et al., 2024)</a></li><li><a href=#1329--13128-dynamic-adapter-meets-prompt-tuning-parameter-efficient-transfer-learning-for-point-cloud-analysis-xin-zhou-et-al-2024>(13/29 | 13/128) Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis (Xin Zhou et al., 2024)</a></li><li><a href=#1429--14128-gptsee-enhancing-moment-retrieval-and-highlight-detection-via-description-based-similarity-features-yunzhuo-sun-et-al-2024>(14/29 | 14/128) GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features (Yunzhuo Sun et al., 2024)</a></li><li><a href=#1529--15128-region-transformer-self-attention-region-based-class-agnostic-point-cloud-segmentation-dipesh-gyawali-et-al-2024>(15/29 | 15/128) Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation (Dipesh Gyawali et al., 2024)</a></li><li><a href=#1629--16128-learning-a-physical-aware-diffusion-model-based-on-transformer-for-underwater-image-enhancement-chen-zhao-et-al-2024>(16/29 | 16/128) Learning A Physical-aware Diffusion Model Based on Transformer for Underwater Image Enhancement (Chen Zhao et al., 2024)</a></li><li><a href=#1729--17128-regeneration-based-training-free-attribution-of-fake-images-generated-by-text-to-image-generative-models-meiling-li-et-al-2024>(17/29 | 17/128) Regeneration Based Training-free Attribution of Fake Images Generated by Text-to-Image Generative Models (Meiling Li et al., 2024)</a></li><li><a href=#1829--18128-pyramid-feature-attention-network-for-monocular-depth-prediction-yifang-xu-et-al-2024>(18/29 | 18/128) Pyramid Feature Attention Network for Monocular Depth Prediction (Yifang Xu et al., 2024)</a></li><li><a href=#1929--19128-logit-standardization-in-knowledge-distillation-shangquan-sun-et-al-2024>(19/29 | 19/128) Logit Standardization in Knowledge Distillation (Shangquan Sun et al., 2024)</a></li><li><a href=#2029--20128-a-simple-but-effective-baseline-for-training-free-class-agnostic-counting-yuhao-lin-et-al-2024>(20/29 | 20/128) A Simple-but-effective Baseline for Training-free Class-Agnostic Counting (Yuhao Lin et al., 2024)</a></li><li><a href=#2129--21128-sa-mixnet-structure-aware-mixup-and-invariance-learning-for-scribble-supervised-road-extraction-in-remote-sensing-images-jie-feng-et-al-2024>(21/29 | 21/128) SA-MixNet: Structure-aware Mixup and Invariance Learning for Scribble-supervised Road Extraction in Remote Sensing Images (Jie Feng et al., 2024)</a></li><li><a href=#2229--22128-infimm-hd-a-leap-forward-in-high-resolution-multimodal-understanding-haogeng-liu-et-al-2024>(22/29 | 22/128) InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding (Haogeng Liu et al., 2024)</a></li><li><a href=#2329--23128-efficient-action-counting-with-dynamic-queries-zishi-li-et-al-2024>(23/29 | 23/128) Efficient Action Counting with Dynamic Queries (Zishi Li et al., 2024)</a></li><li><a href=#2429--24128-aio2-online-correction-of-object-labels-for-deep-learning-with-incomplete-annotation-in-remote-sensing-image-segmentation-chenying-liu-et-al-2024>(24/29 | 24/128) AIO2: Online Correction of Object Labels for Deep Learning with Incomplete Annotation in Remote Sensing Image Segmentation (Chenying Liu et al., 2024)</a></li><li><a href=#2529--25128-depth-estimation-algorithm-based-on-transformer-encoder-and-feature-fusion-linhan-xia-et-al-2024>(25/29 | 25/128) Depth Estimation Algorithm Based on Transformer-Encoder and Feature Fusion (Linhan Xia et al., 2024)</a></li><li><a href=#2629--26128-matchu-matching-unseen-objects-for-6d-pose-estimation-from-rgb-d-images-junwen-huang-et-al-2024>(26/29 | 26/128) MatchU: Matching Unseen Objects for 6D Pose Estimation from RGB-D Images (Junwen Huang et al., 2024)</a></li><li><a href=#2729--27128-occfusion-a-straightforward-and-effective-multi-sensor-fusion-framework-for-3d-occupancy-prediction-zhenxing-ming-et-al-2024>(27/29 | 27/128) OccFusion: A Straightforward and Effective Multi-Sensor Fusion Framework for 3D Occupancy Prediction (Zhenxing Ming et al., 2024)</a></li><li><a href=#2829--28128-a-unified-model-selection-technique-for-spectral-clustering-based-motion-segmentation-yuxiang-huang-et-al-2024>(28/29 | 28/128) A Unified Model Selection Technique for Spectral Clustering Based Motion Segmentation (Yuxiang Huang et al., 2024)</a></li><li><a href=#2929--29128-rethinking-clip-based-video-learners-in-cross-domain-open-vocabulary-action-recognition-kun-yu-lin-et-al-2024>(29/29 | 29/128) Rethinking CLIP-based Video Learners in Cross-Domain Open-Vocabulary Action Recognition (Kun-Yu Lin et al., 2024)</a></li></ul></li><li><a href=#cscl-21>cs.CL (21)</a><ul><li><a href=#121--30128-enhancing-neural-machine-translation-of-low-resource-languages-corpus-development-human-evaluation-and-explainable-ai-architectures-séamus-lankford-2024>(1/21 | 30/128) Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures (Séamus Lankford, 2024)</a></li><li><a href=#221--31128-align-to-distill-trainable-attention-alignment-for-knowledge-distillation-in-neural-machine-translation-heegon-jin-et-al-2024>(2/21 | 31/128) Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation (Heegon Jin et al., 2024)</a></li><li><a href=#321--32128-serval-synergy-learning-between-vertical-models-and-llms-towards-oracle-level-zero-shot-medical-prediction-jiahuan-yan-et-al-2024>(3/21 | 32/128) SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction (Jiahuan Yan et al., 2024)</a></li><li><a href=#421--33128-fine-tuning-vs-retrieval-augmented-generation-for-less-popular-knowledge-heydar-soudani-et-al-2024>(4/21 | 33/128) Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge (Heydar Soudani et al., 2024)</a></li><li><a href=#521--34128-right-for-right-reasons-large-language-models-for-verifiable-commonsense-knowledge-graph-question-answering-armin-toroghi-et-al-2024>(5/21 | 34/128) Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering (Armin Toroghi et al., 2024)</a></li><li><a href=#621--35128-cr-lt-kgqa-a-knowledge-graph-question-answering-dataset-requiring-commonsense-reasoning-and-long-tail-knowledge-willis-guo-et-al-2024>(6/21 | 35/128) CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring Commonsense Reasoning and Long-Tail Knowledge (Willis Guo et al., 2024)</a></li><li><a href=#721--36128-automatic-question-answer-generation-for-long-tail-knowledge-rohan-kumar-et-al-2024>(7/21 | 36/128) Automatic Question-Answer Generation for Long-Tail Knowledge (Rohan Kumar et al., 2024)</a></li><li><a href=#821--37128-multi-level-product-category-prediction-through-text-classification-wesley-ferreira-maia-et-al-2024>(8/21 | 37/128) Multi-level Product Category Prediction through Text Classification (Wesley Ferreira Maia et al., 2024)</a></li><li><a href=#921--38128-towards-comprehensive-vietnamese-retrieval-augmented-generation-and-large-language-models-nguyen-quang-duc-et-al-2024>(9/21 | 38/128) Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models (Nguyen Quang Duc et al., 2024)</a></li><li><a href=#1021--39128-infusing-knowledge-into-large-language-models-with-contextual-prompts-kinshuk-vasisht-et-al-2024>(10/21 | 39/128) Infusing Knowledge into Large Language Models with Contextual Prompts (Kinshuk Vasisht et al., 2024)</a></li><li><a href=#1121--40128-revisiting-dynamic-evaluation-online-adaptation-for-large-language-models-amal-rannen-triki-et-al-2024>(11/21 | 40/128) Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models (Amal Rannen-Triki et al., 2024)</a></li><li><a href=#1221--41128-fantastic-semantics-and-where-to-find-them-investigating-which-layers-of-generative-llms-reflect-lexical-semantics-zhu-liu-et-al-2024>(12/21 | 41/128) Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics (Zhu Liu et al., 2024)</a></li><li><a href=#1321--42128-what-is-missing-in-multilingual-visual-reasoning-and-how-to-fix-it-yueqi-song-et-al-2024>(13/21 | 42/128) What Is Missing in Multilingual Visual Reasoning and How to Fix It (Yueqi Song et al., 2024)</a></li><li><a href=#1421--43128-in-context-sharpness-as-alerts-an-inner-representation-perspective-for-hallucination-mitigation-shiqi-chen-et-al-2024>(14/21 | 43/128) In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation (Shiqi Chen et al., 2024)</a></li><li><a href=#1521--44128-answerability-in-retrieval-augmented-open-domain-question-answering-rustam-abdumalikov-et-al-2024>(15/21 | 44/128) Answerability in Retrieval-Augmented Open-Domain Question Answering (Rustam Abdumalikov et al., 2024)</a></li><li><a href=#1621--45128-improving-cross-lingual-representation-for-semantic-retrieval-with-code-switching-mieradilijiang-maimaiti-et-al-2024>(16/21 | 45/128) Improving Cross-lingual Representation for Semantic Retrieval with Code-switching (Mieradilijiang Maimaiti et al., 2024)</a></li><li><a href=#1721--46128-kormedmcqa-multi-choice-question-answering-benchmark-for-korean-healthcare-professional-licensing-examinations-sunjun-kweon-et-al-2024>(17/21 | 46/128) KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations (Sunjun Kweon et al., 2024)</a></li><li><a href=#1821--47128-controlling-cloze-test-question-item-difficulty-with-plm-based-surrogate-models-for-irt-assessment-jingshen-zhang-et-al-2024>(18/21 | 47/128) Controlling Cloze-test Question Item Difficulty with PLM-based Surrogate Models for IRT Assessment (Jingshen Zhang et al., 2024)</a></li><li><a href=#1921--48128-ovel-large-language-model-as-memory-manager-for-online-video-entity-linking-haiquan-zhao-et-al-2024>(19/21 | 48/128) OVEL: Large Language Model as Memory Manager for Online Video Entity Linking (Haiquan Zhao et al., 2024)</a></li><li><a href=#2021--49128-evaluating-and-mitigating-number-hallucinations-in-large-vision-language-models-a-consistency-perspective-huixuan-zhang-et-al-2024>(20/21 | 49/128) Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective (Huixuan Zhang et al., 2024)</a></li><li><a href=#2121--50128-leveraging-biomolecule-and-natural-language-through-multi-modal-learning-a-survey-qizhi-pei-et-al-2024>(21/21 | 50/128) Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey (Qizhi Pei et al., 2024)</a></li></ul></li><li><a href=#cslg-30>cs.LG (30)</a><ul><li><a href=#130--51128-transformers-for-supervised-online-continual-learning-jorg-bornschein-et-al-2024>(1/30 | 51/128) Transformers for Supervised Online Continual Learning (Jorg Bornschein et al., 2024)</a></li><li><a href=#230--52128-applying-self-supervised-learning-to-network-intrusion-detection-for-network-flows-with-graph-neural-network-renjie-xu-et-al-2024>(2/30 | 52/128) Applying Self-supervised Learning to Network Intrusion Detection for Network Flows with Graph Neural Network (Renjie Xu et al., 2024)</a></li><li><a href=#330--53128-representation-learning-on-heterophilic-graph-with-directional-neighborhood-attention-qincheng-lu-et-al-2024>(3/30 | 53/128) Representation Learning on Heterophilic Graph with Directional Neighborhood Attention (Qincheng Lu et al., 2024)</a></li><li><a href=#430--54128-improving-uncertainty-sampling-with-bell-curve-weight-function-zan-kai-chong-et-al-2024>(4/30 | 54/128) Improving Uncertainty Sampling with Bell Curve Weight Function (Zan-Kai Chong et al., 2024)</a></li><li><a href=#530--55128-on-the-compressibility-of-quantized-large-language-models-yu-mao-et-al-2024>(5/30 | 55/128) On the Compressibility of Quantized Large Language Models (Yu Mao et al., 2024)</a></li><li><a href=#630--56128-partial-federated-learning-tiantian-feng-et-al-2024>(6/30 | 56/128) Partial Federated Learning (Tiantian Feng et al., 2024)</a></li><li><a href=#730--57128-collaborate-to-adapt-source-free-graph-domain-adaptation-via-bi-directional-adaptation-zhen-zhang-et-al-2024>(7/30 | 57/128) Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation (Zhen Zhang et al., 2024)</a></li><li><a href=#830--58128-improving-llm-code-generation-with-grammar-augmentation-shubham-ugare-et-al-2024>(8/30 | 58/128) Improving LLM Code Generation with Grammar Augmentation (Shubham Ugare et al., 2024)</a></li><li><a href=#930--59128-ml4physim--machine-learning-for-physical-simulations-challenge-the-airfoil-design-mouadh-yagoubi-et-al-2024>(9/30 | 59/128) ML4PhySim : Machine Learning for Physical Simulations Challenge (The airfoil design) (Mouadh Yagoubi et al., 2024)</a></li><li><a href=#1030--60128-quantized-hierarchical-federated-learning-a-robust-approach-to-statistical-heterogeneity-seyed-mohammad-azimi-abarghouyi-et-al-2024>(10/30 | 60/128) Quantized Hierarchical Federated Learning: A Robust Approach to Statistical Heterogeneity (Seyed Mohammad Azimi-Abarghouyi et al., 2024)</a></li><li><a href=#1130--61128-convtimenet-a-deep-hierarchical-fully-convolutional-model-for-multivariate-time-series-analysis-mingyue-cheng-et-al-2024>(11/30 | 61/128) ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for Multivariate Time Series Analysis (Mingyue Cheng et al., 2024)</a></li><li><a href=#1230--62128-the-implicit-bias-of-heterogeneity-towards-invariance-and-causality-yang-xu-et-al-2024>(12/30 | 62/128) The Implicit Bias of Heterogeneity towards Invariance and Causality (Yang Xu et al., 2024)</a></li><li><a href=#1330--63128-a-comprehensive-survey-of-federated-transfer-learning-challenges-methods-and-applications-wei-guo-et-al-2024>(13/30 | 63/128) A Comprehensive Survey of Federated Transfer Learning: Challenges, Methods and Applications (Wei Guo et al., 2024)</a></li><li><a href=#1430--64128-neural-graph-generator-feature-conditioned-graph-generation-using-latent-diffusion-models-iakovos-evdaimon-et-al-2024>(14/30 | 64/128) Neural Graph Generator: Feature-Conditioned Graph Generation using Latent Diffusion Models (Iakovos Evdaimon et al., 2024)</a></li><li><a href=#1530--65128-you-need-to-pay-better-attention-mehran-hosseini-et-al-2024>(15/30 | 65/128) You Need to Pay Better Attention (Mehran Hosseini et al., 2024)</a></li><li><a href=#1630--66128-blue-and-green-mode-energy-efficient-chemiresistive-sensor-array-realized-by-rapid-ensemble-learning-zeheng-wang-et-al-2024>(16/30 | 66/128) Blue and Green-Mode Energy-Efficient Chemiresistive Sensor Array Realized by Rapid Ensemble Learning (Zeheng Wang et al., 2024)</a></li><li><a href=#1730--67128-theoretical-insights-for-diffusion-guidance-a-case-study-for-gaussian-mixture-models-yuchen-wu-et-al-2024>(17/30 | 67/128) Theoretical Insights for Diffusion Guidance: A Case Study for Gaussian Mixture Models (Yuchen Wu et al., 2024)</a></li><li><a href=#1830--68128-critical-windows-non-asymptotic-theory-for-feature-emergence-in-diffusion-models-marvin-li-et-al-2024>(18/30 | 68/128) Critical windows: non-asymptotic theory for feature emergence in diffusion models (Marvin Li et al., 2024)</a></li><li><a href=#1930--69128-respiratory-motion-forecasting-with-online-learning-of-recurrent-neural-networks-for-safety-enhancement-in-externally-guided-radiotherapy-michel-pohl-et-al-2024>(19/30 | 69/128) Respiratory motion forecasting with online learning of recurrent neural networks for safety enhancement in externally guided radiotherapy (Michel Pohl et al., 2024)</a></li><li><a href=#2030--70128-towards-provable-log-density-policy-gradient-pulkit-katdare-et-al-2024>(20/30 | 70/128) Towards Provable Log Density Policy Gradient (Pulkit Katdare et al., 2024)</a></li><li><a href=#2130--71128-the-hidden-attention-of-mamba-models-ameen-ali-et-al-2024>(21/30 | 71/128) The Hidden Attention of Mamba Models (Ameen Ali et al., 2024)</a></li><li><a href=#2230--72128-on-the-model-agnostic-multi-source-free-unsupervised-domain-adaptation-jiangbo-pei-et-al-2024>(22/30 | 72/128) On the Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation (Jiangbo Pei et al., 2024)</a></li><li><a href=#2330--73128-privacy-preserving-collaborative-split-learning-framework-for-smart-grid-load-forecasting-asif-iqbal-et-al-2024>(23/30 | 73/128) Privacy-Preserving Collaborative Split Learning Framework for Smart Grid Load Forecasting (Asif Iqbal et al., 2024)</a></li><li><a href=#2430--74128-introduction-to-algogens-amir-shachar-2024>(24/30 | 74/128) Introduction to Algogens (Amir Shachar, 2024)</a></li><li><a href=#2530--75128-asyn2f-an-asynchronous-federated-learning-framework-with-bidirectional-model-aggregation-tien-dung-cao-et-al-2024>(25/30 | 75/128) Asyn2F: An Asynchronous Federated Learning Framework with Bidirectional Model Aggregation (Tien-Dung Cao et al., 2024)</a></li><li><a href=#2630--76128-bandit-profit-maximization-for-targeted-marketing-joon-suk-huh-et-al-2024>(26/30 | 76/128) Bandit Profit-maximization for Targeted Marketing (Joon Suk Huh et al., 2024)</a></li><li><a href=#2730--77128-sangria-stacked-autoencoder-neural-networks-with-gradient-boosting-for-indoor-localization-danish-gufran-et-al-2024>(27/30 | 77/128) SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for Indoor Localization (Danish Gufran et al., 2024)</a></li><li><a href=#2830--78128-decoupling-weighing-and-selecting-for-integrating-multiple-graph-pre-training-tasks-tianyu-fan-et-al-2024>(28/30 | 78/128) Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks (Tianyu Fan et al., 2024)</a></li><li><a href=#2930--79128-one-step-multi-view-clustering-based-on-transition-probability-wenhui-zhao-et-al-2024>(29/30 | 79/128) One-Step Multi-View Clustering Based on Transition Probability (Wenhui Zhao et al., 2024)</a></li><li><a href=#3030--80128-on-diffusion-process-in-se3-invariant-space-zihan-zhou-et-al-2024>(30/30 | 80/128) On Diffusion Process in SE(3)-invariant Space (Zihan Zhou et al., 2024)</a></li></ul></li><li><a href=#cscr-6>cs.CR (6)</a><ul><li><a href=#16--81128-using-llms-for-tabletop-exercises-within-the-security-domain-sam-hays-et-al-2024>(1/6 | 81/128) Using LLMs for Tabletop Exercises within the Security Domain (Sam Hays et al., 2024)</a></li><li><a href=#26--82128-iot-device-labeling-using-large-language-models-bar-meyuhas-et-al-2024>(2/6 | 82/128) IoT Device Labeling Using Large Language Models (Bar Meyuhas et al., 2024)</a></li><li><a href=#36--83128-warden-multi-directional-backdoor-watermarks-for-embedding-as-a-service-copyright-protection-anudeex-shetty-et-al-2024>(3/6 | 83/128) WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection (Anudeex Shetty et al., 2024)</a></li><li><a href=#46--84128-issf-the-intelligent-security-service-framework-for-cloud-native-operation-yikuan-yan-et-al-2024>(4/6 | 84/128) ISSF: The Intelligent Security Service Framework for Cloud-Native Operation (Yikuan Yan et al., 2024)</a></li><li><a href=#56--85128-collective-certified-robustness-against-graph-injection-attacks-yuni-lai-et-al-2024>(5/6 | 85/128) Collective Certified Robustness against Graph Injection Attacks (Yuni Lai et al., 2024)</a></li><li><a href=#66--86128-enhancing-data-provenance-and-model-transparency-in-federated-learning-systems----a-database-approach-michael-gu-et-al-2024>(6/6 | 86/128) Enhancing Data Provenance and Model Transparency in Federated Learning Systems &ndash; A Database Approach (Michael Gu et al., 2024)</a></li></ul></li><li><a href=#csro-9>cs.RO (9)</a><ul><li><a href=#19--87128-bronchocopilot-towards-autonomous-robotic-bronchoscopy-via-multimodal-reinforcement-learning-jianbo-zhao-et-al-2024>(1/9 | 87/128) BronchoCopilot: Towards Autonomous Robotic Bronchoscopy via Multimodal Reinforcement Learning (Jianbo Zhao et al., 2024)</a></li><li><a href=#29--88128-deep-incremental-model-based-reinforcement-learning-a-one-step-lookback-approach-for-continuous-robotics-control-cong-li-2024>(2/9 | 88/128) Deep Incremental Model Based Reinforcement Learning: A One-Step Lookback Approach for Continuous Robotics Control (Cong Li, 2024)</a></li><li><a href=#39--89128-barrier-functions-inspired-reward-shaping-for-reinforcement-learning-nilaksh-et-al-2024>(3/9 | 89/128) Barrier Functions Inspired Reward Shaping for Reinforcement Learning (Nilaksh et al., 2024)</a></li><li><a href=#49--90128-the-grasp-loop-signature-a-topological-representation-for-manipulation-planning-with-ropes-and-cables-peter-mitrano-et-al-2024>(4/9 | 90/128) The Grasp Loop Signature: A Topological Representation for Manipulation Planning with Ropes and Cables (Peter Mitrano et al., 2024)</a></li><li><a href=#59--91128-localization-matters-too-how-localization-error-affects-uav-flight-suquan-zhang-et-al-2024>(5/9 | 91/128) Localization matters too: How localization error affects UAV flight (Suquan Zhang et al., 2024)</a></li><li><a href=#69--92128-a-human-centered-approach-for-bootstrapping-causal-graph-creation-minh-q-tram-et-al-2024>(6/9 | 92/128) A Human-Centered Approach for Bootstrapping Causal Graph Creation (Minh Q. Tram et al., 2024)</a></li><li><a href=#79--93128-collision-free-robot-navigation-in-crowded-environments-using-learning-based-convex-model-predictive-control-zhuanglei-wen-et-al-2024>(7/9 | 93/128) Collision-Free Robot Navigation in Crowded Environments using Learning based Convex Model Predictive Control (Zhuanglei Wen et al., 2024)</a></li><li><a href=#89--94128-fast-ergodic-search-with-kernel-functions-muchen-sun-et-al-2024>(8/9 | 94/128) Fast Ergodic Search with Kernel Functions (Muchen Sun et al., 2024)</a></li><li><a href=#99--95128-dufomap-efficient-dynamic-awareness-mapping-daniel-duberg-et-al-2024>(9/9 | 95/128) DUFOMap: Efficient Dynamic Awareness Mapping (Daniel Duberg et al., 2024)</a></li></ul></li><li><a href=#statml-2>stat.ML (2)</a><ul><li><a href=#12--96128-approximations-to-the-fisher-information-metric-of-deep-generative-models-for-out-of-distribution-detection-sam-dauncey-et-al-2024>(1/2 | 96/128) Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection (Sam Dauncey et al., 2024)</a></li><li><a href=#22--97128-sample-efficient-myopic-exploration-through-multitask-reinforcement-learning-with-diverse-tasks-ziping-xu-et-al-2024>(2/2 | 97/128) Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks (Ziping Xu et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#12--98128-a-closer-look-at-wav2vec2-embeddings-for-on-device-single-channel-speech-enhancement-ravi-shankar-et-al-2024>(1/2 | 98/128) A Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel Speech Enhancement (Ravi Shankar et al., 2024)</a></li><li><a href=#22--99128-a-dcf-an-architecture-agnostic-metric-with-application-to-spoofing-robust-speaker-verification-hye-jin-shim-et-al-2024>(2/2 | 99/128) a-DCF: an architecture agnostic metric with application to spoofing-robust speaker verification (Hye-jin Shim et al., 2024)</a></li></ul></li><li><a href=#csir-1>cs.IR (1)</a><ul><li><a href=#11--100128-logic-rules-as-explanations-for-legal-case-retrieval-zhongxiang-sun-et-al-2024>(1/1 | 100/128) Logic Rules as Explanations for Legal Case Retrieval (Zhongxiang Sun et al., 2024)</a></li></ul></li><li><a href=#csdb-3>cs.DB (3)</a><ul><li><a href=#13--101128-rematch-retrieval-enhanced-schema-matching-with-llms-eitam-sheetrit-et-al-2024>(1/3 | 101/128) ReMatch: Retrieval Enhanced Schema Matching with LLMs (Eitam Sheetrit et al., 2024)</a></li><li><a href=#23--102128-treetracker-join-turning-the-tide-when-a-tuple-fails-to-join-zeyuan-hu-et-al-2024>(2/3 | 102/128) TreeTracker Join: Turning the Tide When a Tuple Fails to Join (Zeyuan Hu et al., 2024)</a></li><li><a href=#33--103128-relational-to-rdf-data-migration-by-query-co-evaluation-ryan-wisnesky-et-al-2024>(3/3 | 103/128) Relational to RDF Data Migration by Query Co-Evaluation (Ryan Wisnesky et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#12--104128-distributed-least-squares-optimization-solvers-with-differential-privacy-weijia-liu-et-al-2024>(1/2 | 104/128) Distributed Least-Squares Optimization Solvers with Differential Privacy (Weijia Liu et al., 2024)</a></li><li><a href=#22--105128-distributed-discrete-time-dynamic-outer-approximation-of-the-intersection-of-ellipsoids-eduardo-sebastián-et-al-2024>(2/2 | 105/128) Distributed Discrete-time Dynamic Outer Approximation of the Intersection of Ellipsoids (Eduardo Sebastián et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--106128-brainmass-advancing-brain-network-analysis-for-diagnosis-with-large-scale-self-supervised-learning-yanwu-yang-et-al-2024>(1/1 | 106/128) BrainMass: Advancing Brain Network Analysis for Diagnosis with Large-scale Self-Supervised Learning (Yanwu Yang et al., 2024)</a></li></ul></li><li><a href=#cspf-1>cs.PF (1)</a><ul><li><a href=#11--107128-a-continuous-benchmarking-infrastructure-for-high-performance-computing-applications-christoph-alt-et-al-2024>(1/1 | 107/128) A Continuous Benchmarking Infrastructure for High-Performance Computing Applications (Christoph Alt et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--108128-an-rbf-partition-of-unity-method-for-geometry-reconstruction-and-pde-solution-in-thin-structures-elisabeth-larsson-et-al-2024>(1/2 | 108/128) An RBF partition of unity method for geometry reconstruction and PDE solution in thin structures (Elisabeth Larsson et al., 2024)</a></li><li><a href=#22--109128-fast-algorithm-for-quasi-2d-coulomb-systems-zecheng-gan-et-al-2024>(2/2 | 109/128) Fast Algorithm for Quasi-2D Coulomb Systems (Zecheng Gan et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--110128-a-face-centred-finite-volume-method-for-laminar-and-turbulent-incompressible-flows-luan-m-vieira-et-al-2024>(1/1 | 110/128) A face-centred finite volume method for laminar and turbulent incompressible flows (Luan M. Vieira et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--111128-can-poverty-be-reduced-by-acting-on-discrimination-an-agent-based-model-for-policy-making-alba-aguilera-et-al-2024>(1/1 | 111/128) Can Poverty Be Reduced by Acting on Discrimination? An Agent-based Model for Policy Making (Alba Aguilera et al., 2024)</a></li></ul></li><li><a href=#csdl-1>cs.DL (1)</a><ul><li><a href=#11--112128-it-takes-a-village-a-distributed-training-model-for-ai-based-chatbots-colleen-estes-et-al-2024>(1/1 | 112/128) It Takes a Village: A Distributed Training Model for AI-based Chatbots (Colleen Estes et al., 2024)</a></li></ul></li><li><a href=#cshc-3>cs.HC (3)</a><ul><li><a href=#13--113128-a-preliminary-exploration-of-the-disruption-of-a-generative-ai-systems-facultystaff-and-student-perceptions-of-chatgpt-and-its-capability-of-completing-undergraduate-engineering-coursework-lance-white-et-al-2024>(1/3 | 113/128) A Preliminary Exploration of the Disruption of a Generative AI Systems: Faculty/Staff and Student Perceptions of ChatGPT and its Capability of Completing Undergraduate Engineering Coursework (Lance White et al., 2024)</a></li><li><a href=#23--114128-sard-a-human-ai-collaborative-story-generation-ahmed-y-radwan-et-al-2024>(2/3 | 114/128) SARD: A Human-AI Collaborative Story Generation (Ahmed Y. Radwan et al., 2024)</a></li><li><a href=#33--115128-exploring-the-design-of-generative-ai-in-supporting-music-based-reminiscence-for-older-adults-yucheng-jin-et-al-2024>(3/3 | 115/128) Exploring the Design of Generative AI in Supporting Music-based Reminiscence for Older Adults (Yucheng Jin et al., 2024)</a></li></ul></li><li><a href=#csit-4>cs.IT (4)</a><ul><li><a href=#14--116128-deep-learning-based-design-of-uplink-integrated-sensing-and-communication-qiao-qi-et-al-2024>(1/4 | 116/128) Deep Learning-based Design of Uplink Integrated Sensing and Communication (Qiao Qi et al., 2024)</a></li><li><a href=#24--117128-successful-transmission-probability-and-sir-meta-distribution-analysis-for-multi-antenna-cache-enabled-networks-with-interference-nulling-tianming-feng-et-al-2024>(2/4 | 117/128) Successful Transmission Probability and SIR Meta Distribution Analysis for Multi-Antenna Cache-Enabled Networks with Interference Nulling (Tianming Feng et al., 2024)</a></li><li><a href=#34--118128-ultimate-codes-ted-hurley-2024>(3/4 | 118/128) Ultimate codes (Ted Hurley, 2024)</a></li><li><a href=#44--119128-maximum-length-rll-sequences-in-de-bruijn-graph-yeow-meng-chee-et-al-2024>(4/4 | 119/128) Maximum Length RLL Sequences in de Bruijn Graph (Yeow Meng Chee et al., 2024)</a></li></ul></li><li><a href=#eessiv-3>eess.IV (3)</a><ul><li><a href=#13--120128-enhancing-retinal-vascular-structure-segmentation-in-images-with-a-novel-design-two-path-interactive-fusion-module-model-rui-yang-et-al-2024>(1/3 | 120/128) Enhancing Retinal Vascular Structure Segmentation in Images With a Novel Design Two-Path Interactive Fusion Module Model (Rui Yang et al., 2024)</a></li><li><a href=#23--121128-cdse-unet-enhancing-covid-19-ct-image-segmentation-with-canny-edge-detection-and-dual-path-senet-feature-fusion-jiao-ding-et-al-2024>(2/3 | 121/128) CDSE-UNet: Enhancing COVID-19 CT Image Segmentation with Canny Edge Detection and Dual-Path SENet Feature Fusion (Jiao Ding et al., 2024)</a></li><li><a href=#33--122128-apisr-anime-production-inspired-real-world-anime-super-resolution-boyang-wang-et-al-2024>(3/3 | 122/128) APISR: Anime Production Inspired Real-World Anime Super-Resolution (Boyang Wang et al., 2024)</a></li></ul></li><li><a href=#csse-1>cs.SE (1)</a><ul><li><a href=#11--123128-modelwriter-text--model-synchronized-document-engineering-platform-ferhat-erata-et-al-2024>(1/1 | 123/128) ModelWriter: Text & Model-Synchronized Document Engineering Platform (Ferhat Erata et al., 2024)</a></li></ul></li><li><a href=#csai-1>cs.AI (1)</a><ul><li><a href=#11--124128-soft-reasoning-on-uncertain-knowledge-graphs-weizhi-fei-et-al-2024>(1/1 | 124/128) Soft Reasoning on Uncertain Knowledge Graphs (Weizhi Fei et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--125128-measuring-technological-convergence-in-encryption-technologies-with-proximity-indices-a-text-mining-and-bibliometric-analysis-using-openalex-alessandro-tavazzi-et-al-2024>(1/1 | 125/128) Measuring Technological Convergence in Encryption Technologies with Proximity Indices: A Text Mining and Bibliometric Analysis using OpenAlex (Alessandro Tavazzi et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--126128-efficient-fir-filtering-with-bit-layer-multiply-accumulator-vincenzo-liguori-2024>(1/1 | 126/128) Efficient FIR filtering with Bit Layer Multiply Accumulator (Vincenzo Liguori, 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#11--127128-approximations-and-hardness-of-packing-partially-ordered-items-ilan-doron-arad-et-al-2024>(1/1 | 127/128) Approximations and Hardness of Packing Partially Ordered Items (Ilan Doron-Arad et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--128128-spectral-antisymmetry-of-twisted-graph-adjacency-ye-luo-et-al-2024>(1/1 | 128/128) Spectral antisymmetry of twisted graph adjacency (Ye Luo et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>