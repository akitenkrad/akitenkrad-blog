<!doctype html><html><head><title>arXiv @ 2024.03.07</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.07"><meta property="og:description" content="Primary Categories cond-mat.dis-nn (1) cs.AI (30) cs.CG (1) cs.CL (56) cs.CR (9) cs.CV (54) cs.CY (1) cs.DB (1) cs.DC (2) cs.DL (2) cs.DS (4) cs.ET (1) cs.FL (1) cs.GR (2) cs.GT (1) cs.HC (5) cs.IR (6) cs.IT (5) cs.LG (42) cs.MA (1) cs.MM (3) cs.NE (2) cs.NI (1) cs.PL (2) cs.RO (6) cs.SD (1) cs.SE (5) cs.SI (1) econ.GN (1) eess.AS (3) eess.IV (4) eess.SP (2) eess.SY (6) hep-th (1) math."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240307000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-07T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-07T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.07"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240307000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Thursday, Mar 7, 2024</p></div><div class=title><h1>arXiv @ 2024.03.07</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#cond-matdis-nn-1>cond-mat.dis-nn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csai-30>cs.AI (30)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#cscl-56>cs.CL (56)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#cscr-9>cs.CR (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#cscv-54>cs.CV (54)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csdl-2>cs.DL (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csds-4>cs.DS (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#cset-1>cs.ET (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csfl-1>cs.FL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csgr-2>cs.GR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#cshc-5>cs.HC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csir-6>cs.IR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csit-5>cs.IT (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#cslg-42>cs.LG (42)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csmm-3>cs.MM (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#cspl-2>cs.PL (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csro-6>cs.RO (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#cssd-1>cs.SD (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#csse-5>cs.SE (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#econgn-1>econ.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#eessas-3>eess.AS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#eessiv-4>eess.IV (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#eesssp-2>eess.SP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#eesssy-6>eess.SY (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#hep-th-1>hep-th (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#mathco-2>math.CO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#mathna-6>math.NA (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#mathoc-4>math.OC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#mathst-1>math.ST (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#physicsao-ph-1>physics.ao-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#physicscomp-ph-1>physics.comp-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#q-biogn-1>q-bio.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#q-bioqm-2>q-bio.QM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#quant-ph-3>quant-ph (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#statme-2>stat.ME (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Alpaca</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Automatic Evaluation</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>1</td><td></td><td></td></tr><tr><td>BERT</td><td></td><td>1</td><td></td><td></td></tr><tr><td>BLEU</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Bard</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Benchmarking</td><td>3</td><td>18</td><td>11</td><td>10</td></tr><tr><td>Black Box</td><td>2</td><td>1</td><td></td><td>2</td></tr><tr><td>Causal Intervention</td><td></td><td>1</td><td></td><td></td></tr><tr><td>ChatGPT</td><td>3</td><td></td><td>1</td><td>1</td></tr><tr><td>Chatbot</td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>Claude</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Clustering</td><td></td><td></td><td>1</td><td>3</td></tr><tr><td>Code Generation</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Continual Relation Extraction</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td>2</td><td>3</td><td>3</td></tr><tr><td>Convolution</td><td></td><td></td><td>1</td><td>2</td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>3</td><td>2</td></tr><tr><td>Counter-factual</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Counterfactual Reasoning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>3</td><td>2</td><td>1</td></tr><tr><td>Dialogue System</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>2</td><td>7</td><td></td></tr><tr><td>Discrete Time</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Distribution Shift</td><td>2</td><td>2</td><td>2</td><td>2</td></tr><tr><td>Domain Adaptation</td><td>1</td><td>1</td><td>2</td><td></td></tr><tr><td>Event Argument Extraction</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Event Detection</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fact Verification</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Federated Learning</td><td>2</td><td></td><td>1</td><td>6</td></tr><tr><td>Few-shot</td><td></td><td>4</td><td>4</td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>1</td><td>23</td><td>5</td><td></td></tr><tr><td>Foundation Model</td><td></td><td></td><td>4</td><td></td></tr><tr><td>GPT</td><td>6</td><td>9</td><td></td><td>1</td></tr><tr><td>GPT-3</td><td>1</td><td>4</td><td></td><td>1</td></tr><tr><td>GPT-3.5</td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>GPT-4</td><td>4</td><td>6</td><td></td><td></td></tr><tr><td>Gaussian Process</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Gemini</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Generative AI</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Geometry</td><td>1</td><td></td><td>1</td><td>1</td></tr><tr><td>Grammatical Error Correction</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Graph</td><td>4</td><td>5</td><td>1</td><td>5</td></tr><tr><td>Graph Attention Networks</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Graph Classification</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Graph Embedding</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Graph Neural Network</td><td>2</td><td></td><td></td><td>2</td></tr><tr><td>Grounding</td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Hallucination Detection</td><td></td><td>1</td><td></td><td></td></tr><tr><td>High-Resource</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td>1</td><td>5</td><td></td></tr><tr><td>In-context Learning</td><td>1</td><td>7</td><td>1</td><td>2</td></tr><tr><td>Information Retrieval</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Instruction Following</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>1</td><td>5</td><td>2</td><td></td></tr><tr><td>Knowledge Graph</td><td>3</td><td>2</td><td>1</td><td></td></tr><tr><td>Knowledge Transfer</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>LLaMA</td><td></td><td>5</td><td></td><td>1</td></tr><tr><td>Large Language Model</td><td>24</td><td>65</td><td>11</td><td>4</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Low-Resource</td><td></td><td>4</td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Masked Language Model</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Mathematical Reasoning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Meta Learning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Mistral</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Model Distillation</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Model Extraction</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Model Quantization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>2</td><td>13</td><td>16</td><td>6</td></tr><tr><td>Mutual Information</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>6</td><td></td><td>2</td></tr><tr><td>Object Detection</td><td></td><td>1</td><td>5</td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Outlier Detection</td><td></td><td></td><td>1</td><td></td></tr><tr><td>PaLM</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Parameter Sharing</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>2</td><td>1</td></tr><tr><td>Prompt</td><td>6</td><td>15</td><td>6</td><td>2</td></tr><tr><td>Prompt Learning</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Pruning</td><td>2</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Quantization</td><td>2</td><td></td><td></td><td>2</td></tr><tr><td>Question Answering</td><td>1</td><td>4</td><td>2</td><td></td></tr><tr><td>Reasoning</td><td>5</td><td>8</td><td>4</td><td></td></tr><tr><td>Recommendation</td><td>4</td><td>1</td><td></td><td>1</td></tr><tr><td>Recommender System</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Reconstruction Loss</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Reinforcement Learning</td><td>4</td><td>1</td><td></td><td>2</td></tr><tr><td>Relation Extraction</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td></td><td>3</td></tr><tr><td>Rouge</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Rouge-L</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Self-Attention</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Self-Distillation</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td>2</td><td>1</td><td>6</td><td>4</td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Simulation</td><td>1</td><td>1</td><td></td><td>3</td></tr><tr><td>Simulator</td><td>1</td><td>1</td><td></td><td>3</td></tr><tr><td>Stance Detection</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td>4</td></tr><tr><td>Style Transfer</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Summarization</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>1</td><td>1</td><td>7</td><td>1</td></tr><tr><td>Text Classification</td><td></td><td>2</td><td></td><td>1</td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2SQL</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Transformer</td><td>2</td><td>1</td><td>7</td><td>5</td></tr><tr><td>Unsupervised Learning</td><td>1</td><td>1</td><td>5</td><td>3</td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Vision-and-Language</td><td>2</td><td>1</td><td>8</td><td></td></tr><tr><td>Visual Question Answering</td><td>2</td><td></td><td>3</td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Zero-shot</td><td></td><td>8</td><td>2</td><td>1</td></tr><tr><td>Zero-shot Learning</td><td></td><td></td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-56>cs.CL (56)</h2><h3 id=156--1290-mathscale-scaling-instruction-tuning-for-mathematical-reasoning-zhengyang-tang-et-al-2024>(1/56 | 1/290) MathScale: Scaling Instruction Tuning for Mathematical Reasoning (Zhengyang Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyang Tang, Xingxing Zhang, Benyou Wang, Furu Wei. (2024)<br><strong>MathScale: Scaling Instruction Tuning for Mathematical Reasoning</strong><br><button class=copy-to-clipboard title="MathScale: Scaling Instruction Tuning for Mathematical Reasoning" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 106<br>Keywords: Graph, Benchmarking, Fine-tuning, GPT, GPT-3, LLaMA, Mistral, Mathematical Reasoning, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02884v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02884v1.pdf filename=2403.02884v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving <b>mathematical</b> <b>problems</b> remains inadequate. We propose MathScale, a simple and scalable method to create high-quality <b>mathematical</b> <b>reasoning</b> data using frontier <b>LLMs</b> (e.g., {\tt <b>GPT-3.5}).</b> Inspired by the cognitive mechanism in human <b>mathematical</b> <b>learning,</b> it first extracts topics and knowledge points from seed math questions and then build a concept <b>graph,</b> which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a <b>mathematical</b> <b>reasoning</b> dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate <b>mathematical</b> <b>reasoning</b> abilities of <b>LLMs</b> comprehensively, we construct {\sc MwpBench}, a <b>benchmark</b> of Math Word Problems, which is a collection of ten datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to <b>fine-tune</b> open-source <b>LLMs</b> (e.g., <b>LLaMA-2</b> and <b>Mistral),</b> resulting in significantly improved capabilities in <b>mathematical</b> <b>reasoning.</b> Evaluated on {\sc MwpBench}, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.9% in micro average accuracy and 43.7% in macro average accuracy, respectively.</p></p class="citation"></blockquote><h3 id=256--2290-design2code-how-far-are-we-from-automating-front-end-engineering-chenglei-si-et-al-2024>(2/56 | 2/290) Design2Code: How Far Are We From Automating Front-End Engineering? (Chenglei Si et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, Diyi Yang. (2024)<br><strong>Design2Code: How Far Are We From Automating Front-End Engineering?</strong><br><button class=copy-to-clipboard title="Design2Code: How Far Are We From Automating Front-End Engineering?" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-CY, cs.CL<br>Keyword Score: 102<br>Keywords: Automatic Evaluation, Benchmarking, Benchmarking, Fine-tuning, Fine-tuning, Generative AI, Multi-modal, Multi-modal, GPT, Gemini, Code Generation, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03163v1.pdf filename=2403.03163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> has made rapid advancements in recent years, achieving unprecedented capabilities in <b>multimodal</b> understanding and <b>code</b> <b>generation.</b> This can enable a new paradigm of front-end development, in which <b>multimodal</b> <b>LLMs</b> might directly convert visual designs into <b>code</b> <b>implementations.</b> In this work, we formalize this as a Design2Code task and conduct comprehensive <b>benchmarking.</b> Specifically, we manually curate a <b>benchmark</b> of 484 diverse real-world webpages as test cases and develop a set of <b>automatic</b> <b>evaluation</b> metrics to assess how well current <b>multimodal</b> <b>LLMs</b> can generate the <b>code</b> <b>implementations</b> that directly render into the given reference webpages, given the screenshots as input. We also complement <b>automatic</b> <b>metrics</b> with comprehensive human evaluations. We develop a suite of <b>multimodal</b> <b>prompting</b> methods and show their effectiveness on <b>GPT-4V</b> and <b>Gemini</b> Pro Vision. We further <b>finetune</b> an open-source Design2Code-18B model that successfully matches the performance of <b>Gemini</b> Pro Vision. Both human evaluation and <b>automatic</b> <b>metrics</b> show that <b>GPT-4V</b> performs the best on this task compared to other models. Moreover, annotators think <b>GPT-4V</b> generated webpages can replace the original reference webpages in 49% of cases in terms of visual appearance and content; and perhaps surprisingly, in 64% of cases <b>GPT-4V</b> generated webpages are considered better than the original reference webpages. Our fine-grained break-down metrics indicate that open-source models mostly lag in recalling visual elements from the input webpages and in generating correct layout designs, while aspects like text content and coloring can be drastically improved with proper <b>finetuning.</b></p></p class="citation"></blockquote><h3 id=356--3290-zero-shot-cross-lingual-document-level-event-causality-identification-with-heterogeneous-graph-contrastive-transfer-learning-zhitao-he-et-al-2024>(3/56 | 3/290) Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning (Zhitao He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhitao He, Pengfei Cao, Yubo Chen, Kang Liu, Zhiqiang Zhang, Mengshu Sun, Jun Zhao. (2024)<br><strong>Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning</strong><br><button class=copy-to-clipboard title="Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 93<br>Keywords: Graph, Few-shot, Few-shot Learning, High-Resource, Low-Resource, Transfer Learning, Zero-shot, GPT, GPT-3, GPT-3.5<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02893v1.pdf filename=2403.02893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event Causality Identification (ECI) refers to detect causal relations between events in texts. However, most existing studies focus on sentence-level ECI with <b>high-resource</b> language, leaving more challenging document-level ECI (DECI) with <b>low-resource</b> languages under-explored. In this paper, we propose a Heterogeneous <b>Graph</b> Interaction Model with Multi-granularity Contrastive <b>Transfer</b> <b>Learning</b> (GIMC) for <b>zero-shot</b> cross-lingual document-level ECI. Specifically, we introduce a heterogeneous <b>graph</b> interaction network to model the long-distance dependencies between events that are scattered over document. Then, to improve cross-lingual transferability of causal knowledge learned from source language, we propose a multi-granularity contrastive <b>transfer</b> <b>learning</b> module to align the causal representations across languages. Extensive experiments show our framework outperforms previous state-of-the-art model by 9.4% and 8.2% of average F1 score on monolingual and multilingual scenarios respectively. Notably, in multilingual scenario, our <b>zero-shot</b> framework even exceeds <b>GPT-3.5</b> with <b>few-shot</b> <b>learning</b> by 24.3% in overall performance.</p></p class="citation"></blockquote><h3 id=456--4290-improving-event-definition-following-for-zero-shot-event-detection-zefan-cai-et-al-2024>(4/56 | 4/290) Improving Event Definition Following For Zero-Shot Event Detection (Zefan Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zefan Cai, Po-Nien Kung, Ashima Suvarna, Mingyu Derek Ma, Hritik Bansal, Baobao Chang, P. Jeffrey Brantingham, Wei Wang, Nanyun Peng. (2024)<br><strong>Improving Event Definition Following For Zero-Shot Event Detection</strong><br><button class=copy-to-clipboard title="Improving Event Definition Following For Zero-Shot Event Detection" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, Fine-tuning, Zero-shot, GPT, GPT-3, GPT-3.5, LLaMA, Event Detection, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02586v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02586v1.pdf filename=2403.02586v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing approaches on <b>zero-shot</b> <b>event</b> <b>detection</b> usually train models on datasets annotated with known <b>event</b> <b>types,</b> and <b>prompt</b> them with unseen <b>event</b> <b>definitions.</b> These approaches yield sporadic successes, yet generally fall short of expectations. In this work, we aim to improve <b>zero-shot</b> <b>event</b> <b>detection</b> by training models to better follow <b>event</b> <b>definitions.</b> We hypothesize that a diverse set of <b>event</b> <b>types</b> and definitions are the key for models to learn to follow <b>event</b> <b>definitions</b> while existing <b>event</b> <b>extraction</b> datasets focus on annotating many high-quality examples for a few <b>event</b> <b>types.</b> To verify our hypothesis, we construct an automatically generated Diverse <b>Event</b> <b>Definition</b> (DivED) dataset and conduct comparative studies. Our experiments reveal that a <b>large</b> <b>number</b> <b>of</b> <b>event</b> <b>types</b> (200) and diverse <b>event</b> <b>definitions</b> can significantly boost <b>event</b> <b>extraction</b> performance; on the other hand, the performance does not scale with over ten examples per <b>event</b> <b>type.</b> Beyond scaling, we incorporate <b>event</b> <b>ontology</b> information and hard-negative samples during training, further boosting the performance. Based on these findings, we <b>fine-tuned</b> a <b>LLaMA-2-7B</b> model on our DivED dataset, yielding performance that surpasses SOTA <b>large</b> <b>language</b> <b>models</b> like <b>GPT-3.5</b> across three open <b>benchmarks</b> on <b>zero-shot</b> <b>event</b> <b>detection.</b></p></p class="citation"></blockquote><h3 id=556--5290-paradise-evaluating-implicit-planning-skills-of-language-models-with-procedural-warnings-and-tips-dataset-arda-uzunoglu-et-al-2024>(5/56 | 5/290) PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset (Arda Uzunoglu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arda Uzunoglu, Abdalfatah Rashid Safa, Gözde Gül Şahin. (2024)<br><strong>PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset</strong><br><button class=copy-to-clipboard title="PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Zero-shot, BERT, GPT, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03167v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03167v2.pdf filename=2403.03167v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been growing interest within the community regarding whether <b>large</b> <b>language</b> <b>models</b> are capable of planning or executing plans. However, most prior studies use <b>LLMs</b> to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive <b>reasoning</b> task using Q&amp;A format on practical procedural text sourced from wikiHow. It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing <b>fine-tuned</b> language models and <b>zero-shot</b> <b>prompting,</b> reveal the effectiveness of task-specific small models over <b>large</b> <b>language</b> <b>models</b> in most scenarios. Despite advancements, all models fall short of human performance. Notably, our analysis uncovers intriguing insights, such as variations in model behavior with dropped keywords, struggles of <b>BERT-family</b> and <b>GPT-4</b> with physical and abstract goals, and the proposed tasks offering valuable prior knowledge for other unseen procedural tasks. The PARADISE dataset and associated resources are publicly available for further research exploration with <a href=https://github.com/GGLAB-KU/paradise>https://github.com/GGLAB-KU/paradise</a>.</p></p class="citation"></blockquote><h3 id=656--6290-ruleprompt-weakly-supervised-text-classification-with-prompting-plms-and-self-iterative-logical-rules-miaomiao-li-et-al-2024>(6/56 | 6/290) RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules (Miaomiao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miaomiao Li, Jiaqi Zhu, Yang Wang, Yi Yang, Yilin Li, Hongan Wang. (2024)<br><strong>RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules</strong><br><button class=copy-to-clipboard title="RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Self-supervised Learning, Supervised Learning, Weakly-supervised Learning, Zero-shot, Text Classification, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02932v1.pdf filename=2403.02932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Weakly <b>supervised</b> <b>text</b> <b>classification</b> (WSTC), also called <b>zero-shot</b> or dataless <b>text</b> <b>classification,</b> has attracted increasing attention due to its applicability in classifying a mass of <b>texts</b> <b>within</b> the dynamic and open Web environment, since it requires only a limited set of seed words (label names) for each category instead of labeled data. With the help of recently popular <b>prompting</b> <b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs),</b> many studies leveraged manually crafted and/or automatically identified verbalizers to estimate the likelihood of categories, but they failed to differentiate the effects of these category-indicative words, let alone capture their correlations and realize adaptive adjustments according to the unlabeled corpus. In this paper, in order to let the <b>PLM</b> effectively understand each category, we at first propose a novel form of rule-based knowledge using logical expressions to characterize the meanings of categories. Then, we develop a <b>prompting</b> <b>PLM-based</b> approach named RulePrompt for the WSTC task, consisting of a rule mining module and a rule-enhanced pseudo label generation module, plus a <b>self-supervised</b> <b>fine-tuning</b> module to make the <b>PLM</b> align with this task. Within this framework, the inaccurate pseudo labels assigned to <b>texts</b> <b>and</b> the imprecise logical rules associated with categories mutually enhance each other in an alternative manner. That establishes a self-iterative closed loop of knowledge (rule) acquisition and utilization, with seed words serving as the starting point. Extensive experiments validate the effectiveness and robustness of our approach, which markedly outperforms state-of-the-art weakly <b>supervised</b> methods. What is more, our approach yields interpretable category rules, proving its advantage in disambiguating easily-confused categories.</p></p class="citation"></blockquote><h3 id=756--7290-causal-prompting-debiasing-large-language-model-prompting-based-on-front-door-adjustment-congzhi-zhang-et-al-2024>(7/56 | 7/290) Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment (Congzhi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Congzhi Zhang, Linhai Zhang, Deyu Zhou, Guoqiang Xu. (2024)<br><strong>Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment</strong><br><button class=copy-to-clipboard title="Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Causal Intervention, Contrastive Learning, Data Augmentation, Fine-tuning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02738v1.pdf filename=2403.02738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the significant achievements of existing <b>prompting</b> methods such as <b>in-context</b> <b>learning</b> and chain-of-thought for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> they still face challenges of various biases. Traditional debiasing methods primarily focus on the model training stage, including <b>data</b> <b>augmentation-based</b> and reweight-based approaches, with the limitations of addressing the complex biases of <b>LLMs.</b> To address such limitations, the <b>causal</b> <b>relationship</b> behind the <b>prompting</b> methods is uncovered using a structural <b>causal</b> <b>model,</b> and a novel <b>causal</b> <b>prompting</b> method based on front-door adjustment is proposed to effectively mitigate the bias of <b>LLMs.</b> In specific, <b>causal</b> <b>intervention</b> is implemented by designing the <b>prompts</b> without accessing the parameters and logits of <b>LLMs.The</b> chain-of-thoughts generated by <b>LLMs</b> are employed as the mediator variable and the <b>causal</b> <b>effect</b> between the input <b>prompt</b> and the output answers is calculated through front-door adjustment to mitigate model biases. Moreover, to obtain the representation of the samples precisely and estimate the <b>causal</b> <b>effect</b> more accurately, <b>contrastive</b> <b>learning</b> is used to <b>fine-tune</b> the encoder of the samples by aligning the space of the encoder with the <b>LLM.</b> Experimental results show that the proposed <b>causal</b> <b>prompting</b> approach achieves excellent performance on 3 natural language processing datasets on both open-source and closed-source <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=856--8290-evidence-focused-fact-summarization-for-knowledge-augmented-zero-shot-question-answering-sungho-ko-et-al-2024>(8/56 | 8/290) Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering (Sungho Ko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sungho Ko, Hyunjin Cho, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee. (2024)<br><strong>Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering</strong><br><button class=copy-to-clipboard title="Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 83<br>Keywords: Graph, Knowledge Distillation, Knowledge Graph, Knowledge Graph, Zero-shot, Question Answering, Question Answering, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02966v1.pdf filename=2403.02966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have investigated utilizing <b>Knowledge</b> <b>Graphs</b> <b>(KGs)</b> to enhance Quesetion Answering <b>(QA)</b> performance of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> yet structured <b>KG</b> verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact <b>Summarization</b> framework for enhanced <b>QA</b> with <b>knowledge-augmented</b> <b>LLMs.</b> We optimize an open-source <b>LLM</b> as a fact summarizer through <b>distillation</b> and preference alignment. Our extensive experiments show that EFSum improves <b>LLM&rsquo;s</b> <b>zero-shot</b> <b>QA</b> performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.</p></p class="citation"></blockquote><h3 id=956--9290-jmi-at-semeval-2024-task-3-two-step-approach-for-multimodal-ecac-using-in-context-learning-with-gpt-and-instruction-tuned-llama-models-arefa-et-al-2024>(9/56 | 9/290) JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models (Arefa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arefa, Mohammed Abbas Ansari, Chandni Saxena, Tanvir Ahmad. (2024)<br><strong>JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models</strong><br><button class=copy-to-clipboard title="JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 76<br>Keywords: Multi-modal, Multi-modal, GPT, GPT-3, GPT-3.5, LLaMA, In-context Learning, In-context Learning, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04798v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04798v1.pdf filename=2403.04798v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents our system development for SemEval-2024 Task 3: &ldquo;The Competition of <b>Multimodal</b> Emotion Cause Analysis in Conversations&rdquo;. Effectively capturing emotions in human conversations requires integrating multiple modalities such as text, audio, and video. However, the complexities of these diverse modalities pose challenges for developing an efficient <b>multimodal</b> emotion cause analysis (ECA) system. Our proposed approach addresses these challenges by a two-step framework. We adopt two different approaches in our implementation. In Approach 1, we employ <b>instruction-tuning</b> <b>with</b> two separate <b>Llama</b> 2 models for emotion and cause prediction. In Approach 2, we use <b>GPT-4V</b> for conversation-level video description and employ <b>in-context</b> <b>learning</b> with annotated conversation using <b>GPT</b> 3.5. Our system wins rank 4, and system ablation experiments demonstrate that our proposed solutions achieve significant performance gains. All the experimental codes are available on Github.</p></p class="citation"></blockquote><h3 id=1056--10290-towards-training-a-chinese-large-language-model-for-anesthesiology-zhonghai-wang-et-al-2024>(10/56 | 10/290) Towards Training A Chinese Large Language Model for Anesthesiology (Zhonghai Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhonghai Wang, Jie Jiang, Yibing Zhan, Bohao Zhou, Yanhong Li, Chong Zhang, Liang Ding, Hua Jin, Jun Peng, Xu Lin, Weifeng Liu. (2024)<br><strong>Towards Training A Chinese Large Language Model for Anesthesiology</strong><br><button class=copy-to-clipboard title="Towards Training A Chinese Large Language Model for Anesthesiology" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, GPT, GPT-4, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02742v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02742v1.pdf filename=2403.02742v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have gained popularity recently due to their significant practical utility. However, most existing research focuses on general medicine, and there is a need for in-depth study of <b>LLMs</b> in specific fields like anesthesiology. To fill the gap, we introduce Hypnos, a Chinese Anesthesia model built upon existing <b>LLMs,</b> e.g., <b>Llama.</b> Hypnos&rsquo; contributions have three aspects: 1) The data, such as utilizing Self-Instruct, acquired from current <b>LLMs</b> likely includes inaccuracies. Hypnos implements a cross-filtering strategy to improve the data quality. This strategy involves using one <b>LLM</b> to assess the quality of the generated data from another <b>LLM</b> and filtering out the data with low quality. 2) Hypnos employs a general-to-specific training strategy that starts by <b>fine-tuning</b> <b>LLMs</b> using the general medicine data and subsequently improving the <b>fine-tuned</b> <b>LLMs</b> using data specifically from Anesthesiology. The general medical data supplement the medical expertise in Anesthesiology and enhance the effectiveness of Hypnos&rsquo; generation. 3) We introduce a standardized <b>benchmark</b> for evaluating medical <b>LLM</b> in Anesthesiology. Our <b>benchmark</b> includes both publicly available instances from the Internet and privately obtained cases from the Hospital. Hypnos outperforms other medical <b>LLMs</b> in anesthesiology in metrics, <b>GPT-4,</b> and human evaluation on the <b>benchmark</b> dataset.</p></p class="citation"></blockquote><h3 id=1156--11290-benchmarking-the-text-to-sql-capability-of-large-language-models-a-comprehensive-evaluation-bin-zhang-et-al-2024>(11/56 | 11/290) Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation (Bin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bin Zhang, Yuxiao Ye, Guoqing Du, Xiaoru Hu, Zhishuai Li, Sun Yang, Chi Harold Liu, Rui Zhao, Ziyue Li, Hangyu Mao. (2024)<br><strong>Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation</strong><br><button class=copy-to-clipboard title="Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 66<br>Keywords: Benchmarking, Benchmarking, Text2SQL, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02951v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02951v2.pdf filename=2403.02951v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have emerged as a powerful tool in advancing the <b>Text-to-SQL</b> task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal <b>prompt</b> templates and design frameworks. Additionally, existing <b>benchmarks</b> inadequately explore the performance of <b>LLMs</b> across the various sub-tasks of the <b>Text-to-SQL</b> process, which hinders the assessment of <b>LLMs&rsquo;</b> cognitive capabilities and the optimization of <b>LLM-based</b> solutions. To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in <b>LLMs.</b> Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various <b>LLMs</b> throughout the <b>Text-to-SQL</b> process.Our study highlights the performance disparities among <b>LLMs</b> and proposes optimal <b>in-context</b> <b>learning</b> solutions tailored to each task. These findings offer valuable insights for enhancing the development of <b>LLM-based</b> <b>Text-to-SQL</b> systems.</p></p class="citation"></blockquote><h3 id=1256--12290-learning-to-maximize-mutual-information-for-chain-of-thought-distillation-xin-chen-et-al-2024>(12/56 | 12/290) Learning to Maximize Mutual Information for Chain-of-Thought Distillation (Xin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Chen, Hanxian Huang, Yanjun Gao, Yi Wang, Jishen Zhao, Ke Ding. (2024)<br><strong>Learning to Maximize Mutual Information for Chain-of-Thought Distillation</strong><br><button class=copy-to-clipboard title="Learning to Maximize Mutual Information for Chain-of-Thought Distillation" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Model Distillation, Mutual Information, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03348v1.pdf filename=2403.03348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>distillation,</b> the technique of transferring <b>knowledge</b> <b>from</b> large, complex <b>models</b> <b>to</b> smaller ones, marks a pivotal step towards efficient AI deployment. <b>Distilling</b> Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) <b>distillation,</b> has demonstrated promise by imbuing smaller <b>models</b> <b>with</b> the superior <b>reasoning</b> capabilities of their larger counterparts. In DSS, the <b>distilled</b> <b>model</b> <b>acquires</b> the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT <b>knowledge</b> <b>with</b> the task of label prediction. To this end, we investigate the <b>mutual</b> <b>relationship</b> of the two tasks from Information Bottleneck perspective and formulate it as maximizing the <b>mutual</b> <b>information</b> of the representation features of the two tasks. We propose a variational approach to solve this optimization problem using a learning-based method. Our experimental results across four datasets demonstrate that our method outperforms the state-of-the-art DSS. Our findings offer insightful guidance for future research on language <b>model</b> <b>distillation</b> as well as applications involving CoT. Code and <b>models</b> <b>will</b> be released soon.</p></p class="citation"></blockquote><h3 id=1356--13290-scope-of-large-language-models-for-mining-emerging-opinions-in-online-health-discourse-joseph-gatto-et-al-2024>(13/56 | 13/290) Scope of Large Language Models for Mining Emerging Opinions in Online Health Discourse (Joseph Gatto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joseph Gatto, Madhusudan Basak, Yash Srivastava, Philip Bohlman, Sarah M. Preum. (2024)<br><strong>Scope of Large Language Models for Mining Emerging Opinions in Online Health Discourse</strong><br><button class=copy-to-clipboard title="Scope of Large Language Models for Mining Emerging Opinions in Online Health Discourse" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SI, cs.CL<br>Keyword Score: 60<br>Keywords: Zero-shot, GPT, GPT-4, Stance Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03336v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03336v1.pdf filename=2403.03336v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we develop an <b>LLM-powered</b> framework for the curation and evaluation of emerging opinion mining in online health communities. We formulate emerging opinion mining as a pairwise <b>stance</b> <b>detection</b> problem between (title, comment) pairs sourced from Reddit, where post titles contain emerging health-related claims on a topic that is not predefined. The claims are either explicitly or implicitly expressed by the user. We detail (i) a method of claim identification &ndash; the task of identifying if a post title contains a claim and (ii) an opinion mining-driven evaluation framework for <b>stance</b> <b>detection</b> using <b>LLMs.</b> We facilitate our exploration by releasing a novel test dataset, Long COVID-Stance, or LC-stance, which can be used to evaluate <b>LLMs</b> on the tasks of claim identification and <b>stance</b> <b>detection</b> in online health communities. Long Covid is an emerging post-COVID disorder with uncertain and complex treatment guidelines, thus making it a suitable use case for our task. LC-Stance contains long COVID treatment related discourse sourced from a Reddit community. Our evaluation shows that <b>GPT-4</b> significantly outperforms prior works on <b>zero-shot</b> <b>stance</b> <b>detection.</b> We then perform thorough <b>LLM</b> model diagnostics, identifying the role of claim type (i.e. implicit vs explicit claims) and comment length as sources of model error.</p></p class="citation"></blockquote><h3 id=1456--14290-mad-libs-are-all-you-need-augmenting-cross-domain-document-level-event-argument-data-joseph-gatto-et-al-2024>(14/56 | 14/290) Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data (Joseph Gatto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joseph Gatto, Parker Seegmiller, Omar Sharif, Sarah M. Preum. (2024)<br><strong>Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data</strong><br><button class=copy-to-clipboard title="Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Data Augmentation, Few-shot, Low-Resource, Event Argument Extraction, Information Retrieval, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03304v1.pdf filename=2403.03304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Document-Level <b>Event</b> <b>Argument</b> <b>Extraction</b> (DocEAE) is an extremely difficult <b>information</b> <b>extraction</b> problem &ndash; with significant limitations in <b>low-resource</b> cross-domain settings. To address this problem, we introduce Mad Lib Aug (MLA), a novel generative DocEAE <b>data</b> <b>augmentation</b> framework. Our approach leverages the intuition that Mad Libs, which are categorically masked documents used as a part of a popular game, can be generated and solved by <b>LLMs</b> to produce <b>data</b> <b>for</b> DocEAE. Using MLA, we achieve a 2.6-point average improvement in overall F1 score. Moreover, this approach achieves a 3.9 and 5.2 point average increase in zero and <b>few-shot</b> <b>event</b> <b>roles</b> <b>compared</b> to augmentation-free baselines across all experiments. To better facilitate analysis of cross-domain DocEAE, we additionally introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify roles in the target domain which are semantic outliers with respect to roles observed in the source domain. Our experiments show that MLA augmentation can boost RDF1 performance by an average of 5.85 points compared to non-augmented datasets.</p></p class="citation"></blockquote><h3 id=1556--15290-language-guided-exploration-for-rl-agents-in-text-environments-hitesh-golchha-et-al-2024>(15/56 | 15/290) Language Guided Exploration for RL Agents in Text Environments (Hitesh Golchha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hitesh Golchha, Sahil Yerawar, Dhruvesh Patel, Soham Dan, Keerthiram Murugesan. (2024)<br><strong>Language Guided Exploration for RL Agents in Text Environments</strong><br><button class=copy-to-clipboard title="Language Guided Exploration for RL Agents in Text Environments" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Distribution Shift, Distribution Shift, Reinforcement Learning, Transformer, Large Language Model, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03141v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03141v1.pdf filename=2403.03141v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world sequential decision making is characterized by sparse rewards and <b>large</b> <b>decision</b> <b>spaces,</b> posing significant difficulty for experiential learning systems like $\textit{tabula rasa}$ <b>reinforcement</b> <b>learning</b> (RL) agents. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> with a wealth of world knowledge, can help RL agents learn quickly and adapt to <b>distribution</b> <b>shifts.</b> In this work, we introduce Language Guided Exploration (LGE) framework, which uses a <b>pre-trained</b> <b>language</b> <b>model</b> (called GUIDE ) to provide decision-level guidance to an RL agent (called EXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging text environment, LGE outperforms vanilla RL agents significantly and also outperforms other sophisticated methods like Behaviour Cloning and Text Decision <b>Transformer.</b></p></p class="citation"></blockquote><h3 id=1656--16290-a-general-and-flexible-multi-concept-parsing-framework-for-multilingual-semantic-matching-dong-yao-et-al-2024>(16/56 | 16/290) A General and Flexible Multi-concept Parsing Framework for Multilingual Semantic Matching (Dong Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dong Yao, Asaad Alghamdi, Qingrong Xia, Xiaoye Qu, Xinyu Duan, Zhefeng Wang, Yi Zheng, Baoxing Huai, Peilun Cheng, Zhou Zhao. (2024)<br><strong>A General and Flexible Multi-concept Parsing Framework for Multilingual Semantic Matching</strong><br><button class=copy-to-clipboard title="A General and Flexible Multi-concept Parsing Framework for Multilingual Semantic Matching" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Low-Resource, Recommendation, Chatbot, Named Entity Recognition, Question Answering, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02975v1.pdf filename=2403.02975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sentence semantic matching is a research hotspot in natural language processing, which is considerably significant in various key scenarios, such as community <b>question</b> <b>answering,</b> searching, <b>chatbot,</b> and <b>recommendation.</b> Since most of the advanced models directly model the semantic relevance among words between two sentences while neglecting the \textit{keywords} and \textit{intents} concepts of them, DC-Match is proposed to disentangle keywords from intents and utilizes them to optimize the matching performance. Although DC-Match is a simple yet effective method for semantic matching, it highly depends on the external <b>NER</b> techniques to identify the keywords of sentences, which limits the performance of semantic matching for minor languages since satisfactory <b>NER</b> tools are usually hard to obtain. In this paper, we propose to generally and flexibly resolve the text into multi concepts for multilingual semantic matching to liberate the model from the reliance on <b>NER</b> models. To this end, we devise a \underline{M}ulti-\underline{C}oncept \underline{P}arsed \underline{S}emantic \underline{M}atching framework based on the <b>pre-trained</b> <b>language</b> <b>models,</b> abbreviated as \textbf{MCP-SM}, to extract various concepts and infuse them into the classification tokens. We conduct comprehensive experiments on English datasets QQP and MRPC, and Chinese dataset Medical-SM. Besides, we experiment on Arabic datasets MQ2Q and XNLI, the outstanding performance further prove MCP-SM&rsquo;s applicability in <b>low-resource</b> languages.</p></p class="citation"></blockquote><h3 id=1756--17290-role-prompting-guided-domain-adaptation-with-general-capability-preserve-for-large-language-models-rui-wang-et-al-2024>(17/56 | 17/290) Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models (Rui Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Wang, Fei Mi, Yi Chen, Boyang Xue, Hongru Wang, Qi Zhu, Kam-Fai Wong, Ruifeng Xu. (2024)<br><strong>Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models</strong><br><button class=copy-to-clipboard title="Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Self-Distillation, Domain Adaptation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02756v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02756v1.pdf filename=2403.02756v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing interest in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for specialized applications has revealed a significant challenge: when tailored to specific <b>domains,</b> <b>LLMs</b> tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple <b>domains</b> <b>simultaneously</b> often results in a decline in overall performance due to confusion between <b>domains.</b> <b>In</b> response to these issues, we present the RolE <b>Prompting</b> Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain <b>LLM</b> adaptation through three key components: 1) <b>Self-Distillation</b> constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role <b>Prompting</b> assigns a central <b>prompt</b> to the general <b>domain</b> <b>and</b> a unique role <b>prompt</b> to each specific <b>domain</b> <b>to</b> minimize inter-domain confusion during training. 3) Role Integration reuses and integrates a small portion of <b>domain-specific</b> <b>data</b> to the general-domain data, which are trained under the guidance of the central <b>prompt.</b> The central <b>prompt</b> is used for a streamlined inference process, removing the necessity to switch <b>prompts</b> for different <b>domains.</b> <b>Empirical</b> results demonstrate that REGA effectively alleviates catastrophic forgetting and inter-domain confusion. This leads to improved <b>domain-specific</b> <b>performance</b> compared to standard <b>fine-tuned</b> models, while still preserving robust general capabilities.</p></p class="citation"></blockquote><h3 id=1856--18290-eliciting-better-multilingual-structured-reasoning-from-llms-through-code-bryan-li-et-al-2024>(18/56 | 18/290) Eliciting Better Multilingual Structured Reasoning from LLMs through Code (Bryan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bryan Li, Tamer Alkhouli, Daniele Bonadiman, Nikolaos Pappas, Saab Mansour. (2024)<br><strong>Eliciting Better Multilingual Structured Reasoning from LLMs through Code</strong><br><button class=copy-to-clipboard title="Eliciting Better Multilingual Structured Reasoning from LLMs through Code" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Common-sense Reasoning, Neural Machine Translation, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02567v1.pdf filename=2403.02567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Development of <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> have shown progress on <b>reasoning,</b> though studies have been limited to English or simple <b>reasoning</b> tasks. We thus introduce a multilingual structured <b>reasoning</b> and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base <b>LLM</b> performance between English and non-English <b>reasoning</b> tasks. We then propose two methods to remedy this gap, building on the insight that <b>LLMs</b> trained on code are better reasoners. First, at training time, we augment a code dataset with multi-lingual comments using <b>machine</b> <b>translation</b> while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a <b>prompt</b> structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific <b>commonsense</b> <b>reasoning</b> subtask. Furthermore, the models show no regression on non-reasoning tasks, thus showing our techniques maintain general-purpose abilities.</p></p class="citation"></blockquote><h3 id=1956--19290-injecagent-benchmarking-indirect-prompt-injections-in-tool-integrated-large-language-model-agents-qiusi-zhan-et-al-2024>(19/56 | 19/290) InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents (Qiusi Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiusi Zhan, Zhixiang Liang, Zifan Ying, Daniel Kang. (2024)<br><strong>InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents</strong><br><button class=copy-to-clipboard title="InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs.CL<br>Keyword Score: 56<br>Keywords: Benchmarking, Benchmarking, GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02691v1.pdf filename=2403.02691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work has embodied <b>LLMs</b> as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect <b>prompt</b> injection (IPI) attacks, where malicious instructions are embedded within the content processed by <b>LLMs,</b> aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing <b>benchmarks</b> to assess and mitigate these risks is imperative. In this work, we introduce InjecAgent, a <b>benchmark</b> designed to assess the vulnerability of tool-integrated <b>LLM</b> agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different <b>LLM</b> agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted <b>GPT-4</b> vulnerable to attacks 24% of the time. Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking <b>prompt,</b> shows additional increases in success rates, nearly doubling the attack success rate on the ReAct-prompted <b>GPT-4.</b> Our findings raise questions about the widespread deployment of <b>LLM</b> Agents. Our <b>benchmark</b> is available at <a href=https://github.com/uiuc-kang-lab/InjecAgent>https://github.com/uiuc-kang-lab/InjecAgent</a>.</p></p class="citation"></blockquote><h3 id=2056--20290-hargpt-are-llms-zero-shot-human-activity-recognizers-sijie-ji-et-al-2024>(20/56 | 20/290) HARGPT: Are LLMs Zero-Shot Human Activity Recognizers? (Sijie Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sijie Ji, Xinzhe Zheng, Chenshu Wu. (2024)<br><strong>HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?</strong><br><button class=copy-to-clipboard title="HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Zero-shot, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02727v1.pdf filename=2403.02727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is an ongoing debate regarding the potential of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as foundational models seamlessly integrated with Cyber-Physical Systems (CPS) for interpreting the physical world. In this paper, we carry out a case study to answer the following question: Are <b>LLMs</b> capable of <b>zero-shot</b> human activity recognition (HAR). Our study, HARGPT, presents an affirmative answer by demonstrating that <b>LLMs</b> can comprehend raw IMU data and perform HAR tasks in a <b>zero-shot</b> manner, with only appropriate <b>prompts.</b> HARGPT inputs raw IMU data into <b>LLMs</b> and utilizes the role-play and think step-by-step strategies for <b>prompting.</b> We <b>benchmark</b> HARGPT on <b>GPT4</b> using two public datasets of different inter-class similarities and compare various baselines both based on traditional machine learning and state-of-the-art deep classification models. Remarkably, <b>LLMs</b> successfully recognize human activities from raw IMU data and consistently outperform all the baselines on both datasets. Our findings indicate that by effective <b>prompting,</b> <b>LLMs</b> can interpret raw IMU data based on their knowledge base, possessing a promising potential to analyze raw sensor data of the physical world effectively.</p></p class="citation"></blockquote><h3 id=2156--21290-causal-walk-debiasing-multi-hop-fact-verification-with-front-door-adjustment-congzhi-zhang-et-al-2024>(21/56 | 21/290) Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment (Congzhi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Congzhi Zhang, Linhai Zhang, Deyu Zhou. (2024)<br><strong>Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment</strong><br><button class=copy-to-clipboard title="Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Graph, Counter-factual, Counterfactual Reasoning, Fact Verification, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02698v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02698v1.pdf filename=2403.02698v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional multi-hop <b>fact</b> <b>verification</b> models are prone to rely on spurious correlations from the annotation artifacts, leading to an obvious performance decline on unbiased datasets. Among the various debiasing works, the causal inference-based methods become popular by performing theoretically guaranteed debiasing such as casual intervention or <b>counterfactual</b> <b>reasoning.</b> However, existing causal inference-based debiasing methods, which mainly formulate <b>fact</b> <b>verification</b> as a single-hop <b>reasoning</b> task to tackle shallow bias patterns, cannot deal with the complicated bias patterns hidden in multiple hops of evidence. To address the challenge, we propose Causal Walk, a novel method for debiasing multi-hop <b>fact</b> <b>verification</b> from a causal perspective with front-door adjustment. Specifically, in the structural causal model, the <b>reasoning</b> path between the treatment (the input claim-evidence <b>graph)</b> and the outcome (the veracity label) is introduced as the mediator to block the confounder. With the front-door adjustment, the causal effect between the treatment and the outcome is decomposed into the causal effect between the treatment and the mediator, which is estimated by applying the idea of random walk, and the causal effect between the mediator and the outcome, which is estimated with normalized weighted geometric mean approximation. To investigate the effectiveness of the proposed method, an adversarial multi-hop <b>fact</b> <b>verification</b> dataset and a symmetric multi-hop <b>fact</b> <b>verification</b> dataset are proposed with the help of the <b>large</b> <b>language</b> <b>model.</b> Experimental results show that Causal Walk outperforms some previous debiasing methods on both existing datasets and the newly constructed datasets. Code and data will be released at <a href=https://github.com/zcccccz/CausalWalk>https://github.com/zcccccz/CausalWalk</a>.</p></p class="citation"></blockquote><h3 id=2256--22290-in-dialogues-we-learn-towards-personalized-dialogue-without-pre-defined-profiles-through-in-dialogue-learning-chuanqi-cheng-et-al-2024>(22/56 | 22/290) &lsquo;In Dialogues We Learn&rsquo;: Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning (Chuanqi Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuanqi Cheng, Quan Tu, Wei Wu, Shuo Shang, Cunli Mao, Zhengtao Yu, Rui Yan. (2024)<br><strong>&lsquo;In Dialogues We Learn&rsquo;: Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning</strong><br><button class=copy-to-clipboard title="'In Dialogues We Learn': Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Dialogue System, BLEU, Large Language Model, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03102v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03102v2.pdf filename=2403.03102v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personalized <b>dialogue</b> <b>systems</b> have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a <b>fine-tuning</b> framework that enhances the ability of pre-trained <b>large</b> <b>language</b> <b>models</b> to leverage <b>dialogue</b> <b>history</b> to characterize persona for completing personalized <b>dialogue</b> <b>generation</b> tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with <b>BLEU</b> and <b>ROUGE</b> scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.</p></p class="citation"></blockquote><h3 id=2356--23290-an-empirical-study-of-llm-as-a-judge-for-llm-evaluation-fine-tuned-judge-models-are-task-specific-classifiers-hui-huang-et-al-2024>(23/56 | 23/290) An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers (Hui Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, Tiejun Zhao. (2024)<br><strong>An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers</strong><br><button class=copy-to-clipboard title="An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fairness, Fine-tuning, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02839v1.pdf filename=2403.02839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been a growing trend of utilizing <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> to evaluate the quality of other <b>LLMs.</b> Many studies have employed proprietary close-source models, especially <b>GPT4,</b> as the evaluator. Alternatively, other works have <b>fine-tuned</b> judge models based on open-source <b>LLMs</b> as the evaluator. In this study, we conduct an empirical study of different judge models on their evaluation capability. Our findings indicate that although the <b>fine-tuned</b> judge models achieve high accuracy on in-domain test sets, even surpassing <b>GPT4,</b> they are inherently task-specific classifiers, and their generalizability and <b>fairness</b> severely underperform <b>GPT4.</b></p></p class="citation"></blockquote><h3 id=2456--24290-revisiting-meta-evaluation-for-grammatical-error-correction-masamune-kobayashi-et-al-2024>(24/56 | 24/290) Revisiting Meta-evaluation for Grammatical Error Correction (Masamune Kobayashi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masamune Kobayashi, Masato Mita, Mamoru Komachi. (2024)<br><strong>Revisiting Meta-evaluation for Grammatical Error Correction</strong><br><button class=copy-to-clipboard title="Revisiting Meta-evaluation for Grammatical Error Correction" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Automatic Evaluation, Grammatical Error Correction, Grammatical Error Correction, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02674v1.pdf filename=2403.02674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Metrics are the foundation for <b>automatic</b> <b>evaluation</b> in <b>grammatical</b> <b>error</b> <b>correction</b> <b>(GEC),</b> with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English <b>GEC</b> encounter several challenges including biases caused by inconsistencies in evaluation granularity, and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of <b>GEC</b> techniques. To address these issues, this paper proposes SEEDA, a new dataset for <b>GEC</b> meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation, suggest that edit-based metrics may have been underestimated in existing studies. Furthermore, correlations of most metrics decrease when changing from classical to neural systems, indicating that traditional metrics are relatively poor at evaluating fluently corrected sentences with many edits.</p></p class="citation"></blockquote><h3 id=2556--25290-updating-the-minimum-information-about-clinical-artificial-intelligence-mi-claim-checklist-for-generative-modeling-research-brenda-y-miao-et-al-2024>(25/56 | 25/290) Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research (Brenda Y. Miao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brenda Y. Miao, Irene Y. Chen, Christopher YK Williams, Jaysón Davidson, Augusto Garcia-Agundez, Harry Sun, Travis Zack, Atul J. Butte, Madhumita Sushil. (2024)<br><strong>Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research</strong><br><button class=copy-to-clipboard title="Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 50<br>Keywords: Diffusion Model, Few-shot, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02558v1.pdf filename=2403.02558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in generative models, including <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> vision language models (VLMs), and <b>diffusion</b> <b>models,</b> have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed. While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks. In particular, the ability of these models to produce useful outputs with little to no specialized training data (&ldquo;zero-&rdquo; or <b>&ldquo;few-shot&rdquo;</b> approaches), as well as the open-ended nature of their outputs, necessitate the development of updated guidelines in using and evaluating these models. In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we begin to formalize some of these guidelines by building on the &ldquo;Minimum information about clinical artificial intelligence modeling&rdquo; (MI-CLAIM) checklist. The MI-CLAIM checklist, originally developed in 2020, provided a set of six steps with guidelines on the minimum information necessary to encourage transparent, reproducible research for artificial intelligence (AI) in medicine. Here, we propose modifications to the original checklist that highlight differences in training, evaluation, interpretability, and reproducibility of generative models compared to traditional AI models for clinical research. This updated checklist also seeks to clarify cohort selection reporting and adds additional items on alignment with ethical standards.</p></p class="citation"></blockquote><h3 id=2656--26290-adding-multimodal-capabilities-to-a-text-only-translation-model-vipin-vijayan-et-al-2024>(26/56 | 26/290) Adding Multimodal Capabilities to a Text-only Translation Model (Vipin Vijayan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vipin Vijayan, Braeden Bowen, Scott Grigsby, Timothy Anderson, Jeremy Gwinnup. (2024)<br><strong>Adding Multimodal Capabilities to a Text-only Translation Model</strong><br><button class=copy-to-clipboard title="Adding Multimodal Capabilities to a Text-only Translation Model" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 46<br>Keywords: Graph Attention Networks, Fine-tuning, Multi-modal, Multi-modal, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03045v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03045v1.pdf filename=2403.03045v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While most current work in <b>multimodal</b> <b>machine</b> <b>translation</b> (MMT) uses the Multi30k dataset for training and evaluation, we find that the resulting models overfit to the Multi30k dataset to an extreme degree. Consequently, these models perform very badly when evaluated against typical text-only testing sets such as the WMT newstest datasets. In order to perform well on both Multi30k and typical text-only datasets, we use a performant text-only <b>machine</b> <b>translation</b> <b>(MT)</b> model as the starting point of our MMT model. We add vision-text adapter layers connected via <b>gating</b> mechanisms to the <b>MT</b> model, and incrementally transform the <b>MT</b> model into an MMT model by 1) pre-training using vision-based masking of the source text and 2) <b>fine-tuning</b> on Multi30k.</p></p class="citation"></blockquote><h3 id=2756--27290-simucourt-building-judicial-decision-making-agents-with-real-world-judgement-documents-zhitao-he-et-al-2024>(27/56 | 27/290) SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents (Zhitao He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhitao He, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, Jun Zhao. (2024)<br><strong>SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents</strong><br><button class=copy-to-clipboard title="SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Simulation, Simulator, Information Retrieval, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02959v1.pdf filename=2403.02959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by <b>large</b> <b>language</b> <b>models</b> are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial <b>benchmark</b> that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a <b>large-scale</b> <b>judicial</b> <b>knowledge</b> base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt. Our framework follows the real-world classic court trial process, consisting of court debate <b>simulation,</b> legal <b>information</b> <b>retrieval</b> and judgement refinement to simulate the decision-making of judge. (3) we perform extensive experiments, the results demonstrate that, our framework outperforms the existing advanced methods in various aspects, especially in generating legal grounds, where our model achieves significant improvements of 8.6% and 9.1% F1 score in the first and second instance settings, respectively.</p></p class="citation"></blockquote><h3 id=2856--28290-crossing-linguistic-horizons-finetuning-and-comprehensive-evaluation-of-vietnamese-large-language-models-sang-t-truong-et-al-2024>(28/56 | 28/290) Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models (Sang T. Truong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sang T. Truong, Duc Q. Nguyen, Toan Nguyen, Dong D. Le, Nhi N. Truong, Tho Quan, Sanmi Koyejo. (2024)<br><strong>Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models</strong><br><button class=copy-to-clipboard title="Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02715v1.pdf filename=2403.02715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced <b>LLMs</b> exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic <b>benchmark</b> datasets and metrics tailored for Vietnamese <b>LLM</b> evaluation. To mitigate these issues, we have <b>finetuned</b> <b>LLMs</b> specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the <b>fine-tuned</b> <b>LLMs</b> exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing <b>LLM</b> performance is the quality of the training or <b>fine-tuning</b> datasets. These insights underscore the significance of meticulous <b>fine-tuning</b> with high-quality datasets in enhancing <b>LLM</b> performance.</p></p class="citation"></blockquote><h3 id=2956--29290-android-in-the-zoo-chain-of-action-thought-for-gui-agents-jiwen-zhang-et-al-2024>(29/56 | 29/290) Android in the Zoo: Chain-of-Action-Thought for GUI Agents (Jiwen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, Duyu Tang. (2024)<br><strong>Android in the Zoo: Chain-of-Action-Thought for GUI Agents</strong><br><button class=copy-to-clipboard title="Android in the Zoo: Chain-of-Action-Thought for GUI Agents" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-HC, cs-LG, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02713v1.pdf filename=2403.02713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>model</b> <b>(LLM)</b> leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a <b>zero-shot</b> setting upon an off-the-shell <b>LLM,</b> CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a <b>benchmark</b> Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pairs together with chain-of-action-thought annotations. Experiments show that <b>fine-tuning</b> a 200M model on our AitZ dataset achieves on par performance with CogAgent-Chat-18B.</p></p class="citation"></blockquote><h3 id=3056--30290-guardrail-baselines-for-unlearning-in-llms-pratiksha-thaker-et-al-2024>(30/56 | 30/290) Guardrail Baselines for Unlearning in LLMs (Pratiksha Thaker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pratiksha Thaker, Yash Maurya, Virginia Smith. (2024)<br><strong>Guardrail Baselines for Unlearning in LLMs</strong><br><button class=copy-to-clipboard title="Guardrail Baselines for Unlearning in LLMs" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03329v1.pdf filename=2403.03329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work has demonstrated that <b>fine-tuning</b> is a promising approach to `unlearn&rsquo; concepts from <b>large</b> <b>language</b> <b>models.</b> However, <b>fine-tuning</b> can be expensive, as it requires both generating a set of examples and running iterations of <b>fine-tuning</b> to update the model. In this work, we show that simple guardrail-based approaches such as <b>prompting</b> and filtering can achieve unlearning results comparable to <b>fine-tuning.</b> We recommend that researchers investigate these lightweight baselines when evaluating the performance of more computationally intensive <b>fine-tuning</b> methods. While we do not claim that methods such as <b>prompting</b> or filtering are universal solutions to the problem of unlearning, our work suggests the need for evaluation metrics that can better separate the power of guardrails vs. <b>fine-tuning,</b> and highlights scenarios where guardrails themselves may be advantageous for unlearning, such as in generating examples for <b>fine-tuning</b> or unlearning when only API access is available.</p></p class="citation"></blockquote><h3 id=3156--31290-book2dial-generating-teacher-student-interactions-from-textbooks-for-cost-effective-development-of-educational-chatbots-junling-wang-et-al-2024>(31/56 | 31/290) Book2Dial: Generating Teacher-Student Interactions from Textbooks for Cost-Effective Development of Educational Chatbots (Junling Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junling Wang, Jakub Macina, Nico Daheim, Sankalan Pal Chowdhury, Mrinmaya Sachan. (2024)<br><strong>Book2Dial: Generating Teacher-Student Interactions from Textbooks for Cost-Effective Development of Educational Chatbots</strong><br><button class=copy-to-clipboard title="Book2Dial: Generating Teacher-Student Interactions from Textbooks for Cost-Effective Development of Educational Chatbots" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Chatbot, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03307v1.pdf filename=2403.03307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Educational <b>chatbots</b> are a promising tool for assisting student learning. However, the development of effective <b>chatbots</b> in education has been challenging, as high-quality data is seldom available in this domain. In this paper, we propose a framework for generating synthetic teacher-student interactions grounded in a set of textbooks. Our approaches capture one aspect of learning interactions where curious students with partial knowledge interactively ask a teacher questions about the material in the textbook. We highlight various quality criteria that such dialogues should fulfill and compare several approaches relying on either <b>prompting</b> or <b>fine-tuning</b> <b>large</b> <b>language</b> <b>models.</b> We use synthetic dialogues to train educational <b>chatbots</b> and show benefits of further <b>fine-tuning</b> in different educational domains. However, human evaluation shows that our best data synthesis method still suffers from hallucinations and tends to reiterate information from previous conversations. Our findings offer insights for future efforts in synthesizing conversational data that strikes a balance between size and quality. We will open-source our data and code.</p></p class="citation"></blockquote><h3 id=3256--32290-demonstrating-mutual-reinforcement-effect-through-information-flow-chengguang-gan-et-al-2024>(32/56 | 32/290) Demonstrating Mutual Reinforcement Effect through Information Flow (Chengguang Gan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengguang Gan, Xuzheng He, Qinghao Zhang, Tatsunori Mori. (2024)<br><strong>Demonstrating Mutual Reinforcement Effect through Information Flow</strong><br><button class=copy-to-clipboard title="Demonstrating Mutual Reinforcement Effect through Information Flow" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Text Classification, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02902v1.pdf filename=2403.02902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and <b>text-level</b> <b>classifications</b> in <b>text</b> <b>classification</b> tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demonstrated or explained in prior research. To address this gap, we employ information flow analysis to observe and substantiate the MRE theory. Our experiments on six MRE hybrid datasets revealed the presence of MRE in the model and its impact. Additionally, we conducted <b>fine-tuning</b> experiments, whose results were consistent with those of the information flow experiments. The convergence of findings from both experiments corroborates the existence of MRE. Furthermore, we extended the application of MRE to <b>prompt</b> <b>learning,</b> utilizing word-level information as a verbalizer to bolster the model&rsquo;s prediction of <b>text-level</b> <b>classification</b> labels. In our final experiment, the F1-score significantly surpassed the baseline in five out of six datasets, further validating the notion that word-level information enhances the language model&rsquo;s comprehension of the <b>text</b> <b>as</b> a whole.</p></p class="citation"></blockquote><h3 id=3356--33290-in-search-of-truth-an-interrogation-approach-to-hallucination-detection-yakir-yehuda-et-al-2024>(33/56 | 33/290) In Search of Truth: An Interrogation Approach to Hallucination Detection (Yakir Yehuda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yakir Yehuda, Itzik Malkiel, Oren Barkan, Jonathan Weill, Royi Ronen, Noam Koenigstein. (2024)<br><strong>In Search of Truth: An Interrogation Approach to Hallucination Detection</strong><br><button class=copy-to-clipboard title="In Search of Truth: An Interrogation Approach to Hallucination Detection" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: LLaMA, Hallucination Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02889v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02889v1.pdf filename=2403.02889v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the many advances of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of <b>hallucinations,</b> <b>where</b> <b>LLMs</b> invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting <b>hallucinations</b> <b>in</b> <b>large</b> <b>language</b> <b>models,</b> which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and <b>LLMs,</b> including <b>Llama-2,</b> we study the <b>hallucination</b> <b>levels</b> of various recent <b>LLMs</b> and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 62% <b>hallucinations</b> <b>for</b> <b>Llama-2</b> in a specific experiment, where our method achieves a Balanced Accuracy (B-ACC) of 87%, all without relying on external knowledge.</p></p class="citation"></blockquote><h3 id=3456--34290-magid-an-automated-pipeline-for-generating-synthetic-multi-modal-datasets-hossein-aboutalebi-et-al-2024>(34/56 | 34/290) MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets (Hossein Aboutalebi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hossein Aboutalebi, Hwanjun Song, Yusheng Xie, Arshit Gupta, Justin Sun, Hang Su, Igor Shalyminov, Nikolaos Pappas, Siffi Singh, Saab Mansour. (2024)<br><strong>MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets</strong><br><button class=copy-to-clipboard title="MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Diffusion Model, Multi-modal, Multi-modal, Image2text, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03194v1.pdf filename=2403.03194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Development of <b>multimodal</b> interactive systems is hindered by the lack of rich, <b>multimodal</b> (text, images) conversational data, which is needed in large quantities for <b>LLMs.</b> Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce \textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative \textbf{I}mages \textbf{D}ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images. Subsequently, a <b>diffusion</b> <b>model</b> is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual <b>LLM)</b> and image quality modules (addressing aesthetics, <b>image-text</b> matching, and safety), that work in tandem to generate high-quality and <b>multi-modal</b> dialogues. We compare MAGID to other SOTA baselines on three dialogue datasets, using automated and human evaluation. Our results show that MAGID is comparable to or better than baselines, with significant improvements in human evaluation, especially against retrieval baselines where the image database is small.</p></p class="citation"></blockquote><h3 id=3556--35290-detecting-concrete-visual-tokens-for-multimodal-machine-translation-braeden-bowen-et-al-2024>(35/56 | 35/290) Detecting Concrete Visual Tokens for Multimodal Machine Translation (Braeden Bowen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Braeden Bowen, Vipin Vijayan, Scott Grigsby, Timothy Anderson, Jeremy Gwinnup. (2024)<br><strong>Detecting Concrete Visual Tokens for Multimodal Machine Translation</strong><br><button class=copy-to-clipboard title="Detecting Concrete Visual Tokens for Multimodal Machine Translation" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Object Detection, Multi-modal, Multi-modal, Grounding, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03075v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03075v1.pdf filename=2403.03075v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The challenge of visual <b>grounding</b> and masking in <b>multimodal</b> <b>machine</b> <b>translation</b> (MMT) systems has encouraged varying approaches to the detection and selection of visually-grounded text tokens for masking. We introduce new methods for detection of visually and contextually relevant (concrete) tokens from source sentences, including detection with natural language processing (NLP), detection with <b>object</b> <b>detection,</b> and a joint detection-verification technique. We also introduce new methods for selection of detected tokens, including shortest $n$ tokens, longest $n$ tokens, and all detected concrete tokens. We utilize the GRAM MMT architecture to train models against synthetically collated <b>multimodal</b> datasets of source images with masked sentences, showing performance improvements and improved usage of visual context during translation tasks over the baseline model.</p></p class="citation"></blockquote><h3 id=3656--36290-alpaca-against-vicuna-using-llms-to-uncover-memorization-of-llms-aly-m-kassem-et-al-2024>(36/56 | 36/290) Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs (Aly M. Kassem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aly M. Kassem, Omar Mahmoud, Niloofar Mireshghallah, Hyunwoo Kim, Yulia Tsvetkov, Yejin Choi, Sherif Saad, Santu Rana. (2024)<br><strong>Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs</strong><br><button class=copy-to-clipboard title="Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 35<br>Keywords: Black Box, Alpaca, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04801v1.pdf filename=2403.04801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a <b>black-box</b> <b>prompt</b> optimization method that uses an attacker <b>LLM</b> agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by <b>prompting</b> the target model with the training data directly, which is the dominant approach of quantifying memorization in <b>LLMs.</b> We use an iterative rejection-sampling optimization process to find instruction-based <b>prompts</b> with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2) maximal overlap between the victim model&rsquo;s output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based <b>prompts</b> generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other <b>LLMs</b> can open a new avenue of automated attacks that we should further study and explore. The code can be found at <a href=https://github.com/Alymostafa/Instruction_based_attack>https://github.com/Alymostafa/Instruction_based_attack</a> .</p></p class="citation"></blockquote><h3 id=3756--37290-data-augmentation-using-llms-data-perspectives-learning-paradigms-and-challenges-bosheng-ding-et-al-2024>(37/56 | 37/290) Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges (Bosheng Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen Chen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, Shafiq Joty. (2024)<br><strong>Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges</strong><br><button class=copy-to-clipboard title="Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Data Augmentation, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02990v1.pdf filename=2403.02990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving field of machine learning (ML), <b>data</b> <b>augmentation</b> (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional <b>data</b> <b>collection.</b> This survey explores the transformative impact of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a <b>data</b> <b>perspective</b> and a learning perspective, we examine various strategies that utilize <b>Large</b> <b>Language</b> <b>Models</b> for <b>data</b> <b>augmentation,</b> including a novel exploration of learning paradigms where <b>LLM-generated</b> <b>data</b> <b>is</b> used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable <b>data</b> <b>augmentation</b> to multi modal <b>data</b> <b>augmentation.</b> This survey highlights the paradigm shift introduced by <b>LLMs</b> in DA, aims to serve as a foundational guide for researchers and practitioners in this field.</p></p class="citation"></blockquote><h3 id=3856--38290-breeze-7b-technical-report-chan-jan-hsu-et-al-2024>(38/56 | 38/290) Breeze-7B Technical Report (Chan-Jan Hsu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chan-Jan Hsu, Chang-Le Liu, Feng-Ting Liao, Po-Chun Hsu, Yi-Chang Chen, Da-Shan Shiu. (2024)<br><strong>Breeze-7B Technical Report</strong><br><button class=copy-to-clipboard title="Breeze-7B Technical Report" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Mistral, Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02712v1.pdf filename=2403.02712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Breeze-7B is an open-source language model based on <b>Mistral-7B,</b> designed to address the need for improved language comprehension and <b>chatbot-oriented</b> capabilities in Traditional Chinese. This technical report provides an overview of the additional pretraining, <b>finetuning,</b> and evaluation stages for the Breeze-7B model. The Breeze-7B family of base and chat models exhibits good performance on language comprehension and <b>chatbot-oriented</b> tasks, reaching the top in several <b>benchmarks</b> among models comparable in its complexity class.</p></p class="citation"></blockquote><h3 id=3956--39290-found-in-the-middle-how-language-models-use-long-contexts-better-via-plug-and-play-positional-encoding-zhenyu-zhang-et-al-2024>(39/56 | 39/290) Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding (Zhenyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi Chen, Xiaoxia Wu, Zhangyang Wang. (2024)<br><strong>Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding</strong><br><button class=copy-to-clipboard title="Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04797v1.pdf filename=2403.04797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper aims to overcome the &ldquo;lost-in-the-middle&rdquo; challenge of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> While recent advancements have successfully enabled <b>LLMs</b> to perform stable language modeling with up to 4 million tokens, the persistent difficulty faced by most <b>LLMs</b> in identifying relevant information situated in the middle of the context has not been adequately tackled. To address this problem, this paper introduces Multi-scale Positional Encoding (Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the capacity of <b>LLMs</b> to handle the relevant information located in the middle of the context, without <b>fine-tuning</b> or introducing any additional overhead. Ms-PoE leverages the position indice rescaling to relieve the long-term decay effect introduced by RoPE, while meticulously assigning distinct scaling ratios to different attention heads to preserve essential knowledge learned during the pre-training step, forming a multi-scale context fusion from short to long distance. Extensive experiments with a wide range of <b>LLMs</b> demonstrate the efficacy of our approach. Notably, Ms-PoE achieves an average accuracy gain of up to 3.8 on the Zero-SCROLLS <b>benchmark</b> over the original <b>LLMs.</b> Code are available at <a href=https://github.com/VITA-Group/Ms-PoE>https://github.com/VITA-Group/Ms-PoE</a>.</p></p class="citation"></blockquote><h3 id=4056--40290-exploring-the-limitations-of-large-language-models-in-compositional-relation-reasoning-jinman-zhao-et-al-2024>(40/56 | 40/290) Exploring the Limitations of Large Language Models in Compositional Relation Reasoning (Jinman Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinman Zhao, Xueyan Zhang. (2024)<br><strong>Exploring the Limitations of Large Language Models in Compositional Relation Reasoning</strong><br><button class=copy-to-clipboard title="Exploring the Limitations of Large Language Models in Compositional Relation Reasoning" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02615v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02615v1.pdf filename=2403.02615v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a comprehensive evaluation of <b>large</b> <b>language</b> <b>models(LLMs)&rsquo;</b> ability to reason about composition relations through a <b>benchmark</b> encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations: Positional, Comparative, Personal, Mathematical, Identity, and Other. Acknowledging the significance of multilingual capabilities, we expanded our assessment to include translations of these cases into Chinese, Japanese, French, and Korean. Our Multilingual Composition Relation (MCR) <b>benchmark</b> aims at investigating the robustness and adaptability of <b>LLMs</b> in handling composition relation <b>reasoning</b> across diverse linguistic contexts.</p></p class="citation"></blockquote><h3 id=4156--41290-best-of-both-worlds-a-pliable-and-generalizable-neuro-symbolic-approach-for-relation-classification-robert-vacareanu-et-al-2024>(41/56 | 41/290) Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification (Robert Vacareanu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Vacareanu, Fahmida Alam, Md Asiful Islam, Haris Riaz, Mihai Surdeanu. (2024)<br><strong>Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification</strong><br><button class=copy-to-clipboard title="Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Few-shot, Human Intervention, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03305v1.pdf filename=2403.03305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel neuro-symbolic architecture for relation classification (RC) that combines rule-based methods with contemporary deep learning techniques. This approach capitalizes on the strengths of both paradigms: the adaptability of rule-based systems and the generalization power of neural networks. Our architecture consists of two components: a declarative rule-based model for transparent classification and a neural component to enhance rule generalizability through semantic text matching. Notably, our semantic matcher is trained in an <b>unsupervised</b> domain-agnostic way, solely with synthetic data. Further, these components are loosely coupled, allowing for rule modifications without retraining the semantic matcher. In our evaluation, we focused on two <b>few-shot</b> relation classification datasets: <b>Few-Shot</b> TACRED and a <b>Few-Shot</b> version of NYT29. We show that our proposed method outperforms previous state-of-the-art models in three out of four settings, despite not seeing any <b>human-annotated</b> <b>training</b> data. Further, we show that our approach remains modular and pliable, i.e., the corresponding rules can be locally modified to improve the overall model. <b>Human</b> <b>interventions</b> to the rules for the TACRED relation \texttt{org:parents} boost the performance on that relation by as much as 26% relative improvement, without negatively impacting the other relations, and without retraining the semantic matching component.</p></p class="citation"></blockquote><h3 id=4256--42290-cogenesis-a-framework-collaborating-large-and-small-language-models-for-secure-context-aware-instruction-following-kaiyan-zhang-et-al-2024>(42/56 | 42/290) CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following (Kaiyan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiyan Zhang, Jianyu Wang, Ermo Hua, Biqing Qi, Ning Ding, Bowen Zhou. (2024)<br><strong>CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following</strong><br><button class=copy-to-clipboard title="CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Instruction Following, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03129v1.pdf filename=2403.03129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend. In contexts laden with user information, enabling models to both safeguard user privacy and execute commands efficiently emerges as an essential research imperative. In this paper, we propose CoGenesis, a collaborative generation framework integrating large (hosted on cloud infrastructure) and small models (deployed on local devices) to address privacy concerns logically. Initially, we design a pipeline to create personalized writing <b>instruction</b> <b>datasets</b> enriched with extensive context details as the testbed of this research issue. Subsequently, we introduce two variants of CoGenesis based on sketch and logits respectively. Our experimental findings, based on our synthesized dataset and two additional open-source datasets, indicate that: 1) Large-scale models perform well when provided with user context but struggle in the absence of such context. 2) While specialized smaller models <b>fine-tuned</b> on the synthetic dataset show promise, they still lag behind their larger counterparts. 3) Our CoGenesis framework, utilizing mixed-scale models, showcases competitive performance, providing a feasible solution to privacy issues.</p></p class="citation"></blockquote><h3 id=4356--43290-angry-men-sad-women-large-language-models-reflect-gendered-stereotypes-in-emotion-attribution-flor-miriam-plaza-del-arco-et-al-2024>(43/56 | 43/290) Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution (Flor Miriam Plaza-del-Arco et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Flor Miriam Plaza-del-Arco, Amanda Cercas Curry, Alba Curry, Gavin Abercrombie, Dirk Hovy. (2024)<br><strong>Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution</strong><br><button class=copy-to-clipboard title="Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03121v1.pdf filename=2403.03121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men&rsquo;s anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art <b>LLMs</b> (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We <b>prompt</b> the models to adopt a gendered persona and attribute emotions to an event like &lsquo;When I had a serious argument with a dear person&rsquo;. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in <b>LLMs</b> allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same <b>LLMs</b> for emotion applications.</p></p class="citation"></blockquote><h3 id=4456--44290-knowagent-knowledge-augmented-planning-for-llm-based-agents-yuqi-zhu-et-al-2024>(44/56 | 44/290) KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents (Yuqi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu Zhang, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen. (2024)<br><strong>KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents</strong><br><button class=copy-to-clipboard title="KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs-LG, cs-MA, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03101v1.pdf filename=2403.03101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated great potential in complex <b>reasoning</b> tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of <b>LLMs</b> by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation. Code is available in <a href=https://github.com/zjunlp/KnowAgent>https://github.com/zjunlp/KnowAgent</a>.</p></p class="citation"></blockquote><h3 id=4556--45290-learning-to-use-tools-via-cooperative-and-interactive-agents-zhengliang-shi-et-al-2024>(45/56 | 45/290) Learning to Use Tools via Cooperative and Interactive Agents (Zhengliang Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengliang Shi, Shen Gao, Xiuyi Chen, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Pengjie Ren, Suzan Verberne, Zhaochun Ren. (2024)<br><strong>Learning to Use Tools via Cooperative and Interactive Agents</strong><br><button class=copy-to-clipboard title="Learning to Use Tools via Cooperative and Interactive Agents" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Grounding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03031v1.pdf filename=2403.03031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tool learning empowers <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> as agents to use external tools to extend their capability. Existing methods employ one single <b>LLM-based</b> agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single <b>LLM</b> to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into <b>Grounding,</b> Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the superiority of our ConAgents (e.g., 6 point improvement over the SOTA baseline). We further provide fine-granularity analysis for the efficiency and consistency of our framework.</p></p class="citation"></blockquote><h3 id=4656--46290-socratic-reasoning-improves-positive-text-rewriting-anmol-goel-et-al-2024>(46/56 | 46/290) Socratic Reasoning Improves Positive Text Rewriting (Anmol Goel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anmol Goel, Nico Daheim, Iryna Gurevych. (2024)<br><strong>Socratic Reasoning Improves Positive Text Rewriting</strong><br><button class=copy-to-clipboard title="Socratic Reasoning Improves Positive Text Rewriting" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03029v1.pdf filename=2403.03029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reframing a negative into a positive thought is at the crux of several cognitive approaches to mental health and psychotherapy that could be made more accessible by <b>large</b> <b>language</b> <b>model-based</b> solutions. Such reframing is typically non-trivial and requires multiple rationalization steps to uncover the underlying issue of a negative thought and transform it to be more positive. However, this rationalization process is currently neglected by both datasets and models which reframe thoughts in one step. In this work, we address this gap by augmenting open-source datasets for positive text rewriting with synthetically-generated Socratic rationales using a novel framework called \textsc{SocraticReframe}. \textsc{SocraticReframe} uses a sequence of question-answer pairs to rationalize the thought rewriting process. We show that such Socratic rationales significantly improve positive text rewriting for different open-source <b>LLMs</b> according to both automatic and human evaluations guided by criteria from psychotherapy research.</p></p class="citation"></blockquote><h3 id=4756--47290-dppa-pruning-method-for-large-language-model-to-model-merging-yaochen-zhu-et-al-2024>(47/56 | 47/290) DPPA: Pruning Method for Large Language Model to Model Merging (Yaochen Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaochen Zhu, Rui Xia, Jiajun Zhang. (2024)<br><strong>DPPA: Pruning Method for Large Language Model to Model Merging</strong><br><button class=copy-to-clipboard title="DPPA: Pruning Method for Large Language Model to Model Merging" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Pruning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02799v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02799v1.pdf filename=2403.02799v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model merging is to combine <b>fine-tuned</b> models derived from multiple domains, with the intent of enhancing the model&rsquo;s proficiency across various domains. The principal concern is the resolution of parameter conflicts. A substantial amount of existing research remedy this issue during the merging stage, with the latest study focusing on resolving this issue throughout the <b>pruning</b> stage. The DARE approach has exhibited promising outcomes when applied to a simplistic <b>fine-tuned</b> model. However, the efficacy of this method tends to wane when employed on complex <b>fine-tuned</b> models that show a significant parameter bias relative to the baseline model. In this paper, we introduce a dual-stage method termed Dynamic <b>Pruning</b> Partition Amplification (DPPA), devised to tackle the challenge of merging complex <b>fine-tuned</b> models. Initially, we introduce Dynamically <b>Pruning</b> (DP), an improved approach based on magnitude <b>pruning,</b> which aim is to enhance performance at higher <b>pruning</b> rates. Subsequently, we propose Dynamically Partition Amplification (DPA), a rescaling strategy, is designed to dynamically amplify parameter partitions in relation to their significance levels. The experimental results show that our method maintains a mere 20% of domain-specific parameters and yet delivers a performance comparable to other methodologies that preserve up to 90% of parameters. Furthermore, our method displays outstanding performance post-pruning, leading to a significant improvement of nearly 20% performance in model merging. We make our code on Github.</p></p class="citation"></blockquote><h3 id=4856--48290-dp-cre-continual-relation-extraction-via-decoupled-contrastive-learning-and-memory-structure-preservation-mengyi-huang-et-al-2024>(48/56 | 48/290) DP-CRE: Continual Relation Extraction via Decoupled Contrastive Learning and Memory Structure Preservation (Mengyi Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengyi Huang, Meng Xiao, Ludi Wang, Yi Du. (2024)<br><strong>DP-CRE: Continual Relation Extraction via Decoupled Contrastive Learning and Memory Structure Preservation</strong><br><button class=copy-to-clipboard title="DP-CRE: Continual Relation Extraction via Decoupled Contrastive Learning and Memory Structure Preservation" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Continual Relation Extraction, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02718v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02718v1.pdf filename=2403.02718v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Continuous <b>Relation</b> <b>Extraction</b> (CRE) aims to incrementally learn <b>relation</b> <b>knowledge</b> from a non-stationary stream of data. Since the introduction of new <b>relational</b> <b>tasks</b> can overshadow previously learned information, catastrophic forgetting becomes a significant challenge in this domain. Current replay-based training paradigms prioritize all data uniformly and train memory samples through multiple rounds, which would result in overfitting old tasks and pronounced bias towards new tasks because of the imbalances of the replay set. To handle the problem, we introduce the DecouPled CRE (DP-CRE) framework that decouples the process of prior information preservation and new knowledge acquisition. This framework examines alterations in the embedding space as new <b>relation</b> <b>classes</b> emerge, distinctly managing the preservation and acquisition of knowledge. Extensive experiments show that DP-CRE significantly outperforms other CRE baselines across two datasets.</p></p class="citation"></blockquote><h3 id=4956--49290-the-case-for-evaluating-multimodal-translation-models-on-text-datasets-vipin-vijayan-et-al-2024>(49/56 | 49/290) The Case for Evaluating Multimodal Translation Models on Text Datasets (Vipin Vijayan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vipin Vijayan, Braeden Bowen, Scott Grigsby, Timothy Anderson, Jeremy Gwinnup. (2024)<br><strong>The Case for Evaluating Multimodal Translation Models on Text Datasets</strong><br><button class=copy-to-clipboard title="The Case for Evaluating Multimodal Translation Models on Text Datasets" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03014v1.pdf filename=2403.03014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A good evaluation framework should evaluate <b>multimodal</b> <b>machine</b> <b>translation</b> (MMT) models by measuring 1) their use of visual information to aid in the translation task and 2) their ability to translate complex sentences such as done for text-only <b>machine</b> <b>translation.</b> However, most current work in MMT is evaluated against the Multi30k testing sets, which do not measure these properties. Namely, the use of visual information by the MMT model cannot be shown directly from the Multi30k test set results and the sentences in Multi30k are are image captions, i.e., short, descriptive sentences, as opposed to complex sentences that typical text-only <b>machine</b> <b>translation</b> models are evaluated against. Therefore, we propose that MMT models be evaluated using 1) the CoMMuTE evaluation framework, which measures the use of visual information by MMT models, 2) the text-only WMT news translation task test sets, which evaluates translation performance against complex sentences, and 3) the Multi30k test sets, for measuring MMT model performance against a real MMT dataset. Finally, we evaluate recent MMT models trained solely against the Multi30k dataset against our proposed evaluation framework and demonstrate the dramatic drop performance against text-only testing sets compared to recent text-only <b>MT</b> models.</p></p class="citation"></blockquote><h3 id=5056--50290-diverse-deciphering-internet-views-on-the-us-military-through-video-comment-stance-analysis-a-novel-benchmark-dataset-for-stance-classification-iain-j-cruickshank-et-al-2024>(50/56 | 50/290) DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification (Iain J. Cruickshank et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Iain J. Cruickshank, Lynnette Hui Xian Ng. (2024)<br><strong>DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification</strong><br><button class=copy-to-clipboard title="DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Stance Detection, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03334v1.pdf filename=2403.03334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Stance</b> <b>detection</b> of social media text is a key component of downstream tasks involving the identification of groups of users with opposing opinions on contested topics such as vaccination and within arguments. In particular, <b>stance</b> <b>provides</b> an indication of an opinion towards an entity. This paper introduces DIVERSE, a dataset of over 173,000 YouTube video comments annotated for their <b>stance</b> <b>towards</b> videos of the U.S. military. The <b>stance</b> <b>is</b> annotated through a human-guided, machine-assisted labeling methodology that makes use of weak signals of tone within the sentence as supporting indicators, as opposed to using manual annotations by humans. These weak signals consist of the presence of hate speech and sarcasm, the presence of specific keywords, the sentiment of the text, and the <b>stance</b> <b>inference</b> from two <b>Large</b> <b>Language</b> <b>Models.</b> The weak signals are then consolidated using a data programming model before each comment is annotated with a final <b>stance</b> <b>label.</b> On average, the videos have 200 comments each, and the <b>stance</b> <b>of</b> the comments skews slightly towards the &ldquo;against&rdquo; characterization for both the U.S. Army and the videos posted on the channel.</p></p class="citation"></blockquote><h3 id=5156--51290-in-memory-learning-a-declarative-learning-framework-for-large-language-models-bo-wang-et-al-2024>(51/56 | 51/290) In-Memory Learning: A Declarative Learning Framework for Large Language Models (Bo Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Wang, Tianxiang Sun, Hang Yan, Siyin Wang, Qingyuan Cheng, Xipeng Qiu. (2024)<br><strong>In-Memory Learning: A Declarative Learning Framework for Large Language Models</strong><br><button class=copy-to-clipboard title="In-Memory Learning: A Declarative Learning Framework for Large Language Models" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Knowledge Distillation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02757v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02757v1.pdf filename=2403.02757v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The exploration of whether agents can align with their environment without relying on human-labeled data presents an intriguing research topic. Drawing inspiration from the alignment process observed in intelligent organisms, where declarative memory plays a pivotal role in summarizing past experiences, we propose a novel learning framework. The agents adeptly <b>distill</b> insights from past experiences, refining and updating existing notes to enhance their performance in the environment. This entire process transpires within the memory components and is implemented through natural language, so we character this framework as In-memory Learning. We also delve into the key features of <b>benchmarks</b> designed to evaluate the self-improvement process. Through systematic experiments, we demonstrate the effectiveness of our framework and provide insights into this problem.</p></p class="citation"></blockquote><h3 id=5256--52290-a-second-look-on-bass----boosting-abstractive-summarization-with-unified-semantic-graphs----a-replication-study-osman-alperen-koraş-et-al-2024>(52/56 | 52/290) A Second Look on BASS &ndash; Boosting Abstractive Summarization with Unified Semantic Graphs &ndash; A Replication Study (Osman Alperen Koraş et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Osman Alperen Koraş, Jörg Schlötterer, Christin Seifert. (2024)<br><strong>A Second Look on BASS &ndash; Boosting Abstractive Summarization with Unified Semantic Graphs &ndash; A Replication Study</strong><br><button class=copy-to-clipboard title="A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 13<br>Keywords: Graph, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02930v1.pdf filename=2403.02930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a detailed replication study of the BASS framework, an abstractive <b>summarization</b> system based on the notion of Unified Semantic <b>Graphs.</b> Our investigation includes challenges in replicating key components and an ablation study to systematically isolate error sources rooted in replicating novel components. Our findings reveal discrepancies in performance compared to the original work. We highlight the significance of paying careful attention even to reasonably omitted details for replicating advanced frameworks like BASS, and emphasize key practices for writing replicable papers.</p></p class="citation"></blockquote><h3 id=5356--53290-reliable-adaptable-and-attributable-language-models-with-retrieval-akari-asai-et-al-2024>(53/56 | 53/290) Reliable, Adaptable, and Attributable Language Models with Retrieval (Akari Asai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih. (2024)<br><strong>Reliable, Adaptable, and Attributable Language Models with Retrieval</strong><br><button class=copy-to-clipboard title="Reliable, Adaptable, and Attributable Language Models with Retrieval" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03187v1.pdf filename=2403.03187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as <b>question</b> <b>answering,</b> have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.</p></p class="citation"></blockquote><h3 id=5456--54290-aix-speed-playback-speed-optimization-using-listening-comprehension-of-speech-recognition-models-kazuki-kawamura-et-al-2024>(54/56 | 54/290) AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models (Kazuki Kawamura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kazuki Kawamura, Jun Rekimoto. (2024)<br><strong>AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models</strong><br><button class=copy-to-clipboard title="AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs-LG, cs-SD, cs.CL, eess-AS<br>Keyword Score: 10<br>Keywords: Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02938v1.pdf filename=2403.02938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension. To further utilize this capability, systems that automatically adjust the playback speed according to the user&rsquo;s condition and the type of content to assist in more efficient comprehension of time-series content have been developed. However, there is still room for these systems to further extend human speed-listening ability by generating <b>speech</b> <b>with</b> playback speed optimized for even finer time units and providing it to humans. In this study, we determine whether humans can hear the optimized <b>speech</b> <b>and</b> propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring <b>speech</b> <b>intelligibility.</b> The system uses the <b>speech</b> <b>recognizer</b> score as a proxy for how well a human can hear a certain unit of <b>speech</b> <b>and</b> maximizes the <b>speech</b> <b>playback</b> speed to the extent that a human can hear. This method can be used to produce fast but intelligible <b>speech.</b> <b>In</b> the evaluation experiment, we compared the <b>speech</b> <b>played</b> back at a constant fast speed and the flexibly speed-up <b>speech</b> <b>generated</b> by the proposed method in a blind test and confirmed that the proposed method produced <b>speech</b> <b>that</b> was easier to listen to.</p></p class="citation"></blockquote><h3 id=5556--55290-ai-literacy-in-low-resource-languagesinsights-from-creating-ai-in-yoruba-videos-wuraola-oyewusi-2024>(55/56 | 55/290) AI Literacy in Low-Resource Languages:Insights from creating AI in Yoruba videos (Wuraola Oyewusi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wuraola Oyewusi. (2024)<br><strong>AI Literacy in Low-Resource Languages:Insights from creating AI in Yoruba videos</strong><br><button class=copy-to-clipboard title="AI Literacy in Low-Resource Languages:Insights from creating AI in Yoruba videos" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs.CL<br>Keyword Score: 10<br>Keywords: Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04799v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04799v1.pdf filename=2403.04799v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To effectively navigate the AI revolution, AI literacy is crucial. However, content predominantly exists in dominant languages, creating a gap for <b>low-resource</b> languages like Yoruba (41 million native speakers). This case study explores bridging this gap by creating and distributing AI videos in Yoruba.The project developed 26 videos covering foundational, intermediate, and advanced AI concepts, leveraging storytelling and accessible explanations. These videos were created using a cost-effective methodology and distributed across YouTube, LinkedIn, and Twitter, reaching an estimated global audience of 22 countries. Analysis of YouTube reveals insights into viewing patterns, with the 25-44 age group contributing the most views. Notably, over half of the traffic originated from external sources, highlighting the potential of cross-platform promotion.This study demonstrates the feasibility and impact of creating AI literacy content in <b>low-resource</b> languages. It emphasizes that accurate interpretation requires both technical expertise in AI and fluency in the target language. This work contributes a replicable methodology, a 22-word Yoruba AI vocabulary, and data-driven insights into audience demographics and acquisition channel</p></p class="citation"></blockquote><h3 id=5656--56290-finreport-explainable-stock-earnings-forecasting-via-news-factor-analyzing-model-xiangyu-li-et-al-2024>(56/56 | 56/290) FinReport: Explainable Stock Earnings Forecasting via News Factor Analyzing Model (Xiangyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyu Li, Xinjie Shen, Yawen Zeng, Xiaofen Xing, Jin Xu. (2024)<br><strong>FinReport: Explainable Stock Earnings Forecasting via News Factor Analyzing Model</strong><br><button class=copy-to-clipboard title="FinReport: Explainable Stock Earnings Forecasting via News Factor Analyzing Model" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02647v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02647v1.pdf filename=2403.02647v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of stock earnings forecasting has received considerable attention due to the demand investors in real-world scenarios. However, compared with financial institutions, it is not easy for ordinary investors to mine factors and analyze news. On the other hand, although <b>large</b> <b>language</b> <b>models</b> in the financial field can serve users in the form of dialogue robots, it still requires users to have financial knowledge to ask reasonable questions. To serve the user experience, we aim to build an automatic system, FinReport, for ordinary investors to collect information, analyze it, and generate reports after summarizing. Specifically, our FinReport is based on financial news announcements and a multi-factor model to ensure the professionalism of the report. The FinReport consists of three modules: news factorization module, return forecasting module, risk assessment module. The news factorization module involves understanding news information and combining it with stock factors, the return forecasting module aim to analysis the impact of news on market sentiment, and the risk assessment module is adopted to control investment risk. Extensive experiments on real-world datasets have well verified the effectiveness and explainability of our proposed FinReport. Our codes and datasets are available at <a href=https://github.com/frinkleko/FinReport>https://github.com/frinkleko/FinReport</a>.</p></p class="citation"></blockquote><h2 id=csai-30>cs.AI (30)</h2><h3 id=130--57290-emerging-synergies-between-large-language-models-and-machine-learning-in-ecommerce-recommendations-xiaonan-xu-et-al-2024>(1/30 | 57/290) Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations (Xiaonan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaonan Xu, Zheng Xu, Zhipeng Ling, Zhengyu Jin, ShuQian Du. (2024)<br><strong>Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations</strong><br><button class=copy-to-clipboard title="Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 100<br>Keywords: Fine-tuning, Recommendation, Recommender System, ChatGPT, GPT, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02760v1.pdf filename=2403.02760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the boom of e-commerce and web applications, <b>recommender</b> <b>systems</b> have become an important part of our daily lives, providing personalized <b>recommendations</b> based on the user&rsquo;s preferences. Although deep neural networks (DNNs) have made significant progress in improving <b>recommendation</b> systems by simulating the interaction between users and items and incorporating their textual information, these DNN-based approaches still have some limitations, such as the difficulty of effectively understanding users&rsquo; interests and capturing textual information. It is not possible to generalize to different seen/unseen <b>recommendation</b> scenarios and reason about their predictions. At the same time, the emergence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> represented by <b>ChatGPT</b> and <b>GPT-4,</b> has revolutionized the fields of natural language processing (NLP) and artificial intelligence (AI) due to their superior capabilities in the basic tasks of language understanding and generation, and their impressive generalization and <b>reasoning</b> capabilities. As a result, recent research has sought to harness the power of <b>LLM</b> to improve <b>recommendation</b> systems. Given the rapid development of this research direction in the field of <b>recommendation</b> systems, there is an urgent need for a systematic review of existing <b>LLM-driven</b> <b>recommendation</b> systems for researchers and practitioners in related fields to gain insight into. More specifically, we first introduced a representative approach to learning user and item representations using <b>LLM</b> as a feature encoder. We then reviewed the latest advances in <b>LLMs</b> techniques for collaborative filtering enhanced <b>recommendation</b> systems from the three paradigms of pre-training, <b>fine-tuning,</b> and <b>prompting.</b> Finally, we had a comprehensive discussion on the future direction of this emerging field.</p></p class="citation"></blockquote><h3 id=230--58290-clevr-poc-reasoning-intensive-visual-question-answering-in-partially-observable-environments-savitha-sam-abraham-et-al-2024>(2/30 | 58/290) CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments (Savitha Sam Abraham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Savitha Sam Abraham, Marjan Alirezaie, Luc De Raedt. (2024)<br><strong>CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments</strong><br><button class=copy-to-clipboard title="CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 93<br>Keywords: Benchmarking, GPT, GPT-4, Question Answering, Reasoning, Visual Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03203v1.pdf filename=2403.03203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of learning and <b>reasoning</b> is high on the research agenda in AI. Nevertheless, there is only a little attention to use existing background knowledge for <b>reasoning</b> about partially observed scenes to answer <b>questions</b> <b>about</b> the scene. Yet, we as humans use such knowledge frequently to infer plausible answers to <b>visual</b> <b>questions</b> <b>(by</b> eliminating all inconsistent ones). Such knowledge often comes in the form of constraints about objects and it tends to be highly domain or environment-specific. We contribute a novel <b>benchmark</b> called CLEVR-POC for <b>reasoning-intensive</b> <b>visual</b> <b>question</b> <b>answering</b> <b>(VQA)</b> in partially observable environments under constraints. In CLEVR-POC, knowledge in the form of logical constraints needs to be leveraged to generate plausible answers to <b>questions</b> <b>about</b> a hidden object in a given partial scene. For instance, if one has the knowledge that all cups are colored either red, green or blue and that there is only one green cup, it becomes possible to deduce the color of an occluded cup as either red or blue, provided that all other cups, including the green one, are observed. Through experiments, we observe that the low performance of pre-trained vision language models like CLIP (~ 22%) and a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> like <b>GPT-4</b> (~ 46%) on CLEVR-POC ascertains the necessity for frameworks that can handle <b>reasoning-intensive</b> tasks where environment-specific background knowledge is available and crucial. Furthermore, our demonstration illustrates that a neuro-symbolic model, which integrates an <b>LLM</b> like <b>GPT-4</b> with a <b>visual</b> <b>perception</b> <b>network</b> and a formal logical reasoner, exhibits exceptional performance on CLEVR-POC.</p></p class="citation"></blockquote><h3 id=330--59290-knowledge-graphs-as-context-sources-for-llm-based-explanations-of-learning-recommendations-hasan-abu-rasheed-et-al-2024>(3/30 | 59/290) Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations (Hasan Abu-Rasheed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hasan Abu-Rasheed, Christian Weber, Madjid Fathi. (2024)<br><strong>Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations</strong><br><button class=copy-to-clipboard title="Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 93<br>Keywords: Graph, Generative AI, Knowledge Graph, Knowledge Graph, Recommendation, GPT, Large Language Model, Large Language Model, Prompt, Rouge, Rouge-L<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03008v1.pdf filename=2403.03008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of personalized education, the provision of comprehensible explanations for learning <b>recommendations</b> is of a great value to enhance the learner&rsquo;s understanding and engagement with the recommended learning content. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and <b>generative</b> <b>AI</b> in general have recently opened new doors for generating human-like explanations, for and along learning <b>recommendations.</b> However, their precision is still far away from acceptable in a sensitive field like education. To harness the abilities of <b>LLMs,</b> while still ensuring a high level of precision towards the intent of the learners, this paper proposes an approach to utilize <b>knowledge</b> <b>graphs</b> <b>(KG)</b> as a source of factual context, for <b>LLM</b> <b>prompts,</b> reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context. We utilize the semantic relations in the <b>knowledge</b> <b>graph</b> to offer curated <b>knowledge</b> <b>about</b> learning <b>recommendations.</b> With domain-experts in the loop, we design the explanation as a textual template, which is filled and completed by the <b>LLM.</b> Domain experts were integrated in the <b>prompt</b> engineering phase as part of a study, to ensure that explanations include information that is relevant to the learner. We evaluate our approach quantitatively using <b>Rouge-N</b> and <b>Rouge-L</b> measures, as well as qualitatively with experts and learners. Our results show an enhanced recall and precision of the generated explanations compared to those generated solely by the <b>GPT</b> model, with a greatly reduced risk of generating imprecise information in the final learning explanation.</p></p class="citation"></blockquote><h3 id=430--60290-domain-agnostic-mutual-prompting-for-unsupervised-domain-adaptation-zhekai-du-et-al-2024>(4/30 | 60/290) Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation (Zhekai Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhekai Du, Xinyao Li, Fengling Li, Ke Lu, Lei Zhu, Jingjing Li. (2024)<br><strong>Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation</strong><br><button class=copy-to-clipboard title="Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 53<br>Keywords: Benchmarking, Knowledge Transfer, Unsupervised Learning, Domain Adaptation, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02899v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02899v1.pdf filename=2403.02899v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional <b>Unsupervised</b> <b>Domain</b> <b>Adaptation</b> (UDA) strives to minimize distribution discrepancy between <b>domains,</b> <b>which</b> neglects to harness rich semantics from data and struggles to handle complex <b>domain</b> <b>shifts.</b> A promising technique is to leverage the <b>knowledge</b> <b>of</b> large-scale pre-trained <b>vision-language</b> models for more guided adaptation. Despite some endeavors, current methods often learn textual <b>prompts</b> to embed <b>domain</b> <b>semantics</b> for source and target <b>domains</b> <b>separately</b> and perform classification within each <b>domain,</b> <b>limiting</b> cross-domain <b>knowledge</b> <b>transfer.</b> Moreover, <b>prompting</b> only the language branch lacks flexibility to adapt both modalities dynamically. To bridge this gap, we propose <b>Domain-Agnostic</b> <b>Mutual</b> <b>Prompting</b> (DAMP) to exploit <b>domain-invariant</b> <b>semantics</b> by mutually aligning visual and textual embeddings. Specifically, the image contextual information is utilized to <b>prompt</b> the language branch in a <b>domain-agnostic</b> <b>and</b> instance-conditioned way. Meanwhile, visual <b>prompts</b> are imposed based on the <b>domain-agnostic</b> <b>textual</b> <b>prompt</b> to elicit <b>domain-invariant</b> <b>visual</b> embeddings. These two branches of <b>prompts</b> are learned mutually with a cross-attention module and regularized with a semantic-consistency loss and an instance-discrimination contrastive loss. Experiments on three UDA <b>benchmarks</b> demonstrate the superiority of DAMP over state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=530--61290-ai-insights-a-case-study-on-utilizing-chatgpt-intelligence-for-research-paper-analysis-anjalee-de-silva-et-al-2024>(5/30 | 61/290) AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis (Anjalee De Silva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anjalee De Silva, Janaka L. Wijekoon, Rashini Liyanarachchi, Rrubaa Panchendrarajan, Weranga Rajapaksha. (2024)<br><strong>AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis</strong><br><button class=copy-to-clipboard title="AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 50<br>Keywords: ChatGPT, GPT, GPT-4, Transformer, Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03293v1.pdf filename=2403.03293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper discusses the effectiveness of leveraging <b>Chatbot:</b> Generative Pre-trained <b>Transformer</b> <b>(ChatGPT)</b> versions 3.5 and 4 for analyzing research papers for effective writing of scientific literature surveys. The study selected the \textit{Application of Artificial Intelligence in Breast Cancer Treatment} as the research topic. Research papers related to this topic were collected from three major publication databases Google Scholar, Pubmed, and Scopus. <b>ChatGPT</b> models were used to identify the category, scope, and relevant information from the research papers for automatic identification of relevant papers related to Breast Cancer Treatment (BCT), organization of papers according to scope, and identification of key information for survey paper writing. Evaluations performed using ground truth data annotated using subject experts reveal, that <b>GPT-4</b> achieves 77.3% accuracy in identifying the research paper categories and 50% of the papers were correctly identified by <b>GPT-4</b> for their scopes. Further, the results demonstrate that <b>GPT-4</b> can generate reasons for its decisions with an average of 27% new words, and 67% of the reasons given by the model were completely agreeable to the subject experts.</p></p class="citation"></blockquote><h3 id=630--62290-easyquant-an-efficient-data-free-quantization-algorithm-for-llms-hanlin-tang-et-al-2024>(6/30 | 62/290) EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs (Hanlin Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanlin Tang, Yifu Sun, Decheng Wu, Kai Liu, Jianchen Zhu, Zhanhui Kang. (2024)<br><strong>EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs</strong><br><button class=copy-to-clipboard title="EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 50<br>Keywords: Model Quantization, Quantization, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02775v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02775v1.pdf filename=2403.02775v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. <b>Model</b> <b>quantization</b> is an effective method for reducing this overhead. The problem is that in most previous works, the <b>quantized</b> <b>model</b> <b>was</b> calibrated using few samples from the training data, which might affect the generalization of the <b>quantized</b> <b>LLMs</b> to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent <b>quantization</b> method for <b>LLMs</b> to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only <b>quantization</b> algorithm for <b>LLMs.</b> Our observation indicates that two factors: outliers in the weight and <b>quantization</b> ranges, are essential for reducing the <b>quantization</b> error. Therefore, in EasyQuant, we leave the outliers (less than 1%) unchanged and optimize the <b>quantization</b> range to reduce the reconstruction error. With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original <b>model.</b> <b>Since</b> EasyQuant does not depend on any training data, the generalization performance of <b>quantized</b> <b>LLMs</b> is safely guaranteed. Moreover, EasyQuant can be implemented in parallel so that the <b>quantized</b> <b>model</b> <b>could</b> be attained in a few minutes even for <b>LLMs</b> over 100B. To our best knowledge, we are the first work that achieves almost lossless <b>quantization</b> performance for <b>LLMs</b> under a data-independent setting and our algorithm runs over 10 times faster than the data-dependent methods.</p></p class="citation"></blockquote><h3 id=730--63290-localized-zeroth-order-prompt-optimization-wenyang-hu-et-al-2024>(7/30 | 63/290) Localized Zeroth-Order Prompt Optimization (Wenyang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenyang Hu, Yao Shu, Zongmin Yu, Zhaoxuan Wu, Xiangqiang Lin, Zhongxiang Dai, See-Kiong Ng, Bryan Kian Hsiang Low. (2024)<br><strong>Localized Zeroth-Order Prompt Optimization</strong><br><button class=copy-to-clipboard title="Localized Zeroth-Order Prompt Optimization" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 45<br>Keywords: Black Box, Gaussian Process, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02993v1.pdf filename=2403.02993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The efficacy of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in understanding and generating natural language has aroused a wide interest in developing <b>prompt-based</b> methods to harness the power of <b>black-box</b> <b>LLMs.</b> Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks. This thus motivates us to re-think the necessity of finding a global optimum in <b>prompt</b> optimization. To answer this, we conduct a thorough empirical study on <b>prompt</b> optimization and draw two major insights. Contrasting with the rarity of global optimum, local optima are usually prevalent and well-performed, which can be more worthwhile for efficient <b>prompt</b> optimization (Insight I). The choice of the input domain, covering both the generation and the representation of <b>prompts,</b> affects the identification of well-performing local optima (Insight II). Inspired by these insights, we propose a novel algorithm, namely localized zeroth-order <b>prompt</b> optimization (ZOPO), which incorporates a Neural Tangent Kernel-based derived <b>Gaussian</b> <b>process</b> into standard zeroth-order optimization for an efficient search of well-performing local optima in <b>prompt</b> optimization. Remarkably, ZOPO outperforms existing baselines in terms of both the optimization performance and the query efficiency, which we demonstrate through extensive experiments.</p></p class="citation"></blockquote><h3 id=830--64290-towards-democratized-flood-risk-management-an-advanced-ai-assistant-enabled-by-gpt-4-for-enhanced-interpretability-and-public-engagement-rafaela-martelo-et-al-2024>(8/30 | 64/290) Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement (Rafaela Martelo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rafaela Martelo, Ruo-Qian Wang. (2024)<br><strong>Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement</strong><br><button class=copy-to-clipboard title="Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-1; I-2-7; J-2, cs-AI, cs-CY, cs-HC, cs.AI<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Natural Language Understanding, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03188v1.pdf filename=2403.03188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-time flood forecasting plays a crucial role in enabling timely and effective emergency responses. However, a significant challenge lies in bridging the gap between complex numerical flood models and practical decision-making. Decision-makers often rely on experts to interpret these models for optimizing flood mitigation strategies. And the public requires complex techniques to inquiry and understand socio-cultural and institutional factors, often hinders the public&rsquo;s understanding of flood risks. To overcome these challenges, our study introduces an innovative solution: a customized AI Assistant powered by the <b>GPT-4</b> <b>Large</b> <b>Language</b> <b>Model.</b> This AI Assistant is designed to facilitate effective communication between decision-makers, the general public, and flood forecasters, without the requirement of specialized knowledge. The new framework utilizes <b>GPT-4&rsquo;s</b> advanced <b>natural</b> <b>language</b> <b>understanding</b> and function calling capabilities to provide immediate flood alerts and respond to various flood-related inquiries. Our developed prototype integrates real-time flood warnings with flood maps and social vulnerability data. It also effectively translates complex flood zone information into actionable risk management advice. To assess its performance, we evaluated the prototype using six criteria within three main categories: relevance, error resilience, and understanding of context. Our research marks a significant step towards a more accessible and user-friendly approach in flood risk management. This study highlights the potential of advanced AI tools like <b>GPT-4</b> in democratizing information and enhancing public engagement in critical social and environmental issues.</p></p class="citation"></blockquote><h3 id=930--65290-evolution-transformer-in-context-evolutionary-optimization-robert-tjarko-lange-et-al-2024>(9/30 | 65/290) Evolution Transformer: In-Context Evolutionary Optimization (Robert Tjarko Lange et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Tjarko Lange, Yingtao Tian, Yujin Tang. (2024)<br><strong>Evolution Transformer: In-Context Evolutionary Optimization</strong><br><button class=copy-to-clipboard title="Evolution Transformer: In-Context Evolutionary Optimization" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-NE, cs.AI<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Supervised Learning, Transformer, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02985v1.pdf filename=2403.02985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evolutionary optimization algorithms are often derived from loose biological analogies and struggle to leverage information obtained during the sequential course of optimization. An alternative promising approach is to leverage data and directly discover powerful optimization principles via meta-optimization. In this work, we follow such a paradigm and introduce Evolution <b>Transformer,</b> a causal <b>Transformer</b> architecture, which can flexibly characterize a family of Evolution Strategies. Given a trajectory of evaluations and search distribution statistics, Evolution <b>Transformer</b> outputs a performance-improving update to the search distribution. The architecture imposes a set of suitable inductive biases, i.e. the invariance of the distribution update to the order of population members within a generation and equivariance to the order of the search dimensions. We train the model weights using Evolutionary Algorithm <b>Distillation,</b> a technique for <b>supervised</b> optimization of sequence models using teacher algorithm trajectories. The resulting model exhibits strong <b>in-context</b> optimization performance and shows strong generalization capabilities to otherwise challenging neuroevolution tasks. We analyze the resulting properties of the Evolution <b>Transformer</b> and propose a technique to fully self-referentially train the Evolution <b>Transformer,</b> starting from a random initialization and bootstrapping its own learning progress. We provide an open source implementation under <a href=https://github.com/RobertTLange/evosax>https://github.com/RobertTLange/evosax</a>.</p></p class="citation"></blockquote><h3 id=1030--66290-a-comprehensive-survey-on-process-oriented-automatic-text-summarization-with-exploration-of-llm-based-methods-hanlei-jin-et-al-2024>(10/30 | 66/290) A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods (Hanlei Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanlei Jin, Yang Zhang, Dan Meng, Jun Wang, Jinghua Tan. (2024)<br><strong>A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods</strong><br><button class=copy-to-clipboard title="A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: Text Summarization, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02901v1.pdf filename=2403.02901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic <b>Text</b> <b>Summarization</b> (ATS), utilizing Natural Language Processing (NLP) algorithms, aims to create concise and accurate summaries, thereby significantly reducing the human effort required in processing <b>large</b> <b>volumes</b> <b>of</b> <b>text.</b> <b>ATS</b> has drawn considerable interest in both academic and industrial circles. Many studies have been conducted in the past to survey ATS methods; however, they generally lack practicality for real-world implementations, as they often categorize previous methods from a theoretical standpoint. Moreover, the advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has altered conventional ATS methods. In this survey, we aim to 1) provide a comprehensive overview of ATS from a ``Process-Oriented Schema&rsquo;&rsquo; perspective, which is best aligned with real-world implementations; 2) comprehensively review the latest <b>LLM-based</b> ATS works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in the literature. To the best of our knowledge, this is the first survey to specifically investigate <b>LLM-based</b> ATS methods.</p></p class="citation"></blockquote><h3 id=1130--67290-evaluating-and-optimizing-educational-content-with-large-language-model-judgments-joy-he-yueya-et-al-2024>(11/30 | 67/290) Evaluating and Optimizing Educational Content with Large Language Model Judgments (Joy He-Yueya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joy He-Yueya, Noah D. Goodman, Emma Brunskill. (2024)<br><strong>Evaluating and Optimizing Educational Content with Large Language Model Judgments</strong><br><button class=copy-to-clipboard title="Evaluating and Optimizing Educational Content with Large Language Model Judgments" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 40<br>Keywords: GPT, GPT-3, GPT-3.5, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02795v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02795v1.pdf filename=2403.02795v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Creating effective educational materials generally requires expensive and time-consuming studies of student learning outcomes. To overcome this barrier, one idea is to build computational models of student learning and use them to optimize instructional materials. However, it is difficult to model the cognitive processes of learning dynamics. We propose an alternative approach that uses Language Models (LMs) as educational experts to assess the impact of various instructions on learning outcomes. Specifically, we use <b>GPT-3.5</b> to evaluate the overall effect of instructional materials on different student groups and find that it can replicate well-established educational findings such as the Expertise Reversal Effect and the Variability Effect. This demonstrates the potential of LMs as reliable evaluators of educational content. Building on this insight, we introduce an instruction optimization approach in which one LM generates instructional materials using the judgments of another LM as a reward function. We apply this approach to create math word problem worksheets aimed at maximizing student learning gains. Human teachers&rsquo; evaluations of these LM-generated worksheets show a significant alignment between the LM judgments and human teacher preferences. We conclude by discussing potential divergences between human and LM opinions and the resulting pitfalls of automating instructional design.</p></p class="citation"></blockquote><h3 id=1230--68290-pps-qmix-periodically-parameter-sharing-for-accelerating-convergence-of-multi-agent-reinforcement-learning-ke-zhang-et-al-2024>(12/30 | 68/290) PPS-QMIX: Periodically Parameter Sharing for Accelerating Convergence of Multi-Agent Reinforcement Learning (Ke Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ke Zhang, DanDan Zhu, Qiuhan Xu, Hao Zhou, Ce Zheng. (2024)<br><strong>PPS-QMIX: Periodically Parameter Sharing for Accelerating Convergence of Multi-Agent Reinforcement Learning</strong><br><button class=copy-to-clipboard title="PPS-QMIX: Periodically Parameter Sharing for Accelerating Convergence of Multi-Agent Reinforcement Learning" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, Federated Learning, Parameter Sharing, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02635v1.pdf filename=2403.02635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training for multi-agent <b>reinforcement</b> <b>learning(MARL)</b> is a time-consuming process caused by <b>distribution</b> <b>shift</b> of each agent. One drawback is that strategy of each agent in MARL is independent but actually in cooperation. Thus, a vertical issue in multi-agent <b>reinforcement</b> <b>learning</b> is how to efficiently accelerate training process. To address this problem, current research has leveraged a centralized function(CF) across multiple agents to learn contribution of the team reward for each agent. However, CF based methods introduce joint error from other agents in estimation of value network. In so doing, inspired by <b>federated</b> <b>learning,</b> we propose three simple novel approaches called Average Periodically <b>Parameter</b> <b>Sharing(A-PPS),</b> Reward-Scalability Periodically <b>Parameter</b> <b>Sharing(RS-PPS)</b> and Partial Personalized Periodically <b>Parameter</b> <b>Sharing(PP-PPS)</b> mechanism to accelerate training of MARL. Agents share Q-value network periodically during the training process. Agents which has same identity adapt collected reward as scalability and update partial neural network during period to share different <b>parameters.</b> <b>We</b> apply our approaches in classical MARL method QMIX and evaluate our approaches on various tasks in StarCraft Multi-Agent Challenge(SMAC) environment. Performance of numerical experiments yield enormous enhancement, with an average improvement of 10%-30%, and enable to win tasks that QMIX cannot. Our code can be downloaded from <a href=https://github.com/ColaZhang22/PPS-QMIX>https://github.com/ColaZhang22/PPS-QMIX</a></p></p class="citation"></blockquote><h3 id=1330--69290-word-importance-explains-how-prompts-affect-language-model-outputs-stefan-hackmann-et-al-2024>(13/30 | 69/290) Word Importance Explains How Prompts Affect Language Model Outputs (Stefan Hackmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Hackmann, Haniyeh Mahmoudian, Mark Steadman, Michael Schmidt. (2024)<br><strong>Word Importance Explains How Prompts Affect Language Model Outputs</strong><br><button class=copy-to-clipboard title="Word Importance Explains How Prompts Affect Language Model Outputs" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-7; I-5-2, cs-AI, cs-CL, cs.AI<br>Keyword Score: 35<br>Keywords: Black Box, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03028v1.pdf filename=2403.03028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has revolutionized numerous applications across industries. However, their <b>&ldquo;black</b> <b>box&rdquo;</b> nature often hinders the understanding of how they make specific decisions, raising concerns about their transparency, reliability, and ethical use. This study presents a method to improve the explainability of <b>LLMs</b> by varying individual words in <b>prompts</b> to uncover their statistical impact on the model outputs. This approach, inspired by permutation importance for tabular data, masks each word in the system <b>prompt</b> and evaluates its effect on the outputs based on the available text scores aggregated over multiple user inputs. Unlike classical attention, word importance measures the impact of <b>prompt</b> words on arbitrarily-defined text scores, which enables decomposing the importance of words into the specific measures of interest&ndash;including bias, reading level, verbosity, etc. This procedure also enables measuring impact when attention weights are not available. To test the fidelity of this approach, we explore the effect of adding different suffixes to multiple different system <b>prompts</b> and comparing subsequent generations with different <b>large</b> <b>language</b> <b>models.</b> Results show that word importance scores are closely related to the expected suffix importances for multiple scoring functions.</p></p class="citation"></blockquote><h3 id=1430--70290-multi-scale-subgraph-contrastive-learning-yanbei-liu-et-al-2024>(14/30 | 70/290) Multi-Scale Subgraph Contrastive Learning (Yanbei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanbei Liu, Yu Zhao, Xiao Wang, Lei Geng, Zhitao Xiao. (2024)<br><strong>Multi-Scale Subgraph Contrastive Learning</strong><br><button class=copy-to-clipboard title="Multi-Scale Subgraph Contrastive Learning" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 33<br>Keywords: Graph Classification, Graph, Contrastive Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02719v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02719v1.pdf filename=2403.02719v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph-level</b> <b>contrastive</b> <b>learning,</b> aiming to learn the representations for each <b>graph</b> <b>by</b> contrasting two augmented <b>graphs,</b> <b>has</b> attracted considerable attention. Previous studies usually simply assume that a <b>graph</b> <b>and</b> its augmented <b>graph</b> <b>as</b> a positive pair, otherwise as a negative pair. However, it is well known that <b>graph</b> <b>structure</b> is always complex and multi-scale, which gives rise to a fundamental question: after <b>graph</b> <b>augmentation,</b> will the previous assumption still hold in reality? By an experimental analysis, we discover the semantic information of an augmented <b>graph</b> <b>structure</b> may be not consistent as original <b>graph</b> <b>structure,</b> and whether two augmented <b>graphs</b> <b>are</b> positive or negative pairs is highly related with the multi-scale structures. Based on this finding, we propose a multi-scale subgraph <b>contrastive</b> <b>learning</b> method which is able to characterize the fine-grained semantic information. Specifically, we generate global and local views at different scales based on subgraph sampling, and construct multiple <b>contrastive</b> <b>relationships</b> according to their semantic associations to provide richer <b>self-supervised</b> signals. Extensive experiments and parametric analysis on eight <b>graph</b> <b>classification</b> real-world datasets well demonstrate the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=1530--71290-race-sm-reinforcement-learning-based-autonomous-control-for-social-on-ramp-merging-jordan-poots-2024>(15/30 | 71/290) RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging (Jordan Poots, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jordan Poots. (2024)<br><strong>RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging</strong><br><button class=copy-to-clipboard title="RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-RO, cs.AI<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03359v1.pdf filename=2403.03359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous parallel-style on-ramp merging in human controlled traffic continues to be an existing issue for autonomous vehicle control. Existing non-learning based solutions for vehicle control rely on rules and optimization primarily. These methods have been seen to present significant challenges. Recent advancements in Deep <b>Reinforcement</b> <b>Learning</b> have shown promise and have received significant academic interest however the available learning based approaches show inadequate attention to other highway vehicles and often rely on inaccurate road traffic assumptions. In addition, the parallel-style case is rarely considered. A novel learning based model for acceleration and lane change decision making that explicitly considers the utility to both the ego vehicle and its surrounding vehicles which may be cooperative or uncooperative to produce behaviour that is socially acceptable is proposed. The novel reward function makes use of Social Value Orientation to weight the vehicle&rsquo;s level of social cooperation and is divided into ego vehicle and surrounding vehicle utility which are weighted according to the model&rsquo;s designated Social Value Orientation. A two-lane highway with an on-ramp divided into a taper-style and parallel-style section is considered. <b>Simulation</b> results indicated the importance of considering surrounding vehicles in reward function design and show that the proposed model matches or surpasses those in literature in terms of collisions while also introducing socially courteous behaviour avoiding near misses and anti-social behaviour through direct consideration of the effect of merging on surrounding vehicles.</p></p class="citation"></blockquote><h3 id=1630--72290-should-we-fear-large-language-models-a-structural-analysis-of-the-human-reasoning-system-for-elucidating-llm-capabilities-and-risks-through-the-lens-of-heideggers-philosophy-jianqiiu-zhang-2024>(16/30 | 72/290) Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger&rsquo;s Philosophy (Jianqiiu Zhang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianqiiu Zhang. (2024)<br><strong>Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger&rsquo;s Philosophy</strong><br><button class=copy-to-clipboard title="Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's Philosophy" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03288v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03288v1.pdf filename=2403.03288v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving field of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> there is a critical need to thoroughly analyze their capabilities and risks. Central to our investigation are two novel elements. Firstly, it is the innovative parallels between the statistical patterns of word relationships within <b>LLMs</b> and Martin Heidegger&rsquo;s concepts of &ldquo;ready-to-hand&rdquo; and &ldquo;present-at-hand,&rdquo; which encapsulate the utilitarian and scientific altitudes humans employ in interacting with the world. This comparison lays the groundwork for positioning <b>LLMs</b> as the digital counterpart to the Faculty of Verbal Knowledge, shedding light on their capacity to emulate certain facets of human <b>reasoning.</b> Secondly, a structural analysis of human <b>reasoning,</b> viewed through Heidegger&rsquo;s notion of truth as &ldquo;unconcealment&rdquo; is conducted This foundational principle enables us to map out the inputs and outputs of the <b>reasoning</b> system and divide <b>reasoning</b> into four distinct categories. Respective cognitive faculties are delineated, allowing us to place <b>LLMs</b> within the broader schema of human <b>reasoning,</b> thus clarifying their strengths and inherent limitations. Our findings reveal that while <b>LLMs</b> possess the capability for Direct Explicative <b>Reasoning</b> and Pseudo Rational <b>Reasoning,</b> they fall short in authentic rational <b>reasoning</b> and have no creative <b>reasoning</b> capabilities, due to the current lack of many analogous AI models such as the Faculty of Judgement. The potential and risks of <b>LLMs</b> when they are augmented with other AI technologies are also evaluated. The results indicate that although <b>LLMs</b> have achieved proficiency in some <b>reasoning</b> abilities, the aspiration to match or exceed human intellectual capabilities is yet unattained. This research not only enriches our comprehension of <b>LLMs</b> but also propels forward the discourse on AI&rsquo;s potential and its bounds, paving the way for future explorations into AI&rsquo;s evolving landscape.</p></p class="citation"></blockquote><h3 id=1730--73290-leveraging-federated-learning-and-edge-computing-for-recommendation-systems-within-cloud-computing-networks-yaqian-qi-et-al-2024>(17/30 | 73/290) Leveraging Federated Learning and Edge Computing for Recommendation Systems within Cloud Computing Networks (Yaqian Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaqian Qi, Yaqian Qi, Xiangxiang Wang, Hanzhe Li, Jingxiao Tian. (2024)<br><strong>Leveraging Federated Learning and Edge Computing for Recommendation Systems within Cloud Computing Networks</strong><br><button class=copy-to-clipboard title="Leveraging Federated Learning and Edge Computing for Recommendation Systems within Cloud Computing Networks" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Federated Learning, Recommendation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03165v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03165v1.pdf filename=2403.03165v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To enable large-scale and efficient deployment of artificial intelligence (AI), the combination of AI and edge computing has spawned Edge Intelligence, which leverages the computing and communication capabilities of end devices and edge servers to process data closer to where it is generated. A key technology for edge intelligence is the privacy-protecting machine learning paradigm known as <b>Federated</b> <b>Learning</b> (FL), which enables data owners to train models without having to transfer raw data to third-party servers. However, FL networks are expected to involve thousands of heterogeneous distributed devices. As a result, communication efficiency remains a key bottleneck. To reduce node failures and device exits, a Hierarchical <b>Federated</b> <b>Learning</b> (HFL) framework is proposed, where a designated cluster leader supports the data owner through intermediate model aggregation. Therefore, based on the improvement of edge server resource utilization, this paper can effectively make up for the limitation of cache capacity. In order to mitigate the impact of soft clicks on the quality of user experience (QoE), the authors model the user QoE as a comprehensive system cost. To solve the formulaic problem, the authors propose a decentralized caching algorithm with <b>federated</b> <b>deep</b> <b>reinforcement</b> <b>learning</b> (DRL) and <b>federated</b> <b>learning</b> (FL), where multiple agents learn and make decisions independently</p></p class="citation"></blockquote><h3 id=1830--74290-opex-a-component-wise-analysis-of-llm-centric-agents-in-embodied-instruction-following-haochen-shi-et-al-2024>(18/30 | 74/290) OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following (Haochen Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haochen Shi, Zhiyuan Sun, Xingdi Yuan, Marc-Alexandre Côté, Bang Liu. (2024)<br><strong>OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following</strong><br><button class=copy-to-clipboard title="OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Instruction Following, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03017v1.pdf filename=2403.03017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Embodied <b>Instruction</b> <b>Following</b> (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language <b>instructions.</b> <b>Recent</b> advancements have seen a surge in employing <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> within a framework-centric approach to enhance performance in embodied learning tasks, including EIF. Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components-ranging from visual perception to action execution-on task performance. To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor. Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance. Furthermore, we innovate within this space by deploying a multi-agent dialogue strategy on a TextWorld counterpart, further enhancing task performance. Our findings reveal that <b>LLM-centric</b> design markedly improves EIF outcomes, identify visual perception and low-level action execution as critical bottlenecks, and demonstrate that augmenting <b>LLMs</b> with a multi-agent framework further elevates performance.</p></p class="citation"></blockquote><h3 id=1930--75290-wikitableedit-a-benchmark-for-table-editing-by-natural-language-instruction-zheng-li-et-al-2024>(19/30 | 75/290) WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction (Zheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Li, Xiang Chen, Xiaojun Wan. (2024)<br><strong>WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction</strong><br><button class=copy-to-clipboard title="WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02962v1.pdf filename=2403.02962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tabular data, as a crucial form of data representation, exists in diverse formats on the Web. When confronted with complex and irregular tables, manual modification becomes a laborious task. This paper investigates the performance of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in the context of table editing tasks. Existing research mainly focuses on regular-shaped tables, wherein instructions are used to generate code in SQL, Python, or Excel Office-script for manipulating the tables. Nevertheless, editing tables with irregular structures, particularly those containing merged cells spanning multiple rows, poses a challenge when using code. To address this, we introduce the WikiTableEdit dataset. Leveraging 26,531 tables from the WikiSQL dataset, we automatically generate natural language instructions for six distinct basic operations and the corresponding outcomes, resulting in over 200,000 instances. Subsequently, we evaluate several representative <b>large</b> <b>language</b> <b>models</b> on the WikiTableEdit dataset to demonstrate the challenge of this task. The dataset will be released to the community to promote related researches.</p></p class="citation"></blockquote><h3 id=2030--76290-minimum-topology-attacks-for-graph-neural-networks-mengmei-zhang-et-al-2024>(20/30 | 76/290) Minimum Topology Attacks for Graph Neural Networks (Mengmei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengmei Zhang, Xiao Wang, Chuan Shi, Lingjuan Lyu, Tianchi Yang, Junping Du. (2024)<br><strong>Minimum Topology Attacks for Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Minimum Topology Attacks for Graph Neural Networks" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02723v1.pdf filename=2403.02723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the great popularity of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs),</b> their robustness to adversarial topology attacks has received significant attention. Although many attack methods have been proposed, they mainly focus on fixed-budget attacks, aiming at finding the most adversarial perturbations within a fixed budget for target node. However, considering the varied robustness of each node, there is an inevitable dilemma caused by the fixed budget, i.e., no successful perturbation is found when the budget is relatively small, while if it is too large, the yielding redundant perturbations will hurt the invisibility. To break this dilemma, we propose a new type of topology attack, named minimum-budget topology attack, aiming to adaptively find the minimum perturbation sufficient for a successful attack on each node. To this end, we propose an attack model, named MiBTack, based on a dynamic projected gradient descent algorithm, which can effectively solve the involving non-convex constraint optimization on discrete topology. Extensive results on three <b>GNNs</b> and four real-world datasets show that MiBTack can successfully lead all target nodes misclassified with the minimum perturbation edges. Moreover, the obtained minimum budget can be used to measure node robustness, so we can explore the relationships of robustness, topology, and uncertainty for nodes, which is beyond what the current fixed-budget topology attacks can offer.</p></p class="citation"></blockquote><h3 id=2130--77290-the-case-for-globalizing-fairness-a-mixed-methods-study-on-colonialism-ai-and-health-in-africa-mercy-asiedu-et-al-2024>(21/30 | 77/290) The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism, AI, and Health in Africa (Mercy Asiedu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mercy Asiedu, Awa Dieng, Iskandar Haykel, Negar Rostamzadeh, Stephen Pfohl, Chirag Nagpal, Maria Nagawa, Abigail Oppong, Sanmi Koyejo, Katherine Heller. (2024)<br><strong>The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism, AI, and Health in Africa</strong><br><button class=copy-to-clipboard title="The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism, AI, and Health in Africa" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CY, cs.AI<br>Keyword Score: 20<br>Keywords: Fairness, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03357v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03357v2.pdf filename=2403.03357v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With growing application of machine learning (ML) technologies in healthcare, there have been calls for developing techniques to understand and mitigate biases these systems may exhibit. Fair-ness considerations in the development of ML-based solutions for health have particular implications for Africa, which already faces inequitable power imbalances between the Global North and South.This paper seeks to explore <b>fairness</b> for global health, with Africa as a case study. We conduct a scoping review to propose axes of disparities for <b>fairness</b> consideration in the African context and delineate where they may come into play in different ML-enabled medical modalities. We then conduct qualitative research studies with 672 general population study participants and 28 experts inML, health, and policy focused on Africa to obtain corroborative evidence on the proposed axes of disparities. Our analysis focuses on colonialism as the attribute of interest and examines the interplay between artificial intelligence (AI), health, and colonialism. Among the pre-identified attributes, we found that colonial history, country of origin, and national income level were specific axes of disparities that participants believed would cause an AI system to be biased.However, there was also divergence of opinion between experts and general population participants. Whereas experts generally expressed a shared view about the relevance of colonial history for the development and implementation of AI technologies in Africa, the majority of the general population participants surveyed did not think there was a direct link between AI and colonialism. Based on these findings, we provide practical <b>recommendations</b> for developing <b>fairness-aware</b> ML solutions for health in Africa.</p></p class="citation"></blockquote><h3 id=2230--78290-reaching-consensus-in-cooperative-multi-agent-reinforcement-learning-with-goal-imagination-liangzhou-wang-et-al-2024>(22/30 | 78/290) Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination (Liangzhou Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liangzhou Wang, Kaiwen Zhu, Fengming Zhu, Xinghu Yao, Shujie Zhang, Deheng Ye, Haobo Fu, Qiang Fu, Wei Yang. (2024)<br><strong>Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination</strong><br><button class=copy-to-clipboard title="Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03172v1.pdf filename=2403.03172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reaching consensus is key to multi-agent coordination. To accomplish a cooperative task, agents need to coherently select optimal joint actions to maximize the team reward. However, current cooperative multi-agent <b>reinforcement</b> <b>learning</b> (MARL) methods usually do not explicitly take consensus into consideration, which may cause miscoordination problem. In this paper, we propose a model-based consensus mechanism to explicitly coordinate multiple agents. The proposed Multi-agent Goal Imagination (MAGI) framework guides agents to reach consensus with an Imagined common goal. The common goal is an achievable state with high value, which is obtained by sampling from the distribution of future states. We directly model this distribution with a <b>self-supervised</b> generative model, thus alleviating the &ldquo;curse of dimensinality&rdquo; problem induced by multi-agent multi-step policy rollout commonly used in model-based methods. We show that such efficient consensus mechanism can guide all agents cooperatively reaching valuable future states. Results on Multi-agent Particle-Environments and Google Research Football environment demonstrate the superiority of MAGI in both sample efficiency and performance.</p></p class="citation"></blockquote><h3 id=2330--79290-curatron-complete-robust-preference-data-for-robust-alignment-of-large-language-models-son-the-nguyen-et-al-2024>(23/30 | 79/290) CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models (Son The Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Son The Nguyen, Niranjan Uma Naresh, Theja Tulabandhula. (2024)<br><strong>CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models</strong><br><button class=copy-to-clipboard title="CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02745v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02745v1.pdf filename=2403.02745v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the challenges of aligning <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with human values via preference learning (PL), with a focus on the issues of incomplete and corrupted data in preference datasets. We propose a novel method for robustly and completely recalibrating values within these datasets to enhance <b>LLMs</b> resilience against the issues. In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley&ndash;Terry&ndash;Luce (BTL) (Bradley and Terry, 1952) model and certain generalizations of it. To the best of our knowledge, our present work is the first to propose an algorithm that provably recovers an {\epsilon}-optimal ranking with high probability while allowing as <b>large</b> <b>as</b> <b>O(n)</b> perturbed pairwise comparison results per model response. Furthermore, we show robust recovery results in the partially observed setting. Our experiments confirm that our algorithms handle adversarial noise and unobserved comparisons well in both general and <b>LLM</b> preference dataset settings. This work contributes to the development and scaling of more reliable and ethically aligned AI models by equipping the dataset curation pipeline with the ability to handle missing and maliciously manipulated inputs.</p></p class="citation"></blockquote><h3 id=2430--80290-chatgpt4pcg-2-competition-prompt-engineering-for-science-birds-level-generation-pittawat-taveekitworachai-et-al-2024>(24/30 | 80/290) ChatGPT4PCG 2 Competition: Prompt Engineering for Science Birds Level Generation (Pittawat Taveekitworachai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pittawat Taveekitworachai, Febri Abdullah, Mury F. Dewantoro, Yi Xia, Pratch Suntichaikul, Ruck Thawonmas, Julian Togelius, Jochen Renz. (2024)<br><strong>ChatGPT4PCG 2 Competition: Prompt Engineering for Science Birds Level Generation</strong><br><button class=copy-to-clipboard title="ChatGPT4PCG 2 Competition: Prompt Engineering for Science Birds Level Generation" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-7; I-2-8, cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: ChatGPT, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02610v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02610v1.pdf filename=2403.02610v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the second ChatGPT4PCG competition at the 2024 IEEE Conference on Games. In this edition of the competition, we follow the first edition, but make several improvements and changes. We introduce a new evaluation metric along with allowing a more flexible format for participants&rsquo; submissions and making several improvements to the evaluation pipeline. Continuing from the first edition, we aim to foster and explore the realm of <b>prompt</b> engineering (PE) for procedural content generation (PCG). While the first competition saw success, it was hindered by various limitations; we aim to mitigate these limitations in this edition. We introduce diversity as a new metric to discourage submissions aimed at producing repetitive structures. Furthermore, we allow submission of a Python program instead of a <b>prompt</b> text file for greater flexibility in implementing advanced PE approaches, which may require control flow, including conditions and iterations. We also make several improvements to the evaluation pipeline with a better classifier for similarity evaluation and better-performing function signatures. We thoroughly evaluate the effectiveness of the new metric and the improved classifier. Additionally, we perform an ablation study to select a function signature to instruct <b>ChatGPT</b> for level generation. Finally, we provide implementation examples of various PE techniques in Python and evaluate their preliminary performance. We hope this competition serves as a resource and platform for learning about PE and PCG in general.</p></p class="citation"></blockquote><h3 id=2530--81290-fuzzy-datalogexists-over-arbitrary-t-norms-matthias-lanzinger-et-al-2024>(25/30 | 81/290) Fuzzy Datalog$^\exists$ over Arbitrary t-Norms (Matthias Lanzinger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthias Lanzinger, Stefano Sferrazza, Przemysław A. Wałęga, Georg Gottlob. (2024)<br><strong>Fuzzy Datalog$^\exists$ over Arbitrary t-Norms</strong><br><button class=copy-to-clipboard title="Fuzzy Datalog$^\exists$ over Arbitrary t-Norms" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LO, cs.AI<br>Keyword Score: 18<br>Keywords: Graph, Knowledge Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02933v1.pdf filename=2403.02933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the main challenges in the area of Neuro-Symbolic AI is to perform logical <b>reasoning</b> in the presence of both neural and symbolic data. This requires combining heterogeneous data sources such as <b>knowledge</b> <b>graphs,</b> neural model predictions, structured databases, crowd-sourced data, and many more. To allow for such <b>reasoning,</b> we generalise the standard rule-based language Datalog with existential rules (commonly referred to as tuple-generating dependencies) to the fuzzy setting, by allowing for arbitrary t-norms in the place of classical conjunctions in rule bodies. The resulting formalism allows us to perform <b>reasoning</b> about data associated with degrees of uncertainty while preserving computational complexity results and the applicability of <b>reasoning</b> techniques established for the standard Datalog setting. In particular, we provide fuzzy extensions of Datalog chases which produce fuzzy universal models and we exploit them to show that in important fragments of the language, <b>reasoning</b> has the same complexity as in the classical setting.</p></p class="citation"></blockquote><h3 id=2630--82290-towards-general-computer-control-a-multimodal-agent-for-red-dead-redemption-ii-as-a-case-study-weihao-tan-et-al-2024>(26/30 | 82/290) Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study (Weihao Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, Yifei Bi, Pengjie Gu, Xinrun Wang, Börje F. Karlsson, Bo An, Zongqing Lu. (2024)<br><strong>Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study</strong><br><button class=copy-to-clipboard title="Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03186v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03186v2.pdf filename=2403.03186v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the success in specific tasks and scenarios, existing foundation agents, empowered by large models (LMs) and advanced tools, still cannot generalize to different scenarios, mainly due to dramatic differences in the observations and actions across scenarios. In this work, we propose the General Computer Control (GCC) setting: building foundation agents that can master any computer task by taking only screen images (and possibly audio) of the computer as input, and producing keyboard and mouse operations as output, similar to human-computer interaction. The main challenges of achieving GCC are: 1) the <b>multimodal</b> observations for decision-making, 2) the requirements of accurate control of keyboard and mouse, 3) the need for long-term memory and <b>reasoning,</b> and 4) the abilities of efficient exploration and self-improvement. To target GCC, we introduce Cradle, an agent framework with six main modules, including: 1) information gathering to extract multi-modality information, 2) self-reflection to rethink past experiences, 3) task inference to choose the best next task, 4) skill curation for generating and updating relevant skills for given tasks, 5) action planning to generate specific operations for keyboard and mouse control, and 6) memory for storage and retrieval of past experiences and known skills. To demonstrate the capabilities of generalization and self-improvement of Cradle, we deploy it in the complex AAA game Red Dead Redemption II, serving as a preliminary attempt towards GCC with a challenging target. To our best knowledge, our work is the first to enable LMM-based agents to follow the main storyline and finish real missions in complex AAA games, with minimal reliance on prior knowledge or resources. The project website is at <a href=https://baai-agents.github.io/Cradle/>https://baai-agents.github.io/Cradle/</a>.</p></p class="citation"></blockquote><h3 id=2730--83290-a-general-approach-to-enhance-the-survivability-of-backdoor-attacks-by-decision-path-coupling-yufei-zhao-et-al-2024>(27/30 | 83/290) A general approach to enhance the survivability of backdoor attacks by decision path coupling (Yufei Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufei Zhao, Dingji Wang, Bihuan Chen, Ziqian Chen, Xin Peng. (2024)<br><strong>A general approach to enhance the survivability of backdoor attacks by decision path coupling</strong><br><button class=copy-to-clipboard title="A general approach to enhance the survivability of backdoor attacks by decision path coupling" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CR, cs.AI<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02950v1.pdf filename=2403.02950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Backdoor attacks have been one of the emerging security threats to deep neural networks (DNNs), leading to serious consequences. One of the mainstream backdoor defenses is model reconstruction-based. Such defenses adopt model unlearning or <b>pruning</b> to eliminate backdoors. However, little attention has been paid to survive from such defenses. To bridge the gap, we propose Venom, the first generic backdoor attack enhancer to improve the survivability of existing backdoor attacks against model reconstruction-based defenses. We formalize Venom as a binary-task optimization problem. The first is the original backdoor attack task to preserve the original attack capability, while the second is the attack enhancement task to improve the attack survivability. To realize the second task, we propose attention imitation loss to force the decision path of poisoned samples in backdoored models to couple with the crucial decision path of benign samples, which makes backdoors difficult to eliminate. Our extensive evaluation on two DNNs and three datasets has demonstrated that Venom significantly improves the survivability of eight state-of-the-art attacks against eight state-of-the-art defenses without impacting the capability of the original attacks.</p></p class="citation"></blockquote><h3 id=2830--84290-dynst-dynamic-sparse-training-for-resource-constrained-spatio-temporal-forecasting-hao-wu-et-al-2024>(28/30 | 84/290) DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal Forecasting (Hao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wu, Haomin Wen, Guibin Zhang, Yutong Xia, Kai Wang, Yuxuan Liang, Yu Zheng, Kun Wang. (2024)<br><strong>DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal Forecasting</strong><br><button class=copy-to-clipboard title="DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal Forecasting" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02914v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02914v1.pdf filename=2403.02914v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ever-increasing sensor service, though opening a precious path and providing a deluge of earth system data for deep-learning-oriented earth science, sadly introduce a daunting obstacle to their industrial level deployment. Concretely, earth science systems rely heavily on the extensive deployment of sensors, however, the data collection from sensors is constrained by complex geographical and social factors, making it challenging to achieve comprehensive coverage and uniform deployment. To alleviate the obstacle, traditional approaches to sensor deployment utilize specific algorithms to design and deploy sensors. These methods dynamically adjust the activation times of sensors to optimize the detection process across each sub-region. Regrettably, formulating an activation strategy generally based on historical observations and geographic characteristics, which make the methods and resultant models were neither simple nor practical. Worse still, the complex technical design may ultimately lead to a model with weak generalizability. In this paper, we introduce for the first time the concept of spatio-temporal data dynamic sparse training and are committed to adaptively, dynamically filtering important sensor distributions. To our knowledge, this is the first proposal (termed DynST) of an industry-level deployment optimization concept at the data level. However, due to the existence of the temporal dimension, <b>pruning</b> of spatio-temporal data may lead to conflicts at different timestamps. To achieve this goal, we employ dynamic merge technology, along with ingenious dimensional mapping to mitigate potential impacts caused by the temporal aspect. During the training process, DynST utilize iterative <b>pruning</b> and sparse training, repeatedly identifying and dynamically removing sensor perception areas that contribute the least to future predictions.</p></p class="citation"></blockquote><h3 id=2930--85290-precise-extraction-of-deep-learning-models-via-side-channel-attacks-on-edgeendpoint-devices-younghan-lee-et-al-2024>(29/30 | 85/290) Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices (Younghan Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Younghan Lee, Sohee Jun, Yungi Cho, Woorim Han, Hyungon Moon, Yunheung Paek. (2024)<br><strong>Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices</strong><br><button class=copy-to-clipboard title="Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CR, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Model Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02870v1.pdf filename=2403.02870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With growing popularity, deep learning (DL) <b>models</b> <b>are</b> becoming larger-scale, and only the companies with vast training datasets and immense computing power can manage their business serving such large <b>models.</b> <b>Most</b> of those DL <b>models</b> <b>are</b> proprietary to the companies who thus strive to keep their private <b>models</b> <b>safe</b> from the <b>model</b> <b>extraction</b> attack (MEA), whose aim is to steal the <b>model</b> <b>by</b> training surrogate <b>models.</b> <b>Nowadays,</b> companies are inclined to offload the <b>models</b> <b>from</b> central servers to edge/endpoint devices. As revealed in the latest studies, adversaries exploit this opportunity as new attack vectors to launch side-channel attack (SCA) on the device running victim <b>model</b> <b>and</b> obtain various pieces of the <b>model</b> <b>information,</b> such as the <b>model</b> <b>architecture</b> (MA) and image dimension (ID). Our work provides a comprehensive understanding of such a relationship for the first time and would benefit future MEA studies in both offensive and defensive sides in that they may learn which pieces of information exposed by SCA are more important than the others. Our analysis additionally reveals that by grasping the victim <b>model</b> <b>information</b> from SCA, MEA can get highly effective and successful even without any prior knowledge of the <b>model.</b> <b>Finally,</b> to evince the practicality of our analysis results, we empirically apply SCA, and subsequently, carry out MEA under realistic threat assumptions. The results show up to 5.8 times better performance than when the adversary has no <b>model</b> <b>information</b> about the victim model.</p></p class="citation"></blockquote><h3 id=3030--86290-reconstruction-for-sparse-view-tomography-of-long-objects-applied-to-imaging-in-the-wood-industry-buda-bajić-et-al-2024>(30/30 | 86/290) Reconstruction for Sparse View Tomography of Long Objects Applied to Imaging in the Wood Industry (Buda Bajić et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Buda Bajić, Johannes A. J. Huber, Benedikt Neyses, Linus Olofsson, Ozan Öktem. (2024)<br><strong>Reconstruction for Sparse View Tomography of Long Objects Applied to Imaging in the Wood Industry</strong><br><button class=copy-to-clipboard title="Reconstruction for Sparse View Tomography of Long Objects Applied to Imaging in the Wood Industry" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02820v1.pdf filename=2403.02820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the wood industry, logs are commonly quality screened by discrete X-ray scans on a moving conveyor belt from a few source positions. Typically, two-dimensional (2D) slice-wise measurements are obtained by a sequential scanning <b>geometry.</b> Each 2D slice alone does not carry sufficient information for a three-dimensional tomographic reconstruction in which biological features of interest in the log are well preserved. In the present work, we propose a learned iterative reconstruction method based on the Learned Primal-Dual neural network, suited for sequential scanning geometries. Our method accumulates information between neighbouring slices, instead of only accounting for single slices during reconstruction. Our quantitative and qualitative evaluations with as few as five source positions show that our method yields reconstructions of logs that are sufficiently accurate to identify biological features like knots (branches), heartwood and sapwood.</p></p class="citation"></blockquote><h2 id=cslg-42>cs.LG (42)</h2><h3 id=142--87290-privacy-aware-semantic-cache-for-large-language-models-waris-gill-et-al-2024>(1/42 | 87/290) Privacy-Aware Semantic Cache for Large Language Models (Waris Gill et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Waris Gill, Mohamed Elidrisi, Pallavi Kalapatapu, Ali Anwar, Muhammad Ali Gulzar. (2024)<br><strong>Privacy-Aware Semantic Cache for Large Language Models</strong><br><button class=copy-to-clipboard title="Privacy-Aware Semantic Cache for Large Language Models" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-7, cs-AI, cs-CL, cs-CR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 93<br>Keywords: Benchmarking, Federated Learning, Bard, ChatGPT, Claude, GPT, GPT-3, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02694v1.pdf filename=2403.02694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>ChatGPT,</b> Google <b>Bard,</b> <b>Claude,</b> and <b>Llama</b> 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, <b>GPT-3</b> consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce <b>LLM</b> inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among <b>LLM</b> queries, leading to unacceptable false hit-and-miss rates. This paper introduces MeanCache, a semantic cache for <b>LLMs</b> that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user&rsquo;s semantically similar query can be retrieved from a local cache rather than re-querying the <b>LLM,</b> thus reducing costs, service provider load, and environmental impact. MeanCache leverages <b>Federated</b> <b>Learning</b> (FL) to collaboratively train a query similarity model in a distributed manner across numerous users without violating privacy. By placing a local cache in each user&rsquo;s device and using FL, MeanCache reduces the latency and costs and enhances model performance, resulting in lower cache false hit rates. Our experiments, <b>benchmarked</b> against the GPTCache, reveal that MeanCache attains an approximately 17% higher F-score and a 20% increase in precision during semantic cache hit-and-miss decisions. Furthermore, MeanCache reduces the storage requirement by 83% and accelerates semantic cache hit-and-miss decisions by 11%, while still surpassing GPTCache.</p></p class="citation"></blockquote><h3 id=242--88290-rehabilitation-exercise-quality-assessment-through-supervised-contrastive-learning-with-hard-and-soft-negatives-mark-karlov-et-al-2024>(2/42 | 88/290) Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives (Mark Karlov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mark Karlov, Ali Abedi, Shehroz S. Khan. (2024)<br><strong>Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives</strong><br><button class=copy-to-clipboard title="Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-CY, cs-LG, cs.LG<br>Keyword Score: 59<br>Keywords: Graph Convolutional Network, Graph, Benchmarking, Contrastive Learning, Convolution, Convolutional Neural Network, Sample Size, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02772v1.pdf filename=2403.02772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exercise-based rehabilitation programs have proven to be effective in enhancing the quality of life and reducing mortality and rehospitalization rates. AI-driven virtual rehabilitation, which allows patients to independently complete exercises at home, utilizes AI algorithms to analyze exercise data, providing feedback to patients and updating clinicians on their progress. These programs commonly prescribe a variety of exercise types, leading to a distinct challenge in rehabilitation exercise assessment datasets: while abundant in overall training <b>samples,</b> <b>these</b> datasets often have a limited number of <b>samples</b> <b>for</b> each individual exercise type. This disparity hampers the ability of existing approaches to train generalizable models with such a small <b>sample</b> <b>size</b> per exercise. Addressing this issue, our paper introduces a novel <b>supervised</b> <b>contrastive</b> <b>learning</b> framework with hard and soft negative <b>samples</b> <b>that</b> effectively utilizes the entire dataset to train a single model applicable to all exercise types. This model, with a Spatial-Temporal <b>Graph</b> <b>Convolutional</b> <b>Network</b> (ST-GCN) architecture, demonstrated enhanced generalizability across exercises and a decrease in overall complexity. Through extensive experiments on three publicly available rehabilitation exercise assessment datasets, the University of Idaho-Physical Rehabilitation Movement Data (UI-PRMD), IntelliRehabDS (IRDS), and KInematic assessment of MOvement and clinical scores for remote monitoring of physical REhabilitation (KIMORE), our method has shown to surpass existing methods, setting a new <b>benchmark</b> in rehabilitation exercise assessment accuracy.</p></p class="citation"></blockquote><h3 id=342--89290-how-well-can-transformers-emulate-in-context-newtons-method-angeliki-giannou-et-al-2024>(3/42 | 89/290) How Well Can Transformers Emulate In-context Newton&rsquo;s Method? (Angeliki Giannou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angeliki Giannou, Liu Yang, Tianhao Wang, Dimitris Papailiopoulos, Jason D. Lee. (2024)<br><strong>How Well Can Transformers Emulate In-context Newton&rsquo;s Method?</strong><br><button class=copy-to-clipboard title="How Well Can Transformers Emulate In-context Newton's Method?" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 50<br>Keywords: Logistic Regression, Transformer, In-context Learning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03183v1.pdf filename=2403.03183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> models have demonstrated remarkable <b>in-context</b> <b>learning</b> capabilities, <b>prompting</b> extensive research into its underlying mechanisms. Recent studies have suggested that <b>Transformers</b> can implement first-order optimization algorithms for <b>in-context</b> <b>learning</b> and even second order ones for the case of linear regression. In this work, we study whether <b>Transformers</b> can perform higher order optimization methods, beyond the case of linear regression. We establish that linear attention <b>Transformers</b> with ReLU layers can approximate second order optimization algorithms for the task of <b>logistic</b> <b>regression</b> and achieve $\epsilon$ error with only a logarithmic to the error more layers. As a by-product we demonstrate the ability of even linear attention-only <b>Transformers</b> in implementing a single step of Newton&rsquo;s iteration for matrix inversion with merely two layers. These results suggest the ability of the <b>Transformer</b> architecture to implement complex algorithms, beyond gradient descent.</p></p class="citation"></blockquote><h3 id=442--90290-unsupervised-spatio-temporal-state-estimation-for-fine-grained-adaptive-anomaly-diagnosis-of-industrial-cyber-physical-systems-haili-sun-et-al-2024>(4/42 | 90/290) Unsupervised Spatio-Temporal State Estimation for Fine-grained Adaptive Anomaly Diagnosis of Industrial Cyber-physical Systems (Haili Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haili Sun, Yan Huang, Lansheng Han, Cai Fu, Chunjie Zhou. (2024)<br><strong>Unsupervised Spatio-Temporal State Estimation for Fine-grained Adaptive Anomaly Diagnosis of Industrial Cyber-physical Systems</strong><br><button class=copy-to-clipboard title="Unsupervised Spatio-Temporal State Estimation for Fine-grained Adaptive Anomaly Diagnosis of Industrial Cyber-physical Systems" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs-NI, cs-SY, cs.LG, eess-SY<br>Keyword Score: 50<br>Keywords: Reconstruction Loss, Simulation, Simulator, Unsupervised Learning, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02616v1.pdf filename=2403.02616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate detection and diagnosis of abnormal behaviors such as network attacks from multivariate time series <b>(MTS)</b> are crucial for ensuring the stable and effective operation of industrial cyber-physical systems (CPS). However, existing researches pay little attention to the logical dependencies among system working states, and have difficulties in explaining the evolution mechanisms of abnormal signals. To reveal the spatio-temporal association relationships and evolution mechanisms of the working states of industrial CPS, this paper proposes a fine-grained adaptive anomaly diagnosis method (i.e. MAD-Transformer) to identify and diagnose anomalies in <b>MTS.</b> MAD-Transformer first constructs a temporal state matrix to characterize and estimate the change patterns of the system states in the temporal dimension. Then, to better locate the anomalies, a spatial state matrix is also constructed to capture the inter-sensor state correlation relationships within the system. Subsequently, based on these two types of state matrices, a three-branch structure of series-temporal-spatial attention module is designed to simultaneously capture the series, temporal, and space dependencies among <b>MTS.</b> Afterwards, three associated alignment loss functions and a <b>reconstruction</b> <b>loss</b> are constructed to jointly optimize the model. Finally, anomalies are determined and diagnosed by comparing the residual matrices with the original matrices. We conducted comparative experiments on five publicly datasets spanning three application domains (service monitoring, spatial and earth exploration, and water treatment), along with a petroleum refining <b>simulation</b> dataset collected by ourselves. The results demonstrate that MAD-Transformer can adaptively detect fine-grained anomalies with short duration, and outperforms the state-of-the-art baselines in terms of noise robustness and localization performance.</p></p class="citation"></blockquote><h3 id=542--91290-flguard-byzantine-robust-federated-learning-via-ensemble-of-contrastive-models-younghan-lee-et-al-2024>(5/42 | 91/290) FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive Models (Younghan Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Younghan Lee, Yungi Cho, Woorim Han, Ho Bae, Yunheung Paek. (2024)<br><strong>FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive Models</strong><br><button class=copy-to-clipboard title="FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive Models" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Federated Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02846v1.pdf filename=2403.02846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) thrives in training a global model with numerous clients by only sharing the parameters of their local models trained with their private training datasets. Therefore, without revealing the private dataset, the clients can obtain a deep learning (DL) model with high performance. However, recent research proposed poisoning attacks that cause a catastrophic loss in the accuracy of the global model when adversaries, posed as benign clients, are present in a group of clients. Therefore, recent studies suggested byzantine-robust FL methods that allow the server to train an accurate global model even with the adversaries present in the system. However, many existing methods require the knowledge of the number of malicious clients or the auxiliary (clean) dataset or the effectiveness reportedly decreased hugely when the private dataset was non-independently and identically distributed (non-IID). In this work, we propose FLGuard, a novel byzantine-robust FL method that detects malicious clients and discards malicious local updates by utilizing the <b>contrastive</b> <b>learning</b> technique, which showed a tremendous improvement as a <b>self-supervised</b> <b>learning</b> method. With <b>contrastive</b> <b>models,</b> we design FLGuard as an ensemble scheme to maximize the defensive capability. We evaluate FLGuard extensively under various poisoning attacks and compare the accuracy of the global model with existing byzantine-robust FL methods. FLGuard outperforms the state-of-the-art defense methods in most cases and shows drastic improvement, especially in non-IID settings. <a href=https://github.com/201younghanlee/FLGuard>https://github.com/201younghanlee/FLGuard</a></p></p class="citation"></blockquote><h3 id=642--92290-controllable-prompt-tuning-for-balancing-group-distributional-robustness-hoang-phan-et-al-2024>(6/42 | 92/290) Controllable Prompt Tuning For Balancing Group Distributional Robustness (Hoang Phan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hoang Phan, Andrew Gordon Wilson, Qi Lei. (2024)<br><strong>Controllable Prompt Tuning For Balancing Group Distributional Robustness</strong><br><button class=copy-to-clipboard title="Controllable Prompt Tuning For Balancing Group Distributional Robustness" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 39<br>Keywords: Benchmarking, Distribution Shift, Distribution Shift, Multi-modal, Multi-modal, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02695v1.pdf filename=2403.02695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Models trained on data composed of different groups or domains can suffer from severe performance degradation under <b>distribution</b> <b>shifts.</b> While recent methods have largely focused on optimizing the worst-group objective, this often comes at the expense of good performance on other groups. To address this problem, we introduce an optimization scheme to achieve good performance across groups and find a good solution for all without severely sacrificing performance on any of them. However, directly applying such optimization involves updating the parameters of the entire network, making it both computationally expensive and challenging. Thus, we introduce Controllable <b>Prompt</b> Tuning (CPT), which couples our approach with <b>prompt-tuning</b> techniques. On spurious correlation <b>benchmarks,</b> our procedures achieve state-of-the-art results across both <b>transformer</b> and non-transformer architectures, as well as unimodal and <b>multimodal</b> data, while requiring only 0.4% tunable parameters.</p></p class="citation"></blockquote><h3 id=742--93290-semi-supervised-graph-representation-learning-with-human-centric-explanation-for-predicting-fatty-liver-disease-so-yeon-kim-et-al-2024>(7/42 | 93/290) Semi-Supervised Graph Representation Learning with Human-centric Explanation for Predicting Fatty Liver Disease (So Yeon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>So Yeon Kim, Sehee Wang, Eun Kyung Choe. (2024)<br><strong>Semi-Supervised Graph Representation Learning with Human-centric Explanation for Predicting Fatty Liver Disease</strong><br><button class=copy-to-clipboard title="Semi-Supervised Graph Representation Learning with Human-centric Explanation for Predicting Fatty Liver Disease" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 38<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Representation Learning, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02786v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02786v1.pdf filename=2403.02786v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing the challenge of limited labeled data in clinical settings, particularly in the prediction of fatty liver disease, this study explores the potential of <b>graph</b> <b>representation</b> <b>learning</b> within a <b>semi-supervised</b> <b>learning</b> framework. Leveraging <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs),</b> our approach constructs a subject similarity <b>graph</b> <b>to</b> <b>identify</b> risk patterns from health checkup data. The effectiveness of various <b>GNN</b> approaches in this context is demonstrated, even with minimal labeled samples. Central to our methodology is the inclusion of human-centric explanations through explainable <b>GNNs,</b> providing personalized feature importance scores for enhanced interpretability and clinical relevance, thereby underscoring the potential of our approach in advancing healthcare practices with a keen focus on <b>graph</b> <b>representation</b> <b>learning</b> and human-centric explanation.</p></p class="citation"></blockquote><h3 id=842--94290-a-zero-shot-reinforcement-learning-strategy-for-autonomous-guidewire-navigation-valentina-scarponi-et-al-2024>(8/42 | 94/290) A Zero-Shot Reinforcement Learning Strategy for Autonomous Guidewire Navigation (Valentina Scarponi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valentina Scarponi, Michel Duprez, Florent Nageotte, Stéphane Cotin. (2024)<br><strong>A Zero-Shot Reinforcement Learning Strategy for Autonomous Guidewire Navigation</strong><br><button class=copy-to-clipboard title="A Zero-Shot Reinforcement Learning Strategy for Autonomous Guidewire Navigation" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs.LG, physics-med-ph<br>Keyword Score: 35<br>Keywords: Geometry, Reinforcement Learning, Zero-shot, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02777v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02777v1.pdf filename=2403.02777v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose: The treatment of cardiovascular diseases requires complex and challenging navigation of a guidewire and catheter. This often leads to lengthy interventions during which the patient and clinician are exposed to X-ray radiation. Deep <b>Reinforcement</b> <b>Learning</b> approaches have shown promise in learning this task and may be the key to automating catheter navigation during robotized interventions. Yet, existing training methods show limited capabilities at generalizing to unseen vascular anatomies, requiring to be retrained each time the <b>geometry</b> changes. Methods: In this paper, we propose a <b>zero-shot</b> <b>learning</b> strategy for three-dimensional autonomous endovascular navigation. Using a very small training set of branching patterns, our <b>reinforcement</b> <b>learning</b> algorithm is able to learn a control that can then be applied to unseen vascular anatomies without retraining. Results: We demonstrate our method on 4 different vascular systems, with an average success rate of 95% at reaching random targets on these anatomies. Our strategy is also computationally efficient, allowing the training of our controller to be performed in only 2 hours. Conclusion: Our training method proved its ability to navigate unseen geometries with different characteristics, thanks to a nearly shape-invariant observation space.</p></p class="citation"></blockquote><h3 id=942--95290-fedhcdr-federated-cross-domain-recommendation-with-hypergraph-signal-decoupling-hongyu-zhang-et-al-2024>(9/42 | 95/290) FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling (Hongyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyu Zhang, Dongyi Zheng, Lin Zhong, Xu Yang, Jiyuan Feng, Yunqing Feng, Qing Liao. (2024)<br><strong>FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling</strong><br><button class=copy-to-clipboard title="FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IR, cs-LG, cs-SI, cs.LG<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Federated Learning, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02630v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02630v2.pdf filename=2403.02630v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, Cross-Domain <b>Recommendation</b> (CDR) has drawn significant attention, which utilizes user data from multiple domains to enhance the <b>recommendation</b> performance. However, current CDR methods require sharing user data across domains, thereby violating the General Data Protection Regulation (GDPR). Consequently, numerous approaches have been proposed for <b>Federated</b> <b>Cross-Domain</b> <b>Recommendation</b> (FedCDR). Nevertheless, the data heterogeneity across different domains inevitably influences the overall performance of <b>federated</b> <b>learning.</b> In this study, we propose FedHCDR, a novel <b>Federated</b> <b>Cross-Domain</b> <b>Recommendation</b> framework with Hypergraph signal decoupling. Specifically, to address the data heterogeneity across domains, we introduce an approach called hypergraph signal decoupling (HSD) to decouple the user features into domain-exclusive and domain-shared features. The approach employs high-pass and low-pass hypergraph filters to decouple domain-exclusive and domain-shared user representations, which are trained by the local-global bi-directional transfer algorithm. In addition, a hypergraph <b>contrastive</b> <b>learning</b> (HCL) module is devised to enhance the learning of domain-shared user relationship information by perturbing the user hypergraph. Extensive experiments conducted on three real-world scenarios demonstrate that FedHCDR outperforms existing baselines significantly.</p></p class="citation"></blockquote><h3 id=1042--96290-training-machine-learning-models-at-the-edge-a-survey-aymen-rayane-khouas-et-al-2024>(10/42 | 96/290) Training Machine Learning models at the Edge: A Survey (Aymen Rayane Khouas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aymen Rayane Khouas, Mohamed Reda Bouadjenek, Hakim Hacid, Sunil Aryal. (2024)<br><strong>Training Machine Learning models at the Edge: A Survey</strong><br><button class=copy-to-clipboard title="Training Machine Learning models at the Edge: A Survey" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02619v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02619v1.pdf filename=2403.02619v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Edge Computing (EC) has gained significant traction in recent years, promising enhanced efficiency by integrating Artificial Intelligence (AI) capabilities at the edge. While the focus has primarily been on the deployment and inference of Machine Learning (ML) models at the edge, the training aspect remains less explored. This survey delves into Edge Learning (EL), specifically the optimization of ML model training at the edge. The objective is to comprehensively explore diverse approaches and methodologies in EL, synthesize existing knowledge, identify challenges, and highlight future trends. Utilizing Scopus&rsquo; advanced search, relevant literature on EL was identified, revealing a concentration of research efforts in distributed learning methods, particularly <b>Federated</b> <b>Learning</b> (FL). This survey further provides a guideline for comparing techniques used to optimize ML for edge learning, along with an exploration of different frameworks, libraries, and <b>simulation</b> tools available for EL. In doing so, the paper contributes to a holistic understanding of the current landscape and future directions in the intersection of edge computing and machine learning, paving the way for informed comparisons between optimization methods and techniques designed for edge learning.</p></p class="citation"></blockquote><h3 id=1142--97290-learning-augmented-online-minimization-of-age-of-information-and-transmission-costs-zhongdong-liu-et-al-2024>(11/42 | 97/290) Learning-augmented Online Minimization of Age of Information and Transmission Costs (Zhongdong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongdong Liu, Keyuan Zhang, Bin Li, Yin Sun, Y. Thomas Hou, Bo Ji. (2024)<br><strong>Learning-augmented Online Minimization of Age of Information and Transmission Costs</strong><br><button class=copy-to-clipboard title="Learning-augmented Online Minimization of Age of Information and Transmission Costs" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Discrete Time, Discrete Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02573v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02573v1.pdf filename=2403.02573v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a <b>discrete-time</b> <b>system</b> where a resource-constrained source (e.g., a small sensor) transmits its time-sensitive data to a destination over a time-varying wireless channel. Each transmission incurs a fixed transmission cost (e.g., energy cost), and no transmission results in a staleness cost represented by the Age-of-Information. The source must balance the tradeoff between transmission and staleness costs. To address this challenge, we develop a robust online algorithm to minimize the sum of transmission and staleness costs, ensuring a worst-case performance guarantee. While online algorithms are robust, they are usually overly conservative and may have a poor average performance in typical scenarios. In contrast, by leveraging historical data and prediction models, machine learning (ML) algorithms perform well in average cases. However, they typically lack worst-case performance guarantees. To achieve the best of both worlds, we design a learning-augmented online algorithm that exhibits two desired properties: (i) consistency: closely approximating the optimal offline algorithm when the ML prediction is accurate and trusted; (ii) robustness: ensuring worst-case performance guarantee even ML predictions are inaccurate. Finally, we perform extensive <b>simulations</b> to show that our online algorithm performs well empirically and that our learning-augmented algorithm achieves both consistency and robustness.</p></p class="citation"></blockquote><h3 id=1242--98290-behavior-generation-with-latent-actions-seungjae-lee-et-al-2024>(12/42 | 98/290) Behavior Generation with Latent Actions (Seungjae Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungjae Lee, Yibin Wang, Haritheja Etukuru, H. Jin Kim, Nur Muhammad Mahi Shafiullah, Lerrel Pinto. (2024)<br><strong>Behavior Generation with Latent Actions</strong><br><button class=copy-to-clipboard title="Behavior Generation with Latent Actions" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 29<br>Keywords: Clustering, Multi-modal, Multi-modal, Quantization, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03181v1.pdf filename=2403.03181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision making. Unlike language or image generation, decision making requires modeling actions - continuous-valued vectors that are <b>multimodal</b> in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior <b>Transformers</b> (BeT) addresses this by discretizing actions using k-means <b>clustering</b> to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior <b>Transformer</b> (VQ-BeT), a versatile model for behavior generation that handles <b>multimodal</b> action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous actions with a hierarchical vector <b>quantization</b> module. Across seven environments including simulated manipulation, autonomous driving, and robotics, VQ-BeT improves on state-of-the-art models such as BeT and Diffusion Policies. Importantly, we demonstrate VQ-BeT&rsquo;s improved ability to capture behavior modes while accelerating inference speed 5x over Diffusion Policies. Videos and code can be found <a href=https://sjlee.cc/vq-bet>https://sjlee.cc/vq-bet</a></p></p class="citation"></blockquote><h3 id=1342--99290-pooling-image-datasets-with-multiple-covariate-shift-and-imbalance-sotirios-panagiotis-chytas-et-al-2024>(13/42 | 99/290) Pooling Image Datasets With Multiple Covariate Shift and Imbalance (Sotirios Panagiotis Chytas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sotirios Panagiotis Chytas, Vishnu Suresh Lokhande, Peiran Li, Vikas Singh. (2024)<br><strong>Pooling Image Datasets With Multiple Covariate Shift and Imbalance</strong><br><button class=copy-to-clipboard title="Pooling Image Datasets With Multiple Covariate Shift and Imbalance" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 28<br>Keywords: Representation Learning, Sample Size, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02598v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02598v1.pdf filename=2403.02598v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Small <b>sample</b> <b>sizes</b> are common in many disciplines, which necessitates pooling roughly similar datasets across multiple institutions to study weak but relevant associations between images and disease outcomes. Such data often manifest shift/imbalance in covariates (i.e., secondary non-imaging data). Controlling for such nuisance variables is common within standard statistical analysis, but the ideas do not directly apply to overparameterized models. Consequently, recent work has shown how strategies from invariant <b>representation</b> <b>learning</b> provides a meaningful starting point, but the current repertoire of methods is limited to accounting for shifts/imbalances in just a couple of covariates at a time. In this paper, we show how viewing this problem from the perspective of Category theory provides a simple and effective solution that completely avoids elaborate multi-stage training pipelines that would otherwise be needed. We show the effectiveness of this approach via extensive experiments on real datasets. Further, we discuss how this style of formulation offers a unified perspective on at least 5+ distinct problem settings, from <b>self-supervised</b> <b>learning</b> to matching problems in 3D reconstruction.</p></p class="citation"></blockquote><h3 id=1442--100290-the-wmdp-benchmark-measuring-and-reducing-malicious-use-with-unlearning-nathaniel-li-et-al-2024>(14/42 | 100/290) The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning (Nathaniel Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao, Ariel Herbert-Voss, Cort B. Breuer, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Liu, Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen Fitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexandr Wang, Dan Hendrycks. (2024)<br><strong>The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning</strong><br><button class=copy-to-clipboard title="The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03218v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03218v2.pdf filename=2403.03218v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The White House Executive Order on Artificial Intelligence highlights the risks of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in <b>LLMs.</b> However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) <b>benchmark,</b> a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in <b>LLMs,</b> and second, as a <b>benchmark</b> for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop CUT, a state-of-the-art unlearning method based on controlling model representations. CUT reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from <b>LLMs.</b> We release our <b>benchmark</b> and code publicly at <a href=https://wmdp.ai>https://wmdp.ai</a></p></p class="citation"></blockquote><h3 id=1542--101290-unsupervised-learning-approaches-for-identifying-icu-patient-subgroups-do-results-generalise-harry-mayne-et-al-2024>(15/42 | 101/290) Unsupervised Learning Approaches for Identifying ICU Patient Subgroups: Do Results Generalise? (Harry Mayne et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harry Mayne, Guy Parsons, Adam Mahdi. (2024)<br><strong>Unsupervised Learning Approaches for Identifying ICU Patient Subgroups: Do Results Generalise?</strong><br><button class=copy-to-clipboard title="Unsupervised Learning Approaches for Identifying ICU Patient Subgroups: Do Results Generalise?" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Clustering, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02945v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02945v1.pdf filename=2403.02945v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The use of <b>unsupervised</b> <b>learning</b> to identify patient subgroups has emerged as a potentially promising direction to improve the efficiency of Intensive Care Units (ICUs). By identifying subgroups of patients with similar levels of medical resource need, ICUs could be restructured into a collection of smaller subunits, each catering to a specific group. However, it is unclear whether common patient subgroups exist across different ICUs, which would determine whether ICU restructuring could be operationalised in a standardised manner. In this paper, we tested the hypothesis that common ICU patient subgroups exist by examining whether the results from one existing study generalise to a different dataset. We extracted 16 features representing medical resource need and used consensus <b>clustering</b> to derive patient subgroups, replicating the previous study. We found limited similarities between our results and those of the previous study, providing evidence against the hypothesis. Our findings imply that there is significant variation between ICUs; thus, a standardised restructuring approach is unlikely to be appropriate. Instead, potential efficiency gains might be greater when the number and nature of the subunits are tailored to each ICU individually.</p></p class="citation"></blockquote><h3 id=1642--102290-taylorshift-shifting-the-complexity-of-self-attention-from-squared-to-linear-and-back-using-taylor-softmax-tobias-christian-nauen-et-al-2024>(16/42 | 102/290) TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax (Tobias Christian Nauen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Christian Nauen, Sebastian Palacio, Andreas Dengel. (2024)<br><strong>TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax</strong><br><button class=copy-to-clipboard title="TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T07, I-5-1; I-2-10; I-2-7, cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02920v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02920v1.pdf filename=2403.02920v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The quadratic complexity of the attention mechanism represents one of the biggest hurdles for processing long sequences using <b>Transformers.</b> Current methods, relying on sparse representations or stateful recurrence, sacrifice token-to-token interactions, which ultimately leads to compromises in performance. This paper introduces TaylorShift, a novel reformulation of the Taylor softmax that enables computing full token-to-token interactions in linear time and space. We analytically determine the crossover points where employing TaylorShift becomes more efficient than traditional attention, aligning closely with empirical measurements. Specifically, our findings demonstrate that TaylorShift enhances memory efficiency for sequences as short as 800 tokens and accelerates inference for inputs of approximately 1700 tokens and beyond. For shorter sequences, TaylorShift scales comparably with the vanilla attention. Furthermore, a classification <b>benchmark</b> across five tasks involving long sequences reveals no degradation in accuracy when employing <b>Transformers</b> equipped with TaylorShift. For reproducibility, we provide access to our code under <a href=https://github.com/tobna/TaylorShift>https://github.com/tobna/TaylorShift</a>.</p></p class="citation"></blockquote><h3 id=1742--103290-sofim-stochastic-optimization-using-regularized-fisher-information-matrix-gayathri-c-et-al-2024>(17/42 | 103/290) SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix (Gayathri C et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gayathri C, Mrinmay Sen, A. K. Qin, Raghu Kishore N, Yen-Wei Chen, Balasubramanian Raman. (2024)<br><strong>SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix</strong><br><button class=copy-to-clipboard title="SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02833v1.pdf filename=2403.02833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a new <b>stochastic</b> <b>optimization</b> <b>method</b> based on the regularized Fisher information matrix (FIM), named SOFIM, which can efficiently utilize the FIM to approximate the Hessian matrix for finding Newton&rsquo;s gradient update in large-scale <b>stochastic</b> <b>optimization</b> <b>of</b> machine learning models. It can be viewed as a variant of natural gradient descent (NGD), where the challenge of storing and calculating the full FIM is addressed through making use of the regularized FIM and directly finding the gradient update direction via Sherman-Morrison matrix inversion. Additionally, like the popular Adam method, SOFIM uses the first moment of the gradient to address the issue of non-stationary objectives across mini-batches due to heterogeneous data. The utilization of the regularized FIM and Sherman-Morrison matrix inversion leads to the improved convergence rate with the same space and time complexities as <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> with momentum. The extensive experiments on training deep learning models on several <b>benchmark</b> image classification datasets demonstrate that the proposed SOFIM outperforms <b>SGD</b> with momentum and several state-of-the-art Newton optimization methods, such as Nystrom-SGD, L-BFGS, and AdaHessian, in term of the convergence speed for achieving the pre-specified objectives of training and test losses as well as test accuracy.</p></p class="citation"></blockquote><h3 id=1842--104290-time-weaver-a-conditional-time-series-generation-model-sai-shankar-narasimhan-et-al-2024>(18/42 | 104/290) Time Weaver: A Conditional Time Series Generation Model (Sai Shankar Narasimhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sai Shankar Narasimhan, Shubhankar Agarwal, Oguzhan Akcin, Sujay Sanghavi, Sandeep Chinchali. (2024)<br><strong>Time Weaver: A Conditional Time Series Generation Model</strong><br><button class=copy-to-clipboard title="Time Weaver: A Conditional Time Series Generation Model" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 23<br>Keywords: Benchmarking, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02682v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02682v1.pdf filename=2403.02682v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imagine generating a city&rsquo;s electricity demand pattern based on weather, the presence of an electric vehicle, and location, which could be used for capacity planning during a winter freeze. Such real-world time series are often enriched with paired heterogeneous contextual metadata (weather, location, etc.). Current approaches to time series generation often ignore this paired metadata, and its heterogeneity poses several practical challenges in adapting existing conditional generation approaches from the image, audio, and video domains to the time series domain. To address this gap, we introduce Time Weaver, a novel diffusion-based model that leverages the heterogeneous metadata in the form of categorical, continuous, and even time-variant variables to significantly improve time series generation. Additionally, we show that naive extensions of standard evaluation metrics from the image to the time series domain are insufficient. These metrics do not penalize conditional generation approaches for their poor specificity in reproducing the metadata-specific features in the generated time series. Thus, we innovate a novel evaluation metric that accurately captures the specificity of conditional generation and the realism of the generated time series. We show that Time Weaver outperforms state-of-the-art <b>benchmarks,</b> such as <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs),</b> by up to 27% in downstream classification tasks on real-world energy, medical, air quality, and traffic data sets.</p></p class="citation"></blockquote><h3 id=1942--105290-testam-a-time-enhanced-spatio-temporal-attention-model-with-mixture-of-experts-hyunwook-lee-et-al-2024>(19/42 | 105/290) TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts (Hyunwook Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyunwook Lee, Sungahn Ko. (2024)<br><strong>TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts</strong><br><button class=copy-to-clipboard title="TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 23<br>Keywords: Graph Attention Networks, Graph, Graph Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02600v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02600v1.pdf filename=2403.02600v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate traffic forecasting is challenging due to the complex dependency on road networks, various types of roads, and the abrupt speed change due to the events. Recent works mainly focus on dynamic spatial modeling with adaptive <b>graph</b> <b>embedding</b> or <b>graph</b> <b>attention</b> having less consideration for temporal characteristics and in-situ modeling. In this paper, we propose a novel deep learning model named TESTAM, which individually models recurring and non-recurring traffic patterns by a mixture-of-experts model with three experts on temporal modeling, spatio-temporal modeling with static <b>graph,</b> <b>and</b> dynamic spatio-temporal dependency modeling with dynamic <b>graph.</b> <b>By</b> introducing different experts and properly routing them, TESTAM could better model various circumstances, including spatially isolated nodes, highly related nodes, and recurring and non-recurring events. For the proper routing, we reformulate a <b>gating</b> problem into a classification problem with pseudo labels. Experimental results on three public traffic network datasets, METR-LA, PEMS-BAY, and EXPY-TKY, demonstrate that TESTAM achieves a better indication and modeling of recurring and non-recurring traffic. We published the official code at <a href=https://github.com/HyunWookL/TESTAM>https://github.com/HyunWookL/TESTAM</a></p></p class="citation"></blockquote><h3 id=2042--106290-improving-variational-autoencoder-estimation-from-incomplete-data-with-mixture-variational-families-vaidotas-simkus-et-al-2024>(20/42 | 106/290) Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families (Vaidotas Simkus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vaidotas Simkus, Michael U. Gutmann. (2024)<br><strong>Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families</strong><br><button class=copy-to-clipboard title="Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 62D10, I-2-6; G-3, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03069v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03069v1.pdf filename=2403.03069v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the task of estimating <b>variational</b> <b>autoencoders</b> (VAEs) when the training data is incomplete. We show that missing data increases the complexity of the model&rsquo;s posterior distribution over the latent variables compared to the fully-observed case. The increased complexity may adversely affect the fit of the model due to a mismatch between the <b>variational</b> <b>and</b> model posterior distributions. We introduce two strategies based on (i) finite <b>variational-mixture</b> <b>and</b> (ii) imputation-based <b>variational-mixture</b> <b>distributions</b> to address the increased posterior complexity. Through a comprehensive evaluation of the proposed approaches, we show that <b>variational</b> <b>mixtures</b> are effective at improving the accuracy of VAE estimation from incomplete data.</p></p class="citation"></blockquote><h3 id=2142--107290-injecttst-a-transformer-method-of-injecting-global-information-into-independent-channels-for-long-time-series-forecasting-ce-chi-et-al-2024>(21/42 | 107/290) InjectTST: A Transformer Method of Injecting Global Information into Independent Channels for Long Time Series Forecasting (Ce Chi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ce Chi, Xing Wang, Kexin Yang, Zhiyan Song, Di Jin, Lin Zhu, Chao Deng, Junlan Feng. (2024)<br><strong>InjectTST: A Transformer Method of Injecting Global Information into Independent Channels for Long Time Series Forecasting</strong><br><button class=copy-to-clipboard title="InjectTST: A Transformer Method of Injecting Global Information into Independent Channels for Long Time Series Forecasting" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Transformer, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02814v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02814v1.pdf filename=2403.02814v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer</b> has become one of the most popular architectures for multivariate time series <b>(MTS)</b> forecasting. Recent <b>Transformer-based</b> <b>MTS</b> models generally prefer channel-independent structures with the observation that channel independence can alleviate noise and distribution drift issues, leading to more robustness. Nevertheless, it is essential to note that channel dependency remains an inherent characteristic of <b>MTS,</b> carrying valuable information. Designing a model that incorporates merits of both channel-independent and channel-mixing structures is a key to further improvement of <b>MTS</b> forecasting, which poses a challenging conundrum. To address the problem, an injection method for global information into channel-independent <b>Transformer,</b> InjectTST, is proposed in this paper. Instead of designing a channel-mixing model directly, we retain the channel-independent backbone and gradually inject global information into individual channels in a selective way. A channel identifier, a global mixing module and a self-contextual attention module are devised in InjectTST. The channel identifier can help <b>Transformer</b> distinguish channels for better representation. The global mixing module produces cross-channel global information. Through the self-contextual attention module, the independent channels can selectively concentrate on useful global information without robustness degradation, and channel mixing is achieved implicitly. Experiments indicate that InjectTST can achieve stable improvement compared with state-of-the-art models.</p></p class="citation"></blockquote><h3 id=2242--108290-g4-attention-deep-learning-model-with-attention-for-predicting-dna-g-quadruplexes-shrimon-mukherjee-et-al-2024>(22/42 | 108/290) G4-Attention: Deep Learning Model with Attention for predicting DNA G-Quadruplexes (Shrimon Mukherjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shrimon Mukherjee, Pulakesh Pramanik, Partha Basuchowdhuri, Santanu Bhattacharya. (2024)<br><strong>G4-Attention: Deep Learning Model with Attention for predicting DNA G-Quadruplexes</strong><br><button class=copy-to-clipboard title="G4-Attention: Deep Learning Model with Attention for predicting DNA G-Quadruplexes" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02765v1.pdf filename=2403.02765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>G-Quadruplexes are the four-stranded non-canonical nucleic acid secondary structures, formed by the stacking arrangement of the guanine tetramers. They are involved in a wide range of biological roles because of their exceptionally unique and distinct structural characteristics. After the completion of the human genome sequencing project, a lot of bioinformatic algorithms were introduced to predict the active G4s regions \textit{in vitro} based on the canonical G4 sequence elements, G-\textit{richness}, and G-\textit{skewness}, as well as the non-canonical sequence features. Recently, sequencing techniques like G4-seq and G4-ChIP-seq were developed to map the G4s \textit{in vitro}, and \textit{in vivo} respectively at a few hundred base resolution. Subsequently, several machine learning approaches were developed for predicting the G4 regions using the existing databases. However, their prediction models were simplistic, and the prediction accuracy was notably poor. In response, here, we propose a novel <b>convolutional</b> <b>neural</b> <b>network</b> with Bi-LSTM and attention layers, named G4-attention, to predict the G4 forming sequences with improved accuracy. G4-attention achieves high accuracy and attains state-of-the-art results in the G4 prediction task. Our model also predicts the G4 regions accurately in the highly class-imbalanced datasets. In addition, the developed model trained on the human genome dataset can be applied to any non-human genome DNA sequences to predict the G4 formation propensities.</p></p class="citation"></blockquote><h3 id=2342--109290-sgd-with-partial-hessian-for-deep-neural-networks-optimization-ying-sun-et-al-2024>(23/42 | 109/290) SGD with Partial Hessian for Deep Neural Networks Optimization (Ying Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Sun, Hongwei Yong, Lei Zhang. (2024)<br><strong>SGD with Partial Hessian for Deep Neural Networks Optimization</strong><br><button class=copy-to-clipboard title="SGD with Partial Hessian for Deep Neural Networks Optimization" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02681v1.pdf filename=2403.02681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the effectiveness of second-order algorithms in solving classical optimization problems, designing second-order optimizers to train deep neural networks (DNNs) has attracted much research interest in recent years. However, because of the very high dimension of intermediate features in DNNs, it is difficult to directly compute and store the Hessian matrix for network optimization. Most of the previous second-order methods approximate the Hessian information imprecisely, resulting in unstable performance. In this work, we propose a compound optimizer, which is a combination of a second-order optimizer with a precise partial Hessian matrix for updating channel-wise parameters and the first-order <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> optimizer for updating the other parameters. We show that the associated Hessian matrices of channel-wise parameters are diagonal and can be extracted directly and precisely from Hessian-free methods. The proposed method, namely <b>SGD</b> with Partial Hessian <b>(SGD-PH),</b> inherits the advantages of both first-order and second-order optimizers. Compared with first-order optimizers, it adopts a certain amount of information from the Hessian matrix to assist optimization, while compared with the existing second-order optimizers, it keeps the good generalization performance of first-order optimizers. Experiments on image classification tasks demonstrate the effectiveness of our proposed optimizer <b>SGD-PH.</b> The code is publicly available at \url{https://github.com/myingysun/SGDPH}.</p></p class="citation"></blockquote><h3 id=2442--110290-splagger-split-aggregation-for-meta-reinforcement-learning-jacob-beck-et-al-2024>(24/42 | 110/290) SplAgger: Split Aggregation for Meta-Reinforcement Learning (Jacob Beck et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacob Beck, Matthew Jackson, Risto Vuorio, Zheng Xiong, Shimon Whiteson. (2024)<br><strong>SplAgger: Split Aggregation for Meta-Reinforcement Learning</strong><br><button class=copy-to-clipboard title="SplAgger: Split Aggregation for Meta-Reinforcement Learning" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Black Box, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03020v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03020v2.pdf filename=2403.03020v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A core ambition of <b>reinforcement</b> <b>learning</b> (RL) is the creation of agents capable of rapid learning in novel tasks. Meta-RL aims to achieve this by directly learning such agents. <b>Black</b> <b>box</b> methods do so by training off-the-shelf sequence models end-to-end. By contrast, task inference methods explicitly infer a posterior distribution over the unknown task, typically using distinct objectives and sequence models designed to enable task inference. Recent work has shown that task inference methods are not necessary for strong performance. However, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not. In this paper, we present strong evidence that task inference sequence models are still beneficial. In particular, we investigate sequence models with permutation invariant aggregation, which exploit the fact that, due to the Markov property, the task posterior does not depend on the order of data. We empirically confirm the advantage of permutation invariant sequence models without the use of task inference objectives. However, we also find, surprisingly, that there are multiple conditions under which permutation variance remains useful. Therefore, we propose SplAgger, which uses both permutation variant and invariant components to achieve the best of both worlds, outperforming all baselines on continuous control and memory environments.</p></p class="citation"></blockquote><h3 id=2542--111290-solution-simplex-clustering-for-heterogeneous-federated-learning-dennis-grinwald-et-al-2024>(25/42 | 111/290) Solution Simplex Clustering for Heterogeneous Federated Learning (Dennis Grinwald et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dennis Grinwald, Philipp Wiesner, Shinichi Nakajima. (2024)<br><strong>Solution Simplex Clustering for Heterogeneous Federated Learning</strong><br><button class=copy-to-clipboard title="Solution Simplex Clustering for Heterogeneous Federated Learning" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03333v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03333v1.pdf filename=2403.03333v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We tackle a major challenge in <b>federated</b> <b>learning</b> (FL) &ndash; achieving good performance under highly heterogeneous client distributions. The difficulty partially arises from two seemingly contradictory goals: learning a common model by aggregating the information from clients, and learning local personalized models that should be adapted to each local distribution. In this work, we propose Solution Simplex Clustered <b>Federated</b> <b>Learning</b> (SosicFL) for dissolving such contradiction. Based on the recent ideas of learning solution simplices, SosicFL assigns a subregion in a simplex to each client, and performs FL to learn a common solution simplex. This allows the client models to possess their characteristics within the degrees of freedom in the solution simplex, and at the same time achieves the goal of learning a global common model. Our experiments show that SosicFL improves the performance and accelerates the training process for global and personalized FL with minimal computational overhead.</p></p class="citation"></blockquote><h3 id=2642--112290-dynamic-gaussian-graph-operator-learning-parametric-partial-differential-equations-in-arbitrary-discrete-mechanics-problems-chu-wang-et-al-2024>(26/42 | 112/290) Dynamic Gaussian Graph Operator: Learning parametric partial differential equations in arbitrary discrete mechanics problems (Chu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chu Wang, Jinhong Wu, Yanzhi Wang, Zhijian Zha, Qi Zhou. (2024)<br><strong>Dynamic Gaussian Graph Operator: Learning parametric partial differential equations in arbitrary discrete mechanics problems</strong><br><button class=copy-to-clipboard title="Dynamic Gaussian Graph Operator: Learning parametric partial differential equations in arbitrary discrete mechanics problems" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Message-Passing, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02810v1.pdf filename=2403.02810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning methods have access to be employed for solving physical systems governed by parametric partial differential equations (PDEs) due to massive scientific data. It has been refined to operator learning that focuses on learning non-linear mapping between infinite-dimensional function spaces, offering interface from observations to solutions. However, state-of-the-art neural operators are limited to constant and uniform discretization, thereby leading to deficiency in generalization on arbitrary discretization schemes for computational domain. In this work, we propose a novel operator learning algorithm, referred to as Dynamic Gaussian <b>Graph</b> Operator (DGGO) that expands neural operators to learning parametric PDEs in arbitrary discrete mechanics problems. The Dynamic Gaussian <b>Graph</b> (DGG) kernel learns to map the observation vectors defined in general Euclidean space to metric vectors defined in high-dimensional uniform metric space. The DGG integral kernel is parameterized by Gaussian kernel weighted Riemann sum approximating and using dynamic message passing <b>graph</b> to depict the interrelation within the integral term. Fourier Neural Operator is selected to localize the metric vectors on spatial and frequency domains. Metric vectors are regarded as located on latent uniform domain, wherein spatial and spectral transformation offer highly regular constraints on solution space. The efficiency and robustness of DGGO are validated by applying it to solve numerical arbitrary discrete mechanics problems in comparison with mainstream neural operators. Ablation experiments are implemented to demonstrate the effectiveness of spatial transformation in the DGG kernel. The proposed method is utilized to forecast stress field of hyper-elastic material with geometrically variable void as engineering application.</p></p class="citation"></blockquote><h3 id=2742--113290-learning-to-defer-to-a-population-a-meta-learning-approach-dharmesh-tailor-et-al-2024>(27/42 | 113/290) Learning to Defer to a Population: A Meta-Learning Approach (Dharmesh Tailor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dharmesh Tailor, Aditya Patra, Rajeev Verma, Putra Manggala, Eric Nalisnick. (2024)<br><strong>Learning to Defer to a Population: A Meta-Learning Approach</strong><br><button class=copy-to-clipboard title="Learning to Defer to a Population: A Meta-Learning Approach" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 13<br>Keywords: Benchmarking, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02683v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02683v1.pdf filename=2403.02683v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The learning to defer (L2D) framework allows autonomous systems to be safe and robust by allocating difficult decisions to a human expert. All existing work on L2D assumes that each expert is well-identified, and if any expert were to change, the system should be re-trained. In this work, we alleviate this constraint, formulating an L2D system that can cope with never-before-seen experts at test-time. We accomplish this by using <b>meta-learning,</b> <b>considering</b> both optimization- and model-based variants. Given a small context set to characterize the currently available expert, our framework can quickly adapt its deferral policy. For the model-based approach, we employ an attention mechanism that is able to look for points in the context set that are similar to a given test point, leading to an even more precise assessment of the expert&rsquo;s abilities. In the experiments, we validate our methods on image recognition, traffic sign detection, and skin lesion diagnosis <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=2842--114290-leveraging-federated-learning-for-automatic-detection-of-clopidogrel-treatment-failures-samuel-kim-et-al-2024>(28/42 | 114/290) Leveraging Federated Learning for Automatic Detection of Clopidogrel Treatment Failures (Samuel Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Kim, Min Sang Kim. (2024)<br><strong>Leveraging Federated Learning for Automatic Detection of Clopidogrel Treatment Failures</strong><br><button class=copy-to-clipboard title="Leveraging Federated Learning for Automatic Detection of Clopidogrel Treatment Failures" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03368v1.pdf filename=2403.03368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The effectiveness of clopidogrel, a widely used antiplatelet medication, varies significantly among individuals, necessitating the development of precise predictive models to optimize patient care. In this study, we leverage <b>federated</b> <b>learning</b> strategies to address clopidogrel treatment failure detection. Our research harnesses the collaborative power of multiple healthcare institutions, allowing them to jointly train machine learning models while safeguarding sensitive patient data. Utilizing the UK Biobank dataset, which encompasses a vast and diverse population, we partitioned the data based on geographic centers and evaluated the performance of <b>federated</b> <b>learning.</b> Our results show that while centralized training achieves higher Area Under the Curve (AUC) values and faster convergence, <b>federated</b> <b>learning</b> approaches can substantially narrow this performance gap. Our findings underscore the potential of <b>federated</b> <b>learning</b> in addressing clopidogrel treatment failure detection, offering a promising avenue for enhancing patient care through personalized treatment strategies while respecting data privacy. This study contributes to the growing body of research on <b>federated</b> <b>learning</b> in healthcare and lays the groundwork for secure and privacy-preserving predictive models for various medical conditions.</p></p class="citation"></blockquote><h3 id=2942--115290-not-all-tickets-are-equal-and-we-know-it-guiding-pruning-with-domain-specific-knowledge-intekhab-hossain-et-al-2024>(29/42 | 115/290) Not all tickets are equal and we know it: Guiding pruning with domain-specific knowledge (Intekhab Hossain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Intekhab Hossain, Jonas Fischer, Rebekka Burkholz, John Quackenbush. (2024)<br><strong>Not all tickets are equal and we know it: Guiding pruning with domain-specific knowledge</strong><br><button class=copy-to-clipboard title="Not all tickets are equal and we know it: Guiding pruning with domain-specific knowledge" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-QM, stat-AP, stat-ML<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04805v1.pdf filename=2403.04805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural structure learning is of paramount importance for scientific discovery and interpretability. Yet, contemporary <b>pruning</b> algorithms that focus on computational resource efficiency face algorithmic barriers to select a meaningful model that aligns with domain expertise. To mitigate this challenge, we propose DASH, which guides <b>pruning</b> by available domain-specific structural information. In the context of learning dynamic gene regulatory network models, we show that DASH combined with existing general knowledge on interaction partners provides data-specific insights aligned with biology. For this task, we show on synthetic data with ground truth information and two real world applications the effectiveness of DASH, which outperforms competing methods by a large margin and provides more meaningful biological insights. Our work shows that domain specific structural information bears the potential to improve model-derived scientific insights.</p></p class="citation"></blockquote><h3 id=3042--116290-lc-tsalis-inf-generalized-best-of-both-worlds-linear-contextual-bandits-masahiro-kato-et-al-2024>(30/42 | 116/290) LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits (Masahiro Kato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masahiro Kato, Shinji Ito. (2024)<br><strong>LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits</strong><br><button class=copy-to-clipboard title="LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03219v1.pdf filename=2403.03219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study considers the linear contextual <b>bandit</b> problem with independent and identically distributed (i.i.d.) contexts. In this problem, existing studies have proposed Best-of-Both-Worlds (BoBW) algorithms whose regrets satisfy $O(\log^2(T))$ for the number of rounds $T$ in a stochastic regime with a suboptimality gap lower-bounded by a positive constant, while satisfying $O(\sqrt{T})$ in an adversarial regime. However, the dependency on $T$ has room for improvement, and the suboptimality-gap assumption can be relaxed. For this issue, this study proposes an algorithm whose regret satisfies $O(\log(T))$ in the setting when the suboptimality gap is lower-bounded. Furthermore, we introduce a margin condition, a milder assumption on the suboptimality gap. That condition characterizes the problem difficulty linked to the suboptimality gap using a parameter $\beta \in (0, \infty]$. We then show that the algorithm&rsquo;s regret satisfies $O\left(\left{\log(T)\right}^{\frac{1+\beta}{2+\beta}}T^{\frac{1}{2+\beta}}\right)$. Here, $\beta= \infty$ corresponds to the case in the existing studies where a lower bound exists in the suboptimality gap, and our regret satisfies $O(\log(T))$ in that case. Our proposed algorithm is based on the Follow-The-Regularized-Leader with the Tsallis entropy and referred to as the $\alpha$-Linear-Contextual (LC)-Tsallis-INF.</p></p class="citation"></blockquote><h3 id=3142--117290-deep-learned-compression-for-radio-frequency-signal-classification-armani-rodriguez-et-al-2024>(31/42 | 117/290) Deep-Learned Compression for Radio-Frequency Signal Classification (Armani Rodriguez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Armani Rodriguez, Yagna Kaasaragadda, Silvija Kokalj-Filipovic. (2024)<br><strong>Deep-Learned Compression for Radio-Frequency Signal Classification</strong><br><button class=copy-to-clipboard title="Deep-Learned Compression for Radio-Frequency Signal Classification" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NI, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03150v1.pdf filename=2403.03150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Next-generation cellular concepts rely on the processing of large quantities of radio-frequency (RF) samples. This includes Radio Access Networks (RAN) connecting the cellular front-end based on software defined radios (SDRs) and a framework for the AI processing of spectrum-related data. The RF data collected by the dense RAN radio units and spectrum sensors may need to be jointly processed for intelligent decision making. Moving large amounts of data to AI agents may result in significant bandwidth and latency costs. We propose a deep learned compression (DLC) model, HQARF, based on learned vector <b>quantization</b> (VQ), to compress the complex-valued samples of RF signals comprised of 6 modulation classes. We are assessing the effects of HQARF on the performance of an AI model trained to infer the modulation class of the RF signal. Compression of narrow-band RF samples for the training and off-the-site inference will allow for an efficient use of the bandwidth and storage for non-real-time analytics, and for a decreased delay in real-time applications. While exploring the effectiveness of the HQARF signal reconstructions in modulation classification tasks, we highlight the DLC optimization space and some open problems related to the training of the VQ embedded in HQARF.</p></p class="citation"></blockquote><h3 id=3242--118290-emergent-equivariance-in-deep-ensembles-jan-e-gerken-et-al-2024>(32/42 | 118/290) Emergent Equivariance in Deep Ensembles (Jan E. Gerken et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan E. Gerken, Pan Kessel. (2024)<br><strong>Emergent Equivariance in Deep Ensembles</strong><br><button class=copy-to-clipboard title="Emergent Equivariance in Deep Ensembles" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03103v1.pdf filename=2403.03103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We demonstrate that deep ensembles are secretly equivariant models. More precisely, we show that deep ensembles become equivariant for all inputs and at all training times by simply using <b>data</b> <b>augmentation.</b> Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit. The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments.</p></p class="citation"></blockquote><h3 id=3342--119290-recall-oriented-continual-learning-with-generative-adversarial-meta-model-haneol-kang-et-al-2024>(33/42 | 119/290) Recall-Oriented Continual Learning with Generative Adversarial Meta-Model (Haneol Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haneol Kang, Dong-Wan Choi. (2024)<br><strong>Recall-Oriented Continual Learning with Generative Adversarial Meta-Model</strong><br><button class=copy-to-clipboard title="Recall-Oriented Continual Learning with Generative Adversarial Meta-Model" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03082v1.pdf filename=2403.03082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The stability-plasticity dilemma is a major challenge in <b>continual</b> <b>learning,</b> as it involves balancing the conflicting objectives of maintaining performance on previous tasks while learning new tasks. In this paper, we propose the recall-oriented <b>continual</b> <b>learning</b> framework to address this challenge. Inspired by the human brain&rsquo;s ability to separate the mechanisms responsible for stability and plasticity, our framework consists of a two-level architecture where an inference network effectively acquires new knowledge and a generative network recalls past knowledge when necessary. In particular, to maximize the stability of past knowledge, we investigate the complexity of knowledge depending on different representations, and thereby introducing generative adversarial meta-model (GAMM) that incrementally learns task-specific parameters instead of input data samples of the task. Through our experiments, we show that our framework not only effectively learns new knowledge without any disruption but also achieves high stability of previous knowledge in both task-aware and task-agnostic learning scenarios. Our code is available at: <a href=https://github.com/bigdata-inha/recall-oriented-cl-framework>https://github.com/bigdata-inha/recall-oriented-cl-framework</a>.</p></p class="citation"></blockquote><h3 id=3442--120290-on-the-asymptotic-mean-square-error-optimality-of-diffusion-probabilistic-models-benedikt-fesl-et-al-2024>(34/42 | 120/290) On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models (Benedikt Fesl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benedikt Fesl, Benedikt Böck, Florian Strasser, Michael Baur, Michael Joham, Wolfgang Utschick. (2024)<br><strong>On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models</strong><br><button class=copy-to-clipboard title="On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02957v1.pdf filename=2403.02957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion <b>probabilistic</b> <b>models</b> (DPMs) have recently shown great potential for denoising tasks. Despite their practical utility, there is a notable gap in their theoretical understanding. This paper contributes novel theoretical insights by rigorously proving the asymptotic convergence of a specific DPM denoising strategy to the mean square error (MSE)-optimal conditional mean estimator (CME) over a large number of diffusion steps. The studied DPM-based denoiser shares the training procedure of DPMs but distinguishes itself by forwarding only the conditional mean during the reverse inference process after training. We highlight the unique perspective that DPMs are composed of an asymptotically optimal denoiser while simultaneously inheriting a powerful generator by switching re-sampling in the reverse process on and off. The theoretical findings are validated by numerical results.</p></p class="citation"></blockquote><h3 id=3542--121290-remove-that-square-root-a-new-efficient-scale-invariant-version-of-adagrad-sayantan-choudhury-et-al-2024>(35/42 | 121/290) Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad (Sayantan Choudhury et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayantan Choudhury, Nazarii Tupitsa, Nicolas Loizou, Samuel Horvath, Martin Takac, Eduard Gorbunov. (2024)<br><strong>Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad</strong><br><button class=copy-to-clipboard title="Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02648v1.pdf filename=2403.02648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}} \right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and <b>text</b> <b>classification</b> on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.</p></p class="citation"></blockquote><h3 id=3642--122290-a-note-on-high-probability-analysis-of-algorithms-with-exponential-sub-gaussian-and-general-light-tails-amit-attia-et-al-2024>(36/42 | 122/290) A Note on High-Probability Analysis of Algorithms with Exponential, Sub-Gaussian, and General Light Tails (Amit Attia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amit Attia, Tomer Koren. (2024)<br><strong>A Note on High-Probability Analysis of Algorithms with Exponential, Sub-Gaussian, and General Light Tails</strong><br><button class=copy-to-clipboard title="A Note on High-Probability Analysis of Algorithms with Exponential, Sub-Gaussian, and General Light Tails" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DS, cs-LG, cs.LG, math-PR<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02873v1.pdf filename=2403.02873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This short note describes a simple technique for analyzing probabilistic algorithms that rely on a light-tailed (but not necessarily bounded) source of randomization. We show that the analysis of such an algorithm can be reduced, in a <b>black-box</b> <b>manner</b> and with only a small loss in logarithmic factors, to an analysis of a simpler variant of the same algorithm that uses bounded random variables and often easier to analyze. This approach simultaneously applies to any light-tailed randomization, including exponential, sub-Gaussian, and more general fast-decaying distributions, without needing to appeal to specialized concentration inequalities. Analyses of a generalized Azuma inequality and stochastic optimization with general light-tailed noise are provided to illustrate the technique.</p></p class="citation"></blockquote><h3 id=3742--123290-pareto-optimal-estimation-and-policy-learning-on-short-term-and-long-term-treatment-effects-yingrong-wang-et-al-2024>(37/42 | 123/290) Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects (Yingrong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingrong Wang, Anpeng Wu, Haoxuan Li, Weiming Liu, Qiaowei Miao, Ruoxuan Xiong, Fei Wu, Kun Kuang. (2024)<br><strong>Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects</strong><br><button class=copy-to-clipboard title="Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02624v1.pdf filename=2403.02624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on developing Pareto-optimal estimation and policy learning to identify the most effective treatment that maximizes the total reward from both short-term and long-term effects, which might conflict with each other. For example, a higher dosage of medication might increase the speed of a patient&rsquo;s recovery (short-term) but could also result in severe long-term side effects. Although recent works have investigated the problems about short-term or long-term effects or the both, how to trade-off between them to achieve optimal treatment remains an open challenge. Moreover, when multiple objectives are directly estimated using conventional causal <b>representation</b> <b>learning,</b> the optimization directions among various tasks can conflict as well. In this paper, we systematically investigate these issues and introduce a Pareto-Efficient algorithm, comprising Pareto-Optimal Estimation (POE) and Pareto-Optimal Policy Learning (POPL), to tackle them. POE incorporates a continuous Pareto module with <b>representation</b> <b>balancing,</b> enhancing estimation efficiency across multiple tasks. As for POPL, it involves deriving short-term and long-term outcomes linked with various treatment levels, facilitating an exploration of the Pareto frontier emanating from these outcomes. Results on both the synthetic and real-world datasets demonstrate the superiority of our method.</p></p class="citation"></blockquote><h3 id=3842--124290-tartanaviation-image-speech-and-ads-b-trajectory-datasets-for-terminal-airspace-operations-jay-patrikar-et-al-2024>(38/42 | 124/290) TartanAviation: Image, Speech, and ADS-B Trajectory Datasets for Terminal Airspace Operations (Jay Patrikar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jay Patrikar, Joao Dantas, Brady Moon, Milad Hamidi, Sourish Ghosh, Nikhil Keetha, Ian Higgins, Atharva Chandak, Takashi Yoneyama, Sebastian Scherer. (2024)<br><strong>TartanAviation: Image, Speech, and ADS-B Trajectory Datasets for Terminal Airspace Operations</strong><br><button class=copy-to-clipboard title="TartanAviation: Image, Speech, and ADS-B Trajectory Datasets for Terminal Airspace Operations" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03372v1.pdf filename=2403.03372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce TartanAviation, an open-source <b>multi-modal</b> dataset focused on terminal-area airspace operations. TartanAviation provides a holistic view of the airport environment by concurrently collecting image, speech, and ADS-B trajectory data using setups installed inside airport boundaries. The datasets were collected at both towered and non-towered airfields across multiple months to capture diversity in aircraft operations, seasons, aircraft types, and weather conditions. In total, TartanAviation provides 3.1M images, 3374 hours of Air Traffic Control speech data, and 661 days of ADS-B trajectory data. The data was filtered, processed, and validated to create a curated dataset. In addition to the dataset, we also open-source the code-base used to collect and pre-process the dataset, further enhancing accessibility and usability. We believe this dataset has many potential use cases and would be particularly vital in allowing AI and machine learning technologies to be integrated into air traffic control systems and advance the adoption of autonomous aircraft in the airspace.</p></p class="citation"></blockquote><h3 id=3942--125290-credibility-aware-multi-modal-fusion-using-probabilistic-circuits-sahil-sidheekh-et-al-2024>(39/42 | 125/290) Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits (Sahil Sidheekh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sahil Sidheekh, Pranuthi Tenali, Saurabh Mathur, Erik Blasch, Kristian Kersting, Sriraam Natarajan. (2024)<br><strong>Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits</strong><br><button class=copy-to-clipboard title="Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03281v1.pdf filename=2403.03281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of late <b>multi-modal</b> fusion for discriminative learning. Motivated by noisy, multi-source domains that require understanding the reliability of each data source, we explore the notion of credibility in the context of <b>multi-modal</b> fusion. We propose a combination function that uses probabilistic circuits (PCs) to combine predictive distributions over individual modalities. We also define a probabilistic measure to evaluate the credibility of each modality via inference queries over the PC. Our experimental evaluation demonstrates that our fusion method can reliably infer credibility while maintaining competitive performance with the state-of-the-art.</p></p class="citation"></blockquote><h3 id=4042--126290-crispr-ensemble-model-mohammad-rostami-et-al-2024>(40/42 | 126/290) CRISPR: Ensemble Model (Mohammad Rostami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Rostami, Amin Ghariyazi, Hamed Dashti, Mohammad Hossein Rohban, Hamid R. Rabiee. (2024)<br><strong>CRISPR: Ensemble Model</strong><br><button class=copy-to-clipboard title="CRISPR: Ensemble Model" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-GN<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03018v1.pdf filename=2403.03018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) is a gene editing technology that has revolutionized the fields of biology and medicine. However, one of the challenges of using CRISPR is predicting the on-target efficacy and off-target sensitivity of single-guide RNAs (sgRNAs). This is because most existing methods are trained on separate datasets with different genes and cells, which limits their generalizability. In this paper, we propose a novel ensemble learning method for sgRNA design that is accurate and generalizable. Our method combines the predictions of multiple machine learning models to produce a single, more robust prediction. This approach allows us to learn from a wider range of data, which improves the generalizability of our model. We evaluated our method on a <b>benchmark</b> dataset of sgRNA designs and found that it outperformed existing methods in terms of both accuracy and generalizability. Our results suggest that our method can be used to design sgRNAs with high sensitivity and specificity, even for new genes or cells. This could have important implications for the clinical use of CRISPR, as it would allow researchers to design more effective and safer treatments for a variety of diseases.</p></p class="citation"></blockquote><h3 id=4142--127290-dirichlet-based-per-sample-weighting-by-transition-matrix-for-noisy-label-learning-heesun-bae-et-al-2024>(41/42 | 127/290) Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning (HeeSun Bae et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>HeeSun Bae, Seungjae Shin, Byeonghu Na, Il-Chul Moon. (2024)<br><strong>Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning</strong><br><button class=copy-to-clipboard title="Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02690v1.pdf filename=2403.02690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For learning with noisy labels, the transition matrix, which explicitly models the relation between noisy label distribution and clean label distribution, has been utilized to achieve the statistical consistency of either the classifier or the risk. Previous researches have focused more on how to estimate this transition matrix well, rather than how to utilize it. We propose good utilization of the transition matrix is crucial and suggest a new utilization method based on resampling, coined RENT. Specifically, we first demonstrate current utilizations can have potential limitations for implementation. As an extension to Reweighting, we suggest the Dirichlet distribution-based per-sample Weight Sampling (DWS) framework, and compare reweighting and resampling under DWS framework. With the analyses from DWS, we propose RENT, a REsampling method with Noise Transition matrix. Empirically, RENT consistently outperforms existing transition matrix utilization methods, which includes reweighting, on various <b>benchmark</b> datasets. Our code is available at \url{https://github.com/BaeHeeSun/RENT}.</p></p class="citation"></blockquote><h3 id=4242--128290-dnnlasso-scalable-graph-learning-for-matrix-variate-data-meixia-lin-et-al-2024>(42/42 | 128/290) DNNLasso: Scalable Graph Learning for Matrix-Variate Data (Meixia Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meixia Lin, Yangjing Zhang. (2024)<br><strong>DNNLasso: Scalable Graph Learning for Matrix-Variate Data</strong><br><button class=copy-to-clipboard title="DNNLasso: Scalable Graph Learning for Matrix-Variate Data" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02608v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02608v1.pdf filename=2403.02608v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of jointly learning row-wise and column-wise dependencies of matrix-variate observations, which are modelled separately by two precision matrices. Due to the complicated structure of Kronecker-product precision matrices in the commonly used matrix-variate Gaussian graphical models, a sparser Kronecker-sum structure was proposed recently based on the Cartesian product of <b>graphs.</b> However, existing methods for estimating Kronecker-sum structured precision matrices do not scale well to large scale datasets. In this paper, we introduce DNNLasso, a diagonally non-negative graphical lasso model for estimating the Kronecker-sum structured precision matrix, which outperforms the state-of-the-art methods by a large margin in both accuracy and computational time. Our code is available at <a href=https://github.com/YangjingZhang/DNNLasso>https://github.com/YangjingZhang/DNNLasso</a>.</p></p class="citation"></blockquote><h2 id=eesssp-2>eess.SP (2)</h2><h3 id=12--129290-arnn-attentive-recurrent-neural-network-for-multi-channel-eeg-signals-to-identify-epileptic-seizures-salim-rukhsar-et-al-2024>(1/2 | 129/290) ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures (Salim Rukhsar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salim Rukhsar, Anil Kumar Tiwari. (2024)<br><strong>ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures</strong><br><button class=copy-to-clipboard title="ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-AI, cs-LG, eess-SP, eess.SP<br>Keyword Score: 90<br>Keywords: Vision Transformer, Convolution, LSTM, LSTM, LSTM, Recurrent Neural Network, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03276v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03276v1.pdf filename=2403.03276v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We proposed an Attentive <b>Recurrent</b> <b>Neural</b> <b>Network</b> (ARNN), which recurrently applies attention layers along a sequence and has linear complexity with respect to the sequence length. The proposed model operates on multi-channel EEG signals rather than single channel signals and leverages parallel computation. In this cell, the attention layer is a computational unit that efficiently applies <b>self-attention</b> and cross-attention mechanisms to compute a <b>recurrent</b> <b>function</b> <b>over</b> a wide number of state vectors and input signals. Our architecture is inspired in part by the attention layer and <b>long</b> <b>short-term</b> <b>memory</b> <b>(LSTM)</b> cells, and it uses <b>long-short</b> <b>style</b> <b>gates,</b> <b>but</b> it scales this typical cell up by several orders to parallelize for multi-channel EEG signals. It inherits the advantages of attention layers and <b>LSTM</b> gate while avoiding their respective drawbacks. We evaluated the model effectiveness through extensive experiments with heterogeneous datasets, including the CHB-MIT and UPenn and Mayos Clinic, CHB-MIT datasets. The empirical findings suggest that the ARNN model outperforms baseline methods such as <b>LSTM,</b> <b>Vision</b> <b>Transformer</b> (ViT), Compact <b>Convolution</b> <b>Transformer</b> (CCT), and R-Transformer (RT), showcasing superior performance and faster processing capabilities across a wide range of tasks. The code has been made publicly accessible at \url{https://github.com/Salim-Lysiun/ARNN}.</p></p class="citation"></blockquote><h3 id=22--130290-low-complexity-linear-decoupling-of-users-for-uplink-massive-mu-mimo-detection-s-sowmya-et-al-2024>(2/2 | 130/290) Low-Complexity Linear Decoupling of Users for Uplink Massive MU-MIMO Detection (S. Sowmya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S. Sowmya, Gokularam Muthukrishnan, K. Giridhar. (2024)<br><strong>Low-Complexity Linear Decoupling of Users for Uplink Massive MU-MIMO Detection</strong><br><button class=copy-to-clipboard title="Low-Complexity Linear Decoupling of Users for Uplink Massive MU-MIMO Detection" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03271v1.pdf filename=2403.03271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-user massive MIMO is a promising candidate for future wireless communication systems. It enables users with different requirements to be connected to the same base station (BS) on the same set of resources. In uplink massive MU-MIMO, while users with different requirements are served, decoupled signal detection helps in using a user-specific detection scheme for every user. In this paper, we propose a low-complexity linear decoupling scheme called Sequential Decoupler (SD), which aids in the parallel detection of each user&rsquo;s data streams. The proposed algorithm shows significant complexity reduction, particularly when the number of users in the system increases. In the numerical <b>simulations,</b> it has been observed that the complexity of the proposed scheme is only 0.15% of the conventional Singular Value Decomposition (SVD) based decoupling and 47% to the pseudo-inverse based decoupling schemes when 80 users with two antennas each are served by the BS.</p></p class="citation"></blockquote><h2 id=csro-6>cs.RO (6)</h2><h3 id=16--131290-moka-open-vocabulary-robotic-manipulation-through-mark-based-visual-prompting-fangchen-liu-et-al-2024>(1/6 | 131/290) MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting (Fangchen Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangchen Liu, Kuan Fang, Pieter Abbeel, Sergey Levine. (2024)<br><strong>MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting</strong><br><button class=copy-to-clipboard title="MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 90<br>Keywords: Knowledge Distillation, Zero-shot, Question Answering, Reasoning, Visual Question Answering, In-context Learning, In-context Learning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03174v1.pdf filename=2403.03174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals. While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open <b>question.</b> <b>In</b> this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions. At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM&rsquo;s predictions on RGB images and the robot&rsquo;s motions in the physical world. By <b>prompting</b> a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from broad sources. To scaffold the VLM&rsquo;s <b>reasoning</b> in <b>zero-shot,</b> we propose a <b>visual</b> <b>prompting</b> <b>technique</b> that annotates marks on the images, converting the prediction of keypoints and waypoints into a series of <b>visual</b> <b>question</b> <b>answering</b> problems that are feasible for the VLM to solve. Using the robot experiences collected in this way, we further investigate ways to bootstrap the performance through <b>in-context</b> <b>learning</b> and policy <b>distillation.</b> We evaluate and analyze MOKA&rsquo;s performance on a variety of manipulation tasks specified by free-form language descriptions, such as tool use, deformable body manipulation, and object rearrangement.</p></p class="citation"></blockquote><h3 id=26--132290-spacehopper-a-small-scale-legged-robot-for-exploring-low-gravity-celestial-bodies-alexander-spiridonov-et-al-2024>(2/6 | 132/290) SpaceHopper: A Small-Scale Legged Robot for Exploring Low-Gravity Celestial Bodies (Alexander Spiridonov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Spiridonov, Fabio Buehler, Moriz Berclaz, Valerio Schelbert, Jorit Geurts, Elena Krasnova, Emma Steinke, Jonas Toma, Joschua Wuethrich, Recep Polat, Wim Zimmermann, Philip Arm, Nikita Rudin, Hendrik Kolvenbach, Marco Hutter. (2024)<br><strong>SpaceHopper: A Small-Scale Legged Robot for Exploring Low-Gravity Celestial Bodies</strong><br><button class=copy-to-clipboard title="SpaceHopper: A Small-Scale Legged Robot for Exploring Low-Gravity Celestial Bodies" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02831v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02831v1.pdf filename=2403.02831v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present SpaceHopper, a three-legged, small-scale robot designed for future mobile exploration of asteroids and moons. The robot weighs 5.2kg and has a body size of 245mm while using space-qualifiable components. Furthermore, SpaceHopper&rsquo;s design and controls make it well-adapted for investigating dynamic locomotion modes with extended flight-phases. Instead of gyroscopes or fly-wheels, the system uses its three legs to reorient the body during flight in preparation for landing. We control the leg motion for reorientation using Deep <b>Reinforcement</b> <b>Learning</b> policies. In a <b>simulation</b> of Ceres&rsquo; gravity (0.029g), the robot can reliably jump to commanded positions up to 6m away. Our real-world experiments show that SpaceHopper can successfully reorient to a safe landing orientation within 9.7 degree inside a rotational gimbal and jump in a counterweight setup in Earth&rsquo;s gravity. Overall, we consider SpaceHopper an important step towards controlled jumping locomotion in low-gravity environments.</p></p class="citation"></blockquote><h3 id=36--133290-single-channel-robot-ego-speech-filtering-during-human-robot-interaction-yue-li-et-al-2024>(3/6 | 133/290) Single-Channel Robot Ego-Speech Filtering during Human-Robot Interaction (Yue Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Li, Koen V Hindriks, Florian Kunneman. (2024)<br><strong>Single-Channel Robot Ego-Speech Filtering during Human-Robot Interaction</strong><br><button class=copy-to-clipboard title="Single-Channel Robot Ego-Speech Filtering during Human-Robot Interaction" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: I-2-9, cs-HC, cs-RO, cs-SD, cs.RO, eess-AS<br>Keyword Score: 20<br>Keywords: Convolution, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02918v1.pdf filename=2403.02918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study how well human speech can automatically be filtered when this overlaps with the voice and fan noise of a social robot, Pepper. We ultimately aim for an HRI scenario where the microphone can remain open when the robot is speaking, enabling a more natural turn-taking scheme where the human can interrupt the robot. To respond appropriately, the robot would need to understand what the interlocutor said in the overlapping part of the speech, which can be accomplished by target speech extraction (TSE). To investigate how well TSE can be accomplished in the context of the popular social robot Pepper, we set out to manufacture a datase composed of a mixture of recorded speech of Pepper itself, its fan noise (which is close to the microphones), and human speech as recorded by the Pepper microphone, in a room with low reverberation and high reverberation. Comparing a signal processing approach, with and without post-filtering, and a <b>convolutional</b> <b>recurrent</b> <b>neural</b> <b>network</b> (CRNN) approach to a state-of-the-art speaker identification-based TSE model, we found that the signal processing approach without post-filtering yielded the best performance in terms of Word Error Rate on the overlapping speech signals with low reverberation, while the CRNN approach is more robust for reverberation. These results show that estimating the human voice in overlapping speech with a robot is possible in real-life application, provided that the room reverberation is low and the human speech has a high volume or high pitch.</p></p class="citation"></blockquote><h3 id=46--134290-splat-nav-safe-real-time-robot-navigation-in-gaussian-splatting-maps-timothy-chen-et-al-2024>(4/6 | 134/290) Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps (Timothy Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timothy Chen, Ola Shorinwa, Weijia Zeng, Joseph Bruno, Philip Dames, Mac Schwager. (2024)<br><strong>Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps</strong><br><button class=copy-to-clipboard title="Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02751v1.pdf filename=2403.02751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Splat-Nav, a navigation pipeline that consists of a real-time safe planning module and a robust state estimation module designed to operate in the Gaussian Splatting (GSplat) environment representation, a popular emerging 3D scene representation from computer vision. We formulate rigorous collision constraints that can be computed quickly to build a guaranteed-safe polytope corridor through the map. We then optimize a B-spline trajectory through this corridor. We also develop a real-time, robust state estimation module by interpreting the GSplat representation as a point cloud. The module enables the robot to localize its global pose with zero prior knowledge from RGB-D images using point cloud alignment, and then track its own pose as it moves through the scene from RGB images using image-to-point cloud localization. We also incorporate semantics into the GSplat in order to obtain better images for localization. All of these modules operate mainly on CPU, freeing up GPU resources for tasks like real-time scene reconstruction. We demonstrate the safety and robustness of our pipeline in both <b>simulation</b> and hardware, where we show re-planning at 5 Hz and pose estimation at 20 Hz, an order of magnitude faster than Neural Radiance Field (NeRF)-based navigation methods, thereby enabling real-time navigation.</p></p class="citation"></blockquote><h3 id=56--135290-unidoormanip-learning-universal-door-manipulation-policy-over-large-scale-and-diverse-door-manipulation-environments-yu-li-et-al-2024>(5/6 | 135/290) UniDoorManip: Learning Universal Door Manipulation Policy Over Large-scale and Diverse Door Manipulation Environments (Yu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Li, Xiaojie Zhang, Ruihai Wu, Zilong Zhang, Yiran Geng, Hao Dong, Zhaofeng He. (2024)<br><strong>UniDoorManip: Learning Universal Door Manipulation Policy Over Large-scale and Diverse Door Manipulation Environments</strong><br><button class=copy-to-clipboard title="UniDoorManip: Learning Universal Door Manipulation Policy Over Large-scale and Diverse Door Manipulation Environments" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02604v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02604v2.pdf filename=2403.02604v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning a universal manipulation policy encompassing doors with diverse categories, geometries and mechanisms, is crucial for future embodied agents to effectively work in complex and broad real-world scenarios. Due to the limited datasets and unrealistic <b>simulation</b> environments, previous works fail to achieve good performance across various doors. In this work, we build a novel door manipulation environment reflecting different realistic door manipulation mechanisms, and further equip this environment with a large-scale door dataset covering 6 door categories with hundreds of door bodies and handles, making up thousands of different door instances. Additionally, to better emulate real-world scenarios, we introduce a mobile robot as the agent and use the partial and occluded point cloud as the observation, which are not considered in previous works while possessing significance for real-world implementations. To learn a universal policy over diverse doors, we propose a novel framework disentangling the whole manipulation process into three stages, and integrating them by training in the reversed order of inference. Extensive experiments validate the effectiveness of our designs and demonstrate our framework&rsquo;s strong performance.</p></p class="citation"></blockquote><h3 id=66--136290-active-information-gathering-for-long-horizon-navigation-under-uncertainty-by-learning-the-value-of-information-raihan-islam-arnob-et-al-2024>(6/6 | 136/290) Active Information Gathering for Long-Horizon Navigation Under Uncertainty by Learning the Value of Information (Raihan Islam Arnob et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raihan Islam Arnob, Gregory J. Stein. (2024)<br><strong>Active Information Gathering for Long-Horizon Navigation Under Uncertainty by Learning the Value of Information</strong><br><button class=copy-to-clipboard title="Active Information Gathering for Long-Horizon Navigation Under Uncertainty by Learning the Value of Information" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03269v1.pdf filename=2403.03269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the task of long-horizon navigation in partially mapped environments for which active gathering of information about faraway unseen space is essential for good behavior. We present a novel planning strategy that, at training time, affords tractable computation of the value of information associated with revealing potentially informative regions of unseen space, data used to train a <b>graph</b> <b>neural</b> <b>network</b> to predict the goodness of temporally-extended exploratory actions. Our learning-augmented model-based planning approach predicts the expected value of information of revealing unseen space and is capable of using these predictions to actively seek information and so improve long-horizon navigation. Across two simulated office-like environments, our planner outperforms competitive learned and non-learned baseline navigation strategies, achieving improvements of up to 63.76% and 36.68%, demonstrating its capacity to actively seek performance-critical information.</p></p class="citation"></blockquote><h2 id=csse-5>cs.SE (5)</h2><h3 id=15--137290-generative-software-engineering-yuan-huang-et-al-2024>(1/5 | 137/290) Generative Software Engineering (Yuan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Huang, Yinan Chen, Xiangping Chen, Junqi Chen, Rui Peng, Zhicao Tang, Jinbo Huang, Furen Xu, Zibin Zheng. (2024)<br><strong>Generative Software Engineering</strong><br><button class=copy-to-clipboard title="Generative Software Engineering" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 90<br>Keywords: Fine-tuning, Transfer Learning, BERT, ChatGPT, Transformer, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02583v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02583v1.pdf filename=2403.02583v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid development of deep learning techniques, improved computational power, and the availability of vast training data have led to significant advancements in pre-trained models and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Pre-trained models based on architectures such as <b>BERT</b> and <b>Transformer,</b> as well as <b>LLMs</b> like <b>ChatGPT,</b> have demonstrated remarkable language capabilities and found applications in Software engineering. Software engineering tasks can be divided into many categories, among which generative tasks are the most concern by researchers, where pre-trained models and <b>LLMs</b> possess powerful language representation and contextual awareness capabilities, enabling them to leverage diverse training data and adapt to generative tasks through <b>fine-tuning,</b> <b>transfer</b> <b>learning,</b> and <b>prompt</b> engineering. These advantages make them effective tools in generative tasks and have demonstrated excellent performance. In this paper, we present a comprehensive literature review of generative tasks in SE using pre-trained models and <b>LLMs.</b> We accurately categorize SE generative tasks based on software engineering methodologies and <b>summarize</b> the advanced pre-trained models and <b>LLMs</b> involved, as well as the datasets and evaluation metrics used. Additionally, we identify key strengths, weaknesses, and gaps in existing approaches, and propose potential research directions. This review aims to provide researchers and practitioners with an in-depth analysis and guidance on the application of pre-trained models and <b>LLMs</b> in generative tasks within SE.</p></p class="citation"></blockquote><h3 id=25--138290-learn-to-code-sustainably-an-empirical-study-on-llm-based-green-code-generation-tina-vartziotis-et-al-2024>(2/5 | 138/290) Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation (Tina Vartziotis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tina Vartziotis, Ippolyti Dellatolas, George Dasoulas, Maximilian Schmidt, Florian Schneider, Tim Hoffmann, Sotirios Kotsopoulos, Michael Keckeisen. (2024)<br><strong>Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation</strong><br><button class=copy-to-clipboard title="Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: ChatGPT, Code Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03344v1.pdf filename=2403.03344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models. The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains. Here, we provide an empirical study on green <b>code</b> <b>and</b> an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models. In this framework, we evaluate the sustainability of auto-generated <b>code.</b> <b>The</b> auto-generate <b>codes</b> <b>considered</b> in this study are produced by generative commercial AI language models, GitHub Copilot, OpenAI <b>ChatGPT-3,</b> and Amazon CodeWhisperer. Within our methodology, in order to quantify the sustainability awareness of these AI models, we propose a definition of the <b>code&rsquo;s</b> <b>&ldquo;green</b> capacity&rdquo;, based on certain sustainability metrics. We compare the performance and green capacity of human-generated <b>code</b> <b>and</b> <b>code</b> <b>generated</b> by the three AI language models in response to easy-to-hard problem statements. Our findings shed light on the current capacity of AI models to contribute to sustainable software development.</p></p class="citation"></blockquote><h3 id=35--139290-tooling-offline-runtime-verification-against-interaction-models--recognizing-sliced-behaviors-using-parameterized-simulation-erwan-mahe-et-al-2024>(3/5 | 139/290) Tooling Offline Runtime Verification against Interaction Models : recognizing sliced behaviors using parameterized simulation (Erwan Mahe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erwan Mahe, Boutheina Bannour, Christophe Gaston, Arnault Lapitre, Pascale Le Gall. (2024)<br><strong>Tooling Offline Runtime Verification against Interaction Models : recognizing sliced behaviors using parameterized simulation</strong><br><button class=copy-to-clipboard title="Tooling Offline Runtime Verification against Interaction Models : recognizing sliced behaviors using parameterized simulation" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03083v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03083v1.pdf filename=2403.03083v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Offline runtime verification involves the static analysis of executions of a system against a specification. For distributed systems, it is generally not possible to characterize executions in the form of global traces, given the absence of a global clock. To account for this, we model executions as collections of local traces called multi-traces, with one local trace per group of co-localized actors that share a common clock. Due to the difficulty of synchronizing the start and end of the recordings of local traces, events may be missing at their beginning or end. Considering such partially observed multi-traces is challenging for runtime verification. To that end, we propose an algorithm that verifies the conformity of such traces against formal specifications called Interactions (akin to Message Sequence Charts). It relies on parameterized <b>simulation</b> to reconstitute unobserved behaviors.</p></p class="citation"></blockquote><h3 id=45--140290-deep-configuration-performance-learning-a-systematic-survey-and-taxonomy-jingzhi-gong-et-al-2024>(4/5 | 140/290) Deep Configuration Performance Learning: A Systematic Survey and Taxonomy (Jingzhi Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingzhi Gong, Tao Chen. (2024)<br><strong>Deep Configuration Performance Learning: A Systematic Survey and Taxonomy</strong><br><button class=copy-to-clipboard title="Deep Configuration Performance Learning: A Systematic Survey and Taxonomy" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-LG, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03322v1.pdf filename=2403.03322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning. In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results <b>summarize</b> the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evaluated and how they are exploited in different tasks related to software configuration. We also identify the good practice and the potentially problematic phenomena from the studies surveyed, together with insights on future opportunities for the field. To promote open science, all the raw results of this survey can be accessed at our repository: <a href=https://github.com/ideas-labo/DCPL-SLR>https://github.com/ideas-labo/DCPL-SLR</a>.</p></p class="citation"></blockquote><h3 id=55--141290-alloyinecore-embedding-of-first-order-relational-logic-into-meta-object-facility-for-automated-model-reasoning-ferhat-erata-et-al-2024>(5/5 | 141/290) AlloyInEcore: Embedding of First-Order Relational Logic into Meta-Object Facility for Automated Model Reasoning (Ferhat Erata et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ferhat Erata, Arda Goknil, Ivan Kurtev, Bedir Tekinerdogan. (2024)<br><strong>AlloyInEcore: Embedding of First-Order Relational Logic into Meta-Object Facility for Automated Model Reasoning</strong><br><button class=copy-to-clipboard title="AlloyInEcore: Embedding of First-Order Relational Logic into Meta-Object Facility for Automated Model Reasoning" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-PL, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02652v1.pdf filename=2403.02652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present AlloyInEcore, a tool for specifying metamodels with their static semantics to facilitate automated, formal <b>reasoning</b> on models. Software development projects require that software systems be specified in various models (e.g., requirements models, architecture models, test models, and source code). It is crucial to reason about those models to ensure the correct and complete system specifications. AlloyInEcore allows the user to specify metamodels with their static semantics, while, using the semantics, it automatically detects inconsistent models, and completes partial models. It has been evaluated on three industrial case studies in the automotive domain (<a href=https://modelwriter.github.io/AlloyInEcore/)>https://modelwriter.github.io/AlloyInEcore/)</a>.</p></p class="citation"></blockquote><h2 id=csmm-3>cs.MM (3)</h2><h3 id=13--142290-sniffer-multimodal-large-language-model-for-explainable-out-of-context-misinformation-detection-peng-qi-et-al-2024>(1/3 | 142/290) SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection (Peng Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Qi, Zehong Yan, Wynne Hsu, Mong Li Lee. (2024)<br><strong>SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection</strong><br><button class=copy-to-clipboard title="SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-AI, cs-CL, cs-CV, cs-CY, cs-MM, cs.MM<br>Keyword Score: 76<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, GPT, GPT-4, Image2text, Reasoning, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03170v1.pdf filename=2403.03170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Misinformation is a prevalent societal issue due to its potential high risks. Out-of-context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences. Current methods focus on assessing <b>image-text</b> consistency but lack convincing explanations for their judgments, which is essential for debunking misinformation. While <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have rich knowledge and innate capability for visual <b>reasoning</b> and explanation generation, they still lack sophistication in understanding and discovering the subtle crossmodal differences. In this paper, we introduce SNIFFER, a novel <b>multimodal</b> <b>large</b> <b>language</b> <b>model</b> specifically engineered for OOC misinformation detection and explanation. SNIFFER employs two-stage <b>instruction</b> <b>tuning</b> on InstructBLIP. The first stage refines the model&rsquo;s concept alignment of generic objects with news-domain entities and the second stage leverages language-only <b>GPT-4</b> generated OOC-specific <b>instruction</b> <b>data</b> to <b>fine-tune</b> the model&rsquo;s discriminatory powers. Enhanced by external tools and retrieval, SNIFFER not only detects inconsistencies between text and image but also utilizes external knowledge for contextual verification. Our experiments show that SNIFFER surpasses the original MLLM by over 40% and outperforms state-of-the-art methods in detection accuracy. SNIFFER also provides accurate and persuasive explanations as validated by quantitative and human evaluations.</p></p class="citation"></blockquote><h3 id=23--143290-mmofusion-multi-modal-co-speech-motion-generation-with-diffusion-model-sen-wang-et-al-2024>(2/3 | 143/290) MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model (Sen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sen Wang, Jiangning Zhang, Weijian Cao, Xiaobin Hu, Moran Li, Xiaozhong Ji, Xin Tan, Mengtian Li, Zhifeng Xie, Chengjie Wang, Lizhuang Ma. (2024)<br><strong>MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model</strong><br><button class=copy-to-clipboard title="MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-MM, cs.MM<br>Keyword Score: 13<br>Keywords: Diffusion Model, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02905v1.pdf filename=2403.02905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The body movements accompanying speech aid speakers in expressing their ideas. Co-speech motion generation is one of the important approaches for synthesizing realistic avatars. Due to the intricate correspondence between speech and motion, generating realistic and diverse motion is a challenging task. In this paper, we propose MMoFusion, a <b>Multi-modal</b> co-speech Motion generation framework based on the <b>diffusion</b> <b>model</b> to ensure both the authenticity and diversity of generated motion. We propose a progressive fusion strategy to enhance the interaction of inter-modal and intra-modal, efficiently integrating <b>multi-modal</b> information. Specifically, we employ a masked style matrix based on emotion and identity information to control the generation of different motion styles. Temporal modeling of speech and motion is partitioned into style-guided specific feature encoding and shared feature encoding, aiming to learn both inter-modal and intra-modal features. Besides, we propose a geometric loss to enforce the joints&rsquo; velocity and acceleration coherence among frames. Our framework generates vivid, diverse, and style-controllable motion of arbitrary length through inputting speech and editing identity and emotion. Extensive experiments demonstrate that our method outperforms current co-speech motion generation methods including upper body and challenging full body.</p></p class="citation"></blockquote><h3 id=33--144290-optimizing-mobile-friendly-viewport-prediction-for-live-360-degree-video-streaming-lei-zhang-et-al-2024>(3/3 | 144/290) Optimizing Mobile-Friendly Viewport Prediction for Live 360-Degree Video Streaming (Lei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Zhang, Tao Long, Weizhen Xu, Laizhong Cui, Jiangchuan Liu. (2024)<br><strong>Optimizing Mobile-Friendly Viewport Prediction for Live 360-Degree Video Streaming</strong><br><button class=copy-to-clipboard title="Optimizing Mobile-Friendly Viewport Prediction for Live 360-Degree Video Streaming" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-MM, cs.MM, eess-IV<br>Keyword Score: 10<br>Keywords: Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02693v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02693v1.pdf filename=2403.02693v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Viewport prediction is the crucial task for adaptive 360-degree video streaming, as the bitrate control algorithms usually require the knowledge of the user&rsquo;s viewing portions of the frames. Various methods are studied and adopted for viewport prediction from less accurate statistic tools to highly calibrated deep neural networks. Conventionally, it is difficult to implement sophisticated deep learning methods on mobile devices, which have limited computation capability. In this work, we propose an advanced learning-based viewport prediction approach and carefully design it to introduce minimal transmission and computation overhead for mobile terminals. We also propose a model-agnostic <b>meta-learning</b> <b>(MAML)</b> based saliency prediction network trainer, which provides a few-sample fast training solution to obtain the prediction model by utilizing the information from the past models. We further discuss how to integrate this mobile-friendly viewport prediction (MFVP) approach into a typical 360-degree video live streaming system by formulating and solving the bitrate adaptation problem. Extensive experiment results show that our prediction approach can work in real-time for live video streaming and can achieve higher accuracies compared to other existing prediction methods on mobile end, which, together with our bitrate adaptation algorithm, significantly improves the streaming QoE from various aspects. We observe the accuracy of MFVP is 8.1$%$ to 28.7$%$ higher than other algorithms and achieves 3.73$%$ to 14.96$%$ higher average quality level and 49.6$%$ to 74.97$%$ less quality level change than other algorithms.</p></p class="citation"></blockquote><h2 id=cscv-54>cs.CV (54)</h2><h3 id=154--145290-promptkd-unsupervised-prompt-distillation-for-vision-language-models-zheng-li-et-al-2024>(1/54 | 145/290) PromptKD: Unsupervised Prompt Distillation for Vision-Language Models (Zheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Li, Xiang Li, Xinyi Fu, Xin Zhang, Weiqiang Wang, Shuo Chen, Jian Yang. (2024)<br><strong>PromptKD: Unsupervised Prompt Distillation for Vision-Language Models</strong><br><button class=copy-to-clipboard title="PromptKD: Unsupervised Prompt Distillation for Vision-Language Models" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Few-shot, Knowledge Distillation, Knowledge Distillation, Unsupervised Learning, Prompt, Prompt Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02781v4 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02781v4.pdf filename=2403.02781v4.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt</b> <b>learning</b> has emerged as a valuable technique in enhancing <b>vision-language</b> models (VLMs) such as CLIP for downstream tasks in specific domains. Existing work mainly focuses on designing various learning forms of <b>prompts,</b> <b>neglecting</b> the potential of <b>prompts</b> <b>as</b> effective distillers for learning from larger teacher models. In this paper, we introduce an <b>unsupervised</b> domain <b>prompt</b> <b>distillation</b> framework, which aims to transfer the <b>knowledge</b> <b>of</b> a larger teacher model to a lightweight target model through <b>prompt-driven</b> <b>imitation</b> using unlabeled domain images. Specifically, our framework consists of two distinct stages. In the initial stage, we pre-train a large CLIP teacher model using domain <b>(few-shot)</b> labels. After pre-training, we leverage the unique decoupled-modality characteristics of CLIP by pre-computing and storing the text features as class vectors only once through the teacher text encoder. In the subsequent stage, the stored class vectors are shared across teacher and student image encoders for calculating the predicted logits. Further, we align the logits of both the teacher and student models via KL divergence, encouraging the student image encoder to generate similar probability distributions to the teacher through the learnable <b>prompts.</b> <b>The</b> proposed <b>prompt</b> <b>distillation</b> process eliminates the reliance on labeled data, enabling the algorithm to leverage a vast amount of unlabeled images within the domain. Finally, the well-trained student image encoders and pre-stored text features (class vectors) are utilized for inference. To our best <b>knowledge,</b> <b>we</b> are the first to (1) perform <b>unsupervised</b> domain-specific <b>prompt-driven</b> <b>knowledge</b> <b>distillation</b> for CLIP, and (2) establish a practical pre-storing mechanism of text features as shared class vectors between teacher and student. Extensive experiments on 11 datasets demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=254--146290-modeling-collaborator-enabling-subjective-vision-classification-with-minimal-human-effort-via-llm-tool-use-imad-eddine-toubal-et-al-2024>(2/54 | 146/290) Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use (Imad Eddine Toubal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Imad Eddine Toubal, Aditya Avinash, Neil Gordon Alldrin, Jan Dlabal, Wenlei Zhou, Enming Luo, Otilia Stretcu, Hao Xiong, Chun-Ta Lu, Howard Zhou, Ranjay Krishna, Ariel Fuxman, Tom Duerig. (2024)<br><strong>Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use</strong><br><button class=copy-to-clipboard title="Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 70<br>Keywords: Foundation Model, Zero-shot, Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02626v1.pdf filename=2403.02626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>From content moderation to wildlife conservation, the number of applications that require models to recognize nuanced or subjective <b>visual</b> <b>concepts</b> <b>is</b> growing. Traditionally, developing classifiers for such concepts requires substantial manual effort measured in hours, days, or even months to identify and annotate data needed for training. Even with recently proposed Agile Modeling techniques, which enable rapid bootstrapping of image classifiers, users are still required to spend 30 minutes or more of monotonous, repetitive data labeling just to train a single classifier. Drawing on Fiske&rsquo;s Cognitive Miser theory, we propose a new framework that alleviates manual effort by replacing human labeling with natural language interactions, reducing the total effort required to define a concept by an order of magnitude: from labeling 2,000 images to only 100 plus some natural language interactions. Our framework leverages recent advances in <b>foundation</b> <b>models,</b> both <b>large</b> <b>language</b> <b>models</b> and <b>vision-language</b> models, to carve out the concept space through conversation and by automatically labeling training data points. Most importantly, our framework eliminates the need for crowd-sourced annotations. Moreover, our framework ultimately produces lightweight classification models that are deployable in cost-sensitive scenarios. Across 15 subjective concepts and across 2 public image classification datasets, our trained models outperform traditional Agile Modeling as well as state-of-the-art <b>zero-shot</b> classification models like ALIGN, CLIP, CuPL, and <b>large</b> <b>visual</b> <b>question-answering</b> <b>models</b> like PaLI-X.</p></p class="citation"></blockquote><h3 id=354--147290-interactive-continual-learning-fast-and-slow-thinking-biqing-qi-et-al-2024>(3/54 | 147/290) Interactive Continual Learning: Fast and Slow Thinking (Biqing Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Biqing Qi, Xingquan Chen, Junqi Gao, Jianxing Liu, Ligang Wu, Bowen Zhou. (2024)<br><strong>Interactive Continual Learning: Fast and Slow Thinking</strong><br><button class=copy-to-clipboard title="Interactive Continual Learning: Fast and Slow Thinking" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 66<br>Keywords: Continual Learning, Multi-modal, Multi-modal, Outlier Detection, Reasoning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02628v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02628v1.pdf filename=2403.02628v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan. In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of <b>continual</b> <b>learning</b> (CL). Nonetheless, the emergence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> presents promising avenues for realizing CL via interactions with these models. Drawing on Complementary Learning System theory, this paper presents a novel Interactive <b>Continual</b> <b>Learning</b> <b>(ICL)</b> framework, enabled by collaborative interactions among models of various sizes. Specifically, we assign the ViT model as System1 and <b>multimodal</b> <b>LLM</b> as System2. To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in System1 through enhanced geometric representation, we introduce the CL-vMF mechanism, based on the von Mises-Fisher (vMF) distribution. Meanwhile, we introduce the von Mises-Fisher <b>Outlier</b> <b>Detection</b> and Interaction (vMF-ODI) strategy to identify hard examples, thus enhancing collaboration between System1 and System2 for complex <b>reasoning</b> realization. Comprehensive evaluation of our proposed <b>ICL</b> demonstrates significant resistance to forgetting and superior performance relative to existing methods.</p></p class="citation"></blockquote><h3 id=454--148290-chatgpt-and-biometrics-an-assessment-of-face-recognition-gender-detection-and-age-estimation-capabilities-ahmad-hassanpour-et-al-2024>(4/54 | 148/290) ChatGPT and biometrics: an assessment of face recognition, gender detection, and age estimation capabilities (Ahmad Hassanpour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmad Hassanpour, Yasamin Kowsari, Hatef Otroshi Shahreza, Bian Yang, Sebastien Marcel. (2024)<br><strong>ChatGPT and biometrics: an assessment of face recognition, gender detection, and age estimation capabilities</strong><br><button class=copy-to-clipboard title="ChatGPT and biometrics: an assessment of face recognition, gender detection, and age estimation capabilities" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Face Recognition, Foundation Model, ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02965v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02965v1.pdf filename=2403.02965v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the application of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> like <b>ChatGPT,</b> for biometric tasks. We specifically examine the capabilities of <b>ChatGPT</b> in performing biometric-related tasks, with an emphasis on <b>face</b> <b>recognition,</b> gender detection, and age estimation. Since biometrics are considered as sensitive information, <b>ChatGPT</b> avoids answering direct <b>prompts,</b> and thus we crafted a <b>prompting</b> strategy to bypass its safeguard and evaluate the capabilities for biometrics tasks. Our study reveals that <b>ChatGPT</b> recognizes facial identities and differentiates between two facial images with considerable accuracy. Additionally, experimental results demonstrate remarkable performance in gender detection and reasonable accuracy for the age estimation tasks. Our findings shed light on the promising potentials in the application of <b>LLMs</b> and <b>foundation</b> <b>models</b> for biometrics.</p></p class="citation"></blockquote><h3 id=554--149290-finetuned-multimodal-language-models-are-high-quality-image-text-data-filters-weizhi-wang-et-al-2024>(5/54 | 149/290) Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters (Weizhi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weizhi Wang, Khalil Mrini, Linjie Yang, Sateesh Kumar, Yu Tian, Xifeng Yan, Heng Wang. (2024)<br><strong>Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters</strong><br><button class=copy-to-clipboard title="Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model, Multi-modal, Multi-modal, Image2text, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02677v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02677v1.pdf filename=2403.02677v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel framework for filtering <b>image-text</b> data by leveraging <b>fine-tuned</b> <b>Multimodal</b> Language Models <b>(MLMs).</b> Our approach outperforms predominant filtering methods (e.g., CLIPScore) via integrating the recent advances in <b>MLMs.</b> We design four distinct yet complementary metrics to holistically measure the quality of <b>image-text</b> data. A new pipeline is established to construct high-quality instruction data for <b>fine-tuning</b> <b>MLMs</b> as data filters. Comparing with CLIPScore, our <b>MLM</b> filters produce more precise and comprehensive scores that directly improve the quality of filtered data and boost the performance of pre-trained models. We achieve significant improvements over CLIPScore on popular <b>foundation</b> <b>models</b> (i.e., CLIP and BLIP2) and various downstream tasks. Our <b>MLM</b> filter can generalize to different models and tasks, and be used as a drop-in replacement for CLIPScore. An additional ablation study is provided to verify our design choices for the <b>MLM</b> filter.</p></p class="citation"></blockquote><h3 id=654--150290-few-shot-learner-parameterization-by-diffusion-time-steps-zhongqi-yue-et-al-2024>(6/54 | 150/290) Few-shot Learner Parameterization by Diffusion Time-steps (Zhongqi Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongqi Yue, Pan Zhou, Richang Hong, Hanwang Zhang, Qianru Sun. (2024)<br><strong>Few-shot Learner Parameterization by Diffusion Time-steps</strong><br><button class=copy-to-clipboard title="Few-shot Learner Parameterization by Diffusion Time-steps" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Diffusion Model, Few-shot, Few-shot Learning, Foundation Model, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02649v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02649v1.pdf filename=2403.02649v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Even when using large <b>multi-modal</b> <b>foundation</b> <b>models,</b> <b>few-shot</b> <b>learning</b> is still challenging &ndash; if there is no proper inductive bias, it is nearly impossible to keep the nuanced class attributes while removing the visually prominent attributes that spuriously correlate with class labels. To this end, we find an inductive bias that the time-steps of a <b>Diffusion</b> <b>Model</b> (DM) can isolate the nuanced class attributes, i.e., as the forward <b>diffusion</b> <b>adds</b> noise to an image at each time-step, nuanced attributes are usually lost at an earlier time-step than the spurious attributes that are visually prominent. Building on this, we propose Time-step <b>Few-shot</b> <b>(TiF)</b> learner. We train class-specific low-rank adapters for a text-conditioned DM to make up for the lost attributes, such that images can be accurately reconstructed from their noisy ones given a <b>prompt.</b> Hence, at a small time-step, the adapter and <b>prompt</b> are essentially a parameterization of only the nuanced class attributes. For a test image, we can use the parameterization to only extract the nuanced class attributes for classification. TiF learner significantly outperforms OpenCLIP and its adapters on a variety of fine-grained and customized <b>few-shot</b> <b>learning</b> tasks. Codes are in <a href=https://github.com/yue-zhongqi/tif>https://github.com/yue-zhongqi/tif</a>.</p></p class="citation"></blockquote><h3 id=754--151290-dinov2-based-self-supervised-learning-for-few-shot-medical-image-segmentation-lev-ayzenberg-et-al-2024>(7/54 | 151/290) DINOv2 based Self Supervised Learning For Few Shot Medical Image Segmentation (Lev Ayzenberg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lev Ayzenberg, Raja Giryes, Hayit Greenspan. (2024)<br><strong>DINOv2 based Self Supervised Learning For Few Shot Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="DINOv2 based Self Supervised Learning For Few Shot Medical Image Segmentation" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Few-shot, Self-supervised Learning, Self-supervised Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03273v1.pdf filename=2403.03273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning models have emerged as the cornerstone of medical image segmentation, but their efficacy hinges on the availability of extensive manually labeled datasets and their adaptability to unforeseen categories remains a challenge. <b>Few-shot</b> segmentation (FSS) offers a promising solution by endowing models with the capacity to learn novel classes from limited labeled examples. A leading method for FSS is ALPNet, which compares features between the query image and the few available support segmented images. A key question about using ALPNet is how to design its features. In this work, we delve into the potential of using features from DINOv2, which is a foundational <b>self-supervised</b> <b>learning</b> model in computer vision. Leveraging the strengths of ALPNet and harnessing the feature extraction capabilities of DINOv2, we present a novel approach to <b>few-shot</b> segmentation that not only enhances performance but also paves the way for more robust and adaptable medical image analysis.</p></p class="citation"></blockquote><h3 id=854--152290-enhancing-generalization-in-medical-visual-question-answering-tasks-via-gradient-guided-model-perturbation-gang-liu-et-al-2024>(8/54 | 152/290) Enhancing Generalization in Medical Visual Question Answering Tasks via Gradient-Guided Model Perturbation (Gang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gang Liu, Hongyang Li, Zerui He, Shenjun Zhong. (2024)<br><strong>Enhancing Generalization in Medical Visual Question Answering Tasks via Gradient-Guided Model Perturbation</strong><br><button class=copy-to-clipboard title="Enhancing Generalization in Medical Visual Question Answering Tasks via Gradient-Guided Model Perturbation" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 50<br>Keywords: Data Augmentation, Fine-tuning, Question Answering, Visual Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02707v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02707v1.pdf filename=2403.02707v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Leveraging pre-trained <b>visual</b> <b>language</b> <b>models</b> has become a widely adopted approach for improving performance in downstream <b>visual</b> <b>question</b> <b>answering</b> <b>(VQA)</b> applications. However, in the specialized field of medical <b>VQA,</b> the scarcity of available <b>data</b> <b>poses</b> a significant barrier to achieving reliable model generalization. Numerous methods have been proposed to enhance model generalization, addressing the issue from <b>data-centric</b> <b>and</b> model-centric perspectives. <b>Data</b> <b>augmentation</b> techniques are commonly employed to enrich the dataset, while various regularization approaches aim to prevent model overfitting, especially when training on limited <b>data</b> <b>samples.</b> In this paper, we introduce a method that incorporates gradient-guided parameter perturbations to the <b>visual</b> <b>encoder</b> <b>of</b> the multimodality model during both pre-training and <b>fine-tuning</b> phases, to improve model generalization for downstream medical <b>VQA</b> tasks. The small perturbation is adaptively generated by aligning with the direction of the moving average gradient in the optimization landscape, which is opposite to the directions of the optimizer&rsquo;s historical updates. It is subsequently injected into the model&rsquo;s <b>visual</b> <b>encoder.</b> <b>The</b> results show that, even with a significantly smaller pre-training image caption dataset, our approach achieves competitive outcomes on both <b>VQA-RAD</b> and SLAKE datasets.</p></p class="citation"></blockquote><h3 id=954--153290-multi-modal-instruction-tuned-llms-with-fine-grained-visual-perception-junwen-he-et-al-2024>(9/54 | 153/290) Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception (Junwen He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Jun-Yan He, Jin-Peng Lan, Bin Luo, Xuansong Xie. (2024)<br><strong>Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception</strong><br><button class=copy-to-clipboard title="Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Grounding, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02969v1.pdf filename=2403.02969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Model</b> (MLLMs) leverages <b>Large</b> <b>Language</b> <b>Models</b> as a cognitive framework for diverse visual-language tasks. Recent efforts have been made to equip MLLMs with visual perceiving and <b>grounding</b> capabilities. However, there still remains a gap in providing fine-grained pixel-level perceptions and extending interactions beyond text-specific inputs. In this work, we propose {\bf{AnyRef}}, a general MLLM model that can generate pixel-wise object perceptions and natural language descriptions from multi-modality references, such as texts, boxes, images, or audio. This innovation empowers users with greater flexibility to engage with the model beyond textual and regional <b>prompts,</b> without modality-specific designs. Through our proposed refocusing mechanism, the generated <b>grounding</b> output is guided to better focus on the referenced object, implicitly incorporating additional pixel-level supervision. This simple modification utilizes attention scores generated during the inference of <b>LLM,</b> eliminating the need for extra computations while exhibiting performance enhancements in both <b>grounding</b> masks and referring expressions. With only publicly available training data, our model achieves state-of-the-art results across multiple <b>benchmarks,</b> including diverse modality referring segmentation and region-level referring expression generation.</p></p class="citation"></blockquote><h3 id=1054--154290-imgtrojan-jailbreaking-vision-language-models-with-one-image-xijia-tao-et-al-2024>(10/54 | 154/290) ImgTrojan: Jailbreaking Vision-Language Models with ONE Image (Xijia Tao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong. (2024)<br><strong>ImgTrojan: Jailbreaking Vision-Language Models with ONE Image</strong><br><button class=copy-to-clipboard title="ImgTrojan: Jailbreaking Vision-Language Models with ONE Image" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02910v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02910v2.pdf filename=2403.02910v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There has been an increasing interest in the alignment of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak <b>prompts,</b> our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack&rsquo;s success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a <b>benchmark</b> for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods.</p></p class="citation"></blockquote><h3 id=1154--155290-towards-robust-federated-learning-via-logits-calibration-on-non-iid-data-yu-qiao-et-al-2024>(11/54 | 155/290) Towards Robust Federated Learning via Logits Calibration on Non-IID Data (Yu Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Qiao, Apurba Adhikary, Chaoning Zhang, Choong Seon Hong. (2024)<br><strong>Towards Robust Federated Learning via Logits Calibration on Non-IID Data</strong><br><button class=copy-to-clipboard title="Towards Robust Federated Learning via Logits Calibration on Non-IID Data" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: MNIST, Adversarial Learning, Benchmarking, Federated Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02803v1.pdf filename=2403.02803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is a privacy-preserving distributed management framework based on collaborative model training of distributed devices in edge networks. However, recent studies have shown that FL is vulnerable to <b>adversarial</b> <b>examples</b> (AEs), leading to a significant drop in its performance. Meanwhile, the non-independent and identically distributed (non-IID) challenge of data distribution between edge devices can further degrade the performance of models. Consequently, both AEs and non-IID pose challenges to deploying robust learning models at the edge. In this work, we adopt the <b>adversarial</b> <b>training</b> (AT) framework to improve the robustness of FL models against <b>adversarial</b> <b>example</b> (AE) attacks, which can be termed as <b>federated</b> <b>adversarial</b> <b>training</b> (FAT). Moreover, we address the non-IID challenge by implementing a simple yet effective logits calibration strategy under the FAT framework, which can enhance the robustness of models when subjected to <b>adversarial</b> <b>attacks.</b> Specifically, we employ a direct strategy to adjust the logits output by assigning higher weights to classes with small samples during training. This approach effectively tackles the class imbalance in the training data, with the goal of mitigating biases between local and global models. Experimental results on three dataset <b>benchmarks,</b> <b>MNIST,</b> Fashion-MNIST, and CIFAR-10 show that our strategy achieves competitive results in natural and robust accuracy compared to several baselines.</p></p class="citation"></blockquote><h3 id=1254--156290-enhancing-vision-language-pre-training-with-rich-supervisions-yuan-gao-et-al-2024>(12/54 | 156/290) Enhancing Vision-Language Pre-training with Rich Supervisions (Yuan Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Gao, Kunyu Shi, Pengkai Zhu, Edouard Belval, Oren Nuriel, Srikar Appalaraju, Shabnam Ghadar, Vijay Mahadevan, Zhuowen Tu, Stefano Soatto. (2024)<br><strong>Enhancing Vision-Language Pre-training with Rich Supervisions</strong><br><button class=copy-to-clipboard title="Enhancing Vision-Language Pre-training with Rich Supervisions" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Supervised Learning, Image2text, Image2text, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03346v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03346v1.pdf filename=2403.03346v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose Strongly <b>Supervised</b> pre-training with ScreenShots (S4) - a novel pre-training paradigm for <b>Vision-Language</b> Models using data from large-scale web screenshot rendering. Using web screenshots unlocks a treasure trove of visual and textual cues that are not present in using <b>image-text</b> pairs. In S4, we leverage the inherent tree-structured hierarchy of HTML elements and the spatial localization to carefully design 10 pre-training tasks with large scale annotated data. These tasks resemble downstream tasks across different domains and the annotations are cheap to obtain. We demonstrate that, compared to current screenshot pre-training objectives, our innovative pre-training method significantly enhances performance of <b>image-to-text</b> model in nine varied and popular downstream tasks - up to 76.1% improvements on Table Detection, and at least 1% on Widget Captioning.</p></p class="citation"></blockquote><h3 id=1354--157290-learning-without-exact-guidance-updating-large-scale-high-resolution-land-cover-maps-from-low-resolution-historical-labels-zhuohong-li-et-al-2024>(13/54 | 157/290) Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels (Zhuohong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuohong Li, Wei He, Jiepan Li, Fangxiao Lu, Hongyan Zhang. (2024)<br><strong>Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels</strong><br><button class=copy-to-clipboard title="Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Convolutional Neural Network, Supervised Learning, Weakly-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02746v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02746v2.pdf filename=2403.02746v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth&rsquo;s surface and resolve many challenges facing humanity. However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area. In this paper, we propose an efficient, weakly <b>supervised</b> framework (Paraformer) to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically, existing land-cover mapping approaches reveal the dominance of <b>CNNs</b> in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore, we design a parallel <b>CNN-Transformer</b> feature extractor in Paraformer, consisting of a downsampling-free <b>CNN</b> branch and a <b>Transformer</b> branch, to jointly capture local and global contextual information. Besides, facing the spatial mismatch of training data, a pseudo-label-assisted training (PLAT) module is adopted to reasonably refine LR labels for weakly <b>supervised</b> semantic segmentation of HR images. Experiments on two large-scale datasets demonstrate the superiority of Paraformer over other state-of-the-art methods for automatically updating HR land-cover maps from LR historical labels.</p></p class="citation"></blockquote><h3 id=1454--158290-madtp-multimodal-alignment-guided-dynamic-token-pruning-for-accelerating-vision-language-transformer-jianjian-cao-et-al-2024>(14/54 | 158/290) MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer (Jianjian Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianjian Cao, Peng Ye, Shengze Li, Chong Yu, Yansong Tang, Jiwen Lu, Tao Chen. (2024)<br><strong>MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer</strong><br><button class=copy-to-clipboard title="MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Pruning, Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02991v1.pdf filename=2403.02991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-Language</b> <b>Transformers</b> (VLTs) have shown great success recently, but are meanwhile accompanied by heavy computation costs, where a major reason can be attributed to the large number of visual and language tokens. Existing token <b>pruning</b> research for compressing VLTs mainly follows a single-modality-based scheme yet ignores the critical role of aligning different modalities for guiding the token <b>pruning</b> process, causing the important tokens for one modality to be falsely pruned in another modality branch. Meanwhile, existing VLT <b>pruning</b> works also lack the flexibility to dynamically compress each layer based on different input samples. To this end, we propose a novel framework named <b>Multimodal</b> Alignment-Guided Dynamic Token <b>Pruning</b> (MADTP) for accelerating various VLTs. Specifically, we first introduce a well-designed Multi-modality Alignment Guidance (MAG) module that can align features of the same semantic concept from different modalities, to ensure the pruned tokens are less important for all modalities. We further design a novel Dynamic Token <b>Pruning</b> (DTP) module, which can adaptively adjust the token compression ratio in each layer based on different input instances. Extensive experiments on various <b>benchmarks</b> demonstrate that MADTP significantly reduces the computational complexity of kinds of <b>multimodal</b> models while preserving competitive performance. Notably, when applied to the BLIP model in the NLVR2 dataset, MADTP can reduce the GFLOPs by 80% with less than 4% performance degradation.</p></p class="citation"></blockquote><h3 id=1554--159290-ddf-a-novel-dual-domain-image-fusion-strategy-for-remote-sensing-image-semantic-segmentation-with-unsupervised-domain-adaptation-lingyan-ran-et-al-2024>(15/54 | 159/290) DDF: A Novel Dual-Domain Image Fusion Strategy for Remote Sensing Image Semantic Segmentation with Unsupervised Domain Adaptation (Lingyan Ran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingyan Ran, Lushuang Wang, Tao Zhuo, Yinghui Xing. (2024)<br><strong>DDF: A Novel Dual-Domain Image Fusion Strategy for Remote Sensing Image Semantic Segmentation with Unsupervised Domain Adaptation</strong><br><button class=copy-to-clipboard title="DDF: A Novel Dual-Domain Image Fusion Strategy for Remote Sensing Image Semantic Segmentation with Unsupervised Domain Adaptation" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02784v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02784v1.pdf filename=2403.02784v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic segmentation of remote sensing images is a challenging and hot issue due to the large amount of unlabeled data. <b>Unsupervised</b> <b>domain</b> <b>adaptation</b> (UDA) has proven to be advantageous in incorporating unclassified information from the target <b>domain.</b> <b>However,</b> independently <b>fine-tuning</b> UDA models on the source and target <b>domains</b> <b>has</b> a limited effect on the outcome. This paper proposes a hybrid training strategy as well as a novel dual-domain image fusion strategy that effectively utilizes the original image, transformation image, and intermediate <b>domain</b> <b>information.</b> Moreover, to enhance the precision of pseudo-labels, we present a pseudo-label region-specific weight strategy. The efficacy of our approach is substantiated by extensive <b>benchmark</b> experiments and ablation studies conducted on the ISPRS Vaihingen and Potsdam datasets.</p></p class="citation"></blockquote><h3 id=1654--160290-domainverse-a-benchmark-towards-real-world-distribution-shifts-for-tuning-free-adaptive-domain-generalization-feng-hou-et-al-2024>(16/54 | 160/290) DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization (Feng Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Hou, Jin Yuan, Ying Yang, Yang Liu, Yang Zhang, Cheng Zhong, Zhongchao Shi, Jianping Fan, Yong Rui, Zhiqiang He. (2024)<br><strong>DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization</strong><br><button class=copy-to-clipboard title="DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Distribution Shift, Distribution Shift, Domain Adaptation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02714v1.pdf filename=2403.02714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional cross-domain tasks, including <b>domain</b> <b>adaptation</b> and <b>domain</b> <b>generalization,</b> rely heavily on training model by source <b>domain</b> <b>data.</b> With the recent advance of <b>vision-language</b> models (VLMs), viewed as natural source models, the cross-domain task changes to directly adapt the pre-trained source model to arbitrary target <b>domains</b> <b>equipped</b> with prior <b>domain</b> <b>knowledge,</b> and we name this task Adaptive <b>Domain</b> <b>Generalization</b> (ADG). However, current cross-domain datasets have many limitations, such as unrealistic <b>domains,</b> <b>unclear</b> <b>domain</b> <b>definitions,</b> and the inability to fine-grained <b>domain</b> <b>decomposition,</b> which drives us to establish a novel dataset DomainVerse for ADG. Benefiting from the introduced hierarchical definition of <b>domain</b> <b>shifts,</b> DomainVerse consists of about 0.5 million images from 390 fine-grained realistic <b>domains.</b> <b>With</b> the help of the constructed DomainVerse and VLMs, we propose two methods called <b>Domain</b> <b>CLIP</b> and Domain++ CLIP for tuning-free adaptive <b>domain</b> <b>generalization.</b> Extensive and comprehensive experiments demonstrate the significance of the dataset and the effectiveness of the proposed methods.</p></p class="citation"></blockquote><h3 id=1754--161290-fastocc-accelerating-3d-occupancy-prediction-by-fusing-the-2d-birds-eye-view-and-perspective-view-jiawei-hou-et-al-2024>(17/54 | 161/290) FastOcc: Accelerating 3D Occupancy Prediction by Fusing the 2D Bird&rsquo;s-Eye View and Perspective View (Jiawei Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Hou, Xiaoyan Li, Wenhao Guan, Gang Zhang, Di Feng, Yuheng Du, Xiangyang Xue, Jian Pu. (2024)<br><strong>FastOcc: Accelerating 3D Occupancy Prediction by Fusing the 2D Bird&rsquo;s-Eye View and Perspective View</strong><br><button class=copy-to-clipboard title="FastOcc: Accelerating 3D Occupancy Prediction by Fusing the 2D Bird's-Eye View and Perspective View" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 33<br>Keywords: Object Detection, Benchmarking, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02710v1.pdf filename=2403.02710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In autonomous driving, 3D occupancy prediction outputs voxel-wise status and semantic labels for more comprehensive understandings of 3D scenes compared with traditional perception tasks, such as 3D <b>object</b> <b>detection</b> and bird&rsquo;s-eye view (BEV) semantic segmentation. Recent researchers have extensively explored various aspects of this task, including view transformation techniques, ground-truth label generation, and elaborate network design, aiming to achieve superior performance. However, the inference speed, crucial for running on an autonomous vehicle, is neglected. To this end, a new method, dubbed FastOcc, is proposed. By carefully analyzing the network effect and latency from four parts, including the input image resolution, image backbone, view transformation, and occupancy prediction head, it is found that the occupancy prediction head holds considerable potential for accelerating the model while keeping its accuracy. Targeted at improving this component, the time-consuming 3D <b>convolution</b> <b>network</b> is replaced with a novel residual-like architecture, where features are mainly digested by a lightweight 2D BEV <b>convolution</b> <b>network</b> and compensated by integrating the 3D voxel features interpolated from the original image features. Experiments on the Occ3D-nuScenes <b>benchmark</b> demonstrate that our FastOcc achieves state-of-the-art results with a fast inference speed.</p></p class="citation"></blockquote><h3 id=1854--162290-scaling-rectified-flow-transformers-for-high-resolution-image-synthesis-patrick-esser-et-al-2024>(18/54 | 162/290) Scaling Rectified Flow Transformers for High-Resolution Image Synthesis (Patrick Esser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, Robin Rombach. (2024)<br><strong>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</strong><br><button class=copy-to-clipboard title="Scaling Rectified Flow Transformers for High-Resolution Image Synthesis" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Transformer, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03206v1.pdf filename=2403.03206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established <b>diffusion</b> <b>formulations</b> for high-resolution <b>text-to-image</b> synthesis. Additionally, we present a novel <b>transformer-based</b> architecture for <b>text-to-image</b> generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved <b>text-to-image</b> synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.</p></p class="citation"></blockquote><h3 id=1954--163290-solving-the-bongard-logo-problem-by-modeling-a-probabilistic-model-ruizhuo-song-et-al-2024>(19/54 | 163/290) Solving the bongard-logo problem by modeling a probabilistic model (Ruizhuo Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruizhuo Song, Beiming Yuan. (2024)<br><strong>Solving the bongard-logo problem by modeling a probabilistic model</strong><br><button class=copy-to-clipboard title="Solving the bongard-logo problem by modeling a probabilistic model" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Probabilistic Model, Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03173v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03173v2.pdf filename=2403.03173v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Abstract <b>reasoning</b> problems challenge the perceptual and cognitive abilities of AI algorithms, demanding deeper pattern discernment and inductive <b>reasoning</b> beyond explicit image features. This study introduces PMoC, a tailored probability model for the Bongard-Logo problem, achieving high <b>reasoning</b> accuracy by constructing independent probability models. Additionally, we present Pose-Transformer, an enhanced <b>Transformer-Encoder</b> designed for complex abstract <b>reasoning</b> tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM. Pose-Transformer incorporates positional information learning, inspired by capsule networks&rsquo; pose matrices, enhancing its focus on local positional relationships in image data processing. When integrated with PMoC, it further improves <b>reasoning</b> accuracy. Our approach effectively addresses <b>reasoning</b> difficulties associated with abstract entities&rsquo; positional changes, outperforming previous models on the OIG, D3$\times$3 subsets of RAVEN, and PGM databases. This research contributes to advancing AI&rsquo;s capabilities in abstract <b>reasoning</b> and cognitive pattern recognition.</p></p class="citation"></blockquote><h3 id=2054--164290-dual-mean-teacher-an-unbiased-semi-supervised-framework-for-audio-visual-source-localization-yuxin-guo-et-al-2024>(20/54 | 164/290) Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization (Yuxin Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxin Guo, Shijie Ma, Hu Su, Zhiqing Wang, Yuhao Zhao, Wei Zou, Siyang Sun, Yun Zheng. (2024)<br><strong>Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization</strong><br><button class=copy-to-clipboard title="Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-MM, cs-SD, cs.CV, eess-AS<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Self-supervised Learning, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03145v1.pdf filename=2403.03145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Audio-Visual Source Localization (AVSL) aims to locate sounding objects within video frames given the paired audio clips. Existing methods predominantly rely on <b>self-supervised</b> <b>contrastive</b> <b>learning</b> of audio-visual correspondence. Without any bounding-box annotations, they struggle to achieve precise localization, especially for small objects, and suffer from blurry boundaries and false positives. Moreover, the naive <b>semi-supervised</b> <b>method</b> is poor in fully leveraging the information of abundant unlabeled data. In this paper, we propose a novel <b>semi-supervised</b> <b>learning</b> framework for AVSL, namely Dual Mean-Teacher (DMT), comprising two teacher-student structures to circumvent the confirmation bias issue. Specifically, two teachers, pre-trained on limited labeled data, are employed to filter out noisy samples via the consensus between their predictions, and then generate high-quality pseudo-labels by intersecting their confidence maps. The sufficient utilization of both labeled and unlabeled data and the proposed unbiased framework enable DMT to outperform current state-of-the-art methods by a large margin, with CIoU of 90.4% and 48.8% on Flickr-SoundNet and VGG-Sound Source, obtaining 8.9%, 9.6% and 4.6%, 6.4% improvements over self- and <b>semi-supervised</b> <b>methods</b> respectively, given only 3% positional-annotations. We also extend our framework to some existing AVSL methods and consistently boost their performance.</p></p class="citation"></blockquote><h3 id=2154--165290-mikasa-multi-key-anchor--scene-aware-transformer-for-3d-visual-grounding-chun-peng-chang-et-al-2024>(21/54 | 165/290) MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding (Chun-Peng Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chun-Peng Chang, Shaoxiang Wang, Alain Pagani, Didier Stricker. (2024)<br><strong>MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding</strong><br><button class=copy-to-clipboard title="MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Transformer, Grounding, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03077v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03077v2.pdf filename=2403.03077v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D visual <b>grounding</b> involves matching natural language descriptions with their corresponding objects in 3D spaces. Existing methods often face challenges with accuracy in object recognition and struggle in interpreting complex linguistic queries, particularly with descriptions that involve multiple anchors or are view-dependent. In response, we present the MiKASA (Multi-Key-Anchor Scene-Aware) <b>Transformer.</b> Our novel end-to-end trained model integrates a <b>self-attention-based</b> scene-aware object encoder and an original multi-key-anchor technique, enhancing object recognition accuracy and the understanding of spatial relationships. Furthermore, MiKASA improves the explainability of decision-making, facilitating error diagnosis. Our model achieves the highest overall accuracy in the Referit3D challenge for both the Sr3D and Nr3D datasets, particularly excelling by a large margin in categories that require viewpoint-dependent descriptions. The source code and additional resources for this project are available on GitHub: <a href=https://github.com/birdy666/MiKASA-3DVG>https://github.com/birdy666/MiKASA-3DVG</a></p></p class="citation"></blockquote><h3 id=2254--166290-doubly-abductive-counterfactual-inference-for-text-based-image-editing-xue-song-et-al-2024>(22/54 | 166/290) Doubly Abductive Counterfactual Inference for Text-based Image Editing (Xue Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xue Song, Jiequan Cui, Hanwang Zhang, Jingjing Chen, Richang Hong, Yu-Gang Jiang. (2024)<br><strong>Doubly Abductive Counterfactual Inference for Text-based Image Editing</strong><br><button class=copy-to-clipboard title="Doubly Abductive Counterfactual Inference for Text-based Image Editing" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Counter-factual, Fine-tuning, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02981v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02981v1.pdf filename=2403.02981v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study text-based image editing (TBIE) of a single image by <b>counterfactual</b> inference because it is an elegant formulation to precisely address the requirement: the edited image should retain the fidelity of the original one. Through the lens of the formulation, we find that the crux of TBIE is that existing techniques hardly achieve a good trade-off between editability and fidelity, mainly due to the overfitting of the single-image <b>fine-tuning.</b> To this end, we propose a Doubly Abductive <b>Counterfactual</b> inference framework (DAC). We first parameterize an exogenous variable as a UNet LoRA, whose abduction can encode all the image details. Second, we abduct another exogenous variable parameterized by a text encoder LoRA, which recovers the lost editability caused by the overfitted first abduction. Thanks to the second abduction, which exclusively encodes the visual transition from post-edit to pre-edit, its inversion &ndash; subtracting the LoRA &ndash; effectively reverts pre-edit back to post-edit, thereby accomplishing the edit. Through extensive experiments, our DAC achieves a good trade-off between editability and fidelity. Thus, we can support a wide spectrum of user editing intents, including addition, removal, manipulation, replacement, <b>style</b> <b>transfer,</b> and facial change, which are extensively validated in both qualitative and quantitative evaluations. Codes are in <a href=https://github.com/xuesong39/DAC>https://github.com/xuesong39/DAC</a>.</p></p class="citation"></blockquote><h3 id=2354--167290-false-positive-sampling-based-data-augmentation-for-enhanced-3d-object-detection-accuracy-jiyong-oh-et-al-2024>(23/54 | 167/290) False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy (Jiyong Oh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiyong Oh, Junhaeng Lee, Woongchan Byun, Minsang Kong, Sang Hun Lee. (2024)<br><strong>False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy</strong><br><button class=copy-to-clipboard title="False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Curriculum Learning, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02639v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02639v2.pdf filename=2403.02639v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have focused on enhancing the performance of 3D <b>object</b> <b>detection</b> models. Among various approaches, ground-truth sampling has been proposed as an augmentation technique to address the challenges posed by limited ground-truth <b>data.</b> <b>However,</b> an inherent issue with ground-truth sampling is its tendency to increase false positives. Therefore, this study aims to overcome the limitations of ground-truth sampling and improve the performance of 3D <b>object</b> <b>detection</b> models by developing a new augmentation technique called false-positive sampling. False-positive sampling involves retraining the model using point clouds that are identified as false positives in the model&rsquo;s predictions. We propose an algorithm that utilizes both ground-truth and false-positive sampling and an algorithm for building the false-positive sample database. Additionally, we analyze the principles behind the performance enhancement due to false-positive sampling and propose a technique that applies the concept of <b>curriculum</b> <b>learning</b> to the sampling strategy that encompasses both false-positive and ground-truth sampling techniques. Our experiments demonstrate that models utilizing false-positive sampling show a reduction in false positives and exhibit improved <b>object</b> <b>detection</b> performance. On the KITTI and Waymo Open datasets, models with false-positive sampling surpass the baseline models by a large margin.</p></p class="citation"></blockquote><h3 id=2454--168290-a-unified-framework-for-microscopy-defocus-deblur-with-multi-pyramid-transformer-and-contrastive-learning-yuelin-zhang-et-al-2024>(24/54 | 168/290) A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning (Yuelin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuelin Zhang, Pengyu Zheng, Wanquan Yan, Chengyu Fang, Shing Shin Cheng. (2024)<br><strong>A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning</strong><br><button class=copy-to-clipboard title="A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Knowledge Transfer, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02611v1.pdf filename=2403.02611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Defocus blur is a persistent problem in microscope imaging that poses harm to pathology interpretation and medical intervention in cell microscopy and microscope surgery. To address this problem, a unified framework including multi-pyramid <b>transformer</b> (MPT) and extended frequency <b>contrastive</b> <b>regularization</b> (EFCR) is proposed to tackle two outstanding challenges in microscopy deblur: longer attention span and feature deficiency. The MPT employs an explicit pyramid structure at each network stage that integrates the cross-scale window attention (CSWA), the intra-scale channel attention (ISCA), and the feature-enhancing feed-forward network (FEFN) to capture long-range cross-scale spatial interaction and global channel context. The EFCR addresses the feature deficiency problem by exploring latent deblur signals from different frequency bands. It also enables deblur <b>knowledge</b> <b>transfer</b> to learn cross-domain information from extra data, improving deblur performance for labeled and unlabeled data. Extensive experiments and downstream task validation show the framework achieves state-of-the-art performance across multiple datasets. Project page: <a href=https://github.com/PieceZhang/MPT-CataBlur>https://github.com/PieceZhang/MPT-CataBlur</a>.</p></p class="citation"></blockquote><h3 id=2554--169290-feast-your-eyes-mixture-of-resolution-adaptation-for-multimodal-large-language-models-gen-luo-et-al-2024>(25/54 | 169/290) Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models (Gen Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, Rongrong Ji. (2024)<br><strong>Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03003v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03003v1.pdf filename=2403.03003v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite remarkable progress, existing <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) are still inferior in granular visual recognition. Contrary to previous works, we study this problem from the perspective of image resolution, and reveal that a combination of low- and high-resolution visual features can effectively mitigate this shortcoming. Based on this observation, we propose a novel and efficient method for MLLMs, termed Mixture-of-Resolution Adaptation (MRA). In particular, MRA adopts two visual pathways for images with different resolutions, where high-resolution visual information is embedded into the low-resolution pathway via the novel mixture-of-resolution adapters (MR-Adapters). This design also greatly reduces the input sequence length of MLLMs. To validate MRA, we apply it to a recent MLLM called LLaVA, and term the new model LLaVA-HR. We conduct extensive experiments on 11 <b>vision-language</b> (VL) tasks, which show that LLaVA-HR outperforms existing MLLMs on 8 VL tasks, e.g., +9.4% on TextVQA. More importantly, both training and inference of LLaVA-HR remain efficient with MRA, e.g., 20 training hours and 3$\times$ inference speed than LLaVA-1.5. Source codes are released at: <a href=https://github.com/luogen1996/LLaVA-HR>https://github.com/luogen1996/LLaVA-HR</a>.</p></p class="citation"></blockquote><h3 id=2654--170290-enhancing-conceptual-understanding-in-multimodal-contrastive-learning-through-hard-negative-samples-philipp-j-rösch-et-al-2024>(26/54 | 170/290) Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples (Philipp J. Rösch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp J. Rösch, Norbert Oswald, Michaela Geierhos, Jindřich Libovický. (2024)<br><strong>Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples</strong><br><button class=copy-to-clipboard title="Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-IR, cs.CV<br>Keyword Score: 26<br>Keywords: Contrastive Learning, Multi-modal, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02875v1.pdf filename=2403.02875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>multimodal</b> models leveraging <b>contrastive</b> <b>learning</b> often face limitations in developing fine-grained conceptual understanding. This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function. Consequently, the models struggle with fine-grained semantic differences. To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples. The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment. Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in <b>vision-language</b> models. We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions. Our results show significant improvements in fine-grained concept understanding across a wide range of <b>vision-language</b> datasets, including our InpaintCOCO dataset.</p></p class="citation"></blockquote><h3 id=2754--171290-veglue-testing-visual-entailment-systems-via-object-aligned-joint-erasing-zhiyuan-chang-et-al-2024>(27/54 | 171/290) VEglue: Testing Visual Entailment Systems via Object-Aligned Joint Erasing (Zhiyuan Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Chang, Mingyang Li, Junjie Wang, Cheng Li, Qing Wang. (2024)<br><strong>VEglue: Testing Visual Entailment Systems via Object-Aligned Joint Erasing</strong><br><button class=copy-to-clipboard title="VEglue: Testing Visual Entailment Systems via Object-Aligned Joint Erasing" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-SE, cs.CV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Image2text, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02581v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02581v1.pdf filename=2403.02581v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual entailment (VE) is a <b>multimodal</b> <b>reasoning</b> task consisting of image-sentence pairs whereby a promise is defined by an image, and a hypothesis is described by a sentence. The goal is to predict whether the image semantically entails the sentence. VE systems have been widely adopted in many downstream tasks. Metamorphic testing is the commonest technique for AI algorithms, but it poses a significant challenge for VE testing. They either only consider perturbations on single modality which would result in ineffective tests due to the destruction of the relationship of <b>image-text</b> pair, or just conduct shallow perturbations on the inputs which can hardly detect the decision error made by VE systems. Motivated by the fact that objects in the image are the fundamental element for <b>reasoning,</b> we propose VEglue, an object-aligned joint erasing approach for VE systems testing. It first aligns the object regions in the premise and object descriptions in the hypothesis to identify linked and un-linked objects. Then, based on the alignment information, three Metamorphic Relations are designed to jointly erase the objects of the two modalities. We evaluate VEglue on four widely-used VE systems involving two public datasets. Results show that VEglue could detect 11,609 issues on average, which is 194%-2,846% more than the baselines. In addition, VEglue could reach 52.5% Issue Finding Rate (IFR) on average, and significantly outperform the baselines by 17.1%-38.2%. Furthermore, we leverage the tests generated by VEglue to retrain the VE systems, which largely improves model performance (50.8% increase in accuracy) on newly generated tests without sacrificing the accuracy on the original test set.</p></p class="citation"></blockquote><h3 id=2854--172290-learning-zero-shot-material-states-segmentation-by-implanting-natural-image-patterns-in-synthetic-data-sagi-eppel-et-al-2024>(28/54 | 172/290) Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data (Sagi Eppel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sagi Eppel, Jolina Li, Manuel Drehwald, Alan Aspuru-Guzik. (2024)<br><strong>Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data</strong><br><button class=copy-to-clipboard title="Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Unsupervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03309v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03309v2.pdf filename=2403.03309v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual understanding and segmentation of materials and their states is fundamental for understanding the physical world. The infinite textures, shapes and often blurry boundaries formed by material make this task particularly hard to generalize. Whether it&rsquo;s identifying wet regions of a surface, minerals in rocks, infected regions in plants, or pollution in water, each material state has its own unique form. For neural nets to learn class-agnostic materials segmentation it is necessary to first collect and annotate data that capture this complexity. Collecting real-world images and manually annotating is limited both by the cost and limited precision of manual labor. In contrast, synthetic data is highly accurate and almost cost-free but fails to replicate the vast diversity of the material world. In this work, we suggest a method to bridge this crucial gap, by implanting patterns extracted from real-world images, in synthetic data. Hence, patterns automatically collected from natural images are used to map materials into synthetic scenes. This <b>unsupervised</b> approach allows the generated data to capture the vast complexity of the real world while maintaining the precision and scale of synthetic data. We also present the first general <b>benchmark</b> for class-agnostic material state segmentation. The <b>benchmark</b> images contain a wide range of real-world images of material states, from cooking, food, rocks, construction, plants, and liquids each in various states (wet/dry/stained/cooked/burned/worned/rusted/sediment/foam&mldr;). The annotation includes both partial similarity between regions with similar but not identical materials, and hard segmentation of only points of the exact same material state. We show that net trains on MatSeg significantly outperform existing state-of-the-art methods on this task.</p></p class="citation"></blockquote><h3 id=2954--173290-self-supervised-3d-patient-modeling-with-multi-modal-attentive-fusion-meng-zheng-et-al-2024>(29/54 | 173/290) Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion (Meng Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng Zheng, Benjamin Planche, Xuan Gong, Fan Yang, Terrence Chen, Ziyan Wu. (2024)<br><strong>Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion</strong><br><button class=copy-to-clipboard title="Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Convolutional Neural Network, Multi-modal, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03217v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03217v1.pdf filename=2403.03217v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D patient body modeling is critical to the success of automated patient positioning for smart medical scanning and operating rooms. Existing <b>CNN-based</b> end-to-end patient modeling solutions typically require a) customized network designs demanding large amount of relevant training data, covering extensive realistic clinical scenarios (e.g., patient covered by sheets), which leads to suboptimal generalizability in practical deployment, b) expensive 3D human model annotations, i.e., requiring huge amount of manual effort, resulting in systems that scale poorly. To address these issues, we propose a generic modularized 3D patient modeling method consists of (a) a <b>multi-modal</b> keypoint detection module with attentive fusion for 2D patient joint localization, to learn complementary cross-modality patient body information, leading to improved keypoint localization robustness and generalizability in a wide variety of imaging (e.g., CT, MRI etc.) and clinical scenarios (e.g., heavy occlusions); and (b) a <b>self-supervised</b> 3D mesh regression module which does not require expensive 3D mesh parameter annotations to train, bringing immediate cost benefits for clinical deployment. We demonstrate the efficacy of the proposed method by extensive patient positioning experiments on both public and clinical data. Our evaluation results achieve superior patient positioning performance across various imaging modalities in real clinical scenarios.</p></p class="citation"></blockquote><h3 id=3054--174290-palmprobnet-a-probabilistic-approach-to-understanding-palm-distributions-in-ecuadorian-tropical-forest-via-transfer-learning-kangning-cui-et-al-2024>(30/54 | 174/290) PalmProbNet: A Probabilistic Approach to Understanding Palm Distributions in Ecuadorian Tropical Forest via Transfer Learning (Kangning Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kangning Cui, Zishan Shao, Gregory Larsen, Victor Pauca, Sarra Alqahtani, David Segurado, João Pinheiro, Manqi Wang, David Lutz, Robert Plemmons, Miles Silman. (2024)<br><strong>PalmProbNet: A Probabilistic Approach to Understanding Palm Distributions in Ecuadorian Tropical Forest via Transfer Learning</strong><br><button class=copy-to-clipboard title="PalmProbNet: A Probabilistic Approach to Understanding Palm Distributions in Ecuadorian Tropical Forest via Transfer Learning" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-9, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Transfer Learning, PaLM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03161v1.pdf filename=2403.03161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Palms</b> play an outsized role in tropical forests and are important resources for humans and wildlife. A central question in tropical ecosystems is understanding <b>palm</b> distribution and abundance. However, accurately identifying and localizing <b>palms</b> in geospatial imagery presents significant challenges due to dense vegetation, overlapping canopies, and variable lighting conditions in mixed-forest landscapes. Addressing this, we introduce PalmProbNet, a probabilistic approach utilizing <b>transfer</b> <b>learning</b> to analyze high-resolution UAV-derived orthomosaic imagery, enabling the detection of <b>palm</b> trees within the dense canopy of the Ecuadorian Rainforest. This approach represents a substantial advancement in automated <b>palm</b> detection, effectively pinpointing <b>palm</b> presence and locality in mixed tropical rainforests. Our process begins by generating an orthomosaic image from UAV images, from which we extract and label <b>palm</b> and non-palm image patches in two distinct sizes. These patches are then used to train models with an identical architecture, consisting of an unaltered pre-trained ResNet-18 and a Multilayer Perceptron (MLP) with specifically trained parameters. Subsequently, PalmProbNet employs a sliding window technique on the landscape orthomosaic, using both small and large window sizes to generate a probability heatmap. This heatmap effectively visualizes the distribution of <b>palms,</b> showcasing the scalability and adaptability of our approach in various forest densities. Despite the challenging terrain, our method demonstrated remarkable performance, achieving an accuracy of 97.32% and a Cohen&rsquo;s kappa of 94.59% in testing.</p></p class="citation"></blockquote><h3 id=3154--175290-zero-led-zero-reference-lighting-estimation-diffusion-model-for-low-light-image-enhancement-jinhong-he-et-al-2024>(31/54 | 175/290) Zero-LED: Zero-Reference Lighting Estimation Diffusion Model for Low-Light Image Enhancement (Jinhong He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinhong He, Minglong Xue, Zhipu Liu, Chengyun Song, Senming Zhong. (2024)<br><strong>Zero-LED: Zero-Reference Lighting Estimation Diffusion Model for Low-Light Image Enhancement</strong><br><button class=copy-to-clipboard title="Zero-LED: Zero-Reference Lighting Estimation Diffusion Model for Low-Light Image Enhancement" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02879v1.pdf filename=2403.02879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>model-based</b> low-light image enhancement methods rely heavily on paired training data, leading to limited extensive application. Meanwhile, existing <b>unsupervised</b> methods lack effective bridging capabilities for unknown degradation. To address these limitations, we propose a novel zero-reference lighting estimation <b>diffusion</b> <b>model</b> for low-light image enhancement called Zero-LED. It utilizes the stable convergence ability of <b>diffusion</b> <b>models</b> to bridge the gap between low-light domains and real normal-light domains and successfully alleviates the dependence on pairwise training data via zero-reference learning. Specifically, we first design the initial optimization network to preprocess the input image and implement bidirectional constraints between the <b>diffusion</b> <b>model</b> and the initial optimization network through multiple objective functions. Subsequently, the degradation factors of the real-world scene are optimized iteratively to achieve effective light enhancement. In addition, we explore a frequency-domain based and semantically guided appearance reconstruction module that encourages feature alignment of the recovered image at a fine-grained level and satisfies subjective expectations. Finally, extensive experiments demonstrate the superiority of our approach to other state-of-the-art methods and more significant generalization capabilities. We will open the source code upon acceptance of the paper.</p></p class="citation"></blockquote><h3 id=3254--176290-are-dense-labels-always-necessary-for-3d-object-detection-from-point-cloud-chenqiang-gao-et-al-2024>(32/54 | 176/290) Are Dense Labels Always Necessary for 3D Object Detection from Point Cloud? (Chenqiang Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenqiang Gao, Chuandong Liu, Jun Shu, Fangcen Liu, Jiang Liu, Luyu Yang, Xinbo Gao, Deyu Meng. (2024)<br><strong>Are Dense Labels Always Necessary for 3D Object Detection from Point Cloud?</strong><br><button class=copy-to-clipboard title="Are Dense Labels Always Necessary for 3D Object Detection from Point Cloud?" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02818v1.pdf filename=2403.02818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current state-of-the-art (SOTA) 3D <b>object</b> <b>detection</b> methods often require a large amount of 3D bounding box annotations for training. However, collecting such large-scale densely-supervised datasets is notoriously costly. To reduce the cumbersome data annotation process, we propose a novel sparsely-annotated framework, in which we just annotate one 3D <b>object</b> <b>per</b> scene. Such a sparse annotation strategy could significantly reduce the heavy annotation burden, while inexact and incomplete sparse supervision may severely deteriorate the detection performance. To address this issue, we develop the SS3D++ method that alternatively improves 3D detector training and confident fully-annotated scene generation in a unified learning scheme. Using sparse annotations as seeds, we progressively generate confident fully-annotated scenes based on designing a missing-annotated instance mining module and reliable background mining module. Our proposed method produces competitive results when compared with SOTA <b>weakly-supervised</b> methods using the same or even more annotation costs. Besides, compared with SOTA fully-supervised methods, we achieve on-par or even better performance on the KITTI dataset with about 5x less annotation cost, and 90% of their performance on the Waymo dataset with about 15x less annotation cost. The additional unlabeled training scenes could further boost the performance. The code will be available at <a href=https://github.com/gaocq/SS3D2>https://github.com/gaocq/SS3D2</a>.</p></p class="citation"></blockquote><h3 id=3354--177290-hunter-unsupervised-human-centric-3d-detection-via-transferring-knowledge-from-synthetic-instances-to-real-scenes-yichen-yao-et-al-2024>(33/54 | 177/290) HUNTER: Unsupervised Human-centric 3D Detection via Transferring Knowledge from Synthetic Instances to Real Scenes (Yichen Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichen Yao, Zimo Jiang, Yujing Sun, Zhencai Zhu, Xinge Zhu, Runnan Chen, Yuexin Ma. (2024)<br><strong>HUNTER: Unsupervised Human-centric 3D Detection via Transferring Knowledge from Synthetic Instances to Real Scenes</strong><br><button class=copy-to-clipboard title="HUNTER: Unsupervised Human-centric 3D Detection via Transferring Knowledge from Synthetic Instances to Real Scenes" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02769v1.pdf filename=2403.02769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human-centric 3D scene understanding has recently drawn increasing attention, driven by its critical impact on robotics. However, human-centric real-life scenarios are extremely diverse and complicated, and humans have intricate motions and interactions. With limited labeled data, <b>supervised</b> methods are difficult to generalize to general scenarios, hindering real-life applications. Mimicking human intelligence, we propose an <b>unsupervised</b> 3D detection method for human-centric scenarios by transferring the knowledge from synthetic human instances to real scenes. To bridge the gap between the distinct data representations and feature distributions of synthetic models and real point clouds, we introduce novel modules for effective instance-to-scene representation transfer and synthetic-to-real feature alignment. Remarkably, our method exhibits superior performance compared to current state-of-the-art techniques, achieving a substantial 87.8% improvement in mAP and closely approaching the performance of fully <b>supervised</b> methods (62.15 mAP vs. 69.02 mAP) on HuCenLife.</p></p class="citation"></blockquote><h3 id=3454--178290-learning-group-activity-features-through-person-attribute-prediction-chihiro-nakatani-et-al-2024>(34/54 | 178/290) Learning Group Activity Features Through Person Attribute Prediction (Chihiro Nakatani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chihiro Nakatani, Hiroaki Kawashima, Norimichi Ukita. (2024)<br><strong>Learning Group Activity Features Through Person Attribute Prediction</strong><br><button class=copy-to-clipboard title="Learning Group Activity Features Through Person Attribute Prediction" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02753v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02753v2.pdf filename=2403.02753v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes Group Activity Feature (GAF) learning in which features of multi-person activity are learned as a compact latent vector. Unlike prior work in which the manual annotation of group activities is required for <b>supervised</b> <b>learning,</b> our method learns the GAF through person attribute prediction without group activity annotations. By learning the whole network in an end-to-end manner so that the GAF is required for predicting the person attributes of people in a group, the GAF is trained as the features of multi-person activity. As a person attribute, we propose to use a person&rsquo;s action class and appearance features because the former is easy to annotate due to its simpleness, and the latter requires no manual annotation. In addition, we introduce a location-guided attribute prediction to disentangle the complex GAF for extracting the features of each target person properly. Various experimental results validate that our method outperforms SOTA methods quantitatively and qualitatively on two public datasets. Visualization of our GAF also demonstrates that our method learns the GAF representing fined-grained group activity classes. Code: <a href=https://github.com/chihina/GAFL-CVPR2024>https://github.com/chihina/GAFL-CVPR2024</a>.</p></p class="citation"></blockquote><h3 id=3554--179290-semantic-human-mesh-reconstruction-with-textures-xiaoyu-zhan-et-al-2024>(35/54 | 179/290) Semantic Human Mesh Reconstruction with Textures (Xiaoyu Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyu Zhan, Jianxin Yang, Yuanqi Li, Jie Guo, Yanwen Guo, Wenping Wang. (2024)<br><strong>Semantic Human Mesh Reconstruction with Textures</strong><br><button class=copy-to-clipboard title="Semantic Human Mesh Reconstruction with Textures" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02561v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02561v1.pdf filename=2403.02561v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of 3D detailed human mesh reconstruction has made significant progress in recent years. However, current methods still face challenges when used in industrial applications due to unstable results, low-quality meshes, and a lack of UV unwrapping and skinning weights. In this paper, we present SHERT, a novel pipeline that can reconstruct semantic human meshes with textures and high-precision details. SHERT applies semantic- and normal-based sampling between the detailed surface (eg mesh and SDF) and the corresponding SMPL-X model to obtain a partially sampled semantic mesh and then generates the complete semantic mesh by our specifically designed <b>self-supervised</b> completion and refinement networks. Using the complete semantic mesh as a basis, we employ a texture <b>diffusion</b> <b>model</b> to create human textures that are driven by both images and texts. Our reconstructed meshes have stable UV unwrapping, high-quality triangle meshes, and consistent semantic information. The given SMPL-X model provides semantic information and shape priors, allowing SHERT to perform well even with incorrect and incomplete inputs. The semantic information also makes it easy to substitute and animate different body parts such as the face, body, and hands. Quantitative and qualitative experiments demonstrate that SHERT is capable of producing high-fidelity and robust semantic meshes that outperform state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=3654--180290-cracknex-a-few-shot-low-light-crack-segmentation-model-based-on-retinex-theory-for-uav-inspections-zhen-yao-et-al-2024>(36/54 | 180/290) CrackNex: a Few-shot Low-light Crack Segmentation Model Based on Retinex Theory for UAV Inspections (Zhen Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Yao, Jiawei Xu, Shuhang Hou, Mooi Choo Chuah. (2024)<br><strong>CrackNex: a Few-shot Low-light Crack Segmentation Model Based on Retinex Theory for UAV Inspections</strong><br><button class=copy-to-clipboard title="CrackNex: a Few-shot Low-light Crack Segmentation Model Based on Retinex Theory for UAV Inspections" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03063v1.pdf filename=2403.03063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Routine visual inspections of concrete structures are imperative for upholding the safety and integrity of critical infrastructure. Such visual inspections sometimes happen under low-light conditions, e.g., checking for bridge health. Crack segmentation under such conditions is challenging due to the poor contrast between cracks and their surroundings. However, most deep learning methods are designed for well-illuminated crack images and hence their performance drops dramatically in low-light scenes. In addition, conventional approaches require many annotated low-light crack images which is time-consuming. In this paper, we address these challenges by proposing CrackNex, a framework that utilizes reflectance information based on Retinex Theory to help the model learn a unified illumination-invariant representation. Furthermore, we utilize <b>few-shot</b> segmentation to solve the inefficient training data problem. In CrackNex, both a support prototype and a reflectance prototype are extracted from the support set. Then, a prototype fusion module is designed to integrate the features from both prototypes. CrackNex outperforms the SOTA methods on multiple datasets. Additionally, we present the first <b>benchmark</b> dataset, LCSD, for low-light crack segmentation. LCSD consists of 102 well-illuminated crack images and 41 low-light crack images. The dataset and code are available at <a href=https://github.com/zy1296/CrackNex>https://github.com/zy1296/CrackNex</a>.</p></p class="citation"></blockquote><h3 id=3754--181290-f3loc-fusion-and-filtering-for-floorplan-localization-changan-chen-et-al-2024>(37/54 | 181/290) F$^3$Loc: Fusion and Filtering for Floorplan Localization (Changan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changan Chen, Rui Wang, Christoph Vogel, Marc Pollefeys. (2024)<br><strong>F$^3$Loc: Fusion and Filtering for Floorplan Localization</strong><br><button class=copy-to-clipboard title="F$^3$Loc: Fusion and Filtering for Floorplan Localization" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03370v1.pdf filename=2403.03370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we propose an efficient data-driven solution to self-localization within a floorplan. Floorplan data is readily available, long-term persistent and inherently robust to changes in the visual appearance. Our method does not require retraining per map and location or demand a large database of images of the area of interest. We propose a novel <b>probabilistic</b> <b>model</b> consisting of an observation and a novel temporal filtering module. Operating internally with an efficient ray-based representation, the observation module consists of a single and a multiview module to predict horizontal depth from images and fuses their results to benefit from advantages offered by either methodology. Our method operates on conventional consumer hardware and overcomes a common limitation of competing methods that often demand upright images. Our full system meets real-time requirements, while outperforming the state-of-the-art by a significant margin.</p></p class="citation"></blockquote><h3 id=3854--182290-far-flexible-accurate-and-robust-6dof-relative-camera-pose-estimation-chris-rockwell-et-al-2024>(38/54 | 182/290) FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation (Chris Rockwell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chris Rockwell, Nilesh Kulkarni, Linyi Jin, Jeong Joon Park, Justin Johnson, David F. Fouhey. (2024)<br><strong>FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation</strong><br><button class=copy-to-clipboard title="FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03221v1.pdf filename=2403.03221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating relative camera poses between images has been a central problem in computer vision. Methods that find correspondences and solve for the fundamental matrix offer high precision in most cases. Conversely, methods predicting pose directly using neural networks are more robust to limited overlap and can infer absolute translation scale, but at the expense of reduced precision. We show how to combine the best of both methods; our approach yields results that are both precise and robust, while also accurately inferring translation scales. At the heart of our model lies a <b>Transformer</b> that (1) learns to balance between solved and learned pose estimations, and (2) provides a prior to guide a solver. A comprehensive analysis supports our design choices and demonstrates that our method adapts flexibly to various feature extractors and correspondence estimators, showing state-of-the-art performance in 6DoF pose estimation on Matterport3D, InteriorNet, StreetLearn, and Map-free Relocalization.</p></p class="citation"></blockquote><h3 id=3954--183290-triple-cfn-restructuring-conceptual-spaces-for-enhancing-abstract-reasoning-process-ruizhuo-song-et-al-2024>(39/54 | 183/290) Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract Reasoning process (Ruizhuo Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruizhuo Song, Beiming Yuan. (2024)<br><strong>Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract Reasoning process</strong><br><button class=copy-to-clipboard title="Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract Reasoning process" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03190v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03190v3.pdf filename=2403.03190v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Abstract <b>reasoning</b> problems pose significant challenges to artificial intelligence algorithms, demanding cognitive capabilities beyond those required for perception tasks. This study introduces the Triple-CFN approach to tackle the Bongard-Logo problem, achieving notable <b>reasoning</b> accuracy by implicitly reorganizing the concept space of conflicting instances. Additionally, the Triple-CFN paradigm proves effective for the RPM problem with necessary modifications, yielding competitive results. To further enhance performance on the RPM issue, we develop the Meta Triple-CFN network, which explicitly structures the problem space while maintaining interpretability on progressive patterns. The success of Meta Triple-CFN is attributed to its paradigm of modeling the conceptual space, equivalent to normalizing <b>reasoning</b> information. Based on this ideology, we introduce the Re-space layer, enhancing the performance of both Meta Triple-CFN and Triple-CFN. This paper aims to contribute to advancements in machine intelligence by exploring innovative network designs for addressing abstract <b>reasoning</b> problems, paving the way for further breakthroughs in this domain.</p></p class="citation"></blockquote><h3 id=4054--184290-neural-image-compression-with-text-guided-encoding-for-both-pixel-level-and-perceptual-fidelity-hagyeong-lee-et-al-2024>(40/54 | 184/290) Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity (Hagyeong Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hagyeong Lee, Minkyu Kim, Jun-Hyuk Kim, Seungeon Kim, Dokwan Oh, Jaeho Lee. (2024)<br><strong>Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity</strong><br><button class=copy-to-clipboard title="Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02944v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02944v1.pdf filename=2403.02944v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in text-guided image compression have shown great potential to enhance the perceptual quality of reconstructed images. These methods, however, tend to have significantly degraded pixel-wise fidelity, limiting their practicality. To fill this gap, we develop a new text-guided image compression algorithm that achieves both high perceptual and pixel-wise fidelity. In particular, we propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint <b>image-text</b> loss. By doing so, we avoid decoding based on text-guided generative models &ndash; known for high generative diversity &ndash; and effectively utilize the semantic information of text at a global level. Experimental results on various datasets show that our method can achieve high pixel-level and perceptual quality, with either human- or machine-generated captions. In particular, our method outperforms all baselines in terms of LPIPS, with some room for even more improvements when we use more carefully generated captions.</p></p class="citation"></blockquote><h3 id=4154--185290-cross-domain-image-conversion-by-cycledm-sho-shimotsumagari-et-al-2024>(41/54 | 185/290) Cross-Domain Image Conversion by CycleDM (Sho Shimotsumagari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sho Shimotsumagari, Shumpei Takezaki, Daichi Haraguchi, Seiichi Uchida. (2024)<br><strong>Cross-Domain Image Conversion by CycleDM</strong><br><button class=copy-to-clipboard title="Cross-Domain Image Conversion by CycleDM" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02919v1.pdf filename=2403.02919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The purpose of this paper is to enable the conversion between machine-printed character images (i.e., font images) and handwritten character images through machine learning. For this purpose, we propose a novel unpaired image-to-image domain conversion method, CycleDM, which incorporates the concept of CycleGAN into the <b>diffusion</b> <b>model.</b> Specifically, CycleDM has two internal conversion models that bridge the denoising processes of two image domains. These conversion models are efficiently trained without explicit correspondence between the domains. By applying machine-printed and handwritten character images to the two modalities, CycleDM realizes the conversion between them. Our experiments for evaluating the converted images quantitatively and qualitatively found that ours performs better than other comparable approaches.</p></p class="citation"></blockquote><h3 id=4254--186290-enhancing-the-rate-distortion-perception-flexibility-of-learned-image-codecs-with-conditional-diffusion-decoders-daniele-mari-et-al-2024>(42/54 | 186/290) Enhancing the Rate-Distortion-Perception Flexibility of Learned Image Codecs with Conditional Diffusion Decoders (Daniele Mari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniele Mari, Simone Milani. (2024)<br><strong>Enhancing the Rate-Distortion-Perception Flexibility of Learned Image Codecs with Conditional Diffusion Decoders</strong><br><button class=copy-to-clipboard title="Enhancing the Rate-Distortion-Perception Flexibility of Learned Image Codecs with Conditional Diffusion Decoders" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02887v1.pdf filename=2403.02887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learned image compression codecs have recently achieved impressive compression performances surpassing the most efficient image coding architectures. However, most approaches are trained to minimize rate and distortion which often leads to unsatisfactory visual results at low bitrates since perceptual metrics are not taken into account. In this paper, we show that conditional <b>diffusion</b> <b>models</b> can lead to promising results in the generative compression task when used as a decoder, and that, given a compressed representation, they allow creating new tradeoff points between distortion and perception at the decoder side based on the sampling method.</p></p class="citation"></blockquote><h3 id=4354--187290-revisiting-confidence-estimation-towards-reliable-failure-prediction-fei-zhu-et-al-2024>(43/54 | 187/290) Revisiting Confidence Estimation: Towards Reliable Failure Prediction (Fei Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fei Zhu, Xu-Yao Zhang, Zhen Cheng, Cheng-Lin Liu. (2024)<br><strong>Revisiting Confidence Estimation: Towards Reliable Failure Prediction</strong><br><button class=copy-to-clipboard title="Revisiting Confidence Estimation: Towards Reliable Failure Prediction" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02886v1.pdf filename=2403.02886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reliable confidence estimation is a challenging yet fundamental requirement in many risk-sensitive applications. However, modern deep neural networks are often overconfident for their incorrect predictions, i.e., misclassified samples from known classes, and <b>out-of-distribution</b> (OOD) samples from unknown classes. In recent years, many confidence calibration and OOD detection methods have been developed. In this paper, we find a general, widely existing but actually-neglected phenomenon that most confidence estimation methods are harmful for detecting misclassification errors. We investigate this problem and reveal that popular calibration and OOD detection methods often lead to worse confidence separation between correctly classified and misclassified examples, making it difficult to decide whether to trust a prediction or not. Finally, we propose to enlarge the confidence gap by finding flat minima, which yields state-of-the-art failure prediction performance under various settings including balanced, long-tailed, and covariate-shift classification scenarios. Our study not only provides a strong baseline for reliable confidence estimation but also acts as a bridge between understanding calibration, OOD detection, and failure prediction. The code is available at \url{https://github.com/Impression2805/FMFP}.</p></p class="citation"></blockquote><h3 id=4454--188290-activead-planning-oriented-active-learning-for-end-to-end-autonomous-driving-han-lu-et-al-2024>(44/54 | 188/290) ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving (Han Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Lu, Xiaosong Jia, Yichen Xie, Wenlong Liao, Xiaokang Yang, Junchi Yan. (2024)<br><strong>ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving</strong><br><button class=copy-to-clipboard title="ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02877v1.pdf filename=2403.02877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>End-to-end differentiable learning for autonomous driving (AD) has recently become a prominent paradigm. One main bottleneck lies in its voracious appetite for high-quality labeled data e.g. 3D bounding boxes and semantic segmentation, which are notoriously expensive to manually annotate. The difficulty is further pronounced due to the prominent fact that the behaviors within samples in AD often suffer from long tailed distribution. In other words, a large part of collected data can be trivial (e.g. simply driving forward in a straight road) and only a few cases are safety-critical. In this paper, we explore a practically important yet under-explored problem about how to achieve sample and label efficiency for end-to-end AD. Specifically, we design a planning-oriented <b>active</b> <b>learning</b> method which progressively annotates part of collected raw data according to the proposed diversity and usefulness criteria for planning routes. Empirically, we show that our planning-oriented approach could outperform general <b>active</b> <b>learning</b> methods by a large margin. Notably, our method achieves comparable performance with state-of-the-art end-to-end AD methods - by using only 30% nuScenes data. We hope our work could inspire future works to explore end-to-end AD from a data-centric perspective in addition to methodology efforts.</p></p class="citation"></blockquote><h3 id=4554--189290-tuning-free-noise-rectification-for-high-fidelity-image-to-video-generation-weijie-li-et-al-2024>(45/54 | 189/290) Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation (Weijie Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weijie Li, Litong Gong, Yiran Zhu, Fanda Fan, Biao Wang, Tiezheng Ge, Bo Zheng. (2024)<br><strong>Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation</strong><br><button class=copy-to-clipboard title="Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02827v1.pdf filename=2403.02827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image-to-video (I2V) generation tasks always suffer from keeping high fidelity in the open domains. Traditional image animation techniques primarily focus on specific domains such as faces or human poses, making them difficult to generalize to open domains. Several recent I2V frameworks based on <b>diffusion</b> <b>models</b> can generate dynamic content for open domain images but fail to maintain fidelity. We found that two main factors of low fidelity are the loss of image details and the noise prediction biases during the denoising process. To this end, we propose an effective method that can be applied to mainstream video <b>diffusion</b> <b>models.</b> This method achieves high fidelity based on supplementing more precise image information and noise rectification. Specifically, given a specified image, our method first adds noise to the input image latent to keep more details, then denoises the noisy latent with proper rectification to alleviate the noise prediction biases. Our method is tuning-free and plug-and-play. The experimental results demonstrate the effectiveness of our approach in improving the fidelity of generated videos. For more image-to-video generated results, please refer to the project website: <a href=https://noise-rectification.github.io>https://noise-rectification.github.io</a>.</p></p class="citation"></blockquote><h3 id=4654--190290-bootstrapping-rare-object-detection-in-high-resolution-satellite-imagery-akram-zaytar-et-al-2024>(46/54 | 190/290) Bootstrapping Rare Object Detection in High-Resolution Satellite Imagery (Akram Zaytar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akram Zaytar, Caleb Robinson, Gilles Q. Hacheme, Girmaw A. Tadesse, Rahul Dodhia, Juan M. Lavista Ferres, Lacey F. Hughey, Jared A. Stabach, Irene Amoke. (2024)<br><strong>Bootstrapping Rare Object Detection in High-Resolution Satellite Imagery</strong><br><button class=copy-to-clipboard title="Bootstrapping Rare Object Detection in High-Resolution Satellite Imagery" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02736v1.pdf filename=2403.02736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rare <b>object</b> <b>detection</b> is a fundamental task in applied geospatial machine learning, however is often challenging due to large amounts of high-resolution satellite or aerial imagery and few or no labeled positive samples to start with. This paper addresses the problem of bootstrapping such a rare <b>object</b> <b>detection</b> task assuming there is no labeled data and no spatial prior over the area of interest. We propose novel offline and online cluster-based approaches for sampling patches that are significantly more efficient, in terms of exposing positive samples to a human annotator, than random sampling. We apply our methods for identifying bomas, or small enclosures for herd animals, in the Serengeti Mara region of Kenya and Tanzania. We demonstrate a significant enhancement in detection efficiency, achieving a positive sampling rate increase from 2% (random) to 30%. This advancement enables effective machine learning mapping even with minimal labeling budgets, exemplified by an F1 score on the boma detection task of 0.51 with a budget of 300 total patches.</p></p class="citation"></blockquote><h3 id=4754--191290-deep-common-feature-mining-for-efficient-video-semantic-segmentation-yaoyan-zheng-et-al-2024>(47/54 | 191/290) Deep Common Feature Mining for Efficient Video Semantic Segmentation (Yaoyan Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaoyan Zheng, Hongyu Yang, Di Huang. (2024)<br><strong>Deep Common Feature Mining for Efficient Video Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Deep Common Feature Mining for Efficient Video Semantic Segmentation" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02689v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02689v1.pdf filename=2403.02689v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in video semantic segmentation have made substantial progress by exploiting temporal correlations. Nevertheless, persistent challenges, including redundant computation and the reliability of the feature propagation process, underscore the need for further innovation. In response, we present Deep Common Feature Mining (DCFM), a novel approach strategically designed to address these challenges by leveraging the concept of feature sharing. DCFM explicitly decomposes features into two complementary components. The common representation extracted from a key-frame furnishes essential high-level information to neighboring non-key frames, allowing for direct re-utilization without feature propagation. Simultaneously, the independent feature, derived from each video frame, captures rapidly changing information, providing frame-specific clues crucial for segmentation. To achieve such decomposition, we employ a symmetric training strategy tailored for sparsely annotated data, empowering the backbone to learn a robust high-level representation enriched with common information. Additionally, we incorporate a <b>self-supervised</b> loss function to reinforce intra-class feature similarity and enhance temporal consistency. Experimental evaluations on the VSPW and Cityscapes datasets demonstrate the effectiveness of our method, showing a superior balance between accuracy and efficiency.</p></p class="citation"></blockquote><h3 id=4854--192290-bsdp-brain-inspired-streaming-dual-level-perturbations-for-online-open-world-object-detection-yu-chen-et-al-2024>(48/54 | 192/290) BSDP: Brain-inspired Streaming Dual-level Perturbations for Online Open World Object Detection (Yu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Chen, Liyan Ma, Liping Jing, Jian Yu. (2024)<br><strong>BSDP: Brain-inspired Streaming Dual-level Perturbations for Online Open World Object Detection</strong><br><button class=copy-to-clipboard title="BSDP: Brain-inspired Streaming Dual-level Perturbations for Online Open World Object Detection" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02637v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02637v1.pdf filename=2403.02637v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans can easily distinguish the known and unknown categories and can recognize the unknown <b>object</b> <b>by</b> learning it once instead of repeating it many times without forgetting the learned <b>object.</b> <b>Hence,</b> we aim to make deep learning models simulate the way people learn. We refer to such a learning manner as OnLine Open World <b>Object</b> <b>Detection(OLOWOD).</b> Existing OWOD approaches pay more attention to the identification of unknown categories, while the incremental learning part is also very important. Besides, some neuroscience research shows that specific noises allow the brain to form new connections and neural pathways which may improve learning speed and efficiency. In this paper, we take the dual-level information of old samples as perturbations on new samples to make the model good at learning new knowledge without forgetting the old knowledge. Therefore, we propose a simple plug-and-play method, called Brain-inspired Streaming Dual-level Perturbations(BSDP), to solve the OLOWOD problem. Specifically, (1) we first calculate the prototypes of previous categories and use the distance between samples and the prototypes as the sample selecting strategy to choose old samples for replay; (2) then take the prototypes as the streaming feature-level perturbations of new samples, so as to improve the plasticity of the model through revisiting the old knowledge; (3) and also use the distribution of the features of the old category samples to generate adversarial data in the form of streams as the data-level perturbations to enhance the robustness of the model to new categories. We empirically evaluate BSDP on PASCAL VOC and MS-COCO, and the excellent results demonstrate the promising performance of our proposed method and learning manner.</p></p class="citation"></blockquote><h3 id=4954--193290-what-do-we-learn-from-inverting-clip-models-hamid-kazemi-et-al-2024>(49/54 | 193/290) What do we learn from inverting CLIP models? (Hamid Kazemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamid Kazemi, Atoosa Chegini, Jonas Geiping, Soheil Feizi, Tom Goldstein. (2024)<br><strong>What do we learn from inverting CLIP models?</strong><br><button class=copy-to-clipboard title="What do we learn from inverting CLIP models?" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02580v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02580v1.pdf filename=2403.02580v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We employ an inversion-based approach to examine CLIP models. Our examination reveals that inverting CLIP models results in the generation of images that exhibit semantic alignment with the specified target <b>prompts.</b> We leverage these inverted images to gain insights into various aspects of CLIP models, such as their ability to blend concepts and inclusion of gender biases. We notably observe instances of NSFW (Not Safe For Work) images during model inversion. This phenomenon occurs even for semantically innocuous <b>prompts,</b> like &ldquo;a beautiful landscape,&rdquo; as well as for <b>prompts</b> involving the names of celebrities.</p></p class="citation"></blockquote><h3 id=5054--194290-why-not-use-your-textbook-knowledge-enhanced-procedure-planning-of-instructional-videos-kumaranage-ravindu-yasas-nagasinghe-et-al-2024>(50/54 | 194/290) Why Not Use Your Textbook? Knowledge-Enhanced Procedure Planning of Instructional Videos (Kumaranage Ravindu Yasas Nagasinghe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kumaranage Ravindu Yasas Nagasinghe, Honglu Zhou, Malitha Gunawardhana, Martin Renqiang Min, Daniel Harari, Muhammad Haris Khan. (2024)<br><strong>Why Not Use Your Textbook? Knowledge-Enhanced Procedure Planning of Instructional Videos</strong><br><button class=copy-to-clipboard title="Why Not Use Your Textbook? Knowledge-Enhanced Procedure Planning of Instructional Videos" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 8<br>Keywords: Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02782v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02782v1.pdf filename=2403.02782v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the capability of an agent to construct a logical sequence of action steps, thereby assembling a strategic procedural plan. This plan is crucial for navigating from an initial visual observation to a target visual outcome, as depicted in real-life instructional videos. Existing works have attained partial success by extensively leveraging various sources of information available in the datasets, such as heavy intermediate visual observations, procedural names, or natural language step-by-step instructions, for features or supervision signals. However, the task remains formidable due to the implicit causal constraints in the sequencing of steps and the variability inherent in multiple feasible plans. To tackle these intricacies that previous efforts have overlooked, we propose to enhance the capabilities of the agent by infusing it with procedural <b>knowledge.</b> <b>This</b> <b>knowledge,</b> <b>sourced</b> from training procedure plans and structured as a directed weighted <b>graph,</b> equips the agent to better navigate the complexities of step sequencing and its potential variations. We coin our approach KEPP, a novel <b>Knowledge-Enhanced</b> <b>Procedure</b> Planning system, which harnesses a probabilistic procedural <b>knowledge</b> <b>graph</b> extracted from training data, effectively acting as a comprehensive textbook for the training domain. Experimental evaluations across three widely-used datasets under settings of varying complexity reveal that KEPP attains superior, state-of-the-art results while requiring only minimal supervision.</p></p class="citation"></blockquote><h3 id=5154--195290-motion-corrected-moving-average-including-post-hoc-temporal-information-for-improved-video-segmentation-robert-mendel-et-al-2024>(51/54 | 195/290) Motion-Corrected Moving Average: Including Post-Hoc Temporal Information for Improved Video Segmentation (Robert Mendel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Mendel, Tobias Rueckert, Dirk Wilhelm, Daniel Rueckert, Christoph Palm. (2024)<br><strong>Motion-Corrected Moving Average: Including Post-Hoc Temporal Information for Improved Video Segmentation</strong><br><button class=copy-to-clipboard title="Motion-Corrected Moving Average: Including Post-Hoc Temporal Information for Improved Video Segmentation" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03120v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03120v1.pdf filename=2403.03120v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-time computational speed and a high degree of precision are requirements for computer-assisted interventions. Applying a segmentation network to a medical video processing task can introduce significant inter-frame prediction noise. Existing approaches can reduce inconsistencies by including temporal information but often impose requirements on the architecture or dataset. This paper proposes a method to include temporal information in any segmentation model and, thus, a technique to improve video segmentation performance without alterations during training or additional labeling. With Motion-Corrected Moving Average, we refine the exponential moving average between the current and previous predictions. Using optical flow to estimate the movement between consecutive frames, we can shift the prior term in the moving-average calculation to align with the <b>geometry</b> of the current frame. The optical flow calculation does not require the output of the model and can therefore be performed in parallel, leading to no significant runtime penalty for our approach. We evaluate our approach on two publicly available segmentation datasets and two proprietary endoscopic datasets and show improvements over a baseline approach.</p></p class="citation"></blockquote><h3 id=5254--196290-a-backpack-full-of-skills-egocentric-video-understanding-with-diverse-task-perspectives-simone-alberto-peirone-et-al-2024>(52/54 | 196/290) A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives (Simone Alberto Peirone et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simone Alberto Peirone, Francesca Pistilli, Antonio Alliegro, Giuseppe Averta. (2024)<br><strong>A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives</strong><br><button class=copy-to-clipboard title="A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03037v1.pdf filename=2403.03037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once. We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills. To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills. We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpack of skills that a robot can carry around and use when needed. We demonstrate the effectiveness and efficiency of our approach on four Ego4D <b>benchmarks,</b> outperforming current state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=5354--197290-enhancing-long-term-person-re-identification-using-global-local-body-part-and-head-streams-duy-tran-thanh-et-al-2024>(53/54 | 197/290) Enhancing Long-Term Person Re-Identification Using Global, Local Body Part, and Head Streams (Duy Tran Thanh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duy Tran Thanh, Yeejin Lee, Byeongkeun Kang. (2024)<br><strong>Enhancing Long-Term Person Re-Identification Using Global, Local Body Part, and Head Streams</strong><br><button class=copy-to-clipboard title="Enhancing Long-Term Person Re-Identification Using Global, Local Body Part, and Head Streams" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02892v1.pdf filename=2403.02892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work addresses the task of long-term person re-identification. Typically, person re-identification assumes that people do not change their clothes, which limits its applications to short-term scenarios. To overcome this limitation, we investigate long-term person re-identification, which considers both clothes-changing and clothes-consistent scenarios. In this paper, we propose a novel framework that effectively learns and utilizes both global and local information. The proposed framework consists of three streams: global, local body part, and head streams. The global and head streams encode identity-relevant information from an entire image and a cropped image of the head region, respectively. Both streams encode the most distinct, less distinct, and average features using the combinations of adversarial erasing, max pooling, and average pooling. The local body part stream extracts identity-related information for each body part, allowing it to be compared with the same body part from another image. Since body part annotations are not available in re-identification datasets, pseudo-labels are generated using <b>clustering.</b> These labels are then utilized to train a body part segmentation head in the local body part stream. The proposed framework is trained by backpropagating the weighted summation of the identity classification loss, the pair-based loss, and the pseudo body part segmentation loss. To demonstrate the effectiveness of the proposed method, we conducted experiments on three publicly available datasets (Celeb-reID, PRCC, and VC-Clothes). The experimental results demonstrate that the proposed method outperforms the previous state-of-the-art method.</p></p class="citation"></blockquote><h3 id=5454--198290-holovic-large-scale-dataset-and-benchmark-for-multi-sensor-holographic-intersection-and-vehicle-infrastructure-cooperative-cong-ma-et-al-2024>(54/54 | 198/290) HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative (Cong Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cong Ma, Lei Qiao, Chengkai Zhu, Kai Liu, Zelong Kong, Qing Li, Xueqi Zhou, Yuheng Kan, Wei Wu. (2024)<br><strong>HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative</strong><br><button class=copy-to-clipboard title="HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02640v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02640v2.pdf filename=2403.02640v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous Driving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one of the important research area. Due to the complexity of traffic conditions such as blind spots and occlusion, it greatly limits the perception capabilities of single-view roadside sensing systems. To further enhance the accuracy of roadside perception and provide better information to the vehicle side, in this paper, we constructed holographic intersections with various layouts to build a large-scale multi-sensor holographic vehicle-infrastructure cooperation dataset, called HoloVIC. Our dataset includes 3 different types of sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the different intersections. Each intersection is equipped with 6-18 sensors to capture synchronous data. While autonomous vehicles pass through these intersections for collecting VIC data. HoloVIC contains in total on 100k+ synchronous frames from different sensors. Additionally, we annotated 3D bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs of the same objects across different devices and consecutive frames in sequence. Based on HoloVIC, we formulated four tasks to facilitate the development of related research. We also provide <b>benchmarks</b> for these tasks.</p></p class="citation"></blockquote><h2 id=eessiv-4>eess.IV (4)</h2><h3 id=14--199290-enhancing-weakly-supervised-3d-medical-image-segmentation-through-probabilistic-aware-learning-zhaoxin-fan-et-al-2024>(1/4 | 199/290) Enhancing Weakly Supervised 3D Medical Image Segmentation through Probabilistic-aware Learning (Zhaoxin Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoxin Fan, Runmin Jiang, Junhao Wu, Xin Huang, Tianyang Wang, Heng Huang, Min Xu. (2024)<br><strong>Enhancing Weakly Supervised 3D Medical Image Segmentation through Probabilistic-aware Learning</strong><br><button class=copy-to-clipboard title="Enhancing Weakly Supervised 3D Medical Image Segmentation through Probabilistic-aware Learning" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 70<br>Keywords: Supervised Learning, Supervised Learning, Weakly-supervised Learning, Weakly-supervised Learning, Transformer, Self-Attention, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02566v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02566v1.pdf filename=2403.02566v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D medical image segmentation is a challenging task with crucial implications for disease diagnosis and treatment planning. Recent advances in deep learning have significantly enhanced fully <b>supervised</b> <b>medical</b> image segmentation. However, this approach heavily relies on labor-intensive and time-consuming fully annotated ground-truth labels, particularly for 3D volumes. To overcome this limitation, we propose a novel probabilistic-aware <b>weakly</b> <b>supervised</b> <b>learning</b> pipeline, specifically designed for 3D medical imaging. Our pipeline integrates three innovative components: a probability-based pseudo-label generation technique for synthesizing dense segmentation masks from sparse annotations, a Probabilistic Multi-head <b>Self-Attention</b> network for robust feature extraction within our Probabilistic <b>Transformer</b> Network, and a Probability-informed Segmentation Loss Function to enhance training with annotation confidence. Demonstrating significant advances, our approach not only rivals the performance of fully <b>supervised</b> <b>methods</b> but also surpasses existing <b>weakly</b> <b>supervised</b> <b>methods</b> in CT and MRI datasets, achieving up to 18.1% improvement in Dice scores for certain organs. The code is available at <a href=https://github.com/runminjiang/PW4MedSeg>https://github.com/runminjiang/PW4MedSeg</a>.</p></p class="citation"></blockquote><h3 id=24--200290-low-res-leads-the-way-improving-generalization-for-super-resolution-by-self-supervised-learning-haoyu-chen-et-al-2024>(2/4 | 200/290) Low-Res Leads the Way: Improving Generalization for Super-Resolution by Self-Supervised Learning (Haoyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Chen, Wenbo Li, Jinjin Gu, Jingjing Ren, Haoze Sun, Xueyi Zou, Zhensong Zhang, Youliang Yan, Lei Zhu. (2024)<br><strong>Low-Res Leads the Way: Improving Generalization for Super-Resolution by Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="Low-Res Leads the Way: Improving Generalization for Super-Resolution by Self-Supervised Learning" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Self-supervised Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02601v1.pdf filename=2403.02601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For image super-resolution (SR), bridging the gap between the performance on synthetic datasets and real-world degradation scenarios remains a challenge. This work introduces a novel &ldquo;Low-Res Leads the Way&rdquo; (LWay) training framework, merging <b>Supervised</b> Pre-training with <b>Self-supervised</b> <b>Learning</b> to enhance the adaptability of SR models to real-world images. Our approach utilizes a low-resolution (LR) reconstruction network to extract degradation embeddings from LR images, merging them with super-resolved outputs for LR reconstruction. Leveraging unseen LR images for <b>self-supervised</b> <b>learning</b> guides the model to adapt its modeling space to the target domain, facilitating <b>fine-tuning</b> of SR models without requiring paired high-resolution (HR) images. The integration of Discrete Wavelet Transform (DWT) further refines the focus on high-frequency details. Extensive evaluations show that our method significantly improves the generalization and detail restoration capabilities of SR models on unseen real-world datasets, outperforming existing methods. Our training regime is universally compatible, requiring no network architecture modifications, making it a practical solution for real-world SR applications.</p></p class="citation"></blockquote><h3 id=34--201290-anatomix-anatomy-aware-data-augmentation-for-multi-organ-segmentation-chang-liu-et-al-2024>(3/4 | 201/290) AnatoMix: Anatomy-aware Data Augmentation for Multi-organ Segmentation (Chang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chang Liu, Fuxin Fan, Annette Schwarz, Andreas Maier. (2024)<br><strong>AnatoMix: Anatomy-aware Data Augmentation for Multi-organ Segmentation</strong><br><button class=copy-to-clipboard title="AnatoMix: Anatomy-aware Data Augmentation for Multi-organ Segmentation" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03326v1.pdf filename=2403.03326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-organ segmentation in medical images is a widely researched task and can save much manual efforts of clinicians in daily routines. Automating the organ segmentation process using deep learning (DL) is a promising solution and state-of-the-art segmentation models are achieving promising accuracy. In this work, We proposed a novel <b>data</b> <b>augmentation</b> strategy for increasing the generalizibility of multi-organ segmentation datasets, namely AnatoMix. By object-level matching and manipulation, our method is able to generate new images with correct anatomy, i.e. organ segmentation mask, exponentially increasing the size of the segmentation dataset. Initial experiments have been done to investigate the segmentation performance influenced by our method on a public CT dataset. Our augmentation method can lead to mean dice of 76.1, compared with 74.8 of the baseline method.</p></p class="citation"></blockquote><h3 id=44--202290-speckle-noise-reduction-in-ultrasound-images-using-denoising-auto-encoder-with-skip-connection-suraj-bhute-et-al-2024>(4/4 | 202/290) Speckle Noise Reduction in Ultrasound Images using Denoising Auto-encoder with Skip Connection (Suraj Bhute et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suraj Bhute, Subhamoy Mandal, Debashree Guha. (2024)<br><strong>Speckle Noise Reduction in Ultrasound Images using Denoising Auto-encoder with Skip Connection</strong><br><button class=copy-to-clipboard title="Speckle Noise Reduction in Ultrasound Images using Denoising Auto-encoder with Skip Connection" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, eess-IV, eess.IV, physics-med-ph<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02750v1.pdf filename=2403.02750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ultrasound is a widely used medical tool for non-invasive diagnosis, but its images often contain speckle noise which can lower their resolution and contrast-to-noise ratio. This can make it more difficult to extract, recognize, and analyze features in the images, as well as impair the accuracy of computer-assisted diagnostic techniques and the ability of doctors to interpret the images. Reducing speckle noise, therefore, is a crucial step in the preprocessing of ultrasound images. Researchers have proposed several speckle reduction methods, but no single method takes all relevant factors into account. In this paper, we compare seven such methods: Median, Gaussian, Bilateral, Average, Weiner, Anisotropic and Denoising auto-encoder without and with skip connections in terms of their ability to preserve features and edges while effectively reducing noise. In an experimental study, a <b>convolutional</b> noise-removing auto-encoder with skip connection, a deep learning method, was used to improve ultrasound images of breast cancer. This method involved adding speckle noise at various levels. The results of the deep learning method were compared to those of traditional image enhancement methods, and it was found that the proposed method was more effective. To assess the performance of these algorithms, we use three established evaluation metrics and present both filtered images and statistical data.</p></p class="citation"></blockquote><h2 id=physicscomp-ph-1>physics.comp-ph (1)</h2><h3 id=11--203290-quantum-many-body-physics-calculations-with-large-language-models-haining-pan-et-al-2024>(1/1 | 203/290) Quantum Many-Body Physics Calculations with Large Language Models (Haining Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haining Pan, Nayantara Mudur, Will Taranto, Maria Tikhanovskaya, Subhashini Venugopalan, Yasaman Bahri, Michael P. Brenner, Eun-Ah Kim. (2024)<br><strong>Quantum Many-Body Physics Calculations with Large Language Models</strong><br><button class=copy-to-clipboard title="Quantum Many-Body Physics Calculations with Large Language Models" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.comp-ph<br>Categories: cond-mat-other, cs-AI, physics-comp-ph, physics.comp-ph<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03154v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03154v1.pdf filename=2403.03154v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated an unprecedented ability to perform complex tasks in multiple domains, including mathematical and scientific <b>reasoning.</b> We demonstrate that with carefully designed <b>prompts,</b> <b>LLMs</b> can accurately carry out key calculations in research papers in theoretical physics. We focus on a broadly used approximation method in quantum physics: the Hartree-Fock method, requiring an analytic multi-step calculation deriving approximate Hamiltonian and corresponding self-consistency equations. To carry out the calculations using <b>LLMs,</b> we design multi-step <b>prompt</b> templates that break down the analytic calculation into standardized steps with placeholders for problem-specific information. We evaluate <b>GPT-4&rsquo;s</b> performance in executing the calculation for 15 research papers from the past decade, demonstrating that, with correction of intermediate steps, it can correctly derive the final Hartree-Fock Hamiltonian in 13 cases and makes minor errors in 2 cases. Aggregating across all research papers, we find an average score of 87.5 (out of 100) on the execution of individual calculation steps. Overall, the requisite skill for doing these calculations is at the graduate level in quantum condensed matter theory. We further use <b>LLMs</b> to mitigate the two primary bottlenecks in this evaluation process: (i) extracting information from papers to fill in templates and (ii) automatic scoring of the calculation steps, demonstrating good results in both cases. The strong performance is the first step for developing algorithms that automatically explore theoretical hypotheses at an unprecedented scale.</p></p class="citation"></blockquote><h2 id=eessas-3>eess.AS (3)</h2><h3 id=13--204290-naturalspeech-3-zero-shot-speech-synthesis-with-factorized-codec-and-diffusion-models-zeqian-ju-et-al-2024>(1/3 | 204/290) NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models (Zeqian Ju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao. (2024)<br><strong>NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models</strong><br><button class=copy-to-clipboard title="NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-CL, cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 60<br>Keywords: Diffusion Model, Quantization, Zero-shot, Text-to-speech, Text-to-speech, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03100v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03100v1.pdf filename=2403.03100v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While recent large-scale <b>text-to-speech</b> <b>(TTS)</b> models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a <b>TTS</b> system with novel factorized <b>diffusion</b> <b>models</b> to generate natural speech in a <b>zero-shot</b> way. Specifically, 1) we design a neural codec with factorized vector <b>quantization</b> (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized <b>diffusion</b> <b>model</b> to generate attributes in each subspace following its corresponding <b>prompt.</b> With this factorization design, NaturalSpeech 3 can effectively and efficiently model the intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art <b>TTS</b> systems on quality, similarity, prosody, and intelligibility. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.</p></p class="citation"></blockquote><h3 id=23--205290-unpaired-signal-to-signal-translation-with-1d-conditional-gans-eric-easthope-2024>(2/3 | 205/290) (Un)paired signal-to-signal translation with 1D conditional GANs (Eric Easthope, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric Easthope. (2024)<br><strong>(Un)paired signal-to-signal translation with 1D conditional GANs</strong><br><button class=copy-to-clipboard title="(Un)paired signal-to-signal translation with 1D conditional GANs" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CV, cs-GR, cs-LG, eess-AS, eess.AS<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Convolution, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04800v1.pdf filename=2403.04800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>I show that a one-dimensional (1D) conditional <b>generative</b> <b>adversarial</b> <b>network</b> (cGAN) with an <b>adversarial</b> <b>training</b> architecture is capable of unpaired signal-to-signal (&ldquo;sig2sig&rdquo;) translation. Using a simplified CycleGAN model with 1D layers and wider <b>convolutional</b> kernels, mirroring WaveGAN to reframe two-dimensional (2D) image generation as 1D audio generation, I show that recasting the 2D image-to-image translation task to a 1D signal-to-signal translation task with deep <b>convolutional</b> <b>GANs</b> is possible without substantial modification to the conventional U-Net model and <b>adversarial</b> <b>architecture</b> developed as CycleGAN. With this I show for a small tunable dataset that noisy test signals unseen by the 1D CycleGAN model and without paired training transform from the source domain to signals similar to paired test signals in the translated domain, especially in terms of frequency, and I quantify these differences in terms of correlation and error.</p></p class="citation"></blockquote><h3 id=33--206290-attentionstitch-how-attention-solves-the-speech-editing-problem-antonios-alexos-et-al-2024>(3/3 | 206/290) AttentionStitch: How Attention Solves the Speech Editing Problem (Antonios Alexos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonios Alexos, Pierre Baldi. (2024)<br><strong>AttentionStitch: How Attention Solves the Speech Editing Problem</strong><br><button class=copy-to-clipboard title="AttentionStitch: How Attention Solves the Speech Editing Problem" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-LG, cs-MM, eess-AS, eess.AS<br>Keyword Score: 30<br>Keywords: Human Intervention, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04804v1.pdf filename=2403.04804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The generation of natural and high-quality speech from text is a challenging problem in the field of natural language processing. In addition to speech generation, speech editing is also a crucial task, which requires the seamless and unnoticeable integration of edited speech into synthesized speech. We propose a novel approach to speech editing by leveraging a pre-trained <b>text-to-speech</b> <b>(TTS)</b> model, such as FastSpeech 2, and incorporating a double attention block network on top of it to automatically merge the synthesized mel-spectrogram with the mel-spectrogram of the edited text. We refer to this model as AttentionStitch, as it harnesses attention to stitch audio samples together. We evaluate the proposed AttentionStitch model against state-of-the-art baselines on both single and multi-speaker datasets, namely LJSpeech and VCTK. We demonstrate its superior performance through an objective and a subjective evaluation test involving 15 <b>human</b> <b>participants.</b> AttentionStitch is capable of producing high-quality speech, even for words not seen during training, while operating automatically without the need for <b>human</b> <b>intervention.</b> Moreover, AttentionStitch is fast during both training and inference and is able to generate <b>human-sounding</b> <b>edited</b> speech.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--207290-rethinking-clustered-federated-learning-in-noma-enhanced-wireless-networks-yushen-lin-et-al-2024>(1/1 | 207/290) Rethinking Clustered Federated Learning in NOMA Enhanced Wireless Networks (Yushen Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yushen Lin, Kaidi Wang, Zhiguo Ding. (2024)<br><strong>Rethinking Clustered Federated Learning in NOMA Enhanced Wireless Networks</strong><br><button class=copy-to-clipboard title="Rethinking Clustered Federated Learning in NOMA Enhanced Wireless Networks" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-IT, cs-LG, cs-NI, cs.NI, math-IT<br>Keyword Score: 53<br>Keywords: Clustering, Federated Learning, Karush-Kuhn-Tucker, Karush-Kuhn-Tucker, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03157v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03157v1.pdf filename=2403.03157v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study explores the benefits of integrating the novel clustered <b>federated</b> <b>learning</b> (CFL) approach with non-orthogonal multiple access (NOMA) under non-independent and identically distributed (non-IID) datasets, where multiple devices participate in the aggregation with time limitations and a finite number of sub-channels. A detailed theoretical analysis of the generalization gap that measures the degree of non-IID in the data distribution is presented. Following that, solutions to address the challenges posed by non-IID conditions are proposed with the analysis of the properties. Specifically, users&rsquo; data distributions are parameterized as concentration parameters and grouped using spectral <b>clustering,</b> with Dirichlet distribution serving as the prior. The investigation into the generalization gap and convergence rate guides the design of sub-channel assignments through the matching-based algorithm, and the power allocation is achieved by <b>Karush-Kuhn-Tucker</b> <b>(KKT)</b> conditions with the derived closed-form solution. The extensive <b>simulation</b> results show that the proposed cluster-based FL framework can outperform FL baselines in terms of both test accuracy and convergence rate. Moreover, jointly optimizing sub-channel and power allocation in NOMA-enhanced networks can lead to a significant improvement.</p></p class="citation"></blockquote><h2 id=cshc-5>cs.HC (5)</h2><h3 id=15--208290-its-the-only-thing-i-can-trust-envisioning-large-language-model-use-by-autistic-workers-for-communication-assistance-jiwoong-jang-et-al-2024>(1/5 | 208/290) &lsquo;It&rsquo;s the only thing I can trust&rsquo;: Envisioning Large Language Model Use by Autistic Workers for Communication Assistance (JiWoong Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>JiWoong Jang, Sanika Moharana, Patrick Carrington, Andrew Begel. (2024)<br><strong>&lsquo;It&rsquo;s the only thing I can trust&rsquo;: Envisioning Large Language Model Use by Autistic Workers for Communication Assistance</strong><br><button class=copy-to-clipboard title="'It's the only thing I can trust': Envisioning Large Language Model Use by Autistic Workers for Communication Assistance" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03297v1.pdf filename=2403.03297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autistic adults often experience stigma and discrimination at work, leading them to seek social communication support from coworkers, friends, and family despite emotional risks. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are increasingly considered an alternative. In this work, we investigate the phenomenon of <b>LLM</b> use by autistic adults at work and explore opportunities and risks of <b>LLMs</b> as a source of social communication advice. We asked 11 autistic participants to present questions about their own workplace-related social difficulties to (1) a <b>GPT-4-based</b> <b>chatbot</b> and (2) a disguised human confederate. Our evaluation shows that participants strongly preferred <b>LLM</b> over confederate interactions. However, a coach specializing in supporting autistic job-seekers raised concerns that the <b>LLM</b> was dispensing questionable advice. We highlight how this divergence in participant and practitioner attitudes reflects existing schisms in HCI on the relative privileging of end-user wants versus normative good and propose design considerations for <b>LLMs</b> to center autistic experiences.</p></p class="citation"></blockquote><h3 id=25--209290-hints-sensemaking-on-large-collections-of-documents-with-hypergraph-visualization-and-intelligent-agents-sam-yu-te-lee-et-al-2024>(2/5 | 209/290) HINTs: Sensemaking on large collections of documents with Hypergraph visualization and INTelligent agents (Sam Yu-Te Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sam Yu-Te Lee, Kwan-Liu Ma. (2024)<br><strong>HINTs: Sensemaking on large collections of documents with Hypergraph visualization and INTelligent agents</strong><br><button class=copy-to-clipboard title="HINTs: Sensemaking on large collections of documents with Hypergraph visualization and INTelligent agents" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 33<br>Keywords: Clustering, Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02752v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02752v1.pdf filename=2403.02752v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sensemaking on a <b>large</b> <b>collection</b> <b>of</b> documents (corpus) is a challenging task often found in fields such as market research, legal studies, intelligence analysis, political science, computational linguistics, etc. Previous works approach this problem either from a topic- or entity-based perspective, but they lack interpretability and trust due to poor model alignment. In this paper, we present HINTs, a visual analytics approach that combines topic- and entity-based techniques seamlessly and integrates <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as both a general NLP task solver and an intelligent agent. By leveraging the extraction capability of <b>LLMs</b> in the data preparation stage, we model the corpus as a hypergraph that matches the user&rsquo;s mental model when making sense of the corpus. The constructed hypergraph is hierarchically organized with an agglomerative <b>clustering</b> algorithm by combining semantic and connectivity similarity. The system further integrates an <b>LLM-based</b> intelligent <b>chatbot</b> agent in the interface to facilitate sensemaking. To demonstrate the generalizability and effectiveness of the HINTs system, we present two case studies on different domains and a comparative user study. We report our insights on the behavior patterns and challenges when intelligent agents are used to facilitate sensemaking. We find that while intelligent agents can address many challenges in sensemaking, the visual hints that visualizations provide are necessary to address the new problems brought by intelligent agents. We discuss limitations and future work for combining interactive visualization and <b>LLMs</b> more profoundly to better support corpus analysis.</p></p class="citation"></blockquote><h3 id=35--210290-large-language-models-and-video-games-a-preliminary-scoping-review-penny-sweetser-2024>(3/5 | 210/290) Large Language Models and Video Games: A Preliminary Scoping Review (Penny Sweetser, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Penny Sweetser. (2024)<br><strong>Large Language Models and Video Games: A Preliminary Scoping Review</strong><br><button class=copy-to-clipboard title="Large Language Models and Video Games: A Preliminary Scoping Review" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02613v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02613v1.pdf filename=2403.02613v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> hold interesting potential for the design, development, and research of video games. Building on the decades of prior research on <b>generative</b> <b>AI</b> in games, many researchers have sped to investigate the power and potential of <b>LLMs</b> for games. Given the recent spike in <b>LLM-related</b> research in games, there is already a wealth of relevant research to survey. In order to capture a snapshot of the state of <b>LLM</b> research in games, and to help lay the foundation for future work, we carried out an initial scoping review of relevant papers published so far. In this paper, we review 76 papers published between 2022 to early 2024 on <b>LLMs</b> and video games, with key focus areas in game AI, game development, narrative, and game research and reviews. Our paper provides an early state of the field and lays the groundwork for future research and reviews on this topic.</p></p class="citation"></blockquote><h3 id=45--211290-citizen-science-and-machine-learning-for-research-and-nature-conservation-the-case-of-eurasian-lynx-free-ranging-rodents-and-insects-kinga-skorupska-et-al-2024>(4/5 | 211/290) Citizen Science and Machine Learning for Research and Nature Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects (Kinga Skorupska et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kinga Skorupska, Rafał Stryjek, Izabela Wierzbowska, Piotr Bebas, Maciej Grzeszczuk, Piotr Gago, Jarosław Kowalski, Maciej Krzywicki, Jagoda Lazarek, Wiesław Kopeć. (2024)<br><strong>Citizen Science and Machine Learning for Research and Nature Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects</strong><br><button class=copy-to-clipboard title="Citizen Science and Machine Learning for Research and Nature Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CV, cs-CY, cs-HC, cs-LG, cs.HC<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02906v1.pdf filename=2403.02906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Technology is increasingly used in Nature Reserves and National Parks around the world to support conservation efforts. Endangered species, such as the Eurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps. Yet, this method produces vast amounts of data, which needs to be prepared, analyzed and interpreted. Therefore, researchers working in this area increasingly need support to process this incoming information. One opportunity is to seek support from volunteer Citizen Scientists who can help label the data, however, it is challenging to retain their interest. Another way is to automate the process with image recognition using <b>convolutional</b> <b>neural</b> <b>networks.</b> During the panel, we will discuss considerations related to nature research and conservation as well as opportunities for the use of Citizen Science and Machine Learning to expedite the process of data preparation, labelling and analysis.</p></p class="citation"></blockquote><h3 id=55--212290-data-driven-ergonomic-risk-assessment-of-complex-hand-intensive-manufacturing-processes-anand-krishnan-et-al-2024>(5/5 | 212/290) Data-Driven Ergonomic Risk Assessment of Complex Hand-intensive Manufacturing Processes (Anand Krishnan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anand Krishnan, Xingjian Yang, Utsav Seth, Jonathan M. Jeyachandran, Jonathan Y. Ahn, Richard Gardner, Samuel F. Pedigo, Adriana, Blom-Schieber, Ashis G. Banerjee, Krithika Manohar. (2024)<br><strong>Data-Driven Ergonomic Risk Assessment of Complex Hand-intensive Manufacturing Processes</strong><br><button class=copy-to-clipboard title="Data-Driven Ergonomic Risk Assessment of Complex Hand-intensive Manufacturing Processes" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-LG, cs.HC<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05591v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05591v1.pdf filename=2403.05591v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hand-intensive manufacturing processes, such as composite layup and textile draping, require significant human dexterity to accommodate task complexity. These strenuous hand motions often lead to musculoskeletal disorders and rehabilitation surgeries. We develop a data-driven ergonomic risk assessment system with a special focus on hand and finger activity to better identify and address ergonomic issues related to hand-intensive manufacturing processes. The system comprises a <b>multi-modal</b> sensor testbed to collect and synchronize operator upper body pose, hand pose and applied forces; a Biometric Assessment of Complete Hand (BACH) formulation to measure high-fidelity hand and finger risks; and industry-standard risk scores associated with upper body posture, RULA, and hand activity, HAL. Our findings demonstrate that BACH captures injurious activity with a higher granularity in comparison to the existing metrics. Machine learning models are also used to automate RULA and HAL scoring, and generalize well to unseen participants. Our assessment system, therefore, provides ergonomic interpretability of the manufacturing processes studied, and could be used to mitigate risks through minor workplace optimization and posture corrections.</p></p class="citation"></blockquote><h2 id=physicsao-ph-1>physics.ao-ph (1)</h2><h3 id=11--213290-fast-scale-adaptive-and-uncertainty-aware-downscaling-of-earth-system-model-fields-with-generative-foundation-models-philipp-hess-et-al-2024>(1/1 | 213/290) Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models (Philipp Hess et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Hess, Michael Aich, Baoxiang Pan, Niklas Boers. (2024)<br><strong>Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models</strong><br><button class=copy-to-clipboard title="Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.ao-ph<br>Categories: cs-CV, cs-LG, physics-ao-ph, physics-geo-ph, physics.ao-ph<br>Keyword Score: 50<br>Keywords: Diffusion Model, Foundation Model, Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02774v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02774v1.pdf filename=2403.02774v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate and high-resolution Earth system model (ESM) <b>simulations</b> are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive. Recent machine learning approaches have shown promising results in downscaling ESM <b>simulations,</b> outperforming state-of-the-art statistical approaches. However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training. We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM <b>simulations</b> without retraining in a <b>zero-shot</b> manner. Our <b>foundation</b> <b>model</b> approach yields probabilistic downscaled fields at resolution only limited by the observational reference data. We show that the CM outperforms state-of-the-art <b>diffusion</b> <b>models</b> at a fraction of computational cost while maintaining high controllability on the downscaling task. Further, our method generalizes to climate states unseen during training without explicitly formulated physical constraints.</p></p class="citation"></blockquote><h2 id=cscr-9>cs.CR (9)</h2><h3 id=19--214290-here-comes-the-ai-worm-unleashing-zero-click-worms-that-target-genai-powered-applications-stav-cohen-et-al-2024>(1/9 | 214/290) Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications (Stav Cohen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stav Cohen, Ron Bitton, Ben Nassi. (2024)<br><strong>Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications</strong><br><button class=copy-to-clipboard title="Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 45<br>Keywords: Black Box, Generative AI, ChatGPT, Gemini, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02817v1.pdf filename=2403.02817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the past year, numerous companies have incorporated <b>Generative</b> <b>AI</b> (GenAI) capabilities into new and existing applications, forming interconnected <b>Generative</b> <b>AI</b> (GenAI) ecosystems consisting of semi/fully autonomous agents powered by GenAI services. While ongoing research highlighted risks associated with the GenAI layer of agents (e.g., dialog poisoning, membership inference, <b>prompt</b> leaking, jailbreaking), a critical question emerges: Can attackers develop malware to exploit the GenAI component of an agent and launch cyber-attacks on the entire GenAI ecosystem? This paper introduces Morris II, the first worm designed to target GenAI ecosystems through the use of adversarial self-replicating <b>prompts.</b> The study demonstrates that attackers can insert such <b>prompts</b> into inputs that, when processed by GenAI models, <b>prompt</b> the model to replicate the input as output (replication), engaging in malicious activities (payload). Additionally, these inputs compel the agent to deliver them (propagate) to new agents by exploiting the connectivity within the GenAI ecosystem. We demonstrate the application of Morris II against GenAIpowered email assistants in two use cases (spamming and exfiltrating personal data), under two settings <b>(black-box</b> <b>and</b> white-box accesses), using two types of input data (text and images). The worm is tested against three different GenAI models <b>(Gemini</b> Pro, <b>ChatGPT</b> 4.0, and LLaVA), and various factors (e.g., propagation rate, replication, malicious activity) influencing the performance of the worm are evaluated.</p></p class="citation"></blockquote><h3 id=29--215290-enhancing-security-in-federated-learning-through-adaptive-consensus-based-model-update-validation-zahir-alsulaimawi-2024>(2/9 | 215/290) Enhancing Security in Federated Learning through Adaptive Consensus-Based Model Update Validation (Zahir Alsulaimawi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zahir Alsulaimawi. (2024)<br><strong>Enhancing Security in Federated Learning through Adaptive Consensus-Based Model Update Validation</strong><br><button class=copy-to-clipboard title="Enhancing Security in Federated Learning through Adaptive Consensus-Based Model Update Validation" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-DC, cs-LG, cs.CR<br>Keyword Score: 33<br>Keywords: MNIST, Anomaly Detection, Benchmarking, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04803v1.pdf filename=2403.04803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces an advanced approach for fortifying <b>Federated</b> <b>Learning</b> (FL) systems against label-flipping attacks. We propose a simplified consensus-based verification process integrated with an adaptive thresholding mechanism. This dynamic thresholding is designed to adjust based on the evolving landscape of model updates, offering a refined layer of <b>anomaly</b> <b>detection</b> that aligns with the real-time needs of distributed learning environments. Our method necessitates a majority consensus among participating clients to validate updates, ensuring that only vetted and consensual modifications are applied to the global model. The efficacy of our approach is validated through experiments on two <b>benchmark</b> datasets in deep learning, CIFAR-10 and <b>MNIST.</b> Our results indicate a significant mitigation of label-flipping attacks, bolstering the FL system&rsquo;s resilience. This method transcends conventional techniques that depend on <b>anomaly</b> <b>detection</b> or statistical validation by incorporating a verification layer reminiscent of blockchain&rsquo;s participatory validation without the associated cryptographic overhead. The innovation of our approach rests in striking an optimal balance between heightened security measures and the inherent limitations of FL systems, such as computational efficiency and data privacy. Implementing a consensus mechanism specifically tailored for FL environments paves the way for more secure, robust, and trustworthy distributed machine learning applications, where safeguarding data integrity and model robustness is critical.</p></p class="citation"></blockquote><h3 id=39--216290-federated-learning-under-attack-exposing-vulnerabilities-through-data-poisoning-attacks-in-computer-networks-ehsan-nowroozi-et-al-2024>(3/9 | 216/290) Federated Learning Under Attack: Exposing Vulnerabilities through Data Poisoning Attacks in Computer Networks (Ehsan Nowroozi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehsan Nowroozi, Imran Haider, Rahim Taheri, Mauro Conti. (2024)<br><strong>Federated Learning Under Attack: Exposing Vulnerabilities through Data Poisoning Attacks in Computer Networks</strong><br><button class=copy-to-clipboard title="Federated Learning Under Attack: Exposing Vulnerabilities through Data Poisoning Attacks in Computer Networks" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-CY, cs-LG, cs-NI, cs.CR<br>Keyword Score: 20<br>Keywords: Federated Learning, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02983v1.pdf filename=2403.02983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) is a machine learning (ML) approach that enables multiple decentralized devices or edge servers to collaboratively train a shared model without exchanging raw data. During the training and sharing of model updates between clients and servers, data and models are susceptible to different data-poisoning attacks. In this study, our motivation is to explore the severity of data poisoning attacks in the computer network domain because they are easy to implement but difficult to detect. We considered two types of data-poisoning attacks, label flipping (LF) and feature poisoning (FP), and applied them with a novel approach. In LF, we randomly flipped the labels of benign data and trained the model on the manipulated data. For FP, we randomly manipulated the highly contributing features determined using the Random Forest algorithm. The datasets used in this experiment were CIC and UNSW related to computer networks. We generated adversarial samples using the two attacks mentioned above, which were applied to a small percentage of datasets. Subsequently, we trained and tested the accuracy of the model on adversarial datasets. We recorded the results for both benign and manipulated datasets and observed significant differences between the accuracy of the models on different datasets. From the experimental results, it is evident that the LF attack failed, whereas the FP attack showed effective results, which proved its significance in fooling a server. With a 1% LF attack on the CIC, the accuracy was approximately 0.0428 and the <b>ASR</b> was 0.9564; hence, the attack is easily detectable, while with a 1% FP attack, the accuracy and <b>ASR</b> were both approximately 0.9600, hence, FP attacks are difficult to detect. We repeated the experiment with different poisoning percentages.</p></p class="citation"></blockquote><h3 id=49--217290-blockchain-enhanced-uav-networks-for-post-disaster-communication-a-decentralized-flocking-approach-sana-hafeez-et-al-2024>(4/9 | 217/290) Blockchain-Enhanced UAV Networks for Post-Disaster Communication: A Decentralized Flocking Approach (Sana Hafeez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sana Hafeez, Runze Cheng, Lina Mohjazi, Yao Sun, Muhammad Ali Imran. (2024)<br><strong>Blockchain-Enhanced UAV Networks for Post-Disaster Communication: A Decentralized Flocking Approach</strong><br><button class=copy-to-clipboard title="Blockchain-Enhanced UAV Networks for Post-Disaster Communication: A Decentralized Flocking Approach" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SY, cs.CR, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04796v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04796v1.pdf filename=2403.04796v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unmanned Aerial Vehicles (UAVs) have significant potential for agile communication and relief coordination in post-disaster scenarios, particularly when ground infrastructure is compromised. However, efficiently coordinating and securing flocks of heterogeneous UAVs from different service providers poses significant challenges related to privacy, scalability, lightweight consensus protocols, and comprehensive cybersecurity mechanisms. This study introduces a robust blockchain-enabled framework designed to tackle these technical challenges through a combination of consensus protocols, smart contracts, and cryptographic techniques. First, we propose a consortium blockchain architecture that ensures secure and private multi-agency coordination by controlling access and safeguarding the privacy of sensitive data. Second, we develop an optimized hybrid consensus protocol that merges Delegated Proof of Stake and Practical Byzantine Fault Tolerance (DPOS-PBFT), aiming to achieve an effective balance between efficiency, security, and resilience against node failures. Finally, we introduce decentralized flocking algorithms that facilitate adaptable and autonomous operations among specialized UAV clusters, ensuring critical disaster relief functions under conditions of uncertain connectivity. Comprehensive <b>simulations</b> demonstrate the system achieved linear scaling of throughput up to 500 UAV nodes, with only a 50ms increase in latency from 10 to 500 nodes. The framework maintained high throughput and low latency despite spoofing, denial-of-service (DoS), and tampering attacks, showing strong cyber resilience. Communication latencies were kept under 10ms for diverse UAV operations through self-optimizing network intelligence, with median values around 2-3ms.</p></p class="citation"></blockquote><h3 id=59--218290-robust-federated-learning-mitigates-client-side-training-data-distribution-inference-attacks-yichang-xu-et-al-2024>(5/9 | 218/290) Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks (Yichang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichang Xu, Ming Yin, Minghong Fang, Neil Zhenqiang Gong. (2024)<br><strong>Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks</strong><br><button class=copy-to-clipboard title="Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DC, cs-LG, cs.CR<br>Keyword Score: 13<br>Keywords: Benchmarking, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03149v1.pdf filename=2403.03149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have revealed that <b>federated</b> <b>learning</b> (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim&rsquo;s data. While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack. In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks. In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives. A client&rsquo;s model update is considered malicious if it significantly deviates from the computed median update. We conduct a thorough evaluation of our proposed InferGuard on five <b>benchmark</b> datasets and perform a comparison with ten baseline methods. The results of our experiments indicate that our defense mechanism is highly effective in protecting against client-side training data distribution inference attacks, even against strong adaptive attacks. Furthermore, our method substantially outperforms the baseline methods in various practical FL scenarios.</p></p class="citation"></blockquote><h3 id=69--219290-towards-an-ai-enhanced-cyber-threat-intelligence-processing-pipeline-lampis-alevizos-et-al-2024>(6/9 | 219/290) Towards an AI-Enhanced Cyber Threat Intelligence Processing Pipeline (Lampis Alevizos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lampis Alevizos, Martijn Dekker. (2024)<br><strong>Towards an AI-Enhanced Cyber Threat Intelligence Processing Pipeline</strong><br><button class=copy-to-clipboard title="Towards an AI-Enhanced Cyber Threat Intelligence Processing Pipeline" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03265v1.pdf filename=2403.03265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cyber threats continue to evolve in complexity, thereby traditional Cyber Threat Intelligence (CTI) methods struggle to keep pace. AI offers a potential solution, automating and enhancing various tasks, from data ingestion to resilience verification. This paper explores the potential of integrating Artificial Intelligence (AI) into CTI. We provide a blueprint of an AI-enhanced CTI processing pipeline, and detail its components and functionalities. The pipeline highlights the collaboration of AI and human expertise, which is necessary to produce timely and high-fidelity cyber threat intelligence. We also explore the automated generation of mitigation <b>recommendations,</b> harnessing AI&rsquo;s capabilities to provide real-time, contextual, and predictive insights. However, the integration of AI into CTI is not without challenges. Thereby, we discuss ethical dilemmas, potential biases, and the imperative for transparency in AI-driven decisions. We address the need for data privacy, consent mechanisms, and the potential misuse of technology. Moreover, we highlights the importance of addressing biases both during CTI analysis and AI models warranting their transparency and interpretability. Lastly, our work points out future research directions such as the exploration of advanced AI models to augment cyber defences, and the human-AI collaboration optimization. Ultimately, the fusion of AI with CTI appears to hold significant potential in cybersecurity domain.</p></p class="citation"></blockquote><h3 id=79--220290-mitigating-label-flipping-attacks-in-malicious-url-detectors-using-ensemble-trees-ehsan-nowroozi-et-al-2024>(7/9 | 220/290) Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees (Ehsan Nowroozi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehsan Nowroozi, Nada Jadalla, Samaneh Ghelichkhani, Alireza Jolfaei. (2024)<br><strong>Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees</strong><br><button class=copy-to-clipboard title="Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-CY, cs-LG, cs-NI, cs.CR<br>Keyword Score: 10<br>Keywords: Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02995v1.pdf filename=2403.02995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Malicious URLs provide adversarial opportunities across various industries, including transportation, healthcare, energy, and banking which could be detrimental to business operations. Consequently, the detection of these URLs is of crucial importance; however, current Machine Learning (ML) models are susceptible to backdoor attacks. These attacks involve manipulating a small percentage of training data labels, such as Label Flipping (LF), which changes benign labels to malicious ones and vice versa. This manipulation results in misclassification and leads to incorrect model behavior. Therefore, integrating defense mechanisms into the architecture of ML models becomes an imperative consideration to fortify against potential attacks. The focus of this study is on backdoor attacks in the context of URL detection using ensemble trees. By illuminating the motivations behind such attacks, highlighting the roles of attackers, and emphasizing the critical importance of effective defense strategies, this paper contributes to the ongoing efforts to fortify ML models against adversarial threats within the ML domain in network security. We propose an innovative alarm system that detects the presence of poisoned labels and a defense mechanism designed to uncover the original class labels with the aim of mitigating backdoor attacks on ensemble tree classifiers. We conducted a case study using the Alexa and Phishing Site URL datasets and showed that LF attacks can be addressed using our proposed defense mechanism. Our experimental results prove that the LF attack achieved an Attack Success Rate <b>(ASR)</b> between 50-65% within 2-5%, and the innovative defense method successfully detected poisoned labels with an accuracy of up to 100%.</p></p class="citation"></blockquote><h3 id=89--221290-xai-based-detection-of-adversarial-attacks-on-deepfake-detectors-ben-pinhasov-et-al-2024>(8/9 | 221/290) XAI-Based Detection of Adversarial Attacks on Deepfake Detectors (Ben Pinhasov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ben Pinhasov, Raz Lapid, Rony Ohayon, Moshe Sipper, Yehudit Aperstein. (2024)<br><strong>XAI-Based Detection of Adversarial Attacks on Deepfake Detectors</strong><br><button class=copy-to-clipboard title="XAI-Based Detection of Adversarial Attacks on Deepfake Detectors" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs.CR<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02955v1.pdf filename=2403.02955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel methodology for identifying <b>adversarial</b> <b>attacks</b> on deepfake detectors using eXplainable Artificial Intelligence (XAI). In an era characterized by digital advancement, deepfakes have emerged as a potent tool, creating a demand for efficient detection systems. However, these systems are frequently targeted by <b>adversarial</b> <b>attacks</b> that inhibit their performance. We address this gap, developing a defensible deepfake detector by leveraging the power of XAI. The proposed methodology uses XAI to generate interpretability maps for a given method, providing explicit visualizations of decision-making factors within the AI models. We subsequently employ a pretrained feature extractor that processes both the input image and its corresponding XAI image. The feature embeddings extracted from this process are then used for training a simple yet effective classifier. Our approach contributes not only to the detection of deepfakes but also enhances the understanding of possible <b>adversarial</b> <b>attacks,</b> pinpointing potential vulnerabilities. Furthermore, this approach does not change the performance of the deepfake detector. The paper demonstrates promising results suggesting a potential pathway for future deepfake detection mechanisms. We believe this study will serve as a valuable contribution to the community, sparking much-needed discourse on safeguarding deepfake detectors.</p></p class="citation"></blockquote><h3 id=99--222290-self-adaptive-traffic-anomaly-detection-system-for-iot-smart-home-environments-naoto-watanabe-et-al-2024>(9/9 | 222/290) Self-adaptive Traffic Anomaly Detection System for IoT Smart Home Environments (Naoto Watanabe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naoto Watanabe, Taku Yamazaki, Takumi Miyoshi, Ryo Yamamoto, Masataka Nakahara, Norihiro Okui, Ayumu Kubota. (2024)<br><strong>Self-adaptive Traffic Anomaly Detection System for IoT Smart Home Environments</strong><br><button class=copy-to-clipboard title="Self-adaptive Traffic Anomaly Detection System for IoT Smart Home Environments" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02744v1.pdf filename=2403.02744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the growth of internet of things (IoT) devices, cyberattacks, such as distributed denial of service, that exploit vulnerable devices infected with malware have increased. Therefore, vendors and users must keep their device firmware updated to eliminate vulnerabilities and quickly handle unknown cyberattacks. However, it is difficult for both vendors and users to continually keep the devices safe because vendors must provide updates quickly and the users must continuously manage the conditions of all deployed devices. Therefore, to ensure security, it is necessary for a system to adapt autonomously to changes in cyberattacks. In addition, it is important to consider network-side security that detects and filters anomalous traffic at the gateway to comprehensively protect those devices. This paper proposes a self-adaptive <b>anomaly</b> <b>detection</b> system for IoT traffic, including unknown attacks. The proposed system comprises a honeypot server and a gateway. The honeypot server continuously captures traffic and adaptively generates an <b>anomaly</b> <b>detection</b> model using real-time captured traffic. Thereafter, the gateway uses the generated model to detect anomalous traffic. Thus, the proposed system can adapt to unknown attacks to reflect pattern changes in anomalous traffic based on real-time captured traffic. Three experiments were conducted to evaluate the proposed system: a virtual experiment using pre-captured traffic from various regions across the world, a demonstration experiment using real-time captured traffic, and a virtual experiment using a public dataset containing the traffic generated by malware. The experimental results indicate that a system adaptable in real time to evolving cyberattacks is a novel approach for ensuring the comprehensive security of IoT devices against both known and unknown attacks.</p></p class="citation"></blockquote><h2 id=cspl-2>cs.PL (2)</h2><h3 id=12--223290-mars-20-a-toolchain-for-modeling-analysis-verification-and-code-generation-of-cyber-physical-systems-bohua-zhan-et-al-2024>(1/2 | 223/290) Mars 2.0: A Toolchain for Modeling, Analysis, Verification and Code Generation of Cyber-Physical Systems (Bohua Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bohua Zhan, Xiong Xu, Qiang Gao, Zekun Ji, Xiangyu Jin, Shuling Wang, Naijun Zhan. (2024)<br><strong>Mars 2.0: A Toolchain for Modeling, Analysis, Verification and Code Generation of Cyber-Physical Systems</strong><br><button class=copy-to-clipboard title="Mars 2.0: A Toolchain for Modeling, Analysis, Verification and Code Generation of Cyber-Physical Systems" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 33<br>Keywords: Benchmarking, Simulation, Simulator, Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03035v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03035v1.pdf filename=2403.03035v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Mars 2.0 for modeling, analysis, verification and <b>code</b> <b>generation</b> of Cyber-Physical Systems. Mars 2.0 integrates Mars 1.0 with several important extensions and improvements, allowing the design of cyber-physical systems using the combination of AADL and Simulink/Stateflow, which provide a unified graphical framework for modeling the functionality, physicality and architecture of the system to be developed. For a safety-critical system, formal analysis and verification of its combined AADL and Simulink/Stateflow model can be conducted via the following steps. First, the toolchain automatically translates AADL and Simulink/Stateflow models into Hybrid CSP (HCSP), an extension of CSP for formally modeling hybrid systems. Second, the HCSP processes can be simulated using the HCSP simulator, and to complement incomplete <b>simulation,</b> they can be verified using the Hybrid Hoare Logic prover in Isabelle/HOL, as well as the more automated HHLPy prover. Finally, implementations in SystemC or C can be automatically generated from the verified HCSP processes. The transformation from AADL and Simulink/Stateflow to HCSP, and the one from HCSP to SystemC or C, are both guaranteed to be correct with formal proofs. This approach allows model-driven design of safety-critical cyber-physical systems based on graphical and formal models and proven-correct translation procedures. We demonstrate the use of the toolchain on several <b>benchmarks</b> of varying complexity, including several industrial-sized examples.</p></p class="citation"></blockquote><h3 id=22--224290-verieql-bounded-equivalence-verification-for-complex-sql-queries-with-integrity-constraints-yang-he-et-al-2024>(2/2 | 224/290) VeriEQL: Bounded Equivalence Verification for Complex SQL Queries with Integrity Constraints (Yang He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang He, Pinhan Zhao, Xinyu Wang, Yuepeng Wang. (2024)<br><strong>VeriEQL: Bounded Equivalence Verification for Complex SQL Queries with Integrity Constraints</strong><br><button class=copy-to-clipboard title="VeriEQL: Bounded Equivalence Verification for Complex SQL Queries with Integrity Constraints" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-DB, cs-PL, cs.PL<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03193v1.pdf filename=2403.03193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of SQL query equivalence checking is important in various real-world applications (including query rewriting and automated grading) that involve complex queries with integrity constraints; yet, state-of-the-art techniques are very limited in their capability of <b>reasoning</b> about complex features (e.g., those that involve sorting, case statement, rich integrity constraints, etc.) in real-life queries. To the best of our knowledge, we propose the first SMT-based approach and its implementation, VeriEQL, capable of proving and disproving bounded equivalence of complex SQL queries. VeriEQL is based on a new logical encoding that models query semantics over symbolic tuples using the theory of integers with uninterpreted functions. It is simple yet highly practical &ndash; our comprehensive evaluation on over 20,000 <b>benchmarks</b> shows that VeriEQL outperforms all state-of-the-art techniques by more than one order of magnitude in terms of the number of <b>benchmarks</b> that can be proved or disproved. VeriEQL can also generate counterexamples that facilitate many downstream tasks (such as finding serious bugs in systems like MySQL and Apache Calcite).</p></p class="citation"></blockquote><h2 id=csit-5>cs.IT (5)</h2><h3 id=15--225290-tensor-decomposition-based-time-varying-channel-estimation-for-mmwave-mimo-ofdm-systems-ruizhe-wang-et-al-2024>(1/5 | 225/290) Tensor Decomposition-based Time Varying Channel Estimation for mmWave MIMO-OFDM Systems (Ruizhe Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruizhe Wang, Hong Ren, Cunhua Pan, Gui Zhou, Jiangzhou Wang. (2024)<br><strong>Tensor Decomposition-based Time Varying Channel Estimation for mmWave MIMO-OFDM Systems</strong><br><button class=copy-to-clipboard title="Tensor Decomposition-based Time Varying Channel Estimation for mmWave MIMO-OFDM Systems" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 33<br>Keywords: Benchmarking, Simulation, Simulator, Tensor Decomposition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02942v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02942v1.pdf filename=2403.02942v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider the time-varying channel estimation in millimeter wave (mmWave) multiple-input multiple-output MIMO systems with hybrid beamforming architectures. Different from the existing contributions that considered single-carrier mmWave systems with high mobility, the wideband orthogonal frequency division multiplexing (OFDM) system is considered in this work. To solve the channel estimation problem under channel double selectivity, we propose a pilot transmission scheme based on 5G OFDM, and the received signals are formed as a fourth-order <b>tensor,</b> <b>which</b> fits the low-rank CANDECOMP/PARAFAC (CP) model. By further exploring the Vandermonde structure of factor matrix, a <b>tensor-subspace</b> <b>decomposition</b> based channel estimation method is proposed to solve the CP decomposition, where the uniqueness condition is analyzed. Based on the decomposed factor matrices, the channel parameters, including angles of arrival/departure, delays, channel gains and Doppler shifts are estimated, and the Cram'{e}r-Rao bound (CRB) results are derived as performance metrics. <b>Simulation</b> results demonstrate the superior performance of the proposed method over other <b>benchmarks.</b> Furthermore, the channel estimation methods are tested based on the channel parameters generated by Wireless InSites, and <b>simulation</b> results show the effectiveness of the proposed method in practical scenarios.</p></p class="citation"></blockquote><h3 id=25--226290-scalable-syndrome-based-neural-decoders-for-bit-interleaved-coded-modulations-gastón-de-boni-rovella-et-al-2024>(2/5 | 226/290) Scalable Syndrome-based Neural Decoders for Bit-Interleaved Coded Modulations (Gastón De Boni Rovella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gastón De Boni Rovella, Meryem Benammar, Tarik Benaddi, Hugo Meric. (2024)<br><strong>Scalable Syndrome-based Neural Decoders for Bit-Interleaved Coded Modulations</strong><br><button class=copy-to-clipboard title="Scalable Syndrome-based Neural Decoders for Bit-Interleaved Coded Modulations" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 30<br>Keywords: Recurrent Neural Network, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02850v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02850v1.pdf filename=2403.02850v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we introduce a framework that enables the use of Syndrome-Based Neural Decoders (SBND) for high-order Bit-Interleaved Coded Modulations (BICM). To this end, we extend the previous results on SBND, for which the validity is limited to Binary Phase-Shift Keying (BPSK), by means of a theoretical channel modeling of the bit Log-Likelihood Ratio (bit-LLR) induced outputs. We implement the proposed SBND system for two polar codes $(64,32)$ and $(128,64)$, using a <b>Recurrent</b> <b>Neural</b> <b>Network</b> <b>(RNN)</b> and a <b>Transformer-based</b> architecture. Both implementations are compared in Bit Error Rate (BER) performance and computational complexity.</p></p class="citation"></blockquote><h3 id=35--227290-spatially-non-stationary-xl-mimo-channel-estimation-a-three-layer-generalized-approximate-message-passing-method-anzheng-tang-et-al-2024>(3/5 | 227/290) Spatially Non-Stationary XL-MIMO Channel Estimation: A Three-Layer Generalized Approximate Message Passing Method (Anzheng Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anzheng Tang, Jun-Bo Wang, Yijin Pan, Wence Zhang, Xiaodan Zhang, Yijian Chen, Hongkang Yu, Rodrigo C. de Lamare. (2024)<br><strong>Spatially Non-Stationary XL-MIMO Channel Estimation: A Three-Layer Generalized Approximate Message Passing Method</strong><br><button class=copy-to-clipboard title="Spatially Non-Stationary XL-MIMO Channel Estimation: A Three-Layer Generalized Approximate Message Passing Method" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Message-Passing, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02633v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02633v1.pdf filename=2403.02633v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, channel estimation problem for extremely large-scale multi-input multi-output (XL-MIMO) systems is investigated with the considerations of the spherical wavefront effect and the spatially non-stationary (SnS) property. Due to the diversities of SnS characteristics among different propagation paths, the concurrent channel estimation of multiple paths becomes intractable. To address this challenge, we propose a two-phase channel estimation scheme. In the first phase, the angles of departure (AoDs) on the user side are estimated, and a carefully designed pilot transmission scheme enables the decomposition of the received signal from different paths. In the second phase, the subchannel estimation corresponding to different paths is formulated as a three-layer Bayesian inference problem. Specifically, the first layer captures block sparsity in the angular domain, the second layer promotes SnS property in the antenna domain, and the third layer decouples the subchannels from the observed signals. To efficiently facilitate Bayesian inference, we propose a novel three-layer generalized approximate message passing (TL-GAMP) algorithm based on structured variational massage passing and belief propagation rules. <b>Simulation</b> results validate the convergence and effectiveness of the proposed algorithm, showcasing its robustness to different channel scenarios.</p></p class="citation"></blockquote><h3 id=45--228290-low-complexity-channel-estimation-for-ris-assisted-thz-systems-with-beam-split-xin-su-et-al-2024>(4/5 | 228/290) Low Complexity Channel Estimation for RIS-Assisted THz Systems with Beam Split (Xin Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Su, Ruisi He, Peng Zhang, Bo Ai. (2024)<br><strong>Low Complexity Channel Estimation for RIS-Assisted THz Systems with Beam Split</strong><br><button class=copy-to-clipboard title="Low Complexity Channel Estimation for RIS-Assisted THz Systems with Beam Split" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03015v1.pdf filename=2403.03015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To support extremely high data rates, reconfigurable intelligent surface (RIS)-assisted terahertz (THz) communication is considered to be a promising technology for future sixth-generation networks. However, due to the typical employment of hybrid beamforming architecture in THz systems, as well as the passive nature of RIS which lacks the capability to process pilot signals, obtaining channel state information (CSI) is facing significant challenges. To accurately estimate the cascaded channel, we propose a novel low-complexity channel estimation scheme, which includes three steps. Specifically, we first estimate full CSI within a small subset of subcarriers (SCs). Then, we acquire angular information at base station and RIS based on the full CSI. Finally, we derive spatial directions and recover full-CSI for the remaining SCs. Theoretical analysis and <b>simulation</b> results demonstrate that the proposed scheme can achieve superior performance in terms of normalized mean-square-error and exhibit a lower computational complexity compared with the existing algorithms.</p></p class="citation"></blockquote><h3 id=55--229290-linear-codes-for-hyperdimensional-computing-netanel-raviv-2024>(5/5 | 229/290) Linear Codes for Hyperdimensional Computing (Netanel Raviv, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Netanel Raviv. (2024)<br><strong>Linear Codes for Hyperdimensional Computing</strong><br><button class=copy-to-clipboard title="Linear Codes for Hyperdimensional Computing" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-NE, cs.IT, math-IT<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03278v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03278v1.pdf filename=2403.03278v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyperdimensional Computing (HDC) is an emerging computational paradigm for representing compositional information as high-dimensional vectors, and has a promising potential in applications ranging from machine learning to neuromorphic computing. One of the long-standing challenges in HDC is factoring a compositional representation to its constituent factors, also known as the recovery problem. In this paper we take a novel approach to solve the recovery problem, and propose the use of random linear codes. These codes are subspaces over the Boolean field, and are a well-studied topic in information theory with various applications in digital communication. We begin by showing that hyperdimensional encoding using random linear codes retains favorable properties of the prevalent (ordinary) random codes, and hence HD representations using the two methods have comparable information storage capabilities. We proceed to show that random linear codes offer a rich subcode structure that can be used to form key-value stores, which encapsulate most use cases of HDC. Most importantly, we show that under the framework we develop, random linear codes admit simple recovery algorithms to factor (either bundled or bound) compositional representations. The former relies on constructing certain linear equation systems over the Boolean field, the solution to which reduces the search space dramatically and strictly outperforms exhaustive search in many cases. The latter employs the subspace structure of these codes to achieve provably correct factorization. Both methods are strictly faster than the state-of-the-art resonator networks, often by an order of magnitude. We implemented our techniques in Python using a <b>benchmark</b> software library, and demonstrated promising experimental results.</p></p class="citation"></blockquote><h2 id=q-bioqm-2>q-bio.QM (2)</h2><h3 id=12--230290-from-noise-to-signal-unveiling-treatment-effects-from-digital-health-data-through-pharmacology-informed-neural-sde-samira-pakravan-et-al-2024>(1/2 | 230/290) From Noise to Signal: Unveiling Treatment Effects from Digital Health Data through Pharmacology-Informed Neural-SDE (Samira Pakravan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samira Pakravan, Nikolaos Evangelou, Maxime Usdin, Logan Brooks, James Lu. (2024)<br><strong>From Noise to Signal: Unveiling Treatment Effects from Digital Health Data through Pharmacology-Informed Neural-SDE</strong><br><button class=copy-to-clipboard title="From Noise to Signal: Unveiling Treatment Effects from Digital Health Data through Pharmacology-Informed Neural-SDE" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: I-2; G-3, cs-AI, cs-LG, math-DS, q-bio-QM, q-bio.QM<br>Keyword Score: 30<br>Keywords: Counter-factual, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03274v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03274v1.pdf filename=2403.03274v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Digital health technologies (DHT), such as wearable devices, provide personalized, continuous, and real-time monitoring of patient. These technologies are contributing to the development of novel therapies and personalized medicine. Gaining insight from these technologies requires appropriate modeling techniques to capture clinically-relevant changes in disease state. The data generated from these devices is characterized by being stochastic in nature, may have missing elements, and exhibits considerable inter-individual variability - thereby making it difficult to analyze using traditional longitudinal modeling techniques. We present a novel pharmacology-informed neural stochastic differential equation (SDE) model capable of addressing these challenges. Using synthetic data, we demonstrate that our approach is effective in identifying treatment effects and learning causal relationships from stochastic data, thereby enabling <b>counterfactual</b> <b>simulation.</b></p></p class="citation"></blockquote><h3 id=22--231290-vqsynery-robust-drug-synergy-prediction-with-vector-quantization-mechanism-jiawei-wu-et-al-2024>(2/2 | 231/290) VQSynery: Robust Drug Synergy Prediction With Vector Quantization Mechanism (Jiawei Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Wu, Mingyuan Yan, Dianbo Liu. (2024)<br><strong>VQSynery: Robust Drug Synergy Prediction With Vector Quantization Mechanism</strong><br><button class=copy-to-clipboard title="VQSynery: Robust Drug Synergy Prediction With Vector Quantization Mechanism" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-AI, cs-LG, q-bio-QM, q-bio.QM<br>Keyword Score: 20<br>Keywords: Graph Attention Networks, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03089v1.pdf filename=2403.03089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pursuit of optimizing cancer therapies is significantly advanced by the accurate prediction of drug synergy. Traditional methods, such as clinical trials, are reliable yet encumbered by extensive time and financial demands. The emergence of high-throughput screening and computational innovations has heralded a shift towards more efficient methodologies for exploring drug interactions. In this study, we present VQSynergy, a novel framework that employs the Vector <b>Quantization</b> (VQ) mechanism, integrated with <b>gated</b> residuals and a tailored attention mechanism, to enhance the precision and generalizability of drug synergy predictions. Our findings demonstrate that VQSynergy surpasses existing models in terms of robustness, particularly under Gaussian noise conditions, highlighting its superior performance and utility in the complex and often noisy domain of drug synergy research. This study underscores the potential of VQSynergy in revolutionizing the field through its advanced predictive capabilities, thereby contributing to the optimization of cancer treatment strategies.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--232290-mem-elements-based-neuromorphic-hardware-for-neural-network-application-ankur-singh-2024>(1/2 | 232/290) Mem-elements based Neuromorphic Hardware for Neural Network Application (Ankur Singh, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ankur Singh. (2024)<br><strong>Mem-elements based Neuromorphic Hardware for Neural Network Application</strong><br><button class=copy-to-clipboard title="Mem-elements based Neuromorphic Hardware for Neural Network Application" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-ET, cs-NE, cs.NE<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03002v1.pdf filename=2403.03002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The thesis investigates the utilization of memristive and memcapacitive crossbar arrays in low-power machine learning accelerators, offering a comprehensive co-design framework for deep neural networks (DNN). The model, implemented through a hybrid Python and PyTorch approach, accounts for various non-idealities, achieving exceptional training accuracies of 90.02% and 91.03% for the CIFAR-10 dataset with memristive and memcapacitive crossbar arrays on an 8-layer VGG network. Additionally, the thesis introduces a novel approach to emulate meminductor devices using Operational Transconductance Amplifiers (OTA) and capacitors, showcasing adjustable behavior. Transistor-level <b>simulations</b> in 180 nm CMOS technology, operating at 60 MHz, demonstrate the proposed meminductor emulator&rsquo;s viability with a power consumption of 0.337 mW. The design is further validated in neuromorphic circuits and <b>CNN</b> accelerators, achieving training and testing accuracies of 91.04% and 88.82%, respectively. Notably, the exclusive use of MOS transistors ensures the feasibility of monolithic IC fabrication. This research significantly contributes to the exploration of advanced hardware solutions for efficient and high-performance machine-learning applications.</p></p class="citation"></blockquote><h3 id=22--233290-g-evonas-evolutionary-neural-architecture-search-based-on-network-growth-juan-zou-et-al-2024>(2/2 | 233/290) G-EvoNAS: Evolutionary Neural Architecture Search Based on Network Growth (Juan Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan Zou, Weiwei Jiang, Yizhang Xia, Yuan Liu, Zhanglu Hou. (2024)<br><strong>G-EvoNAS: Evolutionary Neural Architecture Search Based on Network Growth</strong><br><button class=copy-to-clipboard title="G-EvoNAS: Evolutionary Neural Architecture Search Based on Network Growth" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 13<br>Keywords: Benchmarking, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02667v1.pdf filename=2403.02667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolutionary paradigm has been successfully applied to neural network search(NAS) in recent years. Due to the vast search complexity of the global space, current research mainly seeks to repeatedly stack partial architectures to build the entire model or to seek the entire model based on manually designed <b>benchmark</b> modules. The above two methods are attempts to reduce the search difficulty by narrowing the search space. To efficiently search network architecture in the global space, this paper proposes another solution, namely a computationally efficient neural architecture evolutionary search framework based on network growth (G-EvoNAS). The complete network is obtained by gradually deepening different Blocks. The process begins from a shallow network, grows and evolves, and gradually deepens into a complete network, reducing the search complexity in the global space. Then, to improve the ranking accuracy of the network, we reduce the weight coupling of each network in the SuperNet by <b>pruning</b> the SuperNet according to elite groups at different growth stages. The G-EvoNAS is tested on three commonly used image classification datasets, CIFAR10, CIFAR100, and ImageNet, and compared with various state-of-the-art algorithms, including hand-designed networks and NAS networks. Experimental results demonstrate that G-EvoNAS can find a neural network architecture comparable to state-of-the-art designs in 0.2 GPU days.</p></p class="citation"></blockquote><h2 id=csdl-2>cs.DL (2)</h2><h3 id=12--234290-paperweaver-enriching-topical-paper-alerts-by-contextualizing-recommended-papers-with-user-collected-papers-yoonjoo-lee-et-al-2024>(1/2 | 234/290) PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers (Yoonjoo Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoonjoo Lee, Hyeonsu B. Kang, Matt Latzke, Juho Kim, Jonathan Bragg, Joseph Chee Chang, Pao Siangliulue. (2024)<br><strong>PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers</strong><br><button class=copy-to-clipboard title="PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-AI, cs-CL, cs-DL, cs-HC, cs.DL<br>Keyword Score: 30<br>Keywords: Recommendation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02939v1.pdf filename=2403.02939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid growth of scholarly archives, researchers subscribe to &ldquo;paper alert&rdquo; systems that periodically provide them with <b>recommendations</b> of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to infer users&rsquo; research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers.</p></p class="citation"></blockquote><h3 id=22--235290-acemap-knowledge-discovery-through-academic-graph-xinbing-wang-et-al-2024>(2/2 | 235/290) AceMap: Knowledge Discovery through Academic Graph (Xinbing Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinbing Wang, Luoyi Fu, Xiaoying Gan, Ying Wen, Guanjie Zheng, Jiaxin Ding, Liyao Xiang, Nanyang Ye, Meng Jin, Shiyu Liang, Bin Lu, Haiwen Wang, Yi Xu, Cheng Deng, Shao Zhang, Huquan Kang, Xingli Wang, Qi Li, Zhixin Guo, Jiexing Qi, Pan Liu, Yuyang Ren, Lyuwen Wu, Jungang Yang, Jianping Zhou, Chenghu Zhou. (2024)<br><strong>AceMap: Knowledge Discovery through Academic Graph</strong><br><button class=copy-to-clipboard title="AceMap: Knowledge Discovery through Academic Graph" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-DL, cs-LG, cs-SI, cs.DL<br>Keyword Score: 18<br>Keywords: Graph, Knowledge Graph, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02576v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02576v1.pdf filename=2403.02576v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The exponential growth of scientific literature requires effective management and extraction of valuable insights. While existing scientific search engines excel at delivering search results based on relational databases, they often neglect the analysis of collaborations between scientific entities and the evolution of ideas, as well as the in-depth analysis of content within scientific publications. The representation of heterogeneous <b>graphs</b> and the effective measurement, analysis, and mining of such <b>graphs</b> pose significant challenges. To address these challenges, we present AceMap, an academic system designed for <b>knowledge</b> <b>discovery</b> through academic <b>graph.</b> We present advanced database construction techniques to build the comprehensive AceMap database with <b>large-scale</b> <b>academic</b> <b>publications</b> that contain rich visual, textual, and numerical information. AceMap also employs innovative visualization, quantification, and analysis methods to explore associations and logical relationships among academic entities. AceMap introduces <b>large-scale</b> <b>academic</b> <b>network</b> visualization techniques centered on nebular <b>graphs,</b> providing a comprehensive view of academic networks from multiple perspectives. In addition, AceMap proposes a unified metric based on structural entropy to quantitatively measure the <b>knowledge</b> <b>content</b> of different academic entities. Moreover, AceMap provides advanced analysis capabilities, including tracing the evolution of academic ideas through citation relationships and concept co-occurrence, and generating concise summaries informed by this evolutionary process. In addition, AceMap uses machine reading methods to generate potential new ideas at the intersection of different fields. Exploring the integration of <b>large</b> <b>language</b> <b>models</b> and <b>knowledge</b> <b>graphs</b> is a promising direction for future research in idea evolution. Please visit \url{https://www.acemap.info} for further exploration.</p></p class="citation"></blockquote><h2 id=quant-ph-3>quant-ph (3)</h2><h3 id=13--236290-quantum-mixed-state-self-attention-network-fu-chen-et-al-2024>(1/3 | 236/290) Quantum Mixed-State Self-Attention Network (Fu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fu Chen, Qinglin Zhao, Li Feng, Chuangtao Chen, Yangbin Lin, Jianhong Lin. (2024)<br><strong>Quantum Mixed-State Self-Attention Network</strong><br><button class=copy-to-clipboard title="Quantum Mixed-State Self-Attention Network" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 30<br>Keywords: Information Retrieval, Text Classification, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02871v1.pdf filename=2403.02871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of quantum computing has increasingly highlighted its potential in the realm of machine learning, particularly in the context of natural language processing (NLP) tasks. Quantum machine learning (QML) leverages the unique capabilities of quantum computing to offer novel perspectives and methodologies for complex data processing and pattern recognition challenges. This paper introduces a novel Quantum Mixed-State Attention Network (QMSAN), which integrates the principles of quantum computing with classical machine learning algorithms, especially <b>self-attention</b> networks, to enhance the efficiency and effectiveness in handling NLP tasks. QMSAN model employs a quantum attention mechanism based on mixed states, enabling efficient direct estimation of similarity between queries and keys within the quantum domain, leading to more effective attention weight acquisition. Additionally, we propose an innovative quantum positional encoding scheme, implemented through fixed quantum gates within the quantum circuit, to enhance the model&rsquo;s accuracy. Experimental validation on various datasets demonstrates that QMSAN model outperforms existing quantum and classical models in <b>text</b> <b>classification,</b> achieving significant performance improvements. QMSAN model not only significantly reduces the number of parameters but also exceeds classical <b>self-attention</b> networks in performance, showcasing its strong capability in data representation and <b>information</b> <b>extraction.</b> Furthermore, our study investigates the model&rsquo;s robustness in different quantum noise environments, showing that QMSAN possesses commendable robustness to low noise.</p></p class="citation"></blockquote><h3 id=23--237290-graph-learning-for-parameter-prediction-of-quantum-approximate-optimization-algorithm-zhiding-liang-et-al-2024>(2/3 | 237/290) Graph Learning for Parameter Prediction of Quantum Approximate Optimization Algorithm (Zhiding Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiding Liang, Gang Liu, Zheyuan Liu, Jinglei Cheng, Tianyi Hao, Kecheng Liu, Hang Ren, Zhixin Song, Ji Liu, Fanny Ye, Yiyu Shi. (2024)<br><strong>Graph Learning for Parameter Prediction of Quantum Approximate Optimization Algorithm</strong><br><button class=copy-to-clipboard title="Graph Learning for Parameter Prediction of Quantum Approximate Optimization Algorithm" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03310v1.pdf filename=2403.03310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, quantum computing has emerged as a transformative force in the field of combinatorial optimization, offering novel approaches to tackling complex problems that have long challenged classical computational methods. Among these, the Quantum Approximate Optimization Algorithm (QAOA) stands out for its potential to efficiently solve the Max-Cut problem, a quintessential example of combinatorial optimization. However, practical application faces challenges due to current limitations on quantum computational resource. Our work optimizes QAOA initialization, using <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNN)</b> as a warm-start technique. This sacrifices affordable computational resource on classical computer to reduce quantum computational resource overhead, enhancing QAOA&rsquo;s effectiveness. Experiments with various <b>GNN</b> architectures demonstrate the adaptability and stability of our framework, highlighting the synergy between quantum algorithms and machine learning. Our findings show <b>GNN&rsquo;s</b> potential in improving QAOA performance, opening new avenues for hybrid quantum-classical approaches in quantum computing and contributing to practical applications.</p></p class="citation"></blockquote><h3 id=33--238290-quantum-algorithms-in-a-superposition-of-spacetimes-omri-shmueli-2024>(3/3 | 238/290) Quantum Algorithms in a Superposition of Spacetimes (Omri Shmueli, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omri Shmueli. (2024)<br><strong>Quantum Algorithms in a Superposition of Spacetimes</strong><br><button class=copy-to-clipboard title="Quantum Algorithms in a Superposition of Spacetimes" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CC, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02937v1.pdf filename=2403.02937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum computers are expected to revolutionize our ability to process information. The advancement from classical to quantum computing is a product of our advancement from classical to quantum physics &ndash; the more our understanding of the universe grows, so does our ability to use it for computation. A natural question that arises is, what will physics allow in the future? Can more advanced theories of physics increase our computational power, beyond quantum computing? An active field of research in physics studies theoretical phenomena outside the scope of explainable quantum mechanics, that form when attempting to combine Quantum Mechanics (QM) with General Relativity (GR) into a unified theory of Quantum Gravity (QG). QG is known to present the possibility of a quantum superposition of causal structure and event orderings. In the literature of quantum information theory, this translates to a superposition of unitary evolution orders. In this work we show a first example of a natural computational model based on QG, that provides an exponential speedup over standard quantum computation (under standard hardness assumptions). We define a model and complexity measure for a quantum computer that has the ability to generate a superposition of unitary evolution orders, and show that such computer is able to solve in polynomial time two of the fundamental problems in computer science: The <b>Graph</b> Isomorphism Problem ($\mathsf{GI}$) and the Gap Closest Vector Problem ($\mathsf{GapCVP}$), with gap $O\left( n^{2} \right)$. These problems are believed by experts to be hard to solve for a regular quantum computer. Interestingly, our model does not seem overpowered, and we found no obvious way to solve entire complexity classes that are considered hard in computer science, like the classes $\mathbf{NP}$ and $\mathbf{SZK}$.</p></p class="citation"></blockquote><h2 id=cset-1>cs.ET (1)</h2><h3 id=11--239290-spintronic-implementation-of-unet-for-image-segmentation-venkatesh-vadde-et-al-2024>(1/1 | 239/290) Spintronic Implementation of UNet for Image Segmentation (Venkatesh Vadde et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Venkatesh Vadde, Bhaskaran Muralidharan, Abhishek Sharma. (2024)<br><strong>Spintronic Implementation of UNet for Image Segmentation</strong><br><button class=copy-to-clipboard title="Spintronic Implementation of UNet for Image Segmentation" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cs-ET, cs.ET, eess-IV, physics-app-ph<br>Keyword Score: 30<br>Keywords: Convolution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02863v1.pdf filename=2403.02863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image segmentation plays a crucial role in computer vision applications like self-driving cars, satellite imagery analysis, and medical diagnosis. Implementing these complex deep neural networks on conventional hardware is highly inefficient. In this work, we propose hardware implementation of UNet for segmentation tasks, using spintronic devices. Our approach involves designing hardware for <b>convolution,</b> deconvolution, ReLU, and max pooling layers of the UNet architecture. We demonstrate the synaptic behavior of the domain wall MTJ, and design <b>convolution</b> and deconvolution layers using the domain wall-based crossbar array. We utilize the orthogonal current injected MTJ with its continuous resistance change and showcase the ReLU and max pooling functions. We employ a hybrid <b>simulation</b> setup by coupling micromagnetic <b>simulation,</b> non-equilibrium Green&rsquo;s function, Landau-Lifshitz-Gilbert-Slonczewski equations, and circuit <b>simulation</b> with Python programming to incorporate the diverse physics of spin-transport, magnetization dynamics, and CMOS elements in our proposed designs. We evaluate our UNet design on the CamVid dataset and achieve segmentation accuracies that are comparable to software implementation. During training, our design consumes 43.59pJ of energy for synaptic weight updates.</p></p class="citation"></blockquote><h2 id=csir-6>cs.IR (6)</h2><h3 id=16--240290-contrastive-pre-training-for-deep-session-data-understanding-zixuan-li-et-al-2024>(1/6 | 240/290) Contrastive Pre-training for Deep Session Data Understanding (Zixuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zixuan Li, Lizi Liao, Yunshan Ma, Tat-Seng Chua. (2024)<br><strong>Contrastive Pre-training for Deep Session Data Understanding</strong><br><button class=copy-to-clipboard title="Contrastive Pre-training for Deep Session Data Understanding" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Fine-tuning, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02825v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02825v1.pdf filename=2403.02825v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Session data has been widely used for understanding user&rsquo;s behavior in e-commerce. Researchers are trying to leverage session data for different tasks, such as purchase intention prediction, remaining length prediction, <b>recommendation,</b> etc., as it provides context clues about the user&rsquo;s dynamic interests. However, online shopping session data is semi-structured and complex in nature, which contains both unstructured textual data about the products, search queries, and structured user action sequences. Most existing works focus on leveraging the coarse-grained item sequences for specific tasks, while largely ignore the fine-grained information from text and user action details. In this work, we delve into deep session data understanding via scrutinizing the various clues inside the rich information in user sessions. Specifically, we propose to pre-train a general-purpose User Behavior Model (UBM) over large-scale session data with rich details, such as product title, attributes and various kinds of user actions. A two-stage pre-training scheme is introduced to encourage the model to self-learn from various augmentations with <b>contrastive</b> <b>learning</b> objectives, which spans different granularity levels of session data. Then the well-trained session understanding model can be easily <b>fine-tuned</b> for various downstream tasks. Extensive experiments show that UBM better captures the complex intra-item semantic relations, inter-item connections and inter-interaction dependencies, leading to large performance gains as compared to the baselines on several downstream tasks. And it also demonstrates strong robustness when data is sparse.</p></p class="citation"></blockquote><h3 id=26--241290-chatcite-llm-agent-with-human-workflow-guidance-for-comparative-literature-summary-yutong-li-et-al-2024>(2/6 | 241/290) ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary (Yutong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutong Li, Lu Chen, Aiwei Liu, Kai Yu, Lijie Wen. (2024)<br><strong>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary</strong><br><button class=copy-to-clipboard title="ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: 68T50, I-2-7, cs-AI, cs-CL, cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Automatic Evaluation, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02574v1.pdf filename=2403.02574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous <b>LLM-based</b> studies on literature review mainly focused on the complete process, including literature retrieval, screening, and <b>summarization.</b> However, for the <b>summarization</b> step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature <b>summarization</b> step and introduce ChatCite, an <b>LLM</b> agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a <b>LLM-based</b> <b>automatic</b> <b>evaluation</b> metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.</p></p class="citation"></blockquote><h3 id=36--242290-a-distance-metric-learning-model-based-on-variational-information-bottleneck-yaodan-zhang-et-al-2024>(3/6 | 242/290) A Distance Metric Learning Model Based On Variational Information Bottleneck (YaoDan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>YaoDan Zhang, Zidong Wang, Ru Jia, Ru Li. (2024)<br><strong>A Distance Metric Learning Model Based On Variational Information Bottleneck</strong><br><button class=copy-to-clipboard title="A Distance Metric Learning Model Based On Variational Information Bottleneck" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs-LG, cs.IR<br>Keyword Score: 20<br>Keywords: Mutual Information, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02794v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02794v1.pdf filename=2403.02794v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, personalized <b>recommendation</b> technology has flourished and become one of the hot research directions. The matrix factorization model and the metric learning model which proposed successively have been widely studied and applied. The latter uses the Euclidean distance instead of the dot product used by the former to measure the latent space vector. While avoiding the shortcomings of the dot product, the assumption of Euclidean distance is neglected, resulting in limited <b>recommendation</b> quality of the model. In order to solve this problem, this paper combines the Variationl Information Bottleneck with metric learning model for the first time, and proposes a new metric learning model VIB-DML (Variational Information Bottleneck Distance Metric Learning) for rating prediction, which limits the <b>mutual</b> <b>information</b> of the latent space feature vector to improve the robustness of the model and satisfiy the assumption of Euclidean distance by decoupling the latent space feature vector. In this paper, the experimental results are compared with the root mean square error (RMSE) on the three public datasets. The results show that the generalization ability of VIB-DML is excellent. Compared with the general metric learning model MetricF, the prediction error is reduced by 7.29%. Finally, the paper proves the strong robustness of VIBDML through experiments.</p></p class="citation"></blockquote><h3 id=46--243290-learning-to-ask-critical-questions-for-assisting-product-search-zixuan-li-et-al-2024>(4/6 | 243/290) Learning to Ask Critical Questions for Assisting Product Search (Zixuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zixuan Li, Lizi Liao, Tat-Seng Chua. (2024)<br><strong>Learning to Ask Critical Questions for Assisting Product Search</strong><br><button class=copy-to-clipboard title="Learning to Ask Critical Questions for Assisting Product Search" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02754v1.pdf filename=2403.02754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Product search plays an essential role in eCommerce. It was treated as a special type of <b>information</b> <b>retrieval</b> problem. Most existing works make use of historical data to improve the search performance, which do not take the opportunity to ask for user&rsquo;s current interest directly. Some session-aware methods take the user&rsquo;s clicks within the session as implicit feedback, but it is still just a guess on user&rsquo;s preference. To address this problem, recent conversational or question-based search models interact with users directly for understanding the user&rsquo;s interest explicitly. However, most users do not have a clear picture on what to buy at the initial stage. Asking critical attributes that the user is looking for after they explored for a while should be a more efficient way to help them searching for the target items. In this paper, we propose a dual-learning model that hybrids the best from both implicit session feedback and proactively clarifying with users on the most critical questions. We first establish a novel utility score to measure whether a clicked item provides useful <b>information</b> <b>for</b> finding the target. Then we develop the dual Selection Net and Ranking Net for choosing the critical questions and ranking the items. It innovatively links traditional click-stream data and text-based questions together. To verify our proposal, we did extensive experiments on a public dataset, and our model largely outperformed other state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=56--244290-uplift-modeling-for-target-user-attacks-on-recommender-systems-wenjie-wang-et-al-2024>(5/6 | 244/290) Uplift Modeling for Target User Attacks on Recommender Systems (Wenjie Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenjie Wang, Changsheng Wang, Fuli Feng, Wentao Shi, Daizong Ding, Tat-Seng Chua. (2024)<br><strong>Uplift Modeling for Target User Attacks on Recommender Systems</strong><br><button class=copy-to-clipboard title="Uplift Modeling for Target User Attacks on Recommender Systems" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02692v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02692v1.pdf filename=2403.02692v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommender</b> <b>systems</b> are vulnerable to injective attacks, which inject limited fake users into the platforms to manipulate the exposure of target items to all users. In this work, we identify that conventional injective attackers overlook the fact that each item has its unique potential audience, and meanwhile, the attack difficulty across different users varies. Blindly attacking all users will result in a waste of fake user budgets and inferior attack performance. To address these issues, we focus on an under-explored attack task called target user attacks, aiming at promoting target items to a particular user group. In addition, we formulate the varying attack difficulty as heterogeneous treatment effects through a causal lens and propose an Uplift-guided Budget Allocation (UBA) framework. UBA estimates the treatment effect on each target user and optimizes the allocation of fake user budgets to maximize the attack performance. Theoretical and empirical analysis demonstrates the rationality of treatment effect estimation methods of UBA. By instantiating UBA on multiple attackers, we conduct extensive experiments on three datasets under various settings with different target items, target users, fake user budgets, victim models, and defense models, validating the effectiveness and robustness of UBA.</p></p class="citation"></blockquote><h3 id=66--245290-search-intenion-network-for-personalized-query-auto-completion-in-e-commerce-wei-bao-et-al-2024>(6/6 | 245/290) Search Intenion Network for Personalized Query Auto-Completion in E-Commerce (Wei Bao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Bao, Mi Zhang, Tao Zhang, Chengfu Huo. (2024)<br><strong>Search Intenion Network for Personalized Query Auto-Completion in E-Commerce</strong><br><button class=copy-to-clipboard title="Search Intenion Network for Personalized Query Auto-Completion in E-Commerce" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02609v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02609v1.pdf filename=2403.02609v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Query Auto-Completion(QAC), as an important part of the modern search engine, plays a key role in complementing user queries and helping them refine their search intentions.Today&rsquo;s QAC systems in real-world scenarios face two major challenges:1)intention equivocality(IE): during the user&rsquo;s typing process,the prefix often contains a combination of characters and subwords, which makes the current intention ambiguous and difficult to model.2)intention transfer (IT):previous works make personalized <b>recommendations</b> based on users&rsquo; historical sequences, but ignore the search intention transfer.However, the current intention extracted from prefix may be contrary to the historical preferences.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=12--246290-federated-learning-using-coupled-tensor-train-decomposition-xiangtao-zhang-et-al-2024>(1/2 | 246/290) Federated Learning Using Coupled Tensor Train Decomposition (Xiangtao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangtao Zhang, Eleftherios Kofidis, Ce Zhu, Le Zhang, Yipeng Liu. (2024)<br><strong>Federated Learning Using Coupled Tensor Train Decomposition</strong><br><button class=copy-to-clipboard title="Federated Learning Using Coupled Tensor Train Decomposition" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 26<br>Keywords: Federated Learning, Multi-modal, Multi-modal, Tensor Decomposition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02898v1.pdf filename=2403.02898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Coupled <b>tensor</b> <b>decomposition</b> (CTD) can extract joint features from <b>multimodal</b> data in various applications. It can be employed for <b>federated</b> <b>learning</b> networks with data confidentiality. <b>Federated</b> <b>CTD</b> achieves data privacy protection by sharing common features and keeping individual features. However, traditional CTD schemes based on canonical polyadic decomposition (CPD) may suffer from low computational efficiency and heavy communication costs. Inspired by the efficient <b>tensor</b> <b>train</b> decomposition, we propose a coupled <b>tensor</b> <b>train</b> (CTT) decomposition for <b>federated</b> <b>learning.</b> The distributed coupled multi-way data are decomposed into a series of <b>tensor</b> <b>trains</b> with shared factors. In this way, we can extract common features of coupled modes while maintaining the different features of uncoupled modes. Thus the privacy preservation of information across different network nodes can be ensured. The proposed CTT approach is instantiated for two fundamental network structures, namely master-slave and decentralized networks. Experimental results on synthetic and real datasets demonstrate the superiority of the proposed schemes over existing methods in terms of both computational efficiency and communication rounds. In a classification task, experimental results show that the CTT-based <b>federated</b> <b>learning</b> achieves almost the same accuracy performance as that of the centralized counterpart.</p></p class="citation"></blockquote><h3 id=22--247290-distributed-openmp-offloading-of-openmc-on-intel-gpu-max-accelerators-yehonatan-fridman-et-al-2024>(2/2 | 247/290) Distributed OpenMP Offloading of OpenMC on Intel GPU MAX Accelerators (Yehonatan Fridman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yehonatan Fridman, Guy Tamir, Uri Steinitz, Gal Oren. (2024)<br><strong>Distributed OpenMP Offloading of OpenMC on Intel GPU MAX Accelerators</strong><br><button class=copy-to-clipboard title="Distributed OpenMP Offloading of OpenMC on Intel GPU MAX Accelerators" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02735v1.pdf filename=2403.02735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monte Carlo (MC) <b>simulations</b> play a pivotal role in diverse scientific and engineering domains, with applications ranging from nuclear physics to materials science. Harnessing the computational power of high-performance computing (HPC) systems, especially Graphics Processing Units (GPUs), has become essential for accelerating MC <b>simulations.</b> This paper focuses on the adaptation and optimization of the OpenMC neutron and photon transport Monte Carlo code for Intel GPUs, specifically the Intel Data Center Max 1100 GPU (codename Ponte Vecchio, PVC), through distributed OpenMP offloading. Building upon prior work by Tramm J.R., et al. (2022), which laid the groundwork for GPU adaptation, our study meticulously extends the OpenMC code&rsquo;s capabilities to Intel GPUs. We present a comprehensive <b>benchmarking</b> and scaling analysis, comparing performance on Intel MAX GPUs to state-of-the-art CPU execution (Intel Xeon Platinum 8480+ Processor, codename 4th generation Sapphire Rapids). The results demonstrate a remarkable acceleration factor compared to CPU execution, showcasing the GPU-adapted code&rsquo;s superiority over its CPU counterpart as computational load increases.</p></p class="citation"></blockquote><h2 id=cond-matdis-nn-1>cond-mat.dis-nn (1)</h2><h3 id=11--248290-geometric-dynamics-of-signal-propagation-predict-trainability-of-transformers-aditya-cowsik-et-al-2024>(1/1 | 248/290) Geometric Dynamics of Signal Propagation Predict Trainability of Transformers (Aditya Cowsik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aditya Cowsik, Tamra Nebabu, Xiao-Liang Qi, Surya Ganguli. (2024)<br><strong>Geometric Dynamics of Signal Propagation Predict Trainability of Transformers</strong><br><button class=copy-to-clipboard title="Geometric Dynamics of Signal Propagation Predict Trainability of Transformers" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.dis-nn<br>Categories: cond-mat-dis-nn, cond-mat.dis-nn, cs-LG<br>Keyword Score: 25<br>Keywords: Discrete Time, Discrete Time, Geometry, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02579v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02579v1.pdf filename=2403.02579v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate forward signal propagation and gradient back propagation in deep, randomly initialized <b>transformers,</b> yielding simple necessary and sufficient conditions on initialization hyperparameters that ensure trainability of deep <b>transformers.</b> Our approach treats the evolution of the representations of $n$ tokens as they propagate through the <b>transformer</b> layers in terms of a <b>discrete</b> <b>time</b> dynamical system of $n$ interacting particles. We derive simple update equations for the evolving <b>geometry</b> of this particle system, starting from a permutation symmetric simplex. Our update equations show that without MLP layers, this system will collapse to a line, consistent with prior work on rank collapse in <b>transformers.</b> However, unlike prior work, our evolution equations can quantitatively track particle <b>geometry</b> in the additional presence of nonlinear MLP layers, and it reveals an order-chaos phase transition as a function of initialization hyperparameters, like the strength of attentional and MLP residual connections and weight variances. In the ordered phase the particles are attractive and collapse to a line, while in the chaotic phase the particles are repulsive and converge to a regular $n$-simplex. We analytically derive two Lyapunov exponents: an angle exponent that governs departures from the edge of chaos in this particle system, and a gradient exponent that governs the rate of exponential growth or decay of backpropagated gradients. We show through experiments that, remarkably, the final test loss at the end of training is well predicted just by these two exponents at the beginning of training, and that the simultaneous vanishing of these two exponents yields a simple necessary and sufficient condition to achieve minimal test loss.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--249290-cormf-criticality-ordered-recurrent-mean-field-ising-solver-zhenyu-pan-et-al-2024>(1/3 | 249/290) CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver (Zhenyu Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyu Pan, Ammar Gilani, En-Jui Kuo, Zhuo Liu. (2024)<br><strong>CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver</strong><br><button class=copy-to-clipboard title="CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cond-mat-stat-mech, cs-LG, stat-ML, stat.ML<br>Keyword Score: 23<br>Keywords: Graph, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03391v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03391v2.pdf filename=2403.03391v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose an <b>RNN-based</b> efficient Ising model solver, the Criticality-ordered <b>Recurrent</b> <b>Mean</b> <b>Field</b> (CoRMF), for forward Ising problems. In its core, a criticality-ordered spin sequence of an $N$-spin Ising model is introduced by sorting mission-critical edges with greedy algorithm, such that an autoregressive mean-field factorization can be utilized and optimized with <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNNs).</b> Our method has two notable characteristics: (i) by leveraging the approximated tree structure of the underlying Ising <b>graph,</b> the newly-obtained criticality order enables the unification between variational mean-field and <b>RNN,</b> allowing the generally intractable Ising model to be efficiently probed with probabilistic inference; (ii) it is well-modulized, model-independent while at the same time expressive enough, and hence fully applicable to any forward Ising inference problems with minimal effort. Computationally, by using a variance-reduced Monte Carlo gradient estimator, CoRFM solves the Ising problems in a self-train fashion without data/evidence, and the inference tasks can be executed by directly sampling from <b>RNN.</b> Theoretically, we establish a provably tighter error bound than naive mean-field by using the matrix cut decomposition machineries. Numerically, we demonstrate the utility of this framework on a series of Ising datasets.</p></p class="citation"></blockquote><h3 id=23--250290-active-statistical-inference-tijana-zrnic-et-al-2024>(2/3 | 250/290) Active Statistical Inference (Tijana Zrnic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tijana Zrnic, Emmanuel J. Candès. (2024)<br><strong>Active Statistical Inference</strong><br><button class=copy-to-clipboard title="Active Statistical Inference" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 15<br>Keywords: Active Learning, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03208v1.pdf filename=2403.03208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inspired by the concept of <b>active</b> <b>learning,</b> we propose <b>active</b> <b>inference$\unicode{x2013}$a</b> methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model&rsquo;s predictions where it is confident. <b>Active</b> <b>inference</b> constructs provably valid confidence intervals and hypothesis tests while leveraging any <b>black-box</b> <b>machine</b> learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number of collected samples, <b>active</b> <b>inference</b> enables smaller confidence intervals and more powerful p-values. We evaluate <b>active</b> <b>inference</b> on datasets from public opinion research, census analysis, and proteomics.</p></p class="citation"></blockquote><h3 id=33--251290-chained-information-theoretic-bounds-and-tight-regret-rate-for-linear-bandit-problems-amaury-gouverneur-et-al-2024>(3/3 | 251/290) Chained Information-Theoretic bounds and Tight Regret Rate for Linear Bandit Problems (Amaury Gouverneur et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amaury Gouverneur, Borja Rodríguez-Gálvez, Tobias J. Oechtering, Mikael Skoglund. (2024)<br><strong>Chained Information-Theoretic bounds and Tight Regret Rate for Linear Bandit Problems</strong><br><button class=copy-to-clipboard title="Chained Information-Theoretic bounds and Tight Regret Rate for Linear Bandit Problems" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03361v1.pdf filename=2403.03361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies the Bayesian regret of a variant of the Thompson-Sampling algorithm for <b>bandit</b> problems. It builds upon the information-theoretic framework of [Russo and Van Roy, 2015] and, more specifically, on the rate-distortion analysis from [Dong and Van Roy, 2020], where they proved a bound with regret rate of $O(d\sqrt{T \log(T)})$ for the $d$-dimensional linear <b>bandit</b> setting. We focus on <b>bandit</b> problems with a metric action space and, using a chaining argument, we establish new bounds that depend on the metric entropy of the action space for a variant of Thompson-Sampling. Under suitable continuity assumption of the rewards, our bound offers a tight rate of $O(d\sqrt{T})$ for $d$-dimensional linear <b>bandit</b> problems.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--252290-distributed-policy-gradient-for-linear-quadratic-networked-control-with-limited-communication-range-yuzi-yan-et-al-2024>(1/1 | 252/290) Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range (Yuzi Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuzi Yan, Yuan Shen. (2024)<br><strong>Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range</strong><br><button class=copy-to-clipboard title="Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-LG, cs-MA, cs-RO, cs-SY, cs.MA, eess-SY<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03055v1.pdf filename=2403.03055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a scalable distributed policy gradient method and proves its convergence to near-optimal solution in multi-agent linear quadratic networked systems. The agents engage within a specified network under local communication constraints, implying that each agent can only exchange information with a limited number of neighboring agents. On the underlying <b>graph</b> of the network, each agent implements its control input depending on its nearby neighbors&rsquo; states in the linear quadratic control setting. We show that it is possible to approximate the exact gradient only using local information. Compared with the centralized optimal controller, the performance gap decreases to zero exponentially as the communication and control ranges increase. We also demonstrate how increasing the communication range enhances system stability in the gradient descent process, thereby elucidating a critical trade-off. The <b>simulation</b> results verify our theoretical findings.</p></p class="citation"></blockquote><h2 id=q-biogn-1>q-bio.GN (1)</h2><h3 id=11--253290-caduceus-bi-directional-equivariant-long-range-dna-sequence-modeling-yair-schiff-et-al-2024>(1/1 | 253/290) Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling (Yair Schiff et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, Volodymyr Kuleshov. (2024)<br><strong>Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling</strong><br><button class=copy-to-clipboard title="Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.GN<br>Categories: cs-LG, q-bio-GN, q-bio.GN<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03234v1.pdf filename=2403.03234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale sequence modeling has sparked rapid advances that now extend into biology and genomics. However, modeling genomic sequences introduces challenges such as the need to model long-range token interactions, the effects of upstream and downstream regions of the genome, and the reverse complementarity (RC) of DNA. Here, we propose an architecture motivated by these challenges that builds off the long-range Mamba block, and extends it to a BiMamba component that supports bi-directionality, and to a MambaDNA block that additionally supports RC equivariance. We use MambaDNA as the basis of Caduceus, the first family of RC equivariant bi-directional long-range DNA language models, and we introduce pre-training and <b>fine-tuning</b> strategies that yield Caduceus DNA <b>foundation</b> <b>models.</b> Caduceus outperforms previous long-range models on downstream <b>benchmarks;</b> on a challenging long-range variant effect prediction task, Caduceus exceeds the performance of 10x larger models that do not leverage bi-directionality or equivariance.</p></p class="citation"></blockquote><h2 id=mathoc-4>math.OC (4)</h2><h3 id=14--254290-assortment-optimization-for-conference-goodies-with-indifferent-attendees-fernanda-gutiérrez-et-al-2024>(1/4 | 254/290) Assortment Optimization For Conference Goodies With Indifferent Attendees (Fernanda Gutiérrez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fernanda Gutiérrez, Bernardo Subercaseaux. (2024)<br><strong>Assortment Optimization For Conference Goodies With Indifferent Attendees</strong><br><button class=copy-to-clipboard title="Assortment Optimization For Conference Goodies With Indifferent Attendees" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-DM, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03330v1.pdf filename=2403.03330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conferences such as FUN with Algorithms routinely buy goodies (e.g., t-shirts, coffee mugs, etc) for their attendees. Often, said goodies come in different types, varying by color or design, and organizers need to decide how many goodies of each type to buy. We study the problem of buying optimal amounts of each type under a simple model of preferences by the attendees: they are indifferent to the types but want to be able to choose between more than one type of goodies at the time of their arrival. The indifference of attendees suggests that the optimal policy is to buy roughly equal amounts for every goodie type. Despite how intuitive this conjecture sounds, we show that this simple model of assortment optimization is quite rich, and even though we make progress towards proving the conjecture (e.g., we succeed when the number of goodie types is 2 or 3), the general case with K types remains open. We also present asymptotic results and computer <b>simulations,</b> and finally, to motivate further progress, we offer a reward of $100usd for a full proof.</p></p class="citation"></blockquote><h3 id=24--255290-shuffling-momentum-gradient-algorithm-for-convex-optimization-trang-h-tran-et-al-2024>(2/4 | 255/290) Shuffling Momentum Gradient Algorithm for Convex Optimization (Trang H. Tran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Trang H. Tran, Quoc Tran-Dinh, Lam M. Nguyen. (2024)<br><strong>Shuffling Momentum Gradient Algorithm for Convex Optimization</strong><br><button class=copy-to-clipboard title="Shuffling Momentum Gradient Algorithm for Convex Optimization" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03180v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03180v1.pdf filename=2403.03180v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> method <b>(SGD)</b> and its <b>stochastic</b> <b>variants</b> <b>have</b> become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets. In the last decades, researchers have made substantial effort to study the theoretical performance of <b>SGD</b> and its shuffling variants. However, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and Nesterov&rsquo;s momentum for convex settings. In this work, we extend the analysis of the shuffling momentum gradient method developed in [Tran et al (2021)] to both finite-sum convex and strongly convex optimization problems. We provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $O(1/nT^2)$, where $n$ is the number of samples and $T$ is the number of training epochs. Our analysis is a state-of-the-art, matching the best rates of existing shuffling <b>stochastic</b> <b>gradient</b> <b>algorithms</b> in the literature.</p></p class="citation"></blockquote><h3 id=34--256290-non-convex-stochastic-composite-optimization-with-polyak-momentum-yuan-gao-et-al-2024>(3/4 | 256/290) Non-Convex Stochastic Composite Optimization with Polyak Momentum (Yuan Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Gao, Anton Rodomanov, Sebastian U. Stich. (2024)<br><strong>Non-Convex Stochastic Composite Optimization with Polyak Momentum</strong><br><button class=copy-to-clipboard title="Non-Convex Stochastic Composite Optimization with Polyak Momentum" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02967v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02967v1.pdf filename=2403.02967v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>stochastic</b> <b>proximal</b> <b>gradient</b> method is a powerful generalization of the widely used <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> method and has found numerous applications in Machine Learning. However, it is notoriously known that this method fails to converge in non-convex settings where the <b>stochastic</b> <b>noise</b> <b>is</b> significant (i.e. when only small or bounded batch sizes are used). In this paper, we focus on the <b>stochastic</b> <b>proximal</b> <b>gradient</b> method with Polyak momentum. We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size. Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly. Finally, we provide numerical experiments to validate our theoretical results.</p></p class="citation"></blockquote><h3 id=44--257290-the-vehicle-routing-problem-with-synchronization-constraints-and-support-vehicle-dependent-service-times-david-wittwer-et-al-2024>(4/4 | 257/290) The vehicle routing problem with synchronization constraints and support vehicle-dependent service times (David Wittwer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Wittwer, Felix Tamke. (2024)<br><strong>The vehicle routing problem with synchronization constraints and support vehicle-dependent service times</strong><br><button class=copy-to-clipboard title="The vehicle routing problem with synchronization constraints and support vehicle-dependent service times" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: 68R01, G-2-1; G-2-2; F-2-2, cs-DM, cs-NA, math-NA, math-OC, math.OC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03355v1.pdf filename=2403.03355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many production processes require the cooperation of various resources. Especially when using expensive machines, their utilization plays a decisive role in efficient production. In agricultural production or civil construction processes, e.g., harvesting or road building, the machines are typically mobile, and synchronization of different machine types is required to perform operations. In addition, the productivity of one type often depends on the availability of another type. In this paper, we consider two types of vehicles, called primary and support vehicles. Primary vehicles perform operations and are assisted by at least one support vehicle, with more support vehicles resulting in faster service times for primary vehicles. We call this practical problem the vehicle routing and scheduling problem with support vehicle-dependent service times and introduce two mixed-integer linear programming models. The first represents each support vehicle individually with binary decision variables, while the second considers the cumulative flow of support vehicles with integer decision variables. Furthermore, the models are defined on a <b>graph</b> that allows easy transformation into multiple variants. These variants are based on allowing or prohibiting switching support vehicles between primary vehicles and splitting services among primary vehicles. We show in our extensive computational experiments that: i) the integer representation of support vehicles is superior to the binary representation, ii) the benefit of additional vehicles is subject to saturation effects and depends on the ratio of support and primary vehicles, and iii) switching and splitting lead to problems that are more difficult to solve, but also result in better solutions with higher primary vehicle utilization.</p></p class="citation"></blockquote><h2 id=mathna-6>math.NA (6)</h2><h3 id=16--258290-a-transient-thermal-model-for-power-electronics-systems-neelakantan-padmanabhan-2024>(1/6 | 258/290) A Transient Thermal Model for Power Electronics Systems (Neelakantan Padmanabhan, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neelakantan Padmanabhan. (2024)<br><strong>A Transient Thermal Model for Power Electronics Systems</strong><br><button class=copy-to-clipboard title="A Transient Thermal Model for Power Electronics Systems" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03268v1.pdf filename=2403.03268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An equation based reduced order model applicable to generalized heat equation and thermal <b>simulations</b> of power electronics systems developed in commercial CFD tools, is presented in this work. The model considers the physics of heat transfer between multiple objects in different mediums and presents a set of equations that can be applied to a wide range of heat transfer scenarios including conduction, natural and forced convection problems. A few case studies including heat transfer in a power electronic system are simulated in Ansys Icepak and the temperatures from the <b>simulations</b> are compared with the temperatures predicted by the models. The models are observed to be highly accurate when compared with the <b>simulations.</b> The predictive model described in this work reduces large complex <b>simulations</b> down to a few parameters which tremendously improves the computation speed, uses very low physical disk space and enables fast evaluation of thermal performance of the system for any changes in the input parameters.</p></p class="citation"></blockquote><h3 id=26--259290-on-the-computation-of-lattice-sums-without-translational-invariance-andreas-a-buchheit-et-al-2024>(2/6 | 259/290) On the computation of lattice sums without translational invariance (Andreas A. Buchheit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas A. Buchheit, Torsten Keßler, Kirill Serkh. (2024)<br><strong>On the computation of lattice sums without translational invariance</strong><br><button class=copy-to-clipboard title="On the computation of lattice sums without translational invariance" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cond-mat-str-el, cs-NA, hep-lat, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03213v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03213v2.pdf filename=2403.03213v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a new method for the efficient computation of oscillatory multidimensional lattice sums in geometries with boundaries. Such sums are ubiquitous in both pure and applied mathematics, and have immediate applications in condensed matter physics and topological quantum physics. The challenge in their evaluation results from the combination of singular long-range interactions with the loss of translational invariance caused by the boundaries, rendering standard tools ineffective. Our work shows that these lattice sums can be generated from a generalization of the Riemann zeta function to multidimensional non-periodic lattice sums. We put forth a new representation of this zeta function together with a numerical algorithm that ensures exponential convergence across an extensive range of geometries. Notably, our method&rsquo;s runtime is influenced only by the complexity of the considered geometries and not by the number of particles, providing the foundation for efficient <b>simulations</b> of macroscopic condensed matter systems. We showcase the practical utility of our method by computing interaction energies in a three-dimensional crystal structure with $3\times 10^{23}$ particles. Our method&rsquo;s accuracy is demonstrated through extensive numerical experiments. A reference implementation is provided online along with this article.</p></p class="citation"></blockquote><h3 id=36--260290-scientific-machine-learning-for-closure-models-in-multiscale-problems-a-review-benjamin-sanderse-et-al-2024>(3/6 | 260/290) Scientific machine learning for closure models in multiscale problems: a review (Benjamin Sanderse et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Sanderse, Panos Stinis, Romit Maulik, Shady E. Ahmed. (2024)<br><strong>Scientific machine learning for closure models in multiscale problems: a review</strong><br><button class=copy-to-clipboard title="Scientific machine learning for closure models in multiscale problems: a review" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65MXX, 76MXX, 68T07, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02913v1.pdf filename=2403.02913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Closure problems are omnipresent when simulating multiscale systems, where some quantities and processes cannot be fully prescribed despite their effects on the <b>simulation&rsquo;s</b> accuracy. Recently, scientific machine learning approaches have been proposed as a way to tackle the closure problem, combining traditional (physics-based) modeling with data-driven (machine-learned) techniques, typically through enriching differential equations with neural networks. This paper reviews the different reduced model forms, distinguished by the degree to which they include known physics, and the different objectives of a priori and a posteriori learning. The importance of adhering to physical laws (such as symmetries and conservation laws) in choosing the reduced model form and choosing the learning method is discussed. The effect of spatial and temporal discretization and recent trends toward discretization-invariant models are reviewed. In addition, we make the connections between closure problems and several other research disciplines: inverse problems, Mori-Zwanzig theory, and multi-fidelity methods. In conclusion, much progress has been made with scientific machine learning approaches for solving closure problems, but many challenges remain. In particular, the generalizability and interpretability of learned models is a major issue that needs to be addressed further.</p></p class="citation"></blockquote><h3 id=46--261290-efficient-simulation-of-complex-ginzburg--landau-equations-using-high-order-exponential-type-methods-marco-caliari-et-al-2024>(4/6 | 261/290) Efficient simulation of complex Ginzburg&ndash;Landau equations using high-order exponential-type methods (Marco Caliari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Caliari, Fabio Cassini. (2024)<br><strong>Efficient simulation of complex Ginzburg&ndash;Landau equations using high-order exponential-type methods</strong><br><button class=copy-to-clipboard title="Efficient simulation of complex Ginzburg--Landau equations using high-order exponential-type methods" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02816v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02816v1.pdf filename=2403.02816v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider the task of efficiently computing the numerical solution of evolutionary complex Ginzburg&ndash;Landau equations. To this aim, we employ high-order exponential methods of splitting and Lawson type for the time integration. These schemes enjoy favorable stability properties and, in particular, do not show restrictions on the time step size due to the underlying stiffness of the models. The needed actions of matrix exponentials are efficiently realized with pointwise operations in Fourier space (when the model is considered with periodic boundary conditions) or by using a tensor-oriented approach that suitably employs the so-called $\mu$-mode products (when the semidiscretization in space is performed with finite differences). The overall effectiveness of the approach is demonstrated by running <b>simulations</b> on a variety of two- and three-dimensional (systems of) complex Ginzburg&ndash;Landau equations with cubic and cubic-quintic nonlinearities, which are widely considered in literature to model relevant physical phenomena. In fact, in all instances high-order exponential-type schemes can outperform standard techniques to integrate in time the models under consideration, i.e., the well-known split-step method and the explicit fourth-order Runge&ndash;Kutta integrator.</p></p class="citation"></blockquote><h3 id=56--262290-a-fully-discrete-semi-lagrangian-scheme-for-a-price-formation-mfg-model-yuri-ashrafyan-et-al-2024>(5/6 | 262/290) A Fully-discrete Semi-Lagrangian scheme for a price formation MFG model (Yuri Ashrafyan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuri Ashrafyan, Diogo Gomes. (2024)<br><strong>A Fully-discrete Semi-Lagrangian scheme for a price formation MFG model</strong><br><button class=copy-to-clipboard title="A Fully-discrete Semi-Lagrangian scheme for a price formation MFG model" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 35Q89, 65M22, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02785v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02785v1.pdf filename=2403.02785v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Here, we examine a fully-discrete Semi-Lagrangian scheme for a mean-field game price formation model. We show that the discretization is monotone as a multivalued operator and prove the uniqueness of the discretized solution. Moreover, we show that the limit of the discretization converges to the weak solution of the continuous price formation mean-field game using monotonicity methods. This scheme performs substantially better than standard methods by giving reliable results within a few iterations, as several numerical <b>simulations</b> and comparisons at the end of the paper illustrate.</p></p class="citation"></blockquote><h3 id=66--263290-isc-an-radi-type-method-for-stochastic-continuous-time-algebraic-riccati-equations-zhen-chen-guo-et-al-2024>(6/6 | 263/290) ISC: an RADI-type method for stochastic continuous-time algebraic Riccati equations (Zhen-Chen Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen-Chen Guo, Xin Liang. (2024)<br><strong>ISC: an RADI-type method for stochastic continuous-time algebraic Riccati equations</strong><br><button class=copy-to-clipboard title="ISC: an RADI-type method for stochastic continuous-time algebraic Riccati equations" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65F45, 15A24, cs-NA, math-NA, math-OC, math.NA<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02940v1.pdf filename=2403.02940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose an RADI-type method for large-scale stochastic <b>continuous-time</b> <b>algebraic</b> Riccati equations with sparse and low-rank structures. The so-called ISC method is developed by using the Incorporation idea together with different Shifts to accelerate the convergence and Compressions to reduce the storage and complexity. Numerical experiments are given to show its efficiency.</p></p class="citation"></blockquote><h2 id=statme-2>stat.ME (2)</h2><h3 id=12--264290-tripledebiased-lasso-for-statistical-inference-of-conditional-average-treatment-effects-masahiro-kato-2024>(1/2 | 264/290) Triple/Debiased Lasso for Statistical Inference of Conditional Average Treatment Effects (Masahiro Kato, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masahiro Kato. (2024)<br><strong>Triple/Debiased Lasso for Statistical Inference of Conditional Average Treatment Effects</strong><br><button class=copy-to-clipboard title="Triple/Debiased Lasso for Statistical Inference of Conditional Average Treatment Effects" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-LG, econ-EM, stat-ME, stat-ML, stat.ME<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03240v1.pdf filename=2403.03240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates the estimation and the statistical inference about Conditional Average Treatment Effects (CATEs), which have garnered attention as a metric representing individualized causal effects. In our data-generating process, we assume linear models for the outcomes associated with binary treatments and define the CATE as a difference between the expected outcomes of these linear models. This study allows the linear models to be high-dimensional, and our interest lies in consistent estimation and statistical inference for the CATE. In high-dimensional linear regression, one typical approach is to assume sparsity. However, in our study, we do not assume sparsity directly. Instead, we consider sparsity only in the difference of the linear models. We first use a doubly robust estimator to approximate this difference and then regress the difference on covariates with Lasso regularization. Although this regression estimator is consistent for the CATE, we further reduce the bias using the techniques in double/debiased machine learning (DML) and debiased Lasso, leading to $\sqrt{n}$-consistency and confidence intervals. We refer to the debiased estimator as the triple/debiased Lasso (TDL), applying both DML and debiased Lasso techniques. We confirm the soundness of our proposed method through <b>simulation</b> studies.</p></p class="citation"></blockquote><h3 id=22--265290-a-consensus-constrained-parsimonious-gaussian-mixture-model-for-clustering-hyperspectral-images-ganesh-babu-et-al-2024>(2/2 | 265/290) A consensus-constrained parsimonious Gaussian mixture model for clustering hyperspectral images (Ganesh Babu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ganesh Babu, Aoife Gowen, Michael Fop, Isobel Claire Gormley. (2024)<br><strong>A consensus-constrained parsimonious Gaussian mixture model for clustering hyperspectral images</strong><br><button class=copy-to-clipboard title="A consensus-constrained parsimonious Gaussian mixture model for clustering hyperspectral images" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-CV, eess-IV, stat-ME, stat.ME<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03349v1.pdf filename=2403.03349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The use of hyperspectral imaging to investigate food samples has grown due to the improved performance and lower cost of spectroscopy instrumentation. Food engineers use hyperspectral images to classify the type and quality of a food sample, typically using classification methods. In order to train these methods, every pixel in each training image needs to be labelled. Typically, computationally cheap threshold-based approaches are used to label the pixels, and classification methods are trained based on those labels. However, threshold-based approaches are subjective and cannot be generalized across hyperspectral images taken in different conditions and of different foods. Here a consensus-constrained parsimonious Gaussian mixture model (ccPGMM) is proposed to label pixels in hyperspectral images using a model-based <b>clustering</b> approach. The ccPGMM utilizes available information on the labels of a small number of pixels and the relationship between those pixels and neighbouring pixels as constraints when <b>clustering</b> the rest of the pixels in the image. A latent variable model is used to represent the high-dimensional data in terms of a small number of underlying latent factors. To ensure computational feasibility, a consensus <b>clustering</b> approach is employed, where the data are divided into multiple randomly selected subsets of variables and constrained <b>clustering</b> is applied to each data subset; the <b>clustering</b> results are then consolidated across all data subsets to provide a consensus <b>clustering</b> solution. The ccPGMM approach is applied to simulated datasets and real hyperspectral images of three types of puffed cereal, corn, rice, and wheat. Improved <b>clustering</b> performance and computational efficiency are demonstrated when compared to other current state-of-the-art approaches.</p></p class="citation"></blockquote><h2 id=eesssy-6>eess.SY (6)</h2><h3 id=16--266290-on-the-computation-of-stable-coupled-state-space-models-for-dynamic-substructuring-applications-r-s-o-dias-et-al-2024>(1/6 | 266/290) On the computation of stable coupled state-space models for dynamic substructuring applications (R. S. O. Dias et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>R. S. O. Dias, M. Martarelli, P. Chiariotti. (2024)<br><strong>On the computation of stable coupled state-space models for dynamic substructuring applications</strong><br><button class=copy-to-clipboard title="On the computation of stable coupled state-space models for dynamic substructuring applications" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-DS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03182v1.pdf filename=2403.03182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper aims at introducing a methodology to compute stable coupled state-space models for dynamic substructuring applications by introducing two novel approaches targeted to accomplish this task: a) a procedure to impose Newtons&rsquo;s second law without relying on the use of undamped RCMs (residual compensation modes) and b) a novel approach to impose stability on unstable coupled state-space models. The enforcement of stability is performed by dividing the unstable model into two different models, one composed by the stable poles (stable model) and the other composed by the unstable ones (unstable model). Then, the poles of the unstable state-space model are forced to be stable, leading to the computation of a stabilized state-space model. Afterwards, to make sure that the Frequency Response Functions (FRFs) of the stabilized model well match the FRFs of the unstable model, the Least-Squares Frequency Domain (LSFD) method is exploited to update the modal parameters of the stabilized model composed by the pairs of complex conjugate poles. The validity of the proposed methodologies is presented and discussed by exploiting experimental data. Indeed, by exploiting the FRFs of a real system, accurate state-space models respecting Newton&rsquo;s second law are computed. Then, decoupling and coupling operations are performed with the identified state-space models, no matter the models resultant from the decoupling/coupling operations are unstable. Stability is then imposed on the computed unstable coupled model by following the approach proposed in this paper. The methodology proved to work well on these data. Moreover, the paper also shows that the coupled state-space models obtained using this methodology are suitable to be exploited in time-domain analyses and <b>simulations.</b></p></p class="citation"></blockquote><h3 id=26--267290-design-of-stochastic-quantizers-for-privacy-preservation-le-liu-et-al-2024>(2/6 | 267/290) Design of Stochastic Quantizers for Privacy Preservation (Le Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Le Liu, Yu Kawano, Ming Cao. (2024)<br><strong>Design of Stochastic Quantizers for Privacy Preservation</strong><br><button class=copy-to-clipboard title="Design of Stochastic Quantizers for Privacy Preservation" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-CR, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Quantization, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03048v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03048v1.pdf filename=2403.03048v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we examine the role of stochastic quantizers for privacy preservation. We first employ a static stochastic quantizer and investigate its corresponding privacy-preserving properties. Specifically, we demonstrate that a sufficiently large <b>quantization</b> step guarantees $(0, \delta)$ <b>differential</b> <b>privacy.</b> Additionally, the degradation of control performance caused by <b>quantization</b> is evaluated as the tracking error of output regulation. These two analyses characterize the trade-off between privacy and control performance, determined by the <b>quantization</b> step. This insight enables us to use <b>quantization</b> intentionally as a means to achieve the seemingly conflicting two goals of maintaining control performance and preserving privacy at the same time; towards this end, we further investigate a dynamic stochastic quantizer. Under a stability assumption, the dynamic stochastic quantizer can enhance privacy, more than the static one, while achieving the same control performance. We further handle the unstable case by additionally applying input Gaussian noise.</p></p class="citation"></blockquote><h3 id=36--268290-unifying-controller-design-for-stabilizing-nonlinear-systems-with-norm-bounded-control-inputs-ming-li-et-al-2024>(3/6 | 268/290) Unifying Controller Design for Stabilizing Nonlinear Systems with Norm-Bounded Control Inputs (Ming Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Li, Zhiyong Sun, Siep Weiland. (2024)<br><strong>Unifying Controller Design for Stabilizing Nonlinear Systems with Norm-Bounded Control Inputs</strong><br><button class=copy-to-clipboard title="Unifying Controller Design for Stabilizing Nonlinear Systems with Norm-Bounded Control Inputs" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03030v1.pdf filename=2403.03030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper revisits a classical challenge in the design of stabilizing controllers for nonlinear systems with a norm-bounded input constraint. By extending Lin-Sontag&rsquo;s universal formula and introducing a generic (state-dependent) scaling term, a unifying controller design method is proposed. The incorporation of this generic scaling term gives a unified controller and enables the derivation of alternative universal formulas with various favorable properties, which makes it suitable for tailored control designs to meet specific requirements and provides versatility across different control scenarios. Additionally, we present a constructive approach to determine the optimal scaling term, leading to an explicit solution to an optimization problem, named optimization-based universal formula. The resulting controller ensures asymptotic stability, satisfies a norm-bounded input constraint, and optimizes a predefined cost function. Finally, the essential properties of the unified controllers are analyzed, including smoothness, continuity at the origin, stability margin, and inverse optimality. <b>Simulations</b> validate the approach, showcasing its effectiveness in addressing a challenging stabilizing control problem of a nonlinear system.</p></p class="citation"></blockquote><h3 id=46--269290-single-level-robust-bidding-of-renewable-only-virtual-power-plant-in-energy-and-ancillary-service-markets-for-worst-case-profit-hadi-nemati-et-al-2024>(4/6 | 269/290) Single-level Robust Bidding of Renewable-only Virtual Power Plant in Energy and Ancillary Service Markets for Worst-case Profit (Hadi Nemati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hadi Nemati, Pedro Sánchez-Martín, Ana Baringo, Álvaro Ortega. (2024)<br><strong>Single-level Robust Bidding of Renewable-only Virtual Power Plant in Energy and Ancillary Service Markets for Worst-case Profit</strong><br><button class=copy-to-clipboard title="Single-level Robust Bidding of Renewable-only Virtual Power Plant in Energy and Ancillary Service Markets for Worst-case Profit" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02953v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02953v1.pdf filename=2403.02953v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a novel single-level robust mathematical approach to model the RES-only Virtual Power Plant (RVPP) bidding problem in the simultaneous Day Ahead Market (DAM) and Secondary Reserve Market (SRM). The worst-case profit of RVPP due to uncertainties related to electricity prices, Non-dispatchable Renewable Energy Sources (ND-RES) production, and flexible demand is captured. In order to find the worst-case profit in a single-level model, the relationship between price and energy uncertainties leads to some non-linear constraints, which are appropriately linearized. The <b>simulation</b> results show the superiority of the proposed robust model compared to those in the literature, as well as its computational efficiency.</p></p class="citation"></blockquote><h3 id=56--270290-autonomous-vehicle-decision-and-control-through-reinforcement-learning-with-traffic-flow-randomization-yuan-lin-et-al-2024>(5/6 | 270/290) Autonomous vehicle decision and control through reinforcement learning with traffic flow randomization (Yuan Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Lin, Antai Xie, Xiao Liu. (2024)<br><strong>Autonomous vehicle decision and control through reinforcement learning with traffic flow randomization</strong><br><button class=copy-to-clipboard title="Autonomous vehicle decision and control through reinforcement learning with traffic flow randomization" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02882v1.pdf filename=2403.02882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most of the current studies on autonomous vehicle decision-making and control tasks based on <b>reinforcement</b> <b>learning</b> are conducted in simulated environments. The training and testing of these studies are carried out under rule-based microscopic traffic flow, with little consideration of migrating them to real or near-real environments to test their performance. It may lead to a degradation in performance when the trained model is tested in more realistic traffic scenes. In this study, we propose a method to randomize the driving style and behavior of surrounding vehicles by randomizing certain parameters of the car-following model and the lane-changing model of rule-based microscopic traffic flow in SUMO. We trained policies with deep <b>reinforcement</b> <b>learning</b> algorithms under the domain randomized rule-based microscopic traffic flow in freeway and merging scenes, and then tested them separately in rule-based microscopic traffic flow and high-fidelity microscopic traffic flow. Results indicate that the policy trained under domain randomization traffic flow has significantly better success rate and calculative reward compared to the models trained under other microscopic traffic flows.</p></p class="citation"></blockquote><h3 id=66--271290-faithful-dynamic-timing-analysis-of-digital-circuits-using-continuous-thresholded-mode-switched-odes-arman-ferdowsi-et-al-2024>(6/6 | 271/290) Faithful Dynamic Timing Analysis of Digital Circuits Using Continuous Thresholded Mode-Switched ODEs (Arman Ferdowsi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arman Ferdowsi, Matthias Függer, Thomas Nowak, Michael Drmota, Ulrich Schmid. (2024)<br><strong>Faithful Dynamic Timing Analysis of Digital Circuits Using Continuous Thresholded Mode-Switched ODEs</strong><br><button class=copy-to-clipboard title="Faithful Dynamic Timing Analysis of Digital Circuits Using Continuous Thresholded Mode-Switched ODEs" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AR, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03235v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03235v2.pdf filename=2403.03235v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Thresholded hybrid systems are restricted dynamical systems, where the current mode, and hence the ODE system describing its behavior, is solely determined by externally supplied digital input signals and where the only output signals are digital ones generated by comparing an internal state variable to a threshold value. An attractive feature of such systems is easy composition, which is facilitated by their purely digital interface. A particularly promising application domain of thresholded hybrid systems is digital integrated circuits: Modern digital circuit design considers them as a composition of Millions and even Billions of elementary logic gates, like inverters, GOR and <b>Gand.</b> Since every such logic gate is eventually implemented as an electronic circuit, however, which exhibits a behavior that is governed by some ODE system, thresholded hybrid systems are ideally suited for making the transition from the analog to the digital world rigorous. In this paper, we prove that the mapping from digital input signals to digital output signals is continuous for a large class of thresholded hybrid systems. Moreover, we show that, under some mild conditions regarding causality, this continuity also continues to hold for arbitrary compositions, which in turn guarantees that the composition faithfully captures the analog reality. By applying our generic results to some recently developed thresholded hybrid gate models, both for single-input single-output gates like inverters and for a two-input CMOS NOR gate, we show that they are continuous. Moreover, we provide a novel thresholded hybrid model for the two-input NOR gate, which is not only continuous but also, unlike the existing one, faithfully models all multi-input switching effects.</p></p class="citation"></blockquote><h2 id=csgr-2>cs.GR (2)</h2><h3 id=12--272290-implicit-explicit-simulation-of-mass-spring-charge-systems-zhiyuan-zhang-et-al-2024>(1/2 | 272/290) Implicit-Explicit simulation of Mass-Spring-Charge Systems (Zhiyuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Zhang, Zhaocheng Liu, Stefanos Papanicolopulos, Kartic Subr. (2024)<br><strong>Implicit-Explicit simulation of Mass-Spring-Charge Systems</strong><br><button class=copy-to-clipboard title="Implicit-Explicit simulation of Mass-Spring-Charge Systems" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03005v1.pdf filename=2403.03005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point masses connected by springs, or mass-spring systems, are widely used in computer animation to approximate the behavior of deformable objects. One of the restrictions imposed by these models is that points that are not topologically constrained (linked by a spring) are unable to interact with each other explicitly. Such interactions would introduce a new dimension for artistic control and animation within the computer graphics community. Beyond graphics, such a model could be an effective proxy to use for model-based learning of complex physical systems such as molecular biology. We propose to imbue masses in a mass-spring system with electrostatic charge leading a system with internal forces between all pairs of charged points &ndash; regardless of whether they are linked by a spring. We provide a practical and stable algorithm to simulate charged mass-spring systems over long time horizons. We demonstrate how these systems may be controlled via parameters such as guidance electric fields or external charges, thus presenting fresh opportunities for artistic authoring. Our method is especially appropriate for computer graphics applications due to its robustness at larger <b>simulation</b> time steps.</p></p class="citation"></blockquote><h3 id=22--273290-towards-geometric-photometric-joint-alignment-for-facial-mesh-registration-xizhi-wang-et-al-2024>(2/2 | 273/290) Towards Geometric-Photometric Joint Alignment for Facial Mesh Registration (Xizhi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xizhi Wang, Yaxiong Wang, Mengjian Li. (2024)<br><strong>Towards Geometric-Photometric Joint Alignment for Facial Mesh Registration</strong><br><button class=copy-to-clipboard title="Towards Geometric-Photometric Joint Alignment for Facial Mesh Registration" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-CV, cs-GR, cs.GR<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02629v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02629v1.pdf filename=2403.02629v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a Geometric-Photometric Joint Alignment(GPJA) method, for accurately aligning human expressions by combining <b>geometry</b> and photometric information. Common practices for registering human heads typically involve aligning landmarks with facial template meshes using <b>geometry</b> processing approaches, but often overlook photometric consistency. GPJA overcomes this limitation by leveraging differentiable rendering to align vertices with target expressions, achieving joint alignment in <b>geometry</b> and photometric appearances automatically, without the need for semantic annotation or aligned meshes for training. It features a holistic rendering alignment strategy and a multiscale regularized optimization for robust and fast convergence. The method utilizes derivatives at vertex positions for supervision and employs a gradient-based algorithm which guarantees smoothness and avoids topological defects during the <b>geometry</b> evolution. Experimental results demonstrate faithful alignment under various expressions, surpassing the conventional ICP-based methods and the state-of-the-art deep learning based method. In practical, our method enhances the efficiency of obtaining topology-consistent face models from multi-view stereo facial scanning.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--274290-scalable-continuous-time-diffusion-framework-for-network-inference-and-influence-estimation-keke-huang-et-al-2024>(1/1 | 274/290) Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation (Keke Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keke Huang, Ruize Gao, Bogdan Cautis, Xiaokui Xiao. (2024)<br><strong>Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation</strong><br><button class=copy-to-clipboard title="Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-LG, cs-SI, cs.SI<br>Keyword Score: 20<br>Keywords: Diffusion Model, Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02867v1.pdf filename=2403.02867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study of <b>continuous-time</b> <b>information</b> <b>diffusion</b> <b>has</b> been an important area of research for many applications in recent years. When only the <b>diffusion</b> <b>traces</b> (cascades) are accessible, cascade-based network inference and influence estimation are two essential problems to explore. Alas, existing methods exhibit limited capability to infer and process networks with more than a few thousand nodes, suffering from scalability issues. In this paper, we view the <b>diffusion</b> <b>process</b> as a <b>continuous-time</b> <b>dynamical</b> system, based on which we establish a <b>continuous-time</b> <b>diffusion</b> <b>model.</b> Subsequently, we instantiate the model to a scalable and effective framework (FIM) to approximate the <b>diffusion</b> <b>propagation</b> from available cascades, thereby inferring the underlying network structure. Furthermore, we undertake an analysis of the approximation error of FIM for network inference. To achieve the desired scalability for influence estimation, we devise an advanced sampling technique and significantly boost the efficiency. We also quantify the effect of the approximation error on influence estimation theoretically. Experimental results showcase the effectiveness and superior scalability of FIM on network inference and influence estimation.</p></p class="citation"></blockquote><h2 id=csds-4>cs.DS (4)</h2><h3 id=14--275290-fine-grained-privacy-guarantees-for-coverage-problems-laxman-dhulipala-et-al-2024>(1/4 | 275/290) Fine-Grained Privacy Guarantees for Coverage Problems (Laxman Dhulipala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laxman Dhulipala, George Z. Li. (2024)<br><strong>Fine-Grained Privacy Guarantees for Coverage Problems</strong><br><button class=copy-to-clipboard title="Fine-Grained Privacy Guarantees for Coverage Problems" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CR, cs-DS, cs.DS<br>Keyword Score: 13<br>Keywords: Graph, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03337v1.pdf filename=2403.03337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a new notion of neighboring databases for coverage problems such as Max Cover and Set Cover under <b>differential</b> <b>privacy.</b> In contrast to the standard privacy notion for these problems, which is analogous to node-privacy in <b>graphs,</b> our new definition gives a more fine-grained privacy guarantee, which is analogous to edge-privacy. We illustrate several scenarios of Set Cover and Max Cover where our privacy notion is desired one for the application. Our main result is an $\epsilon$-edge differentially private algorithm for Max Cover which obtains an $(1-1/e-\eta,\tilde{O}(k/\epsilon))$-approximation with high probability. Furthermore, we show that this result is nearly tight: we give a lower bound show that an additive error of $\Omega(k/\epsilon)$ is necessary under edge-differential privacy. Via group privacy properties, this implies a new algorithm for $\epsilon$-node differentially private Max Cover which obtains an $(1-1/e-\eta,\tilde{O}(fk/\epsilon))$-approximation, where $f$ is the maximum degree of an element in the set system. When $f\ll k$, this improves over the best known algorithm for Max Cover under pure (node) <b>differential</b> <b>privacy,</b> which obtains an $(1-1/e,\tilde{O}(k^2/\epsilon))$-approximation.</p></p class="citation"></blockquote><h3 id=24--276290-dgap-efficient-dynamic-graph-analysis-on-persistent-memory-abdullah-al-raqibul-islam-et-al-2024>(2/4 | 276/290) DGAP: Efficient Dynamic Graph Analysis on Persistent Memory (Abdullah Al Raqibul Islam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdullah Al Raqibul Islam, Dong Dai. (2024)<br><strong>DGAP: Efficient Dynamic Graph Analysis on Persistent Memory</strong><br><button class=copy-to-clipboard title="DGAP: Efficient Dynamic Graph Analysis on Persistent Memory" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DC, cs-DS, cs-PF, cs.DS<br>Keyword Score: 13<br>Keywords: Graph, LLaMA<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02665v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02665v1.pdf filename=2403.02665v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic <b>graphs,</b> featuring continuously updated vertices and edges, have grown in importance for numerous real-world applications. To accommodate this, <b>graph</b> frameworks, particularly their internal data structures, must support both persistent <b>graph</b> updates and rapid <b>graph</b> analysis simultaneously, leading to complex designs to orchestrate <code>fast but volatile' and </code>persistent but slow&rsquo; storage devices. Emerging persistent memory technologies, such as Optane DCPMM, offer a promising alternative to simplify the designs by providing data persistence, low latency, and high IOPS together. In light of this, we propose DGAP, a framework for efficient dynamic <b>graph</b> analysis on persistent memory. Unlike traditional dynamic <b>graph</b> frameworks, which combine multiple <b>graph</b> data structures (e.g., edge list or adjacency list) to achieve the required performance, DGAP utilizes a single mutable Compressed Sparse Row (CSR) <b>graph</b> structure with new designs for persistent memory to construct the framework. Specifically, DGAP introduces a \textit{per-section edge log} to reduce write amplification on persistent memory; a \textit{per-thread undo log} to enable high-performance, crash-consistent rebalancing operations; and a data placement schema to minimize in-place updates on persistent memory. Our extensive evaluation results demonstrate that DGAP can achieve up to $3.2\times$ better <b>graph</b> update performance and up to $3.77\times$ better <b>graph</b> analysis performance compared to state-of-the-art dynamic <b>graph</b> frameworks for persistent memory, such as XPGraph, <b>LLAMA,</b> and GraphOne.</p></p class="citation"></blockquote><h3 id=34--277290-cover-edge-based-novel-triangle-counting-david-a-bader-et-al-2024>(3/4 | 277/290) Cover Edge-Based Novel Triangle Counting (David A. Bader et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David A. Bader, Fuhuan Li, Zhihui Du, Palina Pauliuchenka, Oliver Alvarado Rodriguez, Anant Gupta, Sai Sri Vastav Minnal, Valmik Nahata, Anya Ganeshan, Ahmet Gundogdu, Jason Lew. (2024)<br><strong>Cover Edge-Based Novel Triangle Counting</strong><br><button class=copy-to-clipboard title="Cover Edge-Based Novel Triangle Counting" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 9<br>Keywords: Graph, Benchmarking, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02997v1.pdf filename=2403.02997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Listing and counting triangles in <b>graphs</b> is a key algorithmic kernel for network analyses, including community detection, <b>clustering</b> coefficients, k-trusses, and triangle centrality. In this paper, we propose the novel concept of a cover-edge set that can be used to find triangles more efficiently. Leveraging the breadth-first search (BFS) method, we can quickly generate a compact cover-edge set. Novel sequential and parallel triangle counting algorithms that employ cover-edge sets are presented. The novel sequential algorithm performs competitively with the fastest previous approaches on both real and synthetic <b>graphs,</b> such as those from the Graph500 <b>Benchmark</b> and the MIT/Amazon/IEEE <b>Graph</b> Challenge. We implement 22 sequential algorithms for performance evaluation and comparison. At the same time, we employ OpenMP to parallelize 11 sequential algorithms, presenting an in-depth analysis of their parallel performance. Furthermore, we develop a distributed parallel algorithm that can asymptotically reduce communication on massive <b>graphs.</b> In our estimate from massive-scale Graph500 <b>graphs,</b> our distributed parallel algorithm can reduce the communication on a scale~36 <b>graph</b> by 1156x and on a scale~42 <b>graph</b> by 2368x. Comprehensive experiments are conducted on the recently launched Intel Xeon 8480+ processor and shed light on how <b>graph</b> attributes, such as topology, diameter, and degree distribution, can affect the performance of these algorithms.</p></p class="citation"></blockquote><h3 id=44--278290-on-approximate-fully-dynamic-matching-and-online-matrix-vector-multiplication-yang-p-liu-2024>(4/4 | 278/290) On Approximate Fully-Dynamic Matching and Online Matrix-Vector Multiplication (Yang P. Liu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang P. Liu. (2024)<br><strong>On Approximate Fully-Dynamic Matching and Online Matrix-Vector Multiplication</strong><br><button class=copy-to-clipboard title="On Approximate Fully-Dynamic Matching and Online Matrix-Vector Multiplication" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02582v1.pdf filename=2403.02582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study connections between the problem of fully dynamic $(1-\epsilon)$-approximate maximum bipartite matching, and the dual $(1+\epsilon)$-approximate vertex cover problem, with the online matrix-vector ($\mathsf{OMv}$) conjecture which has recently been used in several fine-grained hardness reductions. We prove that there is an online algorithm that maintains a $(1+\epsilon)$-approximate vertex cover in amortized $n^{1-c}\epsilon^{-C}$ time for constants $c, C > 0$ for fully dynamic updates if and only if the $\mathsf{OMv}$ conjecture is false. Similarly, we prove that there is an online algorithm that maintains a $(1-\epsilon)$-approximate maximum matching in amortized $n^{1-c}\epsilon^{-C}$ time if and only if there is a nontrivial algorithm for another dynamic problem, which we call dynamic approximate $\mathsf{OMv}$, that has seemingly no matching structure. This provides some evidence against achieving amortized sublinear update times for approximate fully dynamic matching and vertex cover. Leveraging these connections, we obtain faster algorithms for approximate fully dynamic matching in both the online and offline settings. 1. We give a randomized algorithm that with high probability maintains a $(1-\epsilon)$-approximate bipartite matching and $(1+\epsilon)$-approximate vertex cover in fully dynamic <b>graphs,</b> in amortized $O(\epsilon^{-O(1)} \frac{n}{2^{\Omega(\sqrt{\log n})}})$ update time. Our algorithm leverages fast algorithms for $\mathsf{OMv}$ due to Larsen-Williams [SODA 2017]. 2. We give a randomized offline algorithm for $(1-\epsilon)$-approximate maximum matching with amortized runtime $O(n^{.58}\epsilon^{-O(1)})$ by using fast matrix multiplication, significantly improving over the runtimes achieved via online algorithms. We also give an offline algorithm that maintains a $(1+\epsilon)$-approximate vertex cover in amortized $O(n^{.723}\epsilon^{-O(1)})$ time.</p></p class="citation"></blockquote><h2 id=hep-th-1>hep-th (1)</h2><h3 id=11--279290-neural-network-learning-and-quantum-gravity-stefano-lanza-2024>(1/1 | 279/290) Neural Network Learning and Quantum Gravity (Stefano Lanza, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefano Lanza. (2024)<br><strong>Neural Network Learning and Quantum Gravity</strong><br><button class=copy-to-clipboard title="Neural Network Learning and Quantum Gravity" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-th<br>Categories: cs-LG, hep-th, hep-th<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03245v1.pdf filename=2403.03245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The landscape of low-energy effective field theories <b>stemming</b> from string theory is too vast for a systematic exploration. However, the meadows of the string landscape may be fertile ground for the application of machine learning techniques. Employing neural network learning may allow for inferring novel, undiscovered properties that consistent theories in the landscape should possess, or checking conjectural statements about alleged characteristics thereof. The aim of this work is to describe to what extent the string landscape can be explored with neural network-based learning. Our analysis is motivated by recent studies that show that the string landscape is characterized by finiteness properties, emerging from its underlying tame, o-minimal structures. Indeed, employing these results, we illustrate that any low-energy effective theory of string theory is endowed with certain statistical learnability properties. Consequently, several learning problems therein formulated, including interpolations and multi-class classification problems, can be concretely addressed with machine learning, delivering results with sufficiently high accuracy.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--280290-on-demand-mobility-services-for-urban-resilience-a-review-towards-human-machine-collaborative-future-jiangbo-yu-2024>(1/1 | 280/290) On-demand Mobility Services for Urban Resilience: A Review Towards Human-Machine Collaborative Future (Jiangbo Yu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiangbo Yu. (2024)<br><strong>On-demand Mobility Services for Urban Resilience: A Review Towards Human-Machine Collaborative Future</strong><br><button class=copy-to-clipboard title="On-demand Mobility Services for Urban Resilience: A Review Towards Human-Machine Collaborative Future" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-HC, cs-RO, cs.CY<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03107v1.pdf filename=2403.03107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mobility-on-demand (MOD) services have the potential to significantly improve the adaptiveness and recovery of urban logistics and transportation infrastructure, in the wake of disruptive events. This paper presents a survey on the usage of MOD services for resilience improvement (MOD-R) and finds a noticeable increase within recent years on this topic across four main areas: resilient MOD services, novel usage of MOD-R services for improving supply chain resilience, empirical impact evaluation, and supporting technologies. MOD-R services have been utilized for <b>anomaly</b> <b>detection,</b> essential supply delivery, evacuation and rescue, on-site medical care, power grid stabilization, transit service substitution during downtime, and infrastructure and equipment repair. The review reveals integrating electrification, automation, and advanced communication technologies offers significant synergistic benefits. The review also suggests the importance of harnessing the collective capabilities of humans and intelligent machines to effectively implement versatile, multi-functional MOD-R services during crises.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--281290-note-harnessing-tellurium-nanoparticles-in-the-digital-realm-plasmon-resonance-in-the-context-of-brewsters-angle-and-the-drude-model-for-fake-news-adsorption-in-incomplete-information-games-yasuko-kawahata-2024>(1/1 | 281/290) Note: Harnessing Tellurium Nanoparticles in the Digital Realm Plasmon Resonance, in the Context of Brewster&rsquo;s Angle and the Drude Model for Fake News Adsorption in Incomplete Information Games (Yasuko Kawahata, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasuko Kawahata. (2024)<br><strong>Note: Harnessing Tellurium Nanoparticles in the Digital Realm Plasmon Resonance, in the Context of Brewster&rsquo;s Angle and the Drude Model for Fake News Adsorption in Incomplete Information Games</strong><br><button class=copy-to-clipboard title="Note: Harnessing Tellurium Nanoparticles in the Digital Realm Plasmon Resonance, in the Context of Brewster's Angle and the Drude Model for Fake News Adsorption in Incomplete Information Games" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-AI, physics-soc-ph, physics.soc-ph<br>Keyword Score: 10<br>Keywords: Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03239v1.pdf filename=2403.03239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This note explores the innovative application of soliton theory and plasmonic phenomena in modeling user behavior and engagement within digital health platforms. By introducing the concept of soliton solutions, we present a novel approach to understanding stable patterns of health improvement behaviors over time. Additionally, we delve into the role of tellurium nanoparticles and their plasmonic properties in adsorbing <b>fake</b> <b>news,</b> thereby influencing user interactions and engagement levels. Through a theoretical framework that combines nonlinear dynamics with the unique characteristics of tellurium nanoparticles, we aim to provide new insights into the dynamics of user engagement in digital health environments. Our analysis highlights the potential of soliton theory in capturing the complex, nonlinear dynamics of user behavior, while the application of plasmonic phenomena offers a promising avenue for enhancing the sensitivity and effectiveness of digital health platforms. This research ventures into an uncharted territory where optical phenomena such as Brewster&rsquo;s Angle and Snell&rsquo;s Law, along with the concept of spin solitons, are metaphorically applied to address the challenge of <b>fake</b> <b>news</b> dissemination. By exploring the analogy between light refraction, reflection, and the propagation of information in digital platforms, we unveil a novel perspective on how the &lsquo;angle&rsquo; at which information is presented can significantly affect its acceptance and spread. Additionally, we propose the use of tellurium nanoparticles to manage &lsquo;information waves&rsquo; through mechanisms akin to plasmonic resonance and soliton dynamics. This theoretical exploration aims to bridge the gap between physical sciences and digital communication, offering insights into the development of strategies for mitigating misinformation.</p></p class="citation"></blockquote><h2 id=csfl-1>cs.FL (1)</h2><h3 id=11--282290-efficient-interaction-based-offline-runtime-verification-of-distributed-systems-with-lifeline-removal-erwan-mahe-et-al-2024>(1/1 | 282/290) Efficient Interaction-Based Offline Runtime Verification of Distributed Systems with Lifeline Removal (Erwan Mahe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erwan Mahe, Boutheina Bannour, Christophe Gaston, Pascale Le Gall. (2024)<br><strong>Efficient Interaction-Based Offline Runtime Verification of Distributed Systems with Lifeline Removal</strong><br><button class=copy-to-clipboard title="Efficient Interaction-Based Offline Runtime Verification of Distributed Systems with Lifeline Removal" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.FL<br>Categories: cs-FL, cs.FL<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03057v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03057v1.pdf filename=2403.03057v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Runtime Verification (RV) refers to a family of techniques in which system executions are observed and confronted to formal specifications, with the aim of identifying faults. In Offline RV, observation is done in a first step and verification in a second, on a static artifact collected during observation. In this paper, we define an approach to offline RV of Distributed Systems (DS) against interactions. Interactions are formal models describing communications within a DS. DS are composed of subsystems deployed on different machines and interacting via message passing. Therefore, observing executions of a DS entails logging a collection of local execution traces, one for each subsystem, that we call a multi-trace. A major challenge in analyzing multi-traces is that there are no practical means to synchronize the ends of observations of all local traces. We address this via an operation, called lifeline removal, which we apply on-the-fly on the specification during verification once a local trace has been entirely analyzed. This operation removes from the interaction the specification of actions occurring on the subsystem that is no-longer observed. This may allow further execution of the specification via removing deadlocks due to the partial orders of actions. We prove the correctness of the resulting RV algorithm and introduce two optimization techniques which we also prove correct. We implement a Partial Order Reduction (POR) technique via the selection of a one-unambiguous action (as a unique first step to a linearization) which existence is determined via another use of the lifeline removal operator. Additionally, Local Analyses (LOC) i.e., the verification of local traces, can be leveraged during the global multi-trace analysis to prove failure more quickly. Experiments illustrate the application of our RV approach and the benefits of our optimizations.</p></p class="citation"></blockquote><h2 id=econgn-1>econ.GN (1)</h2><h3 id=11--283290-bias-in-generative-ai-mi-zhou-et-al-2024>(1/1 | 283/290) Bias in Generative AI (Mi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mi Zhou, Vibhanshu Abhishek, Timothy Derdenger, Jaymo Kim, Kannan Srinivasan. (2024)<br><strong>Bias in Generative AI</strong><br><button class=copy-to-clipboard title="Bias in Generative AI" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.GN<br>Categories: cs-AI, cs-CY, econ-GN, econ.GN, q-fin-EC<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02726v1.pdf filename=2403.02726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study analyzed images generated by three popular <b>generative</b> <b>artificial</b> intelligence (AI) tools - Midjourney, Stable Diffusion, and DALLE 2 - representing various occupations to investigate potential bias in AI generators. Our analysis revealed two overarching areas of concern in these AI generators, including (1) systematic gender and racial biases, and (2) subtle biases in facial expressions and appearances. Firstly, we found that all three AI generators exhibited bias against women and African Americans. Moreover, we found that the evident gender and racial biases uncovered in our analysis were even more pronounced than the status quo when compared to labor force statistics or Google images, intensifying the harmful biases we are actively striving to rectify in our society. Secondly, our study uncovered more nuanced prejudices in the portrayal of emotions and appearances. For example, women were depicted as younger with more smiles and happiness, while men were depicted as older with more neutral expressions and anger, posing a risk that <b>generative</b> <b>AI</b> models may unintentionally depict women as more submissive and less competent than men. Such nuanced biases, by their less overt nature, might be more problematic as they can permeate perceptions unconsciously and may be more difficult to rectify. Although the extent of bias varied depending on the model, the direction of bias remained consistent in both commercial and open-source AI generators. As these tools become commonplace, our study highlights the urgency to identify and mitigate various biases in <b>generative</b> <b>AI,</b> reinforcing the commitment to ensuring that AI technologies benefit all of humanity for a more inclusive future.</p></p class="citation"></blockquote><h2 id=cssd-1>cs.SD (1)</h2><h3 id=11--284290-fighting-game-adaptive-background-music-for-improved-gameplay-ibrahim-khan-et-al-2024>(1/1 | 284/290) Fighting Game Adaptive Background Music for Improved Gameplay (Ibrahim Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ibrahim Khan, Thai Van Nguyen, Chollakorn Nimpattanavong, Ruck Thawonmas. (2024)<br><strong>Fighting Game Adaptive Background Music for Improved Gameplay</strong><br><button class=copy-to-clipboard title="Fighting Game Adaptive Background Music for Improved Gameplay" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: I-2; H-5-2; H-5, cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02701v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02701v1.pdf filename=2403.02701v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents our work to enhance the background music (BGM) in DareFightingICE by adding adaptive features. The adaptive BGM consists of three different categories of instruments playing the BGM of the winner sound design from the 2022 DareFightingICE Competition. The BGM adapts by changing the volume of each category of instruments. Each category is connected to a different element of the game. We then run experiments to evaluate the adaptive BGM by using a deep <b>reinforcement</b> <b>learning</b> AI agent that only uses audio as input (Blind DL AI). The results show that the performance of the Blind DL AI improves while playing with the adaptive BGM as compared to playing without the adaptive BGM.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--285290-isummary-workload-based-personalized-summaries-for-knowledge-graphs-giannis-vassiliou-et-al-2024>(1/1 | 285/290) iSummary: Workload-based, Personalized Summaries for Knowledge Graphs (Giannis Vassiliou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giannis Vassiliou, Fanouris Alevizakis, Nikolaos Papadakis, Haridimos Kondylakis. (2024)<br><strong>iSummary: Workload-based, Personalized Summaries for Knowledge Graphs</strong><br><button class=copy-to-clipboard title="iSummary: Workload-based, Personalized Summaries for Knowledge Graphs" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 8<br>Keywords: Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02934v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02934v1.pdf filename=2403.02934v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The explosion in the size and the complexity of the available <b>Knowledge</b> <b>Graphs</b> on the web has led to the need for efficient and effective methods for their understanding and exploration. Semantic summaries have recently emerged as methods to quickly explore and understand the contents of various sources. However in most cases they are static not incorporating user needs and preferences and cannot scale. In this paper we present iSummary a novel scalable approach for constructing personalized summaries. As the size and the complexity of the <b>Knowledge</b> <b>Graphs</b> for constructing personalized summaries prohibit efficient summary construction, in our approach we exploit query logs. The main idea behind our approach is to exploit <b>knowledge</b> <b>captured</b> in existing user queries for identifying the most interesting resources and linking them constructing as such highquality personalized summaries. We present an algorithm with theoretical guarantees on the summarys quality linear in the number of queries available in the query log. We evaluate our approach using three realworld datasets and several baselines showing that our approach dominates other methods in terms of both quality and efficiency.</p></p class="citation"></blockquote><h2 id=mathst-1>math.ST (1)</h2><h3 id=11--286290-finding-super-spreaders-in-network-cascades-elchanan-mossel-et-al-2024>(1/1 | 286/290) Finding Super-spreaders in Network Cascades (Elchanan Mossel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elchanan Mossel, Anirudh Sridhar. (2024)<br><strong>Finding Super-spreaders in Network Cascades</strong><br><button class=copy-to-clipboard title="Finding Super-spreaders in Network Cascades" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: cs-IT, cs-SI, math-IT, math-PR, math-ST, math.ST, stat-TH<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03205v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03205v2.pdf filename=2403.03205v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Suppose that a cascade (e.g., an epidemic) spreads on an unknown <b>graph,</b> and only the infection times of vertices are observed. What can be learned about the <b>graph</b> from the infection times caused by multiple distinct cascades? Most of the literature on this topic focuses on the task of recovering the entire <b>graph,</b> which requires $\Omega ( \log n)$ cascades for an $n$-vertex bounded degree <b>graph.</b> Here we ask a different question: can the important parts of the <b>graph</b> be estimated from just a few (i.e., constant number) of cascades, even as $n$ grows large? In this work, we focus on identifying super-spreaders (i.e., high-degree vertices) from infection times caused by a Susceptible-Infected process on a <b>graph.</b> Our first main result shows that vertices of degree greater than $n^{3/4}$ can indeed be estimated from a constant number of cascades. Our algorithm for doing so leverages a novel connection between vertex degrees and the second derivative of the cumulative infection curve. Conversely, we show that estimating vertices of degree smaller than $n^{1/2}$ requires at least $\log(n) / \log \log (n)$ cascades. Surprisingly, this matches (up to $\log \log n$ factors) the number of cascades needed to learn the \emph{entire} <b>graph</b> if it is a tree.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--287290-equilibria-in-two-stage-facility-location-with-atomic-clients-simon-krogmann-et-al-2024>(1/1 | 287/290) Equilibria in Two-Stage Facility Location with Atomic Clients (Simon Krogmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Krogmann, Pascal Lenzner, Alexander Skopalik, Marc Uetz, Marnix C. Vos. (2024)<br><strong>Equilibria in Two-Stage Facility Location with Atomic Clients</strong><br><button class=copy-to-clipboard title="Equilibria in Two-Stage Facility Location with Atomic Clients" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-AI, cs-GT, cs.GT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03114v1.pdf filename=2403.03114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider competitive facility location as a two-stage multi-agent system with two types of clients. For a given host <b>graph</b> with weighted clients on the vertices, first facility agents strategically select vertices for opening their facilities. Then, the clients strategically select which of the opened facilities in their neighborhood to patronize. Facilities want to attract as much client weight as possible, clients want to minimize congestion on the chosen facility. All recently studied versions of this model assume that clients can split their weight strategically. We consider clients with unsplittable weights, but allow mixed strategies. So clients may randomize over which facility to patronize. Besides modeling a natural client behavior, this subtle change yields drastic changes, e.g., for a given facility placement, qualitatively different client equilibria are possible. As our main result, we show that pure subgame perfect equilibria always exist if all client weights are identical. For this, we use a novel potential function argument, employing a hierarchical classification of the clients and sophisticated rounding in each step. In contrast, for non-identical clients, we show that deciding the existence of even approximately stable states is computationally intractable. On the positive side, we give a tight bound of 2 on the price of anarchy which implies high social welfare of equilibria, if they exist.</p></p class="citation"></blockquote><h2 id=mathco-2>math.CO (2)</h2><h3 id=12--288290-the-clique-chromatic-number-of-sparse-random-graphs-manuel-fernandez-v-et-al-2024>(1/2 | 288/290) The clique chromatic number of sparse random graphs (Manuel Fernandez V et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manuel Fernandez V, Lutz Warnke. (2024)<br><strong>The clique chromatic number of sparse random graphs</strong><br><button class=copy-to-clipboard title="The clique chromatic number of sparse random graphs" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C15, 05C80, 60C05, cs-DM, math-CO, math-PR, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03013v1.pdf filename=2403.03013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The clique chromatic number of a <b>graph</b> is the smallest number of colors in a vertex coloring so that no maximal clique is monochromatic. In this paper, we determine the order of magnitude of the clique chromatic number of the random <b>graph</b> G_{n,p} for most edge-probabilities p in the range n^{-2/5} \ll p \ll 1. This resolves open problems and questions of Lichev, Mitsche and Warnke as well as Alon and Krievelevich. One major proof difficulty stems from high-degree vertices, which prevent maximal cliques in their neighborhoods: we deal with these vertices by an intricate union bound argument, that combines the probabilistic method with new degree counting arguments in order to enable Janson&rsquo;s inequality. This way we determine the asymptotics of the clique chromatic number of G_{n,p} in some ranges, and discover a surprising new phenomenon that contradicts earlier predictions for edge-probabilities p close to n^{-2/5}.</p></p class="citation"></blockquote><h3 id=22--289290-face-hitting-dominating-sets-in-planar-graphs-p-francis-et-al-2024>(2/2 | 289/290) Face-hitting Dominating Sets in Planar Graphs (P. Francis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>P. Francis, Abraham M. Illickan, Lijo M. Jose, Deepak Rajendraprasad. (2024)<br><strong>Face-hitting Dominating Sets in Planar Graphs</strong><br><button class=copy-to-clipboard title="Face-hitting Dominating Sets in Planar Graphs" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02808v1.pdf filename=2403.02808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A dominating set of a <b>graph</b> $G$ is a subset $S$ of its vertices such that each vertex of $G$ not in $S$ has a neighbor in $S$. A face-hitting set of a plane <b>graph</b> $G$ is a set $T$ of vertices in $G$ such that every face of $G$ contains at least one vertex of $T$. We show that the vertex-set of every plane (multi-)graph without isolated vertices, self-loops or $2$-faces can be partitioned into two disjoint sets so that both the sets are dominating and face-hitting. We also show that all the three assumptions above are necessary for the conclusion. As a corollary, we show that every $n$-vertex simple plane triangulation has a dominating set of size at most $(1 - \alpha)n/2$, where $\alpha n$ is the maximum size of an independent set in the triangulation. Matheson and Tarjan [European J. Combin., 1996] conjectured that every plane triangulation with a sufficiently large number of vertices $n$ has a dominating set of size at most $n / 4$. Currently, the best known general bound for this is by Christiansen, Rotenberg and Rutschmann [SODA, 2024] who showed that every plane triangulation on $n > 10$ vertices has a dominating set of size at most $2n/7$. Our corollary improves their bound for $n$-vertex plane triangulations which contain a maximal independent set of size either less than $2n/7$ or more than $3n/7$.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--290290-space-complexity-of-euclidean-clustering-xiaoyi-zhu-et-al-2024>(1/1 | 290/290) Space Complexity of Euclidean Clustering (Xiaoyi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyi Zhu, Yuxiang Tian, Lingxiao Huang, Zengfeng Huang. (2024)<br><strong>Space Complexity of Euclidean Clustering</strong><br><button class=copy-to-clipboard title="Space Complexity of Euclidean Clustering" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs-DS, cs.CG<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02971v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02971v2.pdf filename=2403.02971v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The $(k, z)$-Clustering problem in Euclidean space $\mathbb{R}^d$ has been extensively studied. Given the scale of data involved, compression methods for the Euclidean $(k, z)$-Clustering problem, such as data compression and dimension reduction, have received significant attention in the literature. However, the space complexity of the <b>clustering</b> problem, specifically, the number of bits required to compress the cost function within a multiplicative error $\varepsilon$, remains unclear in existing literature. This paper initiates the study of space complexity for Euclidean $(k, z)$-Clustering and offers both upper and lower bounds. Our space bounds are nearly tight when $k$ is constant, indicating that storing a coreset, a well-known data compression approach, serves as the optimal compression scheme. Furthermore, our lower bound result for $(k, z)$-Clustering establishes a tight space bound of $\Theta( n d )$ for terminal embedding, where $n$ represents the dataset size. Our technical approach leverages new geometric insights for principal angles and discrepancy methods, which may hold independent interest.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.06</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.08</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-56>cs.CL (56)</a><ul><li><a href=#156--1290-mathscale-scaling-instruction-tuning-for-mathematical-reasoning-zhengyang-tang-et-al-2024>(1/56 | 1/290) MathScale: Scaling Instruction Tuning for Mathematical Reasoning (Zhengyang Tang et al., 2024)</a></li><li><a href=#256--2290-design2code-how-far-are-we-from-automating-front-end-engineering-chenglei-si-et-al-2024>(2/56 | 2/290) Design2Code: How Far Are We From Automating Front-End Engineering? (Chenglei Si et al., 2024)</a></li><li><a href=#356--3290-zero-shot-cross-lingual-document-level-event-causality-identification-with-heterogeneous-graph-contrastive-transfer-learning-zhitao-he-et-al-2024>(3/56 | 3/290) Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning (Zhitao He et al., 2024)</a></li><li><a href=#456--4290-improving-event-definition-following-for-zero-shot-event-detection-zefan-cai-et-al-2024>(4/56 | 4/290) Improving Event Definition Following For Zero-Shot Event Detection (Zefan Cai et al., 2024)</a></li><li><a href=#556--5290-paradise-evaluating-implicit-planning-skills-of-language-models-with-procedural-warnings-and-tips-dataset-arda-uzunoglu-et-al-2024>(5/56 | 5/290) PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset (Arda Uzunoglu et al., 2024)</a></li><li><a href=#656--6290-ruleprompt-weakly-supervised-text-classification-with-prompting-plms-and-self-iterative-logical-rules-miaomiao-li-et-al-2024>(6/56 | 6/290) RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules (Miaomiao Li et al., 2024)</a></li><li><a href=#756--7290-causal-prompting-debiasing-large-language-model-prompting-based-on-front-door-adjustment-congzhi-zhang-et-al-2024>(7/56 | 7/290) Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment (Congzhi Zhang et al., 2024)</a></li><li><a href=#856--8290-evidence-focused-fact-summarization-for-knowledge-augmented-zero-shot-question-answering-sungho-ko-et-al-2024>(8/56 | 8/290) Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering (Sungho Ko et al., 2024)</a></li><li><a href=#956--9290-jmi-at-semeval-2024-task-3-two-step-approach-for-multimodal-ecac-using-in-context-learning-with-gpt-and-instruction-tuned-llama-models-arefa-et-al-2024>(9/56 | 9/290) JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models (Arefa et al., 2024)</a></li><li><a href=#1056--10290-towards-training-a-chinese-large-language-model-for-anesthesiology-zhonghai-wang-et-al-2024>(10/56 | 10/290) Towards Training A Chinese Large Language Model for Anesthesiology (Zhonghai Wang et al., 2024)</a></li><li><a href=#1156--11290-benchmarking-the-text-to-sql-capability-of-large-language-models-a-comprehensive-evaluation-bin-zhang-et-al-2024>(11/56 | 11/290) Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation (Bin Zhang et al., 2024)</a></li><li><a href=#1256--12290-learning-to-maximize-mutual-information-for-chain-of-thought-distillation-xin-chen-et-al-2024>(12/56 | 12/290) Learning to Maximize Mutual Information for Chain-of-Thought Distillation (Xin Chen et al., 2024)</a></li><li><a href=#1356--13290-scope-of-large-language-models-for-mining-emerging-opinions-in-online-health-discourse-joseph-gatto-et-al-2024>(13/56 | 13/290) Scope of Large Language Models for Mining Emerging Opinions in Online Health Discourse (Joseph Gatto et al., 2024)</a></li><li><a href=#1456--14290-mad-libs-are-all-you-need-augmenting-cross-domain-document-level-event-argument-data-joseph-gatto-et-al-2024>(14/56 | 14/290) Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data (Joseph Gatto et al., 2024)</a></li><li><a href=#1556--15290-language-guided-exploration-for-rl-agents-in-text-environments-hitesh-golchha-et-al-2024>(15/56 | 15/290) Language Guided Exploration for RL Agents in Text Environments (Hitesh Golchha et al., 2024)</a></li><li><a href=#1656--16290-a-general-and-flexible-multi-concept-parsing-framework-for-multilingual-semantic-matching-dong-yao-et-al-2024>(16/56 | 16/290) A General and Flexible Multi-concept Parsing Framework for Multilingual Semantic Matching (Dong Yao et al., 2024)</a></li><li><a href=#1756--17290-role-prompting-guided-domain-adaptation-with-general-capability-preserve-for-large-language-models-rui-wang-et-al-2024>(17/56 | 17/290) Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models (Rui Wang et al., 2024)</a></li><li><a href=#1856--18290-eliciting-better-multilingual-structured-reasoning-from-llms-through-code-bryan-li-et-al-2024>(18/56 | 18/290) Eliciting Better Multilingual Structured Reasoning from LLMs through Code (Bryan Li et al., 2024)</a></li><li><a href=#1956--19290-injecagent-benchmarking-indirect-prompt-injections-in-tool-integrated-large-language-model-agents-qiusi-zhan-et-al-2024>(19/56 | 19/290) InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents (Qiusi Zhan et al., 2024)</a></li><li><a href=#2056--20290-hargpt-are-llms-zero-shot-human-activity-recognizers-sijie-ji-et-al-2024>(20/56 | 20/290) HARGPT: Are LLMs Zero-Shot Human Activity Recognizers? (Sijie Ji et al., 2024)</a></li><li><a href=#2156--21290-causal-walk-debiasing-multi-hop-fact-verification-with-front-door-adjustment-congzhi-zhang-et-al-2024>(21/56 | 21/290) Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment (Congzhi Zhang et al., 2024)</a></li><li><a href=#2256--22290-in-dialogues-we-learn-towards-personalized-dialogue-without-pre-defined-profiles-through-in-dialogue-learning-chuanqi-cheng-et-al-2024>(22/56 | 22/290) &lsquo;In Dialogues We Learn&rsquo;: Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning (Chuanqi Cheng et al., 2024)</a></li><li><a href=#2356--23290-an-empirical-study-of-llm-as-a-judge-for-llm-evaluation-fine-tuned-judge-models-are-task-specific-classifiers-hui-huang-et-al-2024>(23/56 | 23/290) An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers (Hui Huang et al., 2024)</a></li><li><a href=#2456--24290-revisiting-meta-evaluation-for-grammatical-error-correction-masamune-kobayashi-et-al-2024>(24/56 | 24/290) Revisiting Meta-evaluation for Grammatical Error Correction (Masamune Kobayashi et al., 2024)</a></li><li><a href=#2556--25290-updating-the-minimum-information-about-clinical-artificial-intelligence-mi-claim-checklist-for-generative-modeling-research-brenda-y-miao-et-al-2024>(25/56 | 25/290) Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research (Brenda Y. Miao et al., 2024)</a></li><li><a href=#2656--26290-adding-multimodal-capabilities-to-a-text-only-translation-model-vipin-vijayan-et-al-2024>(26/56 | 26/290) Adding Multimodal Capabilities to a Text-only Translation Model (Vipin Vijayan et al., 2024)</a></li><li><a href=#2756--27290-simucourt-building-judicial-decision-making-agents-with-real-world-judgement-documents-zhitao-he-et-al-2024>(27/56 | 27/290) SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents (Zhitao He et al., 2024)</a></li><li><a href=#2856--28290-crossing-linguistic-horizons-finetuning-and-comprehensive-evaluation-of-vietnamese-large-language-models-sang-t-truong-et-al-2024>(28/56 | 28/290) Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models (Sang T. Truong et al., 2024)</a></li><li><a href=#2956--29290-android-in-the-zoo-chain-of-action-thought-for-gui-agents-jiwen-zhang-et-al-2024>(29/56 | 29/290) Android in the Zoo: Chain-of-Action-Thought for GUI Agents (Jiwen Zhang et al., 2024)</a></li><li><a href=#3056--30290-guardrail-baselines-for-unlearning-in-llms-pratiksha-thaker-et-al-2024>(30/56 | 30/290) Guardrail Baselines for Unlearning in LLMs (Pratiksha Thaker et al., 2024)</a></li><li><a href=#3156--31290-book2dial-generating-teacher-student-interactions-from-textbooks-for-cost-effective-development-of-educational-chatbots-junling-wang-et-al-2024>(31/56 | 31/290) Book2Dial: Generating Teacher-Student Interactions from Textbooks for Cost-Effective Development of Educational Chatbots (Junling Wang et al., 2024)</a></li><li><a href=#3256--32290-demonstrating-mutual-reinforcement-effect-through-information-flow-chengguang-gan-et-al-2024>(32/56 | 32/290) Demonstrating Mutual Reinforcement Effect through Information Flow (Chengguang Gan et al., 2024)</a></li><li><a href=#3356--33290-in-search-of-truth-an-interrogation-approach-to-hallucination-detection-yakir-yehuda-et-al-2024>(33/56 | 33/290) In Search of Truth: An Interrogation Approach to Hallucination Detection (Yakir Yehuda et al., 2024)</a></li><li><a href=#3456--34290-magid-an-automated-pipeline-for-generating-synthetic-multi-modal-datasets-hossein-aboutalebi-et-al-2024>(34/56 | 34/290) MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets (Hossein Aboutalebi et al., 2024)</a></li><li><a href=#3556--35290-detecting-concrete-visual-tokens-for-multimodal-machine-translation-braeden-bowen-et-al-2024>(35/56 | 35/290) Detecting Concrete Visual Tokens for Multimodal Machine Translation (Braeden Bowen et al., 2024)</a></li><li><a href=#3656--36290-alpaca-against-vicuna-using-llms-to-uncover-memorization-of-llms-aly-m-kassem-et-al-2024>(36/56 | 36/290) Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs (Aly M. Kassem et al., 2024)</a></li><li><a href=#3756--37290-data-augmentation-using-llms-data-perspectives-learning-paradigms-and-challenges-bosheng-ding-et-al-2024>(37/56 | 37/290) Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges (Bosheng Ding et al., 2024)</a></li><li><a href=#3856--38290-breeze-7b-technical-report-chan-jan-hsu-et-al-2024>(38/56 | 38/290) Breeze-7B Technical Report (Chan-Jan Hsu et al., 2024)</a></li><li><a href=#3956--39290-found-in-the-middle-how-language-models-use-long-contexts-better-via-plug-and-play-positional-encoding-zhenyu-zhang-et-al-2024>(39/56 | 39/290) Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding (Zhenyu Zhang et al., 2024)</a></li><li><a href=#4056--40290-exploring-the-limitations-of-large-language-models-in-compositional-relation-reasoning-jinman-zhao-et-al-2024>(40/56 | 40/290) Exploring the Limitations of Large Language Models in Compositional Relation Reasoning (Jinman Zhao et al., 2024)</a></li><li><a href=#4156--41290-best-of-both-worlds-a-pliable-and-generalizable-neuro-symbolic-approach-for-relation-classification-robert-vacareanu-et-al-2024>(41/56 | 41/290) Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification (Robert Vacareanu et al., 2024)</a></li><li><a href=#4256--42290-cogenesis-a-framework-collaborating-large-and-small-language-models-for-secure-context-aware-instruction-following-kaiyan-zhang-et-al-2024>(42/56 | 42/290) CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following (Kaiyan Zhang et al., 2024)</a></li><li><a href=#4356--43290-angry-men-sad-women-large-language-models-reflect-gendered-stereotypes-in-emotion-attribution-flor-miriam-plaza-del-arco-et-al-2024>(43/56 | 43/290) Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution (Flor Miriam Plaza-del-Arco et al., 2024)</a></li><li><a href=#4456--44290-knowagent-knowledge-augmented-planning-for-llm-based-agents-yuqi-zhu-et-al-2024>(44/56 | 44/290) KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents (Yuqi Zhu et al., 2024)</a></li><li><a href=#4556--45290-learning-to-use-tools-via-cooperative-and-interactive-agents-zhengliang-shi-et-al-2024>(45/56 | 45/290) Learning to Use Tools via Cooperative and Interactive Agents (Zhengliang Shi et al., 2024)</a></li><li><a href=#4656--46290-socratic-reasoning-improves-positive-text-rewriting-anmol-goel-et-al-2024>(46/56 | 46/290) Socratic Reasoning Improves Positive Text Rewriting (Anmol Goel et al., 2024)</a></li><li><a href=#4756--47290-dppa-pruning-method-for-large-language-model-to-model-merging-yaochen-zhu-et-al-2024>(47/56 | 47/290) DPPA: Pruning Method for Large Language Model to Model Merging (Yaochen Zhu et al., 2024)</a></li><li><a href=#4856--48290-dp-cre-continual-relation-extraction-via-decoupled-contrastive-learning-and-memory-structure-preservation-mengyi-huang-et-al-2024>(48/56 | 48/290) DP-CRE: Continual Relation Extraction via Decoupled Contrastive Learning and Memory Structure Preservation (Mengyi Huang et al., 2024)</a></li><li><a href=#4956--49290-the-case-for-evaluating-multimodal-translation-models-on-text-datasets-vipin-vijayan-et-al-2024>(49/56 | 49/290) The Case for Evaluating Multimodal Translation Models on Text Datasets (Vipin Vijayan et al., 2024)</a></li><li><a href=#5056--50290-diverse-deciphering-internet-views-on-the-us-military-through-video-comment-stance-analysis-a-novel-benchmark-dataset-for-stance-classification-iain-j-cruickshank-et-al-2024>(50/56 | 50/290) DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification (Iain J. Cruickshank et al., 2024)</a></li><li><a href=#5156--51290-in-memory-learning-a-declarative-learning-framework-for-large-language-models-bo-wang-et-al-2024>(51/56 | 51/290) In-Memory Learning: A Declarative Learning Framework for Large Language Models (Bo Wang et al., 2024)</a></li><li><a href=#5256--52290-a-second-look-on-bass----boosting-abstractive-summarization-with-unified-semantic-graphs----a-replication-study-osman-alperen-koraş-et-al-2024>(52/56 | 52/290) A Second Look on BASS &ndash; Boosting Abstractive Summarization with Unified Semantic Graphs &ndash; A Replication Study (Osman Alperen Koraş et al., 2024)</a></li><li><a href=#5356--53290-reliable-adaptable-and-attributable-language-models-with-retrieval-akari-asai-et-al-2024>(53/56 | 53/290) Reliable, Adaptable, and Attributable Language Models with Retrieval (Akari Asai et al., 2024)</a></li><li><a href=#5456--54290-aix-speed-playback-speed-optimization-using-listening-comprehension-of-speech-recognition-models-kazuki-kawamura-et-al-2024>(54/56 | 54/290) AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models (Kazuki Kawamura et al., 2024)</a></li><li><a href=#5556--55290-ai-literacy-in-low-resource-languagesinsights-from-creating-ai-in-yoruba-videos-wuraola-oyewusi-2024>(55/56 | 55/290) AI Literacy in Low-Resource Languages:Insights from creating AI in Yoruba videos (Wuraola Oyewusi, 2024)</a></li><li><a href=#5656--56290-finreport-explainable-stock-earnings-forecasting-via-news-factor-analyzing-model-xiangyu-li-et-al-2024>(56/56 | 56/290) FinReport: Explainable Stock Earnings Forecasting via News Factor Analyzing Model (Xiangyu Li et al., 2024)</a></li></ul></li><li><a href=#csai-30>cs.AI (30)</a><ul><li><a href=#130--57290-emerging-synergies-between-large-language-models-and-machine-learning-in-ecommerce-recommendations-xiaonan-xu-et-al-2024>(1/30 | 57/290) Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations (Xiaonan Xu et al., 2024)</a></li><li><a href=#230--58290-clevr-poc-reasoning-intensive-visual-question-answering-in-partially-observable-environments-savitha-sam-abraham-et-al-2024>(2/30 | 58/290) CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments (Savitha Sam Abraham et al., 2024)</a></li><li><a href=#330--59290-knowledge-graphs-as-context-sources-for-llm-based-explanations-of-learning-recommendations-hasan-abu-rasheed-et-al-2024>(3/30 | 59/290) Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations (Hasan Abu-Rasheed et al., 2024)</a></li><li><a href=#430--60290-domain-agnostic-mutual-prompting-for-unsupervised-domain-adaptation-zhekai-du-et-al-2024>(4/30 | 60/290) Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation (Zhekai Du et al., 2024)</a></li><li><a href=#530--61290-ai-insights-a-case-study-on-utilizing-chatgpt-intelligence-for-research-paper-analysis-anjalee-de-silva-et-al-2024>(5/30 | 61/290) AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis (Anjalee De Silva et al., 2024)</a></li><li><a href=#630--62290-easyquant-an-efficient-data-free-quantization-algorithm-for-llms-hanlin-tang-et-al-2024>(6/30 | 62/290) EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs (Hanlin Tang et al., 2024)</a></li><li><a href=#730--63290-localized-zeroth-order-prompt-optimization-wenyang-hu-et-al-2024>(7/30 | 63/290) Localized Zeroth-Order Prompt Optimization (Wenyang Hu et al., 2024)</a></li><li><a href=#830--64290-towards-democratized-flood-risk-management-an-advanced-ai-assistant-enabled-by-gpt-4-for-enhanced-interpretability-and-public-engagement-rafaela-martelo-et-al-2024>(8/30 | 64/290) Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement (Rafaela Martelo et al., 2024)</a></li><li><a href=#930--65290-evolution-transformer-in-context-evolutionary-optimization-robert-tjarko-lange-et-al-2024>(9/30 | 65/290) Evolution Transformer: In-Context Evolutionary Optimization (Robert Tjarko Lange et al., 2024)</a></li><li><a href=#1030--66290-a-comprehensive-survey-on-process-oriented-automatic-text-summarization-with-exploration-of-llm-based-methods-hanlei-jin-et-al-2024>(10/30 | 66/290) A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods (Hanlei Jin et al., 2024)</a></li><li><a href=#1130--67290-evaluating-and-optimizing-educational-content-with-large-language-model-judgments-joy-he-yueya-et-al-2024>(11/30 | 67/290) Evaluating and Optimizing Educational Content with Large Language Model Judgments (Joy He-Yueya et al., 2024)</a></li><li><a href=#1230--68290-pps-qmix-periodically-parameter-sharing-for-accelerating-convergence-of-multi-agent-reinforcement-learning-ke-zhang-et-al-2024>(12/30 | 68/290) PPS-QMIX: Periodically Parameter Sharing for Accelerating Convergence of Multi-Agent Reinforcement Learning (Ke Zhang et al., 2024)</a></li><li><a href=#1330--69290-word-importance-explains-how-prompts-affect-language-model-outputs-stefan-hackmann-et-al-2024>(13/30 | 69/290) Word Importance Explains How Prompts Affect Language Model Outputs (Stefan Hackmann et al., 2024)</a></li><li><a href=#1430--70290-multi-scale-subgraph-contrastive-learning-yanbei-liu-et-al-2024>(14/30 | 70/290) Multi-Scale Subgraph Contrastive Learning (Yanbei Liu et al., 2024)</a></li><li><a href=#1530--71290-race-sm-reinforcement-learning-based-autonomous-control-for-social-on-ramp-merging-jordan-poots-2024>(15/30 | 71/290) RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging (Jordan Poots, 2024)</a></li><li><a href=#1630--72290-should-we-fear-large-language-models-a-structural-analysis-of-the-human-reasoning-system-for-elucidating-llm-capabilities-and-risks-through-the-lens-of-heideggers-philosophy-jianqiiu-zhang-2024>(16/30 | 72/290) Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger&rsquo;s Philosophy (Jianqiiu Zhang, 2024)</a></li><li><a href=#1730--73290-leveraging-federated-learning-and-edge-computing-for-recommendation-systems-within-cloud-computing-networks-yaqian-qi-et-al-2024>(17/30 | 73/290) Leveraging Federated Learning and Edge Computing for Recommendation Systems within Cloud Computing Networks (Yaqian Qi et al., 2024)</a></li><li><a href=#1830--74290-opex-a-component-wise-analysis-of-llm-centric-agents-in-embodied-instruction-following-haochen-shi-et-al-2024>(18/30 | 74/290) OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following (Haochen Shi et al., 2024)</a></li><li><a href=#1930--75290-wikitableedit-a-benchmark-for-table-editing-by-natural-language-instruction-zheng-li-et-al-2024>(19/30 | 75/290) WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction (Zheng Li et al., 2024)</a></li><li><a href=#2030--76290-minimum-topology-attacks-for-graph-neural-networks-mengmei-zhang-et-al-2024>(20/30 | 76/290) Minimum Topology Attacks for Graph Neural Networks (Mengmei Zhang et al., 2024)</a></li><li><a href=#2130--77290-the-case-for-globalizing-fairness-a-mixed-methods-study-on-colonialism-ai-and-health-in-africa-mercy-asiedu-et-al-2024>(21/30 | 77/290) The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism, AI, and Health in Africa (Mercy Asiedu et al., 2024)</a></li><li><a href=#2230--78290-reaching-consensus-in-cooperative-multi-agent-reinforcement-learning-with-goal-imagination-liangzhou-wang-et-al-2024>(22/30 | 78/290) Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination (Liangzhou Wang et al., 2024)</a></li><li><a href=#2330--79290-curatron-complete-robust-preference-data-for-robust-alignment-of-large-language-models-son-the-nguyen-et-al-2024>(23/30 | 79/290) CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models (Son The Nguyen et al., 2024)</a></li><li><a href=#2430--80290-chatgpt4pcg-2-competition-prompt-engineering-for-science-birds-level-generation-pittawat-taveekitworachai-et-al-2024>(24/30 | 80/290) ChatGPT4PCG 2 Competition: Prompt Engineering for Science Birds Level Generation (Pittawat Taveekitworachai et al., 2024)</a></li><li><a href=#2530--81290-fuzzy-datalogexists-over-arbitrary-t-norms-matthias-lanzinger-et-al-2024>(25/30 | 81/290) Fuzzy Datalog$^\exists$ over Arbitrary t-Norms (Matthias Lanzinger et al., 2024)</a></li><li><a href=#2630--82290-towards-general-computer-control-a-multimodal-agent-for-red-dead-redemption-ii-as-a-case-study-weihao-tan-et-al-2024>(26/30 | 82/290) Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study (Weihao Tan et al., 2024)</a></li><li><a href=#2730--83290-a-general-approach-to-enhance-the-survivability-of-backdoor-attacks-by-decision-path-coupling-yufei-zhao-et-al-2024>(27/30 | 83/290) A general approach to enhance the survivability of backdoor attacks by decision path coupling (Yufei Zhao et al., 2024)</a></li><li><a href=#2830--84290-dynst-dynamic-sparse-training-for-resource-constrained-spatio-temporal-forecasting-hao-wu-et-al-2024>(28/30 | 84/290) DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal Forecasting (Hao Wu et al., 2024)</a></li><li><a href=#2930--85290-precise-extraction-of-deep-learning-models-via-side-channel-attacks-on-edgeendpoint-devices-younghan-lee-et-al-2024>(29/30 | 85/290) Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices (Younghan Lee et al., 2024)</a></li><li><a href=#3030--86290-reconstruction-for-sparse-view-tomography-of-long-objects-applied-to-imaging-in-the-wood-industry-buda-bajić-et-al-2024>(30/30 | 86/290) Reconstruction for Sparse View Tomography of Long Objects Applied to Imaging in the Wood Industry (Buda Bajić et al., 2024)</a></li></ul></li><li><a href=#cslg-42>cs.LG (42)</a><ul><li><a href=#142--87290-privacy-aware-semantic-cache-for-large-language-models-waris-gill-et-al-2024>(1/42 | 87/290) Privacy-Aware Semantic Cache for Large Language Models (Waris Gill et al., 2024)</a></li><li><a href=#242--88290-rehabilitation-exercise-quality-assessment-through-supervised-contrastive-learning-with-hard-and-soft-negatives-mark-karlov-et-al-2024>(2/42 | 88/290) Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives (Mark Karlov et al., 2024)</a></li><li><a href=#342--89290-how-well-can-transformers-emulate-in-context-newtons-method-angeliki-giannou-et-al-2024>(3/42 | 89/290) How Well Can Transformers Emulate In-context Newton&rsquo;s Method? (Angeliki Giannou et al., 2024)</a></li><li><a href=#442--90290-unsupervised-spatio-temporal-state-estimation-for-fine-grained-adaptive-anomaly-diagnosis-of-industrial-cyber-physical-systems-haili-sun-et-al-2024>(4/42 | 90/290) Unsupervised Spatio-Temporal State Estimation for Fine-grained Adaptive Anomaly Diagnosis of Industrial Cyber-physical Systems (Haili Sun et al., 2024)</a></li><li><a href=#542--91290-flguard-byzantine-robust-federated-learning-via-ensemble-of-contrastive-models-younghan-lee-et-al-2024>(5/42 | 91/290) FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive Models (Younghan Lee et al., 2024)</a></li><li><a href=#642--92290-controllable-prompt-tuning-for-balancing-group-distributional-robustness-hoang-phan-et-al-2024>(6/42 | 92/290) Controllable Prompt Tuning For Balancing Group Distributional Robustness (Hoang Phan et al., 2024)</a></li><li><a href=#742--93290-semi-supervised-graph-representation-learning-with-human-centric-explanation-for-predicting-fatty-liver-disease-so-yeon-kim-et-al-2024>(7/42 | 93/290) Semi-Supervised Graph Representation Learning with Human-centric Explanation for Predicting Fatty Liver Disease (So Yeon Kim et al., 2024)</a></li><li><a href=#842--94290-a-zero-shot-reinforcement-learning-strategy-for-autonomous-guidewire-navigation-valentina-scarponi-et-al-2024>(8/42 | 94/290) A Zero-Shot Reinforcement Learning Strategy for Autonomous Guidewire Navigation (Valentina Scarponi et al., 2024)</a></li><li><a href=#942--95290-fedhcdr-federated-cross-domain-recommendation-with-hypergraph-signal-decoupling-hongyu-zhang-et-al-2024>(9/42 | 95/290) FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling (Hongyu Zhang et al., 2024)</a></li><li><a href=#1042--96290-training-machine-learning-models-at-the-edge-a-survey-aymen-rayane-khouas-et-al-2024>(10/42 | 96/290) Training Machine Learning models at the Edge: A Survey (Aymen Rayane Khouas et al., 2024)</a></li><li><a href=#1142--97290-learning-augmented-online-minimization-of-age-of-information-and-transmission-costs-zhongdong-liu-et-al-2024>(11/42 | 97/290) Learning-augmented Online Minimization of Age of Information and Transmission Costs (Zhongdong Liu et al., 2024)</a></li><li><a href=#1242--98290-behavior-generation-with-latent-actions-seungjae-lee-et-al-2024>(12/42 | 98/290) Behavior Generation with Latent Actions (Seungjae Lee et al., 2024)</a></li><li><a href=#1342--99290-pooling-image-datasets-with-multiple-covariate-shift-and-imbalance-sotirios-panagiotis-chytas-et-al-2024>(13/42 | 99/290) Pooling Image Datasets With Multiple Covariate Shift and Imbalance (Sotirios Panagiotis Chytas et al., 2024)</a></li><li><a href=#1442--100290-the-wmdp-benchmark-measuring-and-reducing-malicious-use-with-unlearning-nathaniel-li-et-al-2024>(14/42 | 100/290) The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning (Nathaniel Li et al., 2024)</a></li><li><a href=#1542--101290-unsupervised-learning-approaches-for-identifying-icu-patient-subgroups-do-results-generalise-harry-mayne-et-al-2024>(15/42 | 101/290) Unsupervised Learning Approaches for Identifying ICU Patient Subgroups: Do Results Generalise? (Harry Mayne et al., 2024)</a></li><li><a href=#1642--102290-taylorshift-shifting-the-complexity-of-self-attention-from-squared-to-linear-and-back-using-taylor-softmax-tobias-christian-nauen-et-al-2024>(16/42 | 102/290) TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax (Tobias Christian Nauen et al., 2024)</a></li><li><a href=#1742--103290-sofim-stochastic-optimization-using-regularized-fisher-information-matrix-gayathri-c-et-al-2024>(17/42 | 103/290) SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix (Gayathri C et al., 2024)</a></li><li><a href=#1842--104290-time-weaver-a-conditional-time-series-generation-model-sai-shankar-narasimhan-et-al-2024>(18/42 | 104/290) Time Weaver: A Conditional Time Series Generation Model (Sai Shankar Narasimhan et al., 2024)</a></li><li><a href=#1942--105290-testam-a-time-enhanced-spatio-temporal-attention-model-with-mixture-of-experts-hyunwook-lee-et-al-2024>(19/42 | 105/290) TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts (Hyunwook Lee et al., 2024)</a></li><li><a href=#2042--106290-improving-variational-autoencoder-estimation-from-incomplete-data-with-mixture-variational-families-vaidotas-simkus-et-al-2024>(20/42 | 106/290) Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families (Vaidotas Simkus et al., 2024)</a></li><li><a href=#2142--107290-injecttst-a-transformer-method-of-injecting-global-information-into-independent-channels-for-long-time-series-forecasting-ce-chi-et-al-2024>(21/42 | 107/290) InjectTST: A Transformer Method of Injecting Global Information into Independent Channels for Long Time Series Forecasting (Ce Chi et al., 2024)</a></li><li><a href=#2242--108290-g4-attention-deep-learning-model-with-attention-for-predicting-dna-g-quadruplexes-shrimon-mukherjee-et-al-2024>(22/42 | 108/290) G4-Attention: Deep Learning Model with Attention for predicting DNA G-Quadruplexes (Shrimon Mukherjee et al., 2024)</a></li><li><a href=#2342--109290-sgd-with-partial-hessian-for-deep-neural-networks-optimization-ying-sun-et-al-2024>(23/42 | 109/290) SGD with Partial Hessian for Deep Neural Networks Optimization (Ying Sun et al., 2024)</a></li><li><a href=#2442--110290-splagger-split-aggregation-for-meta-reinforcement-learning-jacob-beck-et-al-2024>(24/42 | 110/290) SplAgger: Split Aggregation for Meta-Reinforcement Learning (Jacob Beck et al., 2024)</a></li><li><a href=#2542--111290-solution-simplex-clustering-for-heterogeneous-federated-learning-dennis-grinwald-et-al-2024>(25/42 | 111/290) Solution Simplex Clustering for Heterogeneous Federated Learning (Dennis Grinwald et al., 2024)</a></li><li><a href=#2642--112290-dynamic-gaussian-graph-operator-learning-parametric-partial-differential-equations-in-arbitrary-discrete-mechanics-problems-chu-wang-et-al-2024>(26/42 | 112/290) Dynamic Gaussian Graph Operator: Learning parametric partial differential equations in arbitrary discrete mechanics problems (Chu Wang et al., 2024)</a></li><li><a href=#2742--113290-learning-to-defer-to-a-population-a-meta-learning-approach-dharmesh-tailor-et-al-2024>(27/42 | 113/290) Learning to Defer to a Population: A Meta-Learning Approach (Dharmesh Tailor et al., 2024)</a></li><li><a href=#2842--114290-leveraging-federated-learning-for-automatic-detection-of-clopidogrel-treatment-failures-samuel-kim-et-al-2024>(28/42 | 114/290) Leveraging Federated Learning for Automatic Detection of Clopidogrel Treatment Failures (Samuel Kim et al., 2024)</a></li><li><a href=#2942--115290-not-all-tickets-are-equal-and-we-know-it-guiding-pruning-with-domain-specific-knowledge-intekhab-hossain-et-al-2024>(29/42 | 115/290) Not all tickets are equal and we know it: Guiding pruning with domain-specific knowledge (Intekhab Hossain et al., 2024)</a></li><li><a href=#3042--116290-lc-tsalis-inf-generalized-best-of-both-worlds-linear-contextual-bandits-masahiro-kato-et-al-2024>(30/42 | 116/290) LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits (Masahiro Kato et al., 2024)</a></li><li><a href=#3142--117290-deep-learned-compression-for-radio-frequency-signal-classification-armani-rodriguez-et-al-2024>(31/42 | 117/290) Deep-Learned Compression for Radio-Frequency Signal Classification (Armani Rodriguez et al., 2024)</a></li><li><a href=#3242--118290-emergent-equivariance-in-deep-ensembles-jan-e-gerken-et-al-2024>(32/42 | 118/290) Emergent Equivariance in Deep Ensembles (Jan E. Gerken et al., 2024)</a></li><li><a href=#3342--119290-recall-oriented-continual-learning-with-generative-adversarial-meta-model-haneol-kang-et-al-2024>(33/42 | 119/290) Recall-Oriented Continual Learning with Generative Adversarial Meta-Model (Haneol Kang et al., 2024)</a></li><li><a href=#3442--120290-on-the-asymptotic-mean-square-error-optimality-of-diffusion-probabilistic-models-benedikt-fesl-et-al-2024>(34/42 | 120/290) On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models (Benedikt Fesl et al., 2024)</a></li><li><a href=#3542--121290-remove-that-square-root-a-new-efficient-scale-invariant-version-of-adagrad-sayantan-choudhury-et-al-2024>(35/42 | 121/290) Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad (Sayantan Choudhury et al., 2024)</a></li><li><a href=#3642--122290-a-note-on-high-probability-analysis-of-algorithms-with-exponential-sub-gaussian-and-general-light-tails-amit-attia-et-al-2024>(36/42 | 122/290) A Note on High-Probability Analysis of Algorithms with Exponential, Sub-Gaussian, and General Light Tails (Amit Attia et al., 2024)</a></li><li><a href=#3742--123290-pareto-optimal-estimation-and-policy-learning-on-short-term-and-long-term-treatment-effects-yingrong-wang-et-al-2024>(37/42 | 123/290) Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects (Yingrong Wang et al., 2024)</a></li><li><a href=#3842--124290-tartanaviation-image-speech-and-ads-b-trajectory-datasets-for-terminal-airspace-operations-jay-patrikar-et-al-2024>(38/42 | 124/290) TartanAviation: Image, Speech, and ADS-B Trajectory Datasets for Terminal Airspace Operations (Jay Patrikar et al., 2024)</a></li><li><a href=#3942--125290-credibility-aware-multi-modal-fusion-using-probabilistic-circuits-sahil-sidheekh-et-al-2024>(39/42 | 125/290) Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits (Sahil Sidheekh et al., 2024)</a></li><li><a href=#4042--126290-crispr-ensemble-model-mohammad-rostami-et-al-2024>(40/42 | 126/290) CRISPR: Ensemble Model (Mohammad Rostami et al., 2024)</a></li><li><a href=#4142--127290-dirichlet-based-per-sample-weighting-by-transition-matrix-for-noisy-label-learning-heesun-bae-et-al-2024>(41/42 | 127/290) Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning (HeeSun Bae et al., 2024)</a></li><li><a href=#4242--128290-dnnlasso-scalable-graph-learning-for-matrix-variate-data-meixia-lin-et-al-2024>(42/42 | 128/290) DNNLasso: Scalable Graph Learning for Matrix-Variate Data (Meixia Lin et al., 2024)</a></li></ul></li><li><a href=#eesssp-2>eess.SP (2)</a><ul><li><a href=#12--129290-arnn-attentive-recurrent-neural-network-for-multi-channel-eeg-signals-to-identify-epileptic-seizures-salim-rukhsar-et-al-2024>(1/2 | 129/290) ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures (Salim Rukhsar et al., 2024)</a></li><li><a href=#22--130290-low-complexity-linear-decoupling-of-users-for-uplink-massive-mu-mimo-detection-s-sowmya-et-al-2024>(2/2 | 130/290) Low-Complexity Linear Decoupling of Users for Uplink Massive MU-MIMO Detection (S. Sowmya et al., 2024)</a></li></ul></li><li><a href=#csro-6>cs.RO (6)</a><ul><li><a href=#16--131290-moka-open-vocabulary-robotic-manipulation-through-mark-based-visual-prompting-fangchen-liu-et-al-2024>(1/6 | 131/290) MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting (Fangchen Liu et al., 2024)</a></li><li><a href=#26--132290-spacehopper-a-small-scale-legged-robot-for-exploring-low-gravity-celestial-bodies-alexander-spiridonov-et-al-2024>(2/6 | 132/290) SpaceHopper: A Small-Scale Legged Robot for Exploring Low-Gravity Celestial Bodies (Alexander Spiridonov et al., 2024)</a></li><li><a href=#36--133290-single-channel-robot-ego-speech-filtering-during-human-robot-interaction-yue-li-et-al-2024>(3/6 | 133/290) Single-Channel Robot Ego-Speech Filtering during Human-Robot Interaction (Yue Li et al., 2024)</a></li><li><a href=#46--134290-splat-nav-safe-real-time-robot-navigation-in-gaussian-splatting-maps-timothy-chen-et-al-2024>(4/6 | 134/290) Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps (Timothy Chen et al., 2024)</a></li><li><a href=#56--135290-unidoormanip-learning-universal-door-manipulation-policy-over-large-scale-and-diverse-door-manipulation-environments-yu-li-et-al-2024>(5/6 | 135/290) UniDoorManip: Learning Universal Door Manipulation Policy Over Large-scale and Diverse Door Manipulation Environments (Yu Li et al., 2024)</a></li><li><a href=#66--136290-active-information-gathering-for-long-horizon-navigation-under-uncertainty-by-learning-the-value-of-information-raihan-islam-arnob-et-al-2024>(6/6 | 136/290) Active Information Gathering for Long-Horizon Navigation Under Uncertainty by Learning the Value of Information (Raihan Islam Arnob et al., 2024)</a></li></ul></li><li><a href=#csse-5>cs.SE (5)</a><ul><li><a href=#15--137290-generative-software-engineering-yuan-huang-et-al-2024>(1/5 | 137/290) Generative Software Engineering (Yuan Huang et al., 2024)</a></li><li><a href=#25--138290-learn-to-code-sustainably-an-empirical-study-on-llm-based-green-code-generation-tina-vartziotis-et-al-2024>(2/5 | 138/290) Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation (Tina Vartziotis et al., 2024)</a></li><li><a href=#35--139290-tooling-offline-runtime-verification-against-interaction-models--recognizing-sliced-behaviors-using-parameterized-simulation-erwan-mahe-et-al-2024>(3/5 | 139/290) Tooling Offline Runtime Verification against Interaction Models : recognizing sliced behaviors using parameterized simulation (Erwan Mahe et al., 2024)</a></li><li><a href=#45--140290-deep-configuration-performance-learning-a-systematic-survey-and-taxonomy-jingzhi-gong-et-al-2024>(4/5 | 140/290) Deep Configuration Performance Learning: A Systematic Survey and Taxonomy (Jingzhi Gong et al., 2024)</a></li><li><a href=#55--141290-alloyinecore-embedding-of-first-order-relational-logic-into-meta-object-facility-for-automated-model-reasoning-ferhat-erata-et-al-2024>(5/5 | 141/290) AlloyInEcore: Embedding of First-Order Relational Logic into Meta-Object Facility for Automated Model Reasoning (Ferhat Erata et al., 2024)</a></li></ul></li><li><a href=#csmm-3>cs.MM (3)</a><ul><li><a href=#13--142290-sniffer-multimodal-large-language-model-for-explainable-out-of-context-misinformation-detection-peng-qi-et-al-2024>(1/3 | 142/290) SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection (Peng Qi et al., 2024)</a></li><li><a href=#23--143290-mmofusion-multi-modal-co-speech-motion-generation-with-diffusion-model-sen-wang-et-al-2024>(2/3 | 143/290) MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model (Sen Wang et al., 2024)</a></li><li><a href=#33--144290-optimizing-mobile-friendly-viewport-prediction-for-live-360-degree-video-streaming-lei-zhang-et-al-2024>(3/3 | 144/290) Optimizing Mobile-Friendly Viewport Prediction for Live 360-Degree Video Streaming (Lei Zhang et al., 2024)</a></li></ul></li><li><a href=#cscv-54>cs.CV (54)</a><ul><li><a href=#154--145290-promptkd-unsupervised-prompt-distillation-for-vision-language-models-zheng-li-et-al-2024>(1/54 | 145/290) PromptKD: Unsupervised Prompt Distillation for Vision-Language Models (Zheng Li et al., 2024)</a></li><li><a href=#254--146290-modeling-collaborator-enabling-subjective-vision-classification-with-minimal-human-effort-via-llm-tool-use-imad-eddine-toubal-et-al-2024>(2/54 | 146/290) Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use (Imad Eddine Toubal et al., 2024)</a></li><li><a href=#354--147290-interactive-continual-learning-fast-and-slow-thinking-biqing-qi-et-al-2024>(3/54 | 147/290) Interactive Continual Learning: Fast and Slow Thinking (Biqing Qi et al., 2024)</a></li><li><a href=#454--148290-chatgpt-and-biometrics-an-assessment-of-face-recognition-gender-detection-and-age-estimation-capabilities-ahmad-hassanpour-et-al-2024>(4/54 | 148/290) ChatGPT and biometrics: an assessment of face recognition, gender detection, and age estimation capabilities (Ahmad Hassanpour et al., 2024)</a></li><li><a href=#554--149290-finetuned-multimodal-language-models-are-high-quality-image-text-data-filters-weizhi-wang-et-al-2024>(5/54 | 149/290) Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters (Weizhi Wang et al., 2024)</a></li><li><a href=#654--150290-few-shot-learner-parameterization-by-diffusion-time-steps-zhongqi-yue-et-al-2024>(6/54 | 150/290) Few-shot Learner Parameterization by Diffusion Time-steps (Zhongqi Yue et al., 2024)</a></li><li><a href=#754--151290-dinov2-based-self-supervised-learning-for-few-shot-medical-image-segmentation-lev-ayzenberg-et-al-2024>(7/54 | 151/290) DINOv2 based Self Supervised Learning For Few Shot Medical Image Segmentation (Lev Ayzenberg et al., 2024)</a></li><li><a href=#854--152290-enhancing-generalization-in-medical-visual-question-answering-tasks-via-gradient-guided-model-perturbation-gang-liu-et-al-2024>(8/54 | 152/290) Enhancing Generalization in Medical Visual Question Answering Tasks via Gradient-Guided Model Perturbation (Gang Liu et al., 2024)</a></li><li><a href=#954--153290-multi-modal-instruction-tuned-llms-with-fine-grained-visual-perception-junwen-he-et-al-2024>(9/54 | 153/290) Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception (Junwen He et al., 2024)</a></li><li><a href=#1054--154290-imgtrojan-jailbreaking-vision-language-models-with-one-image-xijia-tao-et-al-2024>(10/54 | 154/290) ImgTrojan: Jailbreaking Vision-Language Models with ONE Image (Xijia Tao et al., 2024)</a></li><li><a href=#1154--155290-towards-robust-federated-learning-via-logits-calibration-on-non-iid-data-yu-qiao-et-al-2024>(11/54 | 155/290) Towards Robust Federated Learning via Logits Calibration on Non-IID Data (Yu Qiao et al., 2024)</a></li><li><a href=#1254--156290-enhancing-vision-language-pre-training-with-rich-supervisions-yuan-gao-et-al-2024>(12/54 | 156/290) Enhancing Vision-Language Pre-training with Rich Supervisions (Yuan Gao et al., 2024)</a></li><li><a href=#1354--157290-learning-without-exact-guidance-updating-large-scale-high-resolution-land-cover-maps-from-low-resolution-historical-labels-zhuohong-li-et-al-2024>(13/54 | 157/290) Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels (Zhuohong Li et al., 2024)</a></li><li><a href=#1454--158290-madtp-multimodal-alignment-guided-dynamic-token-pruning-for-accelerating-vision-language-transformer-jianjian-cao-et-al-2024>(14/54 | 158/290) MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer (Jianjian Cao et al., 2024)</a></li><li><a href=#1554--159290-ddf-a-novel-dual-domain-image-fusion-strategy-for-remote-sensing-image-semantic-segmentation-with-unsupervised-domain-adaptation-lingyan-ran-et-al-2024>(15/54 | 159/290) DDF: A Novel Dual-Domain Image Fusion Strategy for Remote Sensing Image Semantic Segmentation with Unsupervised Domain Adaptation (Lingyan Ran et al., 2024)</a></li><li><a href=#1654--160290-domainverse-a-benchmark-towards-real-world-distribution-shifts-for-tuning-free-adaptive-domain-generalization-feng-hou-et-al-2024>(16/54 | 160/290) DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization (Feng Hou et al., 2024)</a></li><li><a href=#1754--161290-fastocc-accelerating-3d-occupancy-prediction-by-fusing-the-2d-birds-eye-view-and-perspective-view-jiawei-hou-et-al-2024>(17/54 | 161/290) FastOcc: Accelerating 3D Occupancy Prediction by Fusing the 2D Bird&rsquo;s-Eye View and Perspective View (Jiawei Hou et al., 2024)</a></li><li><a href=#1854--162290-scaling-rectified-flow-transformers-for-high-resolution-image-synthesis-patrick-esser-et-al-2024>(18/54 | 162/290) Scaling Rectified Flow Transformers for High-Resolution Image Synthesis (Patrick Esser et al., 2024)</a></li><li><a href=#1954--163290-solving-the-bongard-logo-problem-by-modeling-a-probabilistic-model-ruizhuo-song-et-al-2024>(19/54 | 163/290) Solving the bongard-logo problem by modeling a probabilistic model (Ruizhuo Song et al., 2024)</a></li><li><a href=#2054--164290-dual-mean-teacher-an-unbiased-semi-supervised-framework-for-audio-visual-source-localization-yuxin-guo-et-al-2024>(20/54 | 164/290) Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization (Yuxin Guo et al., 2024)</a></li><li><a href=#2154--165290-mikasa-multi-key-anchor--scene-aware-transformer-for-3d-visual-grounding-chun-peng-chang-et-al-2024>(21/54 | 165/290) MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding (Chun-Peng Chang et al., 2024)</a></li><li><a href=#2254--166290-doubly-abductive-counterfactual-inference-for-text-based-image-editing-xue-song-et-al-2024>(22/54 | 166/290) Doubly Abductive Counterfactual Inference for Text-based Image Editing (Xue Song et al., 2024)</a></li><li><a href=#2354--167290-false-positive-sampling-based-data-augmentation-for-enhanced-3d-object-detection-accuracy-jiyong-oh-et-al-2024>(23/54 | 167/290) False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy (Jiyong Oh et al., 2024)</a></li><li><a href=#2454--168290-a-unified-framework-for-microscopy-defocus-deblur-with-multi-pyramid-transformer-and-contrastive-learning-yuelin-zhang-et-al-2024>(24/54 | 168/290) A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning (Yuelin Zhang et al., 2024)</a></li><li><a href=#2554--169290-feast-your-eyes-mixture-of-resolution-adaptation-for-multimodal-large-language-models-gen-luo-et-al-2024>(25/54 | 169/290) Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models (Gen Luo et al., 2024)</a></li><li><a href=#2654--170290-enhancing-conceptual-understanding-in-multimodal-contrastive-learning-through-hard-negative-samples-philipp-j-rösch-et-al-2024>(26/54 | 170/290) Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples (Philipp J. Rösch et al., 2024)</a></li><li><a href=#2754--171290-veglue-testing-visual-entailment-systems-via-object-aligned-joint-erasing-zhiyuan-chang-et-al-2024>(27/54 | 171/290) VEglue: Testing Visual Entailment Systems via Object-Aligned Joint Erasing (Zhiyuan Chang et al., 2024)</a></li><li><a href=#2854--172290-learning-zero-shot-material-states-segmentation-by-implanting-natural-image-patterns-in-synthetic-data-sagi-eppel-et-al-2024>(28/54 | 172/290) Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data (Sagi Eppel et al., 2024)</a></li><li><a href=#2954--173290-self-supervised-3d-patient-modeling-with-multi-modal-attentive-fusion-meng-zheng-et-al-2024>(29/54 | 173/290) Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion (Meng Zheng et al., 2024)</a></li><li><a href=#3054--174290-palmprobnet-a-probabilistic-approach-to-understanding-palm-distributions-in-ecuadorian-tropical-forest-via-transfer-learning-kangning-cui-et-al-2024>(30/54 | 174/290) PalmProbNet: A Probabilistic Approach to Understanding Palm Distributions in Ecuadorian Tropical Forest via Transfer Learning (Kangning Cui et al., 2024)</a></li><li><a href=#3154--175290-zero-led-zero-reference-lighting-estimation-diffusion-model-for-low-light-image-enhancement-jinhong-he-et-al-2024>(31/54 | 175/290) Zero-LED: Zero-Reference Lighting Estimation Diffusion Model for Low-Light Image Enhancement (Jinhong He et al., 2024)</a></li><li><a href=#3254--176290-are-dense-labels-always-necessary-for-3d-object-detection-from-point-cloud-chenqiang-gao-et-al-2024>(32/54 | 176/290) Are Dense Labels Always Necessary for 3D Object Detection from Point Cloud? (Chenqiang Gao et al., 2024)</a></li><li><a href=#3354--177290-hunter-unsupervised-human-centric-3d-detection-via-transferring-knowledge-from-synthetic-instances-to-real-scenes-yichen-yao-et-al-2024>(33/54 | 177/290) HUNTER: Unsupervised Human-centric 3D Detection via Transferring Knowledge from Synthetic Instances to Real Scenes (Yichen Yao et al., 2024)</a></li><li><a href=#3454--178290-learning-group-activity-features-through-person-attribute-prediction-chihiro-nakatani-et-al-2024>(34/54 | 178/290) Learning Group Activity Features Through Person Attribute Prediction (Chihiro Nakatani et al., 2024)</a></li><li><a href=#3554--179290-semantic-human-mesh-reconstruction-with-textures-xiaoyu-zhan-et-al-2024>(35/54 | 179/290) Semantic Human Mesh Reconstruction with Textures (Xiaoyu Zhan et al., 2024)</a></li><li><a href=#3654--180290-cracknex-a-few-shot-low-light-crack-segmentation-model-based-on-retinex-theory-for-uav-inspections-zhen-yao-et-al-2024>(36/54 | 180/290) CrackNex: a Few-shot Low-light Crack Segmentation Model Based on Retinex Theory for UAV Inspections (Zhen Yao et al., 2024)</a></li><li><a href=#3754--181290-f3loc-fusion-and-filtering-for-floorplan-localization-changan-chen-et-al-2024>(37/54 | 181/290) F$^3$Loc: Fusion and Filtering for Floorplan Localization (Changan Chen et al., 2024)</a></li><li><a href=#3854--182290-far-flexible-accurate-and-robust-6dof-relative-camera-pose-estimation-chris-rockwell-et-al-2024>(38/54 | 182/290) FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation (Chris Rockwell et al., 2024)</a></li><li><a href=#3954--183290-triple-cfn-restructuring-conceptual-spaces-for-enhancing-abstract-reasoning-process-ruizhuo-song-et-al-2024>(39/54 | 183/290) Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract Reasoning process (Ruizhuo Song et al., 2024)</a></li><li><a href=#4054--184290-neural-image-compression-with-text-guided-encoding-for-both-pixel-level-and-perceptual-fidelity-hagyeong-lee-et-al-2024>(40/54 | 184/290) Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity (Hagyeong Lee et al., 2024)</a></li><li><a href=#4154--185290-cross-domain-image-conversion-by-cycledm-sho-shimotsumagari-et-al-2024>(41/54 | 185/290) Cross-Domain Image Conversion by CycleDM (Sho Shimotsumagari et al., 2024)</a></li><li><a href=#4254--186290-enhancing-the-rate-distortion-perception-flexibility-of-learned-image-codecs-with-conditional-diffusion-decoders-daniele-mari-et-al-2024>(42/54 | 186/290) Enhancing the Rate-Distortion-Perception Flexibility of Learned Image Codecs with Conditional Diffusion Decoders (Daniele Mari et al., 2024)</a></li><li><a href=#4354--187290-revisiting-confidence-estimation-towards-reliable-failure-prediction-fei-zhu-et-al-2024>(43/54 | 187/290) Revisiting Confidence Estimation: Towards Reliable Failure Prediction (Fei Zhu et al., 2024)</a></li><li><a href=#4454--188290-activead-planning-oriented-active-learning-for-end-to-end-autonomous-driving-han-lu-et-al-2024>(44/54 | 188/290) ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving (Han Lu et al., 2024)</a></li><li><a href=#4554--189290-tuning-free-noise-rectification-for-high-fidelity-image-to-video-generation-weijie-li-et-al-2024>(45/54 | 189/290) Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation (Weijie Li et al., 2024)</a></li><li><a href=#4654--190290-bootstrapping-rare-object-detection-in-high-resolution-satellite-imagery-akram-zaytar-et-al-2024>(46/54 | 190/290) Bootstrapping Rare Object Detection in High-Resolution Satellite Imagery (Akram Zaytar et al., 2024)</a></li><li><a href=#4754--191290-deep-common-feature-mining-for-efficient-video-semantic-segmentation-yaoyan-zheng-et-al-2024>(47/54 | 191/290) Deep Common Feature Mining for Efficient Video Semantic Segmentation (Yaoyan Zheng et al., 2024)</a></li><li><a href=#4854--192290-bsdp-brain-inspired-streaming-dual-level-perturbations-for-online-open-world-object-detection-yu-chen-et-al-2024>(48/54 | 192/290) BSDP: Brain-inspired Streaming Dual-level Perturbations for Online Open World Object Detection (Yu Chen et al., 2024)</a></li><li><a href=#4954--193290-what-do-we-learn-from-inverting-clip-models-hamid-kazemi-et-al-2024>(49/54 | 193/290) What do we learn from inverting CLIP models? (Hamid Kazemi et al., 2024)</a></li><li><a href=#5054--194290-why-not-use-your-textbook-knowledge-enhanced-procedure-planning-of-instructional-videos-kumaranage-ravindu-yasas-nagasinghe-et-al-2024>(50/54 | 194/290) Why Not Use Your Textbook? Knowledge-Enhanced Procedure Planning of Instructional Videos (Kumaranage Ravindu Yasas Nagasinghe et al., 2024)</a></li><li><a href=#5154--195290-motion-corrected-moving-average-including-post-hoc-temporal-information-for-improved-video-segmentation-robert-mendel-et-al-2024>(51/54 | 195/290) Motion-Corrected Moving Average: Including Post-Hoc Temporal Information for Improved Video Segmentation (Robert Mendel et al., 2024)</a></li><li><a href=#5254--196290-a-backpack-full-of-skills-egocentric-video-understanding-with-diverse-task-perspectives-simone-alberto-peirone-et-al-2024>(52/54 | 196/290) A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives (Simone Alberto Peirone et al., 2024)</a></li><li><a href=#5354--197290-enhancing-long-term-person-re-identification-using-global-local-body-part-and-head-streams-duy-tran-thanh-et-al-2024>(53/54 | 197/290) Enhancing Long-Term Person Re-Identification Using Global, Local Body Part, and Head Streams (Duy Tran Thanh et al., 2024)</a></li><li><a href=#5454--198290-holovic-large-scale-dataset-and-benchmark-for-multi-sensor-holographic-intersection-and-vehicle-infrastructure-cooperative-cong-ma-et-al-2024>(54/54 | 198/290) HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative (Cong Ma et al., 2024)</a></li></ul></li><li><a href=#eessiv-4>eess.IV (4)</a><ul><li><a href=#14--199290-enhancing-weakly-supervised-3d-medical-image-segmentation-through-probabilistic-aware-learning-zhaoxin-fan-et-al-2024>(1/4 | 199/290) Enhancing Weakly Supervised 3D Medical Image Segmentation through Probabilistic-aware Learning (Zhaoxin Fan et al., 2024)</a></li><li><a href=#24--200290-low-res-leads-the-way-improving-generalization-for-super-resolution-by-self-supervised-learning-haoyu-chen-et-al-2024>(2/4 | 200/290) Low-Res Leads the Way: Improving Generalization for Super-Resolution by Self-Supervised Learning (Haoyu Chen et al., 2024)</a></li><li><a href=#34--201290-anatomix-anatomy-aware-data-augmentation-for-multi-organ-segmentation-chang-liu-et-al-2024>(3/4 | 201/290) AnatoMix: Anatomy-aware Data Augmentation for Multi-organ Segmentation (Chang Liu et al., 2024)</a></li><li><a href=#44--202290-speckle-noise-reduction-in-ultrasound-images-using-denoising-auto-encoder-with-skip-connection-suraj-bhute-et-al-2024>(4/4 | 202/290) Speckle Noise Reduction in Ultrasound Images using Denoising Auto-encoder with Skip Connection (Suraj Bhute et al., 2024)</a></li></ul></li><li><a href=#physicscomp-ph-1>physics.comp-ph (1)</a><ul><li><a href=#11--203290-quantum-many-body-physics-calculations-with-large-language-models-haining-pan-et-al-2024>(1/1 | 203/290) Quantum Many-Body Physics Calculations with Large Language Models (Haining Pan et al., 2024)</a></li></ul></li><li><a href=#eessas-3>eess.AS (3)</a><ul><li><a href=#13--204290-naturalspeech-3-zero-shot-speech-synthesis-with-factorized-codec-and-diffusion-models-zeqian-ju-et-al-2024>(1/3 | 204/290) NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models (Zeqian Ju et al., 2024)</a></li><li><a href=#23--205290-unpaired-signal-to-signal-translation-with-1d-conditional-gans-eric-easthope-2024>(2/3 | 205/290) (Un)paired signal-to-signal translation with 1D conditional GANs (Eric Easthope, 2024)</a></li><li><a href=#33--206290-attentionstitch-how-attention-solves-the-speech-editing-problem-antonios-alexos-et-al-2024>(3/3 | 206/290) AttentionStitch: How Attention Solves the Speech Editing Problem (Antonios Alexos et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--207290-rethinking-clustered-federated-learning-in-noma-enhanced-wireless-networks-yushen-lin-et-al-2024>(1/1 | 207/290) Rethinking Clustered Federated Learning in NOMA Enhanced Wireless Networks (Yushen Lin et al., 2024)</a></li></ul></li><li><a href=#cshc-5>cs.HC (5)</a><ul><li><a href=#15--208290-its-the-only-thing-i-can-trust-envisioning-large-language-model-use-by-autistic-workers-for-communication-assistance-jiwoong-jang-et-al-2024>(1/5 | 208/290) &lsquo;It&rsquo;s the only thing I can trust&rsquo;: Envisioning Large Language Model Use by Autistic Workers for Communication Assistance (JiWoong Jang et al., 2024)</a></li><li><a href=#25--209290-hints-sensemaking-on-large-collections-of-documents-with-hypergraph-visualization-and-intelligent-agents-sam-yu-te-lee-et-al-2024>(2/5 | 209/290) HINTs: Sensemaking on large collections of documents with Hypergraph visualization and INTelligent agents (Sam Yu-Te Lee et al., 2024)</a></li><li><a href=#35--210290-large-language-models-and-video-games-a-preliminary-scoping-review-penny-sweetser-2024>(3/5 | 210/290) Large Language Models and Video Games: A Preliminary Scoping Review (Penny Sweetser, 2024)</a></li><li><a href=#45--211290-citizen-science-and-machine-learning-for-research-and-nature-conservation-the-case-of-eurasian-lynx-free-ranging-rodents-and-insects-kinga-skorupska-et-al-2024>(4/5 | 211/290) Citizen Science and Machine Learning for Research and Nature Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects (Kinga Skorupska et al., 2024)</a></li><li><a href=#55--212290-data-driven-ergonomic-risk-assessment-of-complex-hand-intensive-manufacturing-processes-anand-krishnan-et-al-2024>(5/5 | 212/290) Data-Driven Ergonomic Risk Assessment of Complex Hand-intensive Manufacturing Processes (Anand Krishnan et al., 2024)</a></li></ul></li><li><a href=#physicsao-ph-1>physics.ao-ph (1)</a><ul><li><a href=#11--213290-fast-scale-adaptive-and-uncertainty-aware-downscaling-of-earth-system-model-fields-with-generative-foundation-models-philipp-hess-et-al-2024>(1/1 | 213/290) Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models (Philipp Hess et al., 2024)</a></li></ul></li><li><a href=#cscr-9>cs.CR (9)</a><ul><li><a href=#19--214290-here-comes-the-ai-worm-unleashing-zero-click-worms-that-target-genai-powered-applications-stav-cohen-et-al-2024>(1/9 | 214/290) Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications (Stav Cohen et al., 2024)</a></li><li><a href=#29--215290-enhancing-security-in-federated-learning-through-adaptive-consensus-based-model-update-validation-zahir-alsulaimawi-2024>(2/9 | 215/290) Enhancing Security in Federated Learning through Adaptive Consensus-Based Model Update Validation (Zahir Alsulaimawi, 2024)</a></li><li><a href=#39--216290-federated-learning-under-attack-exposing-vulnerabilities-through-data-poisoning-attacks-in-computer-networks-ehsan-nowroozi-et-al-2024>(3/9 | 216/290) Federated Learning Under Attack: Exposing Vulnerabilities through Data Poisoning Attacks in Computer Networks (Ehsan Nowroozi et al., 2024)</a></li><li><a href=#49--217290-blockchain-enhanced-uav-networks-for-post-disaster-communication-a-decentralized-flocking-approach-sana-hafeez-et-al-2024>(4/9 | 217/290) Blockchain-Enhanced UAV Networks for Post-Disaster Communication: A Decentralized Flocking Approach (Sana Hafeez et al., 2024)</a></li><li><a href=#59--218290-robust-federated-learning-mitigates-client-side-training-data-distribution-inference-attacks-yichang-xu-et-al-2024>(5/9 | 218/290) Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks (Yichang Xu et al., 2024)</a></li><li><a href=#69--219290-towards-an-ai-enhanced-cyber-threat-intelligence-processing-pipeline-lampis-alevizos-et-al-2024>(6/9 | 219/290) Towards an AI-Enhanced Cyber Threat Intelligence Processing Pipeline (Lampis Alevizos et al., 2024)</a></li><li><a href=#79--220290-mitigating-label-flipping-attacks-in-malicious-url-detectors-using-ensemble-trees-ehsan-nowroozi-et-al-2024>(7/9 | 220/290) Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees (Ehsan Nowroozi et al., 2024)</a></li><li><a href=#89--221290-xai-based-detection-of-adversarial-attacks-on-deepfake-detectors-ben-pinhasov-et-al-2024>(8/9 | 221/290) XAI-Based Detection of Adversarial Attacks on Deepfake Detectors (Ben Pinhasov et al., 2024)</a></li><li><a href=#99--222290-self-adaptive-traffic-anomaly-detection-system-for-iot-smart-home-environments-naoto-watanabe-et-al-2024>(9/9 | 222/290) Self-adaptive Traffic Anomaly Detection System for IoT Smart Home Environments (Naoto Watanabe et al., 2024)</a></li></ul></li><li><a href=#cspl-2>cs.PL (2)</a><ul><li><a href=#12--223290-mars-20-a-toolchain-for-modeling-analysis-verification-and-code-generation-of-cyber-physical-systems-bohua-zhan-et-al-2024>(1/2 | 223/290) Mars 2.0: A Toolchain for Modeling, Analysis, Verification and Code Generation of Cyber-Physical Systems (Bohua Zhan et al., 2024)</a></li><li><a href=#22--224290-verieql-bounded-equivalence-verification-for-complex-sql-queries-with-integrity-constraints-yang-he-et-al-2024>(2/2 | 224/290) VeriEQL: Bounded Equivalence Verification for Complex SQL Queries with Integrity Constraints (Yang He et al., 2024)</a></li></ul></li><li><a href=#csit-5>cs.IT (5)</a><ul><li><a href=#15--225290-tensor-decomposition-based-time-varying-channel-estimation-for-mmwave-mimo-ofdm-systems-ruizhe-wang-et-al-2024>(1/5 | 225/290) Tensor Decomposition-based Time Varying Channel Estimation for mmWave MIMO-OFDM Systems (Ruizhe Wang et al., 2024)</a></li><li><a href=#25--226290-scalable-syndrome-based-neural-decoders-for-bit-interleaved-coded-modulations-gastón-de-boni-rovella-et-al-2024>(2/5 | 226/290) Scalable Syndrome-based Neural Decoders for Bit-Interleaved Coded Modulations (Gastón De Boni Rovella et al., 2024)</a></li><li><a href=#35--227290-spatially-non-stationary-xl-mimo-channel-estimation-a-three-layer-generalized-approximate-message-passing-method-anzheng-tang-et-al-2024>(3/5 | 227/290) Spatially Non-Stationary XL-MIMO Channel Estimation: A Three-Layer Generalized Approximate Message Passing Method (Anzheng Tang et al., 2024)</a></li><li><a href=#45--228290-low-complexity-channel-estimation-for-ris-assisted-thz-systems-with-beam-split-xin-su-et-al-2024>(4/5 | 228/290) Low Complexity Channel Estimation for RIS-Assisted THz Systems with Beam Split (Xin Su et al., 2024)</a></li><li><a href=#55--229290-linear-codes-for-hyperdimensional-computing-netanel-raviv-2024>(5/5 | 229/290) Linear Codes for Hyperdimensional Computing (Netanel Raviv, 2024)</a></li></ul></li><li><a href=#q-bioqm-2>q-bio.QM (2)</a><ul><li><a href=#12--230290-from-noise-to-signal-unveiling-treatment-effects-from-digital-health-data-through-pharmacology-informed-neural-sde-samira-pakravan-et-al-2024>(1/2 | 230/290) From Noise to Signal: Unveiling Treatment Effects from Digital Health Data through Pharmacology-Informed Neural-SDE (Samira Pakravan et al., 2024)</a></li><li><a href=#22--231290-vqsynery-robust-drug-synergy-prediction-with-vector-quantization-mechanism-jiawei-wu-et-al-2024>(2/2 | 231/290) VQSynery: Robust Drug Synergy Prediction With Vector Quantization Mechanism (Jiawei Wu et al., 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--232290-mem-elements-based-neuromorphic-hardware-for-neural-network-application-ankur-singh-2024>(1/2 | 232/290) Mem-elements based Neuromorphic Hardware for Neural Network Application (Ankur Singh, 2024)</a></li><li><a href=#22--233290-g-evonas-evolutionary-neural-architecture-search-based-on-network-growth-juan-zou-et-al-2024>(2/2 | 233/290) G-EvoNAS: Evolutionary Neural Architecture Search Based on Network Growth (Juan Zou et al., 2024)</a></li></ul></li><li><a href=#csdl-2>cs.DL (2)</a><ul><li><a href=#12--234290-paperweaver-enriching-topical-paper-alerts-by-contextualizing-recommended-papers-with-user-collected-papers-yoonjoo-lee-et-al-2024>(1/2 | 234/290) PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers (Yoonjoo Lee et al., 2024)</a></li><li><a href=#22--235290-acemap-knowledge-discovery-through-academic-graph-xinbing-wang-et-al-2024>(2/2 | 235/290) AceMap: Knowledge Discovery through Academic Graph (Xinbing Wang et al., 2024)</a></li></ul></li><li><a href=#quant-ph-3>quant-ph (3)</a><ul><li><a href=#13--236290-quantum-mixed-state-self-attention-network-fu-chen-et-al-2024>(1/3 | 236/290) Quantum Mixed-State Self-Attention Network (Fu Chen et al., 2024)</a></li><li><a href=#23--237290-graph-learning-for-parameter-prediction-of-quantum-approximate-optimization-algorithm-zhiding-liang-et-al-2024>(2/3 | 237/290) Graph Learning for Parameter Prediction of Quantum Approximate Optimization Algorithm (Zhiding Liang et al., 2024)</a></li><li><a href=#33--238290-quantum-algorithms-in-a-superposition-of-spacetimes-omri-shmueli-2024>(3/3 | 238/290) Quantum Algorithms in a Superposition of Spacetimes (Omri Shmueli, 2024)</a></li></ul></li><li><a href=#cset-1>cs.ET (1)</a><ul><li><a href=#11--239290-spintronic-implementation-of-unet-for-image-segmentation-venkatesh-vadde-et-al-2024>(1/1 | 239/290) Spintronic Implementation of UNet for Image Segmentation (Venkatesh Vadde et al., 2024)</a></li></ul></li><li><a href=#csir-6>cs.IR (6)</a><ul><li><a href=#16--240290-contrastive-pre-training-for-deep-session-data-understanding-zixuan-li-et-al-2024>(1/6 | 240/290) Contrastive Pre-training for Deep Session Data Understanding (Zixuan Li et al., 2024)</a></li><li><a href=#26--241290-chatcite-llm-agent-with-human-workflow-guidance-for-comparative-literature-summary-yutong-li-et-al-2024>(2/6 | 241/290) ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary (Yutong Li et al., 2024)</a></li><li><a href=#36--242290-a-distance-metric-learning-model-based-on-variational-information-bottleneck-yaodan-zhang-et-al-2024>(3/6 | 242/290) A Distance Metric Learning Model Based On Variational Information Bottleneck (YaoDan Zhang et al., 2024)</a></li><li><a href=#46--243290-learning-to-ask-critical-questions-for-assisting-product-search-zixuan-li-et-al-2024>(4/6 | 243/290) Learning to Ask Critical Questions for Assisting Product Search (Zixuan Li et al., 2024)</a></li><li><a href=#56--244290-uplift-modeling-for-target-user-attacks-on-recommender-systems-wenjie-wang-et-al-2024>(5/6 | 244/290) Uplift Modeling for Target User Attacks on Recommender Systems (Wenjie Wang et al., 2024)</a></li><li><a href=#66--245290-search-intenion-network-for-personalized-query-auto-completion-in-e-commerce-wei-bao-et-al-2024>(6/6 | 245/290) Search Intenion Network for Personalized Query Auto-Completion in E-Commerce (Wei Bao et al., 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#12--246290-federated-learning-using-coupled-tensor-train-decomposition-xiangtao-zhang-et-al-2024>(1/2 | 246/290) Federated Learning Using Coupled Tensor Train Decomposition (Xiangtao Zhang et al., 2024)</a></li><li><a href=#22--247290-distributed-openmp-offloading-of-openmc-on-intel-gpu-max-accelerators-yehonatan-fridman-et-al-2024>(2/2 | 247/290) Distributed OpenMP Offloading of OpenMC on Intel GPU MAX Accelerators (Yehonatan Fridman et al., 2024)</a></li></ul></li><li><a href=#cond-matdis-nn-1>cond-mat.dis-nn (1)</a><ul><li><a href=#11--248290-geometric-dynamics-of-signal-propagation-predict-trainability-of-transformers-aditya-cowsik-et-al-2024>(1/1 | 248/290) Geometric Dynamics of Signal Propagation Predict Trainability of Transformers (Aditya Cowsik et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--249290-cormf-criticality-ordered-recurrent-mean-field-ising-solver-zhenyu-pan-et-al-2024>(1/3 | 249/290) CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver (Zhenyu Pan et al., 2024)</a></li><li><a href=#23--250290-active-statistical-inference-tijana-zrnic-et-al-2024>(2/3 | 250/290) Active Statistical Inference (Tijana Zrnic et al., 2024)</a></li><li><a href=#33--251290-chained-information-theoretic-bounds-and-tight-regret-rate-for-linear-bandit-problems-amaury-gouverneur-et-al-2024>(3/3 | 251/290) Chained Information-Theoretic bounds and Tight Regret Rate for Linear Bandit Problems (Amaury Gouverneur et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--252290-distributed-policy-gradient-for-linear-quadratic-networked-control-with-limited-communication-range-yuzi-yan-et-al-2024>(1/1 | 252/290) Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range (Yuzi Yan et al., 2024)</a></li></ul></li><li><a href=#q-biogn-1>q-bio.GN (1)</a><ul><li><a href=#11--253290-caduceus-bi-directional-equivariant-long-range-dna-sequence-modeling-yair-schiff-et-al-2024>(1/1 | 253/290) Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling (Yair Schiff et al., 2024)</a></li></ul></li><li><a href=#mathoc-4>math.OC (4)</a><ul><li><a href=#14--254290-assortment-optimization-for-conference-goodies-with-indifferent-attendees-fernanda-gutiérrez-et-al-2024>(1/4 | 254/290) Assortment Optimization For Conference Goodies With Indifferent Attendees (Fernanda Gutiérrez et al., 2024)</a></li><li><a href=#24--255290-shuffling-momentum-gradient-algorithm-for-convex-optimization-trang-h-tran-et-al-2024>(2/4 | 255/290) Shuffling Momentum Gradient Algorithm for Convex Optimization (Trang H. Tran et al., 2024)</a></li><li><a href=#34--256290-non-convex-stochastic-composite-optimization-with-polyak-momentum-yuan-gao-et-al-2024>(3/4 | 256/290) Non-Convex Stochastic Composite Optimization with Polyak Momentum (Yuan Gao et al., 2024)</a></li><li><a href=#44--257290-the-vehicle-routing-problem-with-synchronization-constraints-and-support-vehicle-dependent-service-times-david-wittwer-et-al-2024>(4/4 | 257/290) The vehicle routing problem with synchronization constraints and support vehicle-dependent service times (David Wittwer et al., 2024)</a></li></ul></li><li><a href=#mathna-6>math.NA (6)</a><ul><li><a href=#16--258290-a-transient-thermal-model-for-power-electronics-systems-neelakantan-padmanabhan-2024>(1/6 | 258/290) A Transient Thermal Model for Power Electronics Systems (Neelakantan Padmanabhan, 2024)</a></li><li><a href=#26--259290-on-the-computation-of-lattice-sums-without-translational-invariance-andreas-a-buchheit-et-al-2024>(2/6 | 259/290) On the computation of lattice sums without translational invariance (Andreas A. Buchheit et al., 2024)</a></li><li><a href=#36--260290-scientific-machine-learning-for-closure-models-in-multiscale-problems-a-review-benjamin-sanderse-et-al-2024>(3/6 | 260/290) Scientific machine learning for closure models in multiscale problems: a review (Benjamin Sanderse et al., 2024)</a></li><li><a href=#46--261290-efficient-simulation-of-complex-ginzburg--landau-equations-using-high-order-exponential-type-methods-marco-caliari-et-al-2024>(4/6 | 261/290) Efficient simulation of complex Ginzburg&ndash;Landau equations using high-order exponential-type methods (Marco Caliari et al., 2024)</a></li><li><a href=#56--262290-a-fully-discrete-semi-lagrangian-scheme-for-a-price-formation-mfg-model-yuri-ashrafyan-et-al-2024>(5/6 | 262/290) A Fully-discrete Semi-Lagrangian scheme for a price formation MFG model (Yuri Ashrafyan et al., 2024)</a></li><li><a href=#66--263290-isc-an-radi-type-method-for-stochastic-continuous-time-algebraic-riccati-equations-zhen-chen-guo-et-al-2024>(6/6 | 263/290) ISC: an RADI-type method for stochastic continuous-time algebraic Riccati equations (Zhen-Chen Guo et al., 2024)</a></li></ul></li><li><a href=#statme-2>stat.ME (2)</a><ul><li><a href=#12--264290-tripledebiased-lasso-for-statistical-inference-of-conditional-average-treatment-effects-masahiro-kato-2024>(1/2 | 264/290) Triple/Debiased Lasso for Statistical Inference of Conditional Average Treatment Effects (Masahiro Kato, 2024)</a></li><li><a href=#22--265290-a-consensus-constrained-parsimonious-gaussian-mixture-model-for-clustering-hyperspectral-images-ganesh-babu-et-al-2024>(2/2 | 265/290) A consensus-constrained parsimonious Gaussian mixture model for clustering hyperspectral images (Ganesh Babu et al., 2024)</a></li></ul></li><li><a href=#eesssy-6>eess.SY (6)</a><ul><li><a href=#16--266290-on-the-computation-of-stable-coupled-state-space-models-for-dynamic-substructuring-applications-r-s-o-dias-et-al-2024>(1/6 | 266/290) On the computation of stable coupled state-space models for dynamic substructuring applications (R. S. O. Dias et al., 2024)</a></li><li><a href=#26--267290-design-of-stochastic-quantizers-for-privacy-preservation-le-liu-et-al-2024>(2/6 | 267/290) Design of Stochastic Quantizers for Privacy Preservation (Le Liu et al., 2024)</a></li><li><a href=#36--268290-unifying-controller-design-for-stabilizing-nonlinear-systems-with-norm-bounded-control-inputs-ming-li-et-al-2024>(3/6 | 268/290) Unifying Controller Design for Stabilizing Nonlinear Systems with Norm-Bounded Control Inputs (Ming Li et al., 2024)</a></li><li><a href=#46--269290-single-level-robust-bidding-of-renewable-only-virtual-power-plant-in-energy-and-ancillary-service-markets-for-worst-case-profit-hadi-nemati-et-al-2024>(4/6 | 269/290) Single-level Robust Bidding of Renewable-only Virtual Power Plant in Energy and Ancillary Service Markets for Worst-case Profit (Hadi Nemati et al., 2024)</a></li><li><a href=#56--270290-autonomous-vehicle-decision-and-control-through-reinforcement-learning-with-traffic-flow-randomization-yuan-lin-et-al-2024>(5/6 | 270/290) Autonomous vehicle decision and control through reinforcement learning with traffic flow randomization (Yuan Lin et al., 2024)</a></li><li><a href=#66--271290-faithful-dynamic-timing-analysis-of-digital-circuits-using-continuous-thresholded-mode-switched-odes-arman-ferdowsi-et-al-2024>(6/6 | 271/290) Faithful Dynamic Timing Analysis of Digital Circuits Using Continuous Thresholded Mode-Switched ODEs (Arman Ferdowsi et al., 2024)</a></li></ul></li><li><a href=#csgr-2>cs.GR (2)</a><ul><li><a href=#12--272290-implicit-explicit-simulation-of-mass-spring-charge-systems-zhiyuan-zhang-et-al-2024>(1/2 | 272/290) Implicit-Explicit simulation of Mass-Spring-Charge Systems (Zhiyuan Zhang et al., 2024)</a></li><li><a href=#22--273290-towards-geometric-photometric-joint-alignment-for-facial-mesh-registration-xizhi-wang-et-al-2024>(2/2 | 273/290) Towards Geometric-Photometric Joint Alignment for Facial Mesh Registration (Xizhi Wang et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--274290-scalable-continuous-time-diffusion-framework-for-network-inference-and-influence-estimation-keke-huang-et-al-2024>(1/1 | 274/290) Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation (Keke Huang et al., 2024)</a></li></ul></li><li><a href=#csds-4>cs.DS (4)</a><ul><li><a href=#14--275290-fine-grained-privacy-guarantees-for-coverage-problems-laxman-dhulipala-et-al-2024>(1/4 | 275/290) Fine-Grained Privacy Guarantees for Coverage Problems (Laxman Dhulipala et al., 2024)</a></li><li><a href=#24--276290-dgap-efficient-dynamic-graph-analysis-on-persistent-memory-abdullah-al-raqibul-islam-et-al-2024>(2/4 | 276/290) DGAP: Efficient Dynamic Graph Analysis on Persistent Memory (Abdullah Al Raqibul Islam et al., 2024)</a></li><li><a href=#34--277290-cover-edge-based-novel-triangle-counting-david-a-bader-et-al-2024>(3/4 | 277/290) Cover Edge-Based Novel Triangle Counting (David A. Bader et al., 2024)</a></li><li><a href=#44--278290-on-approximate-fully-dynamic-matching-and-online-matrix-vector-multiplication-yang-p-liu-2024>(4/4 | 278/290) On Approximate Fully-Dynamic Matching and Online Matrix-Vector Multiplication (Yang P. Liu, 2024)</a></li></ul></li><li><a href=#hep-th-1>hep-th (1)</a><ul><li><a href=#11--279290-neural-network-learning-and-quantum-gravity-stefano-lanza-2024>(1/1 | 279/290) Neural Network Learning and Quantum Gravity (Stefano Lanza, 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--280290-on-demand-mobility-services-for-urban-resilience-a-review-towards-human-machine-collaborative-future-jiangbo-yu-2024>(1/1 | 280/290) On-demand Mobility Services for Urban Resilience: A Review Towards Human-Machine Collaborative Future (Jiangbo Yu, 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--281290-note-harnessing-tellurium-nanoparticles-in-the-digital-realm-plasmon-resonance-in-the-context-of-brewsters-angle-and-the-drude-model-for-fake-news-adsorption-in-incomplete-information-games-yasuko-kawahata-2024>(1/1 | 281/290) Note: Harnessing Tellurium Nanoparticles in the Digital Realm Plasmon Resonance, in the Context of Brewster&rsquo;s Angle and the Drude Model for Fake News Adsorption in Incomplete Information Games (Yasuko Kawahata, 2024)</a></li></ul></li><li><a href=#csfl-1>cs.FL (1)</a><ul><li><a href=#11--282290-efficient-interaction-based-offline-runtime-verification-of-distributed-systems-with-lifeline-removal-erwan-mahe-et-al-2024>(1/1 | 282/290) Efficient Interaction-Based Offline Runtime Verification of Distributed Systems with Lifeline Removal (Erwan Mahe et al., 2024)</a></li></ul></li><li><a href=#econgn-1>econ.GN (1)</a><ul><li><a href=#11--283290-bias-in-generative-ai-mi-zhou-et-al-2024>(1/1 | 283/290) Bias in Generative AI (Mi Zhou et al., 2024)</a></li></ul></li><li><a href=#cssd-1>cs.SD (1)</a><ul><li><a href=#11--284290-fighting-game-adaptive-background-music-for-improved-gameplay-ibrahim-khan-et-al-2024>(1/1 | 284/290) Fighting Game Adaptive Background Music for Improved Gameplay (Ibrahim Khan et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--285290-isummary-workload-based-personalized-summaries-for-knowledge-graphs-giannis-vassiliou-et-al-2024>(1/1 | 285/290) iSummary: Workload-based, Personalized Summaries for Knowledge Graphs (Giannis Vassiliou et al., 2024)</a></li></ul></li><li><a href=#mathst-1>math.ST (1)</a><ul><li><a href=#11--286290-finding-super-spreaders-in-network-cascades-elchanan-mossel-et-al-2024>(1/1 | 286/290) Finding Super-spreaders in Network Cascades (Elchanan Mossel et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--287290-equilibria-in-two-stage-facility-location-with-atomic-clients-simon-krogmann-et-al-2024>(1/1 | 287/290) Equilibria in Two-Stage Facility Location with Atomic Clients (Simon Krogmann et al., 2024)</a></li></ul></li><li><a href=#mathco-2>math.CO (2)</a><ul><li><a href=#12--288290-the-clique-chromatic-number-of-sparse-random-graphs-manuel-fernandez-v-et-al-2024>(1/2 | 288/290) The clique chromatic number of sparse random graphs (Manuel Fernandez V et al., 2024)</a></li><li><a href=#22--289290-face-hitting-dominating-sets-in-planar-graphs-p-francis-et-al-2024>(2/2 | 289/290) Face-hitting Dominating Sets in Planar Graphs (P. Francis et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--290290-space-complexity-of-euclidean-clustering-xiaoyi-zhu-et-al-2024>(1/1 | 290/290) Space Complexity of Euclidean Clustering (Xiaoyi Zhu et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>