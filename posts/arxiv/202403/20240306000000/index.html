<!doctype html><html><head><title>arXiv @ 2024.03.06</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.06"><meta property="og:description" content="Primary Categories astro-ph.CO (1) astro-ph.IM (1) astro-ph.SR (1) cs.AI (11) cs.CC (1) cs.CE (1) cs.CL (64) cs.CR (8) cs.CV (62) cs.CY (3) cs.DB (1) cs.DC (4) cs.DM (2) cs.DS (5) cs.FL (1) cs.GR (1) cs.GT (1) cs.HC (7) cs.IR (5) cs.IT (1) cs.LG (43) cs.LO (2) cs.MA (1) cs.NE (4) cs.NI (4) cs.PL (6) cs.RO (20) cs.SD (4) cs.SE (2) cs.SI (1) cs.SY (1) econ.EM (1) econ.GN (1) eess.AS (3) eess."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240306000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-06T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-06T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.06"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240306000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Wednesday, Mar 6, 2024</p></div><div class=title><h1>arXiv @ 2024.03.06</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#astro-phco-1>astro-ph.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#astro-phsr-1>astro-ph.SR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csai-11>cs.AI (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#cscl-64>cs.CL (64)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#cscr-8>cs.CR (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#cscv-62>cs.CV (62)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#cscy-3>cs.CY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csdc-4>cs.DC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csdm-2>cs.DM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csds-5>cs.DS (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csfl-1>cs.FL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#cshc-7>cs.HC (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csir-5>cs.IR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csit-1>cs.IT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#cslg-43>cs.LG (43)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#cslo-2>cs.LO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csne-4>cs.NE (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csni-4>cs.NI (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#cspl-6>cs.PL (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csro-20>cs.RO (20)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#cssd-4>cs.SD (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#csse-2>cs.SE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#cssy-1>cs.SY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#econem-1>econ.EM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#econgn-1>econ.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#eessas-3>eess.AS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#eessiv-7>eess.IV (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#eesssy-4>eess.SY (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#mathap-1>math.AP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#mathco-3>math.CO (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#mathds-1>math.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#mathna-5>math.NA (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#mathpr-1>math.PR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#q-bioto-1>q-bio.TO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#q-finpm-1>q-fin.PM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#quant-ph-3>quant-ph (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/#statml-5>stat.ML (5)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td></td><td>2</td><td>1</td></tr><tr><td>Automatic Evaluation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>5</td><td></td><td></td><td></td></tr><tr><td>BART</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>BERT</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td></td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>Bard</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>3</td><td>18</td><td>13</td><td>9</td><td>5</td></tr><tr><td>Black Box</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td>2</td><td>3</td><td>1</td><td>1</td><td></td></tr><tr><td>Chatbot</td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Claude</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td>3</td><td>1</td><td>1</td></tr><tr><td>Code Generation</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Continuous Time</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td>1</td><td>2</td><td>1</td><td></td></tr><tr><td>ControlNet</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td>8</td><td>4</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>7</td><td>6</td><td></td></tr><tr><td>Coreference Resolution</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Data Augmentation</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Dense Retrieval</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Dialogue System</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>2</td><td>8</td><td></td><td></td></tr><tr><td>Direct Preference Optimization</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>2</td><td>4</td><td></td></tr><tr><td>Explainable AI</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fake News Detection</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Few-shot</td><td></td><td>6</td><td>1</td><td>2</td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td>3</td><td>1</td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td></td><td>17</td><td>8</td><td>3</td><td>1</td></tr><tr><td>Foundation Model</td><td></td><td>4</td><td>1</td><td>1</td><td></td></tr><tr><td>GPT</td><td></td><td>9</td><td>1</td><td>1</td><td></td></tr><tr><td>GPT-3</td><td></td><td>7</td><td></td><td>1</td><td></td></tr><tr><td>GPT-3.5</td><td></td><td>6</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Gemini</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Graph</td><td>1</td><td>4</td><td>2</td><td>6</td><td>2</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Graph Embedding</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Grounding</td><td></td><td></td><td>2</td><td></td><td>1</td></tr><tr><td>Hate Speech Detection</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Image2text</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>In-context Learning</td><td></td><td>5</td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Karush-Kuhn-Tucker</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>2</td><td>4</td><td>3</td><td></td></tr><tr><td>Knowledge Graph</td><td></td><td>3</td><td></td><td></td><td>1</td></tr><tr><td>Knowledge Transfer</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>LLaMA</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td>1</td><td></td><td>3</td><td></td></tr><tr><td>Language Generation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>10</td><td>56</td><td>4</td><td>8</td><td></td></tr><tr><td>Low-Resource</td><td></td><td>8</td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Markov Decision Process</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Mistral</td><td></td><td>4</td><td></td><td>1</td><td></td></tr><tr><td>Model Compression</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Multi-modal</td><td>2</td><td>8</td><td>13</td><td>3</td><td>3</td></tr><tr><td>Mutual Information</td><td></td><td></td><td>1</td><td>2</td><td>1</td></tr><tr><td>Natural Language Inference</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>9</td><td></td><td>1</td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Noise-tolerant</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>8</td><td>1</td><td></td></tr><tr><td>Open-Domain Question Answering</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>1</td><td></td><td>3</td><td></td></tr><tr><td>Out-of-domain</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>12</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Prompt</td><td>3</td><td>12</td><td>7</td><td>2</td><td></td></tr><tr><td>Prompt Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Quantization</td><td></td><td></td><td>1</td><td>6</td><td></td></tr><tr><td>Question Answering</td><td></td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>Reasoning</td><td>2</td><td>11</td><td>4</td><td></td><td></td></tr><tr><td>Recommendation</td><td>1</td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td>4</td><td></td><td></td><td>1</td></tr><tr><td>Reinforcement Learning</td><td>3</td><td>2</td><td></td><td>3</td><td>5</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td>2</td><td></td><td>4</td><td></td></tr><tr><td>Relation Extraction</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>RoBERTa</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Scaling Law</td><td></td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>Self-adaption</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td>4</td><td>3</td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Sentiment Analysis</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td>1</td><td>4</td><td>5</td></tr><tr><td>Simulator</td><td></td><td></td><td>1</td><td>4</td><td>5</td></tr><tr><td>Speech-to-Speech Translation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Stemming</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Summarization</td><td>1</td><td>5</td><td>1</td><td></td><td></td></tr><tr><td>SuperGLUE</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>5</td><td>5</td><td>5</td><td>2</td></tr><tr><td>Text Classification</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text Summarization</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text-to-speech</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>5</td><td></td><td></td></tr><tr><td>Tokenization</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Topic Model</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Topic Modeling</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>2</td><td>2</td><td>3</td><td></td></tr><tr><td>Transformer</td><td>1</td><td>8</td><td>10</td><td>4</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td></td><td>4</td><td>3</td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>8</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>2</td><td>8</td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Yolo</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>5</td><td>2</td><td>1</td><td>1</td></tr><tr><td>Zero-shot Learning</td><td></td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>falcon</td><td></td><td></td><td></td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cslg-43>cs.LG (43)</h2><h3 id=143--1309-tpllm-a-traffic-prediction-framework-based-on-pretrained-large-language-models-yilong-ren-et-al-2024>(1/43 | 1/309) TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models (Yilong Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yilong Ren, Yue Chen, Shuai Liu, Boyue Wang, Haiyang Yu, Zhiyong Cui. (2024)<br><strong>TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models</strong><br><button class=copy-to-clipboard title="TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 143<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Graph Embedding, Convolution, Convolutional Neural Network, Convolutional Neural Network, Convolutional Neural Network, Few-shot, Few-shot Learning, Fine-tuning, Knowledge Transfer, Stemming, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02221v1.pdf filename=2403.02221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traffic prediction constitutes a pivotal facet within the purview of Intelligent Transportation Systems (ITS), and the attainment of highly precise predictions holds profound significance for efficacious traffic management. The precision of prevailing deep learning-driven traffic prediction models typically sees an upward trend with a rise in the volume of training data. However, the procurement of comprehensive spatiotemporal datasets for traffic is often fraught with challenges, primarily <b>stemming</b> from the substantial costs associated with data collection and retention. Consequently, developing a model that can achieve accurate predictions and good generalization ability in areas with limited historical traffic data is a challenging problem. It is noteworthy that the rapidly advancing pretrained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> of recent years have demonstrated exceptional proficiency in cross-modality <b>knowledge</b> <b>transfer</b> and <b>few-shot</b> <b>learning.</b> Recognizing the sequential nature of traffic data, similar to language, we introduce TPLLM, a novel traffic prediction framework leveraging <b>LLMs.</b> In this framework, we construct a sequence embedding layer based on <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and a <b>graph</b> <b>embedding</b> <b>layer</b> based on <b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCNs)</b> to extract sequence features and spatial features, respectively. These are subsequently integrated to form inputs that are suitable for <b>LLMs.</b> A Low-Rank Adaptation (LoRA) <b>fine-tuning</b> approach is applied to TPLLM, thereby facilitating efficient learning and minimizing computational demands. Experiments on two real-world datasets demonstrate that TPLLM exhibits commendable performance in both full-sample and <b>few-shot</b> <b>prediction</b> scenarios, effectively supporting the development of ITS in regions with scarce historical traffic data.</p></p class="citation"></blockquote><h3 id=243--2309-enhancing-llm-safety-via-constrained-direct-preference-optimization-zixuan-liu-et-al-2024>(2/43 | 2/309) Enhancing LLM Safety via Constrained Direct Preference Optimization (Zixuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zixuan Liu, Xiaolin Sun, Zizhan Zheng. (2024)<br><strong>Enhancing LLM Safety via Constrained Direct Preference Optimization</strong><br><button class=copy-to-clipboard title="Enhancing LLM Safety via Constrained Direct Preference Optimization" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Direct Preference Optimization, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02475v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02475v1.pdf filename=2403.02475v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapidly increasing capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> raise an urgent need to align AI systems with diverse human preferences to simultaneously enhance their usefulness and safety, despite the often conflicting nature of these goals. To address this important problem, a promising approach is to enforce a safety constraint at the <b>fine-tuning</b> stage through a constrained <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> framework. This approach, however, is computationally expensive and often unstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension of the recently proposed <b>Direct</b> <b>Preference</b> <b>Optimization</b> (DPO) approach for <b>fine-tuning</b> <b>LLMs</b> that is both efficient and lightweight. By integrating dual gradient descent and DPO, our method identifies a nearly optimal trade-off between helpfulness and harmlessness without using <b>reinforcement</b> <b>learning.</b> <b>Empirically,</b> <b>our</b> <b>approach</b> provides a safety guarantee to <b>LLMs</b> that is missing in DPO while achieving significantly higher rewards under the same safety constraint compared to a recently proposed safe <b>RLHF</b> approach. Warning: This paper contains example data that may be offensive or harmful.</p></p class="citation"></blockquote><h3 id=343--3309-towards-efficient-deep-autoencoders-for-multivariate-time-series-anomaly-detection-marcin-pietroń-et-al-2024>(3/43 | 3/309) Towards efficient deep autoencoders for multivariate time series anomaly detection (Marcin Pietroń et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcin Pietroń, Dominik Żurek, Kamil Faber, Roberto Corizzo. (2024)<br><strong>Towards efficient deep autoencoders for multivariate time series anomaly detection</strong><br><button class=copy-to-clipboard title="Towards efficient deep autoencoders for multivariate time series anomaly detection" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Anomaly Detection, Autoencoder, Benchmarking, Model Compression, Pruning, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02429v1.pdf filename=2403.02429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multivariate time series <b>anomaly</b> <b>detection</b> is a crucial problem in many industrial and research applications. Timely detection of anomalies allows, for instance, to prevent defects in manufacturing processes and failures in cyberphysical systems. Deep learning methods are preferred among others for their accuracy and robustness for the analysis of complex multivariate data. However, a key aspect is being able to extract predictions in a timely manner, to accommodate real-time requirements in different applications. In the case of deep learning <b>models,</b> <b>model</b> <b>reduction</b> is extremely important to achieve optimal results in real-time systems with limited time and memory constraints. In this paper, we address this issue by proposing a novel compression method for deep <b>autoencoders</b> that involves three key factors. First, <b>pruning</b> reduces the number of weights, while preventing catastrophic drops in accuracy by means of a fast search process that identifies high sparsity levels. Second, linear and non-linear <b>quantization</b> reduces <b>model</b> <b>complexity</b> by reducing the number of bits for every single weight. The combined contribution of these three aspects allow the <b>model</b> <b>size</b> to be reduced, by removing a subset of the weights <b>(pruning),</b> and decreasing their bit-width <b>(quantization).</b> As a result, the compressed <b>model</b> <b>is</b> faster and easier to adopt in highly constrained hardware environments. Experiments performed on popular multivariate <b>anomaly</b> <b>detection</b> <b>benchmarks,</b> show that our method is capable of achieving significant <b>model</b> <b>compression</b> ratio (between 80% and 95%) without a significant reduction in the <b>anomaly</b> <b>detection</b> performance.</p></p class="citation"></blockquote><h3 id=443--4309-reward-model-learning-vs-direct-policy-optimization-a-comparative-analysis-of-learning-from-human-preferences-andi-nika-et-al-2024>(4/43 | 4/309) Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences (Andi Nika et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andi Nika, Debmalya Mandal, Parameswaran Kamalaruban, Georgios Tzannetos, Goran Radanović, Adish Singla. (2024)<br><strong>Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences</strong><br><button class=copy-to-clipboard title="Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Direct Preference Optimization, Markov Decision Process, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01857v1.pdf filename=2403.01857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we take a step towards a deeper understanding of learning from human preferences by systematically comparing the paradigm of <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF)</b> with the recently proposed paradigm of <b>direct</b> <b>preference</b> <b>optimization</b> (DPO). We focus our attention on the class of loglinear policy parametrization and linear reward functions. In order to compare the two paradigms, we first derive minimax statistical bounds on the suboptimality gap induced by both <b>RLHF</b> and DPO, assuming access to an oracle that exactly solves the optimization problems. We provide a detailed discussion on the relative comparison between the two paradigms, simultaneously taking into account the <b>sample</b> <b>size,</b> policy and reward class dimensions, and the regularization temperature. Moreover, we extend our analysis to the approximate optimization setting and derive exponentially decaying convergence rates for both <b>RLHF</b> and DPO. Next, we analyze the setting where the ground-truth reward is not realizable and find that, while <b>RLHF</b> incurs a constant additional error, DPO retains its asymptotically decaying gap by just tuning the temperature accordingly. Finally, we extend our comparison to the <b>Markov</b> <b>decision</b> <b>process</b> setting, where we generalize our results with exact optimization. To the best of our knowledge, we are the first to provide such a comparative analysis for <b>RLHF</b> and DPO.</p></p class="citation"></blockquote><h3 id=543--5309-wukong-towards-a-scaling-law-for-large-scale-recommendation-buyun-zhang-et-al-2024>(5/43 | 5/309) Wukong: Towards a Scaling Law for Large-Scale Recommendation (Buyun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Buyun Zhang, Liang Luo, Yuxin Chen, Jade Nie, Xi Liu, Daifeng Guo, Yanli Zhao, Shen Li, Yuchen Hao, Yantao Yao, Guna Lakshminarayanan, Ellie Dingqiao Wen, Jongsoo Park, Maxim Naumov, Wenlin Chen. (2024)<br><strong>Wukong: Towards a Scaling Law for Large-Scale Recommendation</strong><br><button class=copy-to-clipboard title="Wukong: Towards a Scaling Law for Large-Scale Recommendation" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Recommendation, GPT, GPT-3, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02545v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02545v2.pdf filename=2403.02545v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Scaling</b> <b>laws</b> play an instrumental role in the sustainable improvement in model quality. Unfortunately, <b>recommendation</b> models to date do not exhibit such laws similar to those observed in the domain of <b>large</b> <b>language</b> <b>models,</b> due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a <b>scaling</b> <b>law</b> in the domain of <b>recommendation.</b> Wukong&rsquo;s unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wukong&rsquo;s scalability on an internal, <b>large-scale</b> <b>dataset.</b> <b>The</b> results show that Wukong retains its superiority in quality over state-of-the-art models, while holding the <b>scaling</b> <b>law</b> across two orders of magnitude in model complexity, extending beyond 100 Gflop or equivalently up to <b>Large</b> <b>Language</b> <b>Model</b> <b>(GPT-3)</b> training compute scale, where prior arts fall short.</p></p class="citation"></blockquote><h3 id=643--6309-distilled-chatgpt-topic--sentiment-modeling-with-applications-in-finance-olivier-gandouet-et-al-2024>(6/43 | 6/309) Distilled ChatGPT Topic & Sentiment Modeling with Applications in Finance (Olivier Gandouet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olivier Gandouet, Mouloud Belbahri, Armelle Jezequel, Yuriy Bodjov. (2024)<br><strong>Distilled ChatGPT Topic & Sentiment Modeling with Applications in Finance</strong><br><button class=copy-to-clipboard title="Distilled ChatGPT Topic & Sentiment Modeling with Applications in Finance" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CE, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Transfer Learning, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02185v1.pdf filename=2403.02185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, <b>ChatGPT</b> is utilized to create streamlined models that generate easily interpretable features. These features are then used to evaluate financial outcomes from earnings calls. We detail a training approach that merges <b>knowledge</b> <b>distillation</b> and <b>transfer</b> <b>learning,</b> resulting in lightweight topic and sentiment classification models without significant loss in accuracy. These models are assessed through a dataset annotated by experts. The paper also delves into two practical case studies, highlighting how the generated features can be effectively utilized in quantitative investing scenarios.</p></p class="citation"></blockquote><h3 id=743--7309-unsupervised-distance-metric-learning-for-anomaly-detection-over-multivariate-time-series-hanyang-yuan-et-al-2024>(7/43 | 7/309) Unsupervised Distance Metric Learning for Anomaly Detection Over Multivariate Time Series (Hanyang Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanyang Yuan, Qinglin Cai, Keting Yin. (2024)<br><strong>Unsupervised Distance Metric Learning for Anomaly Detection Over Multivariate Time Series</strong><br><button class=copy-to-clipboard title="Unsupervised Distance Metric Learning for Anomaly Detection Over Multivariate Time Series" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 46<br>Keywords: Anomaly Detection, Benchmarking, Clustering, Supervised Learning, Unsupervised Learning, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01895v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01895v1.pdf filename=2403.01895v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distance-based time series <b>anomaly</b> <b>detection</b> methods are prevalent due to their relative non-parametric nature and interpretability. However, the commonly used Euclidean distance is sensitive to noise. While existing works have explored dynamic time warping (DTW) for its robustness, they only support <b>supervised</b> tasks over multivariate time series <b>(MTS),</b> leaving a scarcity of <b>unsupervised</b> methods. In this work, we propose FCM-wDTW, an <b>unsupervised</b> distance metric learning method for <b>anomaly</b> <b>detection</b> over <b>MTS,</b> which encodes raw data into latent space and reveals normal dimension relationships through cluster centers. FCM-wDTW introduces locally weighted DTW into fuzzy C-means <b>clustering</b> and learns the optimal latent space efficiently, enabling <b>anomaly</b> <b>identification</b> via data reconstruction. Experiments with 11 different types of <b>benchmarks</b> demonstrate our method&rsquo;s competitive accuracy and efficiency.</p></p class="citation"></blockquote><h3 id=843--8309-better-schedules-for-low-precision-training-of-deep-neural-networks-cameron-r-wolfe-et-al-2024>(8/43 | 8/309) Better Schedules for Low Precision Training of Deep Neural Networks (Cameron R. Wolfe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cameron R. Wolfe, Anastasios Kyrillidis. (2024)<br><strong>Better Schedules for Low Precision Training of Deep Neural Networks</strong><br><button class=copy-to-clipboard title="Better Schedules for Low Precision Training of Deep Neural Networks" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6; I-2-10; I-4-0, cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Node Classification, Graph, Graph Neural Network, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02243v1.pdf filename=2403.02243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low precision training can significantly reduce the computational overhead of training deep neural networks (DNNs). Though many such techniques exist, cyclic precision training (CPT), which dynamically adjusts precision throughout training according to a cyclic schedule, achieves particularly impressive improvements in training efficiency, while actually improving DNN performance. Existing CPT implementations take common learning rate schedules (e.g., cyclical cosine schedules) and use them for low precision training without adequate comparisons to alternative scheduling options. We define a diverse suite of CPT schedules and analyze their performance across a variety of DNN training regimes, some of which are unexplored in the low precision training literature (e.g., <b>node</b> <b>classification</b> with <b>graph</b> <b>neural</b> <b>networks).</b> From these experiments, we discover alternative CPT schedules that offer further improvements in training efficiency and model performance, as well as derive a set of best practices for choosing CPT schedules. Going further, we find that a correlation exists between model performance and training cost, and that changing the underlying CPT schedule can control the tradeoff between these two variables. To explain the direct correlation between model performance and training cost, we draw a connection between <b>quantized</b> training and critical learning periods, suggesting that aggressive <b>quantization</b> is a form of learning impairment that can permanently damage model performance.</p></p class="citation"></blockquote><h3 id=943--9309-a-safe-screening-rule-with-bi-level-optimization-of-ν-support-vector-machine-zhiji-yang-et-al-2024>(9/43 | 9/309) A Safe Screening Rule with Bi-level Optimization of $ν$ Support Vector Machine (Zhiji Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiji Yang, Wanyi Chen, Huan Zhang, Yitian Xu, Lei Shi, Jianhua Zhao. (2024)<br><strong>A Safe Screening Rule with Bi-level Optimization of $ν$ Support Vector Machine</strong><br><button class=copy-to-clipboard title="A Safe Screening Rule with Bi-level Optimization of $ν$ Support Vector Machine" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC<br>Keyword Score: 43<br>Keywords: Benchmarking, Karush-Kuhn-Tucker, Karush-Kuhn-Tucker, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01769v1.pdf filename=2403.01769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Support vector machine (SVM) has achieved many successes in machine learning, especially for a small sample problem. As a famous extension of the traditional SVM, the $\nu$ support vector machine ($\nu$-SVM) has shown outstanding performance due to its great model interpretability. However, it still faces challenges in training overhead for large-scale problems. To address this issue, we propose a safe screening rule with bi-level optimization for $\nu$-SVM (SRBO-$\nu$-SVM) which can screen out inactive samples before training and reduce the computational cost without sacrificing the prediction accuracy. Our SRBO-$\nu$-SVM is strictly deduced by integrating the <b>Karush-Kuhn-Tucker</b> <b>(KKT)</b> conditions, the variational inequalities of convex problems and the $\nu$-property. Furthermore, we develop an efficient dual coordinate descent method (DCDM) to further improve computational speed. Finally, a unified framework for SRBO is proposed to accelerate many SVM-type models, and it is successfully applied to one-class SVM. Experimental results on 6 artificial data sets and 30 <b>benchmark</b> data sets have verified the effectiveness and safety of our proposed methods in <b>supervised</b> and <b>unsupervised</b> tasks.</p></p class="citation"></blockquote><h3 id=1043--10309-towards-foundation-time-series-model-to-synthesize-or-not-to-synthesize-kseniia-kuvshinova-et-al-2024>(10/43 | 10/309) Towards Foundation Time Series Model: To Synthesize Or Not To Synthesize? (Kseniia Kuvshinova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kseniia Kuvshinova, Olga Tsymboi, Alina Kostromina, Dmitry Simakov, Elizaveta Kovtun. (2024)<br><strong>Towards Foundation Time Series Model: To Synthesize Or Not To Synthesize?</strong><br><button class=copy-to-clipboard title="Towards Foundation Time Series Model: To Synthesize Or Not To Synthesize?" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Few-shot, Foundation Model, Supervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02534v1.pdf filename=2403.02534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The industry is rich in cases when we are required to make forecasting for large amounts of time series at once. However, we might be in a situation where we can not afford to train a separate model for each of them. Such issue in time series modeling remains without due attention. The remedy for this setting is the establishment of a <b>foundation</b> <b>model.</b> Such a model is expected to work in <b>zero-shot</b> and <b>few-shot</b> regimes. However, what should we take as a training dataset for such kind of model? Witnessing the benefits from the enrichment of NLP datasets with artificially-generated data, we might want to adopt their experience for time series. In contrast to natural language, the process of generation of synthetic time series data is even more favorable because it provides full control of series patterns, time horizons, and number of samples. In this work, we consider the essential question if it is advantageous to train a <b>foundation</b> <b>model</b> on synthetic data or it is better to utilize only a limited number of real-life examples. Our experiments are conducted only for regular time series and speak in favor of leveraging solely the real time series. Moreover, the choice of the proper source dataset strongly influences the performance during inference. When provided access even to a limited quantity of short time series data, employing it within a <b>supervised</b> framework yields more favorable results than training on a larger volume of synthetic data. The code for our experiments is publicly available on Github \url{https://github.com/sb-ai-lab/synthesize_or_not}.</p></p class="citation"></blockquote><h3 id=1143--11309-taming-throughput-latency-tradeoff-in-llm-inference-with-sarathi-serve-amey-agrawal-et-al-2024>(11/43 | 11/309) Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve (Amey Agrawal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, Alexey Tumanov, Ramachandran Ramjee. (2024)<br><strong>Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve</strong><br><button class=copy-to-clipboard title="Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Mistral, falcon, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02310v1.pdf filename=2403.02310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Each <b>LLM</b> serving request goes through two phases. The first is prefill which processes the entire input <b>prompt</b> to produce one output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input <b>prompt.</b> In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency. We introduce an efficient <b>LLM</b> inference scheduler Sarathi-Serve inspired by the techniques we originally proposed for optimizing throughput in Sarathi. Sarathi-Serve leverages chunked-prefills from Sarathi to create stall-free schedules that can add new requests in a batch without pausing ongoing decodes. Stall-free scheduling unlocks the opportunity to improve throughput with large batch sizes while minimizing the effect of batching on latency. Our evaluation shows that Sarathi-Serve improves serving throughput within desired latency SLOs of <b>Mistral-7B</b> by up to 2.6x on a single A100 GPU and up to 6.9x for <b>Falcon-180B</b> on 8 A100 GPUs over Orca and vLLM.</p></p class="citation"></blockquote><h3 id=1243--12309-cola-cross-city-mobility-transformer-for-human-trajectory-simulation-yu-wang-et-al-2024>(12/43 | 12/309) COLA: Cross-city Mobility Transformer for Human Trajectory Simulation (Yu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Wang, Tongya Zheng, Yuxuan Liang, Shunyu Liu, Mingli Song. (2024)<br><strong>COLA: Cross-city Mobility Transformer for Human Trajectory Simulation</strong><br><button class=copy-to-clipboard title="COLA: Cross-city Mobility Transformer for Human Trajectory Simulation" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Knowledge Transfer, Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01801v1.pdf filename=2403.01801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human trajectory data produced by daily mobile devices has proven its usefulness in various substantial fields such as urban planning and epidemic prevention. In terms of the individual privacy concern, human trajectory <b>simulation</b> has attracted increasing attention from researchers, targeting at offering numerous realistic mobility data for downstream tasks. Nevertheless, the prevalent issue of data scarcity undoubtedly degrades the reliability of existing deep learning models. In this paper, we are motivated to explore the intriguing problem of mobility transfer across cities, grasping the universal patterns of human trajectories to augment the powerful <b>Transformer</b> with external mobility data. There are two crucial challenges arising in the <b>knowledge</b> <b>transfer</b> across cities: 1) how to transfer the <b>Transformer</b> to adapt for domain heterogeneity; 2) how to calibrate the <b>Transformer</b> to adapt for subtly different long-tail frequency distributions of locations. To address these challenges, we have tailored a Cross-city mObiLity <b>trAnsformer</b> (COLA) with a dedicated model-agnostic transfer framework by effectively transferring cross-city <b>knowledge</b> <b>for</b> human trajectory <b>simulation.</b> Firstly, COLA divides the <b>Transformer</b> into the private modules for city-specific characteristics and the shared modules for city-universal mobility patterns. Secondly, COLA leverages a lightweight yet effective post-hoc adjustment strategy for trajectory <b>simulation,</b> without disturbing the complex bi-level optimization of model-agnostic <b>knowledge</b> <b>transfer.</b> Extensive experiments of COLA compared to state-of-the-art single-city baselines and our implemented cross-city baselines have demonstrated its superiority and effectiveness. The code is available at <a href=https://github.com/Star607/Cross-city-Mobility-Transformer>https://github.com/Star607/Cross-city-Mobility-Transformer</a>.</p></p class="citation"></blockquote><h3 id=1343--13309-coms2t-a-complementary-spatiotemporal-learning-system-for-data-adaptive-model-evolution-zhengyang-zhou-et-al-2024>(13/43 | 13/309) ComS2T: A complementary spatiotemporal learning system for data-adaptive model evolution (Zhengyang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyang Zhou, Qihe Huang, Binwu Wang, Jianpeng Hou, Kuo Yang, Yuxuan Liang, Yang Wang. (2024)<br><strong>ComS2T: A complementary spatiotemporal learning system for data-adaptive model evolution</strong><br><button class=copy-to-clipboard title="ComS2T: A complementary spatiotemporal learning system for data-adaptive model evolution" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Convolution, Fine-tuning, Out-of-distribution, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01738v1.pdf filename=2403.01738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spatiotemporal (ST) learning has become a crucial technique to enable smart cities and sustainable urban development. Current ST learning models capture the heterogeneity via various spatial <b>convolution</b> and temporal evolution blocks. However, rapid urbanization leads to fluctuating distributions in urban data and city structures over short periods, resulting in existing methods suffering generalization and data adaptation issues. Despite efforts, existing methods fail to deal with newly arrived observations and those methods with generalization capacity are limited in repeated training. Motivated by complementary learning in neuroscience, we introduce a <b>prompt-based</b> complementary spatiotemporal learning termed ComS2T, to empower the evolution of models for data adaptation. ComS2T partitions the neural architecture into a stable neocortex for consolidating historical memory and a dynamic hippocampus for new knowledge update. We first disentangle two disjoint structures into stable and dynamic weights, and then train spatial and temporal <b>prompts</b> by characterizing distribution of main observations to enable <b>prompts</b> adaptive to new data. This data-adaptive <b>prompt</b> mechanism, combined with a two-stage training process, facilitates <b>fine-tuning</b> of the neural architecture conditioned on <b>prompts,</b> thereby enabling efficient adaptation during testing. Extensive experiments validate the efficacy of ComS2T in adapting to various spatiotemporal <b>out-of-distribution</b> scenarios while maintaining efficient inference capabilities.</p></p class="citation"></blockquote><h3 id=1443--14309-commit-certifying-robustness-of-multi-sensor-fusion-systems-against-semantic-attacks-zijian-huang-et-al-2024>(14/43 | 14/309) COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against Semantic Attacks (Zijian Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijian Huang, Wenda Chu, Linyi Li, Chejian Xu, Bo Li. (2024)<br><strong>COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against Semantic Attacks</strong><br><button class=copy-to-clipboard title="COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against Semantic Attacks" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Object Detection, Benchmarking, Multi-modal, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02329v1.pdf filename=2403.02329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-sensor fusion systems (MSFs) play a vital role as the perception module in modern autonomous vehicles (AVs). Therefore, ensuring their robustness against common and realistic adversarial semantic transformations, such as rotation and shifting in the physical world, is crucial for the safety of AVs. While empirical evidence suggests that MSFs exhibit improved robustness compared to single-modal models, they are still vulnerable to adversarial semantic transformations. Despite the proposal of empirical defenses, several works show that these defenses can be attacked again by new adaptive attacks. So far, there is no certified defense proposed for MSFs. In this work, we propose the first robustness certification framework COMMIT certify robustness of multi-sensor fusion systems against semantic attacks. In particular, we propose a practical anisotropic noise mechanism that leverages randomized smoothing with <b>multi-modal</b> data and performs a grid-based splitting method to characterize complex semantic transformations. We also propose efficient algorithms to compute the certification in terms of <b>object</b> <b>detection</b> accuracy and IoU for large-scale MSF models. Empirically, we evaluate the efficacy of COMMIT in different settings and provide a comprehensive <b>benchmark</b> of certified robustness for different MSF models using the CARLA <b>simulation</b> platform. We show that the certification for MSF models is at most 48.39% higher than that of single-modal models, which validates the advantages of MSF models. We believe our certification framework and <b>benchmark</b> will contribute an important step towards certifiably robust AVs in practice.</p></p class="citation"></blockquote><h3 id=1543--15309-hear----health-acoustic-representations-sebastien-baur-et-al-2024>(15/43 | 15/309) HeAR &ndash; Health Acoustic Representations (Sebastien Baur et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastien Baur, Zaid Nabulsi, Wei-Hung Weng, Jake Garrison, Louis Blankemeier, Sam Fishman, Christina Chen, Sujay Kakarmath, Minyoi Maimbolwa, Nsala Sanjase, Brian Shuma, Yossi Matias, Greg S. Corrado, Shwetak Patel, Shravya Shetty, Shruthi Prabhakara, Monde Muyoyeta, Diego Ardila. (2024)<br><strong>HeAR &ndash; Health Acoustic Representations</strong><br><button class=copy-to-clipboard title="HeAR -- Health Acoustic Representations" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Autoencoder, Benchmarking, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02522v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02522v1.pdf filename=2403.02522v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Health acoustic sounds such as coughs and breaths are known to contain useful health signals with significant potential for monitoring health and disease, yet are underexplored in the medical machine learning community. The existing deep learning systems for health acoustics are often narrowly trained and evaluated on a single task, which is limited by data and may hinder generalization to other tasks. To mitigate these gaps, we develop HeAR, a scalable <b>self-supervised</b> <b>learning-based</b> deep learning system using masked <b>autoencoders</b> trained on a large dataset of 313 million two-second long audio clips. Through linear probes, we establish HeAR as a state-of-the-art health audio embedding model on a <b>benchmark</b> of 33 health acoustic tasks across 6 datasets. By introducing this work, we hope to enable and accelerate further health acoustics research.</p></p class="citation"></blockquote><h3 id=1643--16309-sok-challenges-and-opportunities-in-federated-unlearning-hyejun-jeong-et-al-2024>(16/43 | 16/309) SoK: Challenges and Opportunities in Federated Unlearning (Hyejun Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyejun Jeong, Shiqing Ma, Amir Houmansadr. (2024)<br><strong>SoK: Challenges and Opportunities in Federated Unlearning</strong><br><button class=copy-to-clipboard title="SoK: Challenges and Opportunities in Federated Unlearning" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02437v1.pdf filename=2403.02437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL), introduced in 2017, facilitates collaborative learning between non-trusting parties with no need for the parties to explicitly share their data among themselves. This allows training models on user data while respecting privacy regulations such as GDPR and CPRA. However, emerging privacy requirements may mandate model owners to be able to \emph{forget} some learned data, e.g., when requested by data owners or law enforcement. This has given birth to an active field of research called \emph{machine unlearning}. In the context of FL, many techniques developed for unlearning in centralized settings are not trivially applicable! This is due to the unique differences between centralized and distributed learning, in particular, interactivity, stochasticity, heterogeneity, and limited accessibility in FL. In response, a recent line of work has focused on developing unlearning mechanisms tailored to FL. This SoK paper aims to take a deep look at the \emph{federated unlearning} literature, with the goal of identifying research trends and challenges in this emerging field. By carefully categorizing papers published on FL unlearning (since 2020), we aim to pinpoint the unique complexities of <b>federated</b> <b>unlearning,</b> highlighting limitations on directly applying centralized unlearning methods. We compare existing <b>federated</b> <b>unlearning</b> methods regarding influence removal and performance recovery, compare their threat models and assumptions, and discuss their implications and limitations. For instance, we analyze the experimental setup of FL unlearning studies from various perspectives, including data heterogeneity and its <b>simulation,</b> the datasets used for demonstration, and evaluation metrics. Our work aims to offer insights and suggestions for future research on <b>federated</b> <b>unlearning.</b></p></p class="citation"></blockquote><h3 id=1743--17309-are-more-llm-calls-all-you-need-towards-scaling-laws-of-compound-inference-systems-lingjiao-chen-et-al-2024>(17/43 | 17/309) Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems (Lingjiao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, James Zou. (2024)<br><strong>Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems</strong><br><button class=copy-to-clipboard title="Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02419v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02419v1.pdf filename=2403.02419v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> calls and aggregate their responses. However, there is little understanding of how the number of <b>LLM</b> calls &ndash; e.g., when asking the <b>LLM</b> to answer each question multiple times and taking a consensus &ndash; affects such a compound system&rsquo;s performance. In this paper, we initiate the study of <b>scaling</b> <b>laws</b> of compound inference systems. We analyze, theoretically and empirically, how the number of <b>LLM</b> calls affects the performance of one-layer Voting Inference Systems &ndash; one of the simplest compound systems, which aggregates <b>LLM</b> responses via majority voting. We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems&rsquo; performance first increases but then decreases as a function of the number of <b>LLM</b> calls. Our theoretical results suggest that this non-monotonicity is due to the diversity of query difficulties within a task: more <b>LLM</b> calls lead to higher performance on &ldquo;easy&rdquo; queries, but lower performance on &ldquo;hard&rdquo; queries, and non-monotone behavior emerges when a task contains both types of queries. This insight then allows us to compute, from a small number of samples, the number of <b>LLM</b> calls that maximizes system performance, and define a <b>scaling</b> <b>law</b> of Voting Inference Systems. Experiments show that our <b>scaling</b> <b>law</b> can predict the performance of Voting Inference Systems and find the optimal number of <b>LLM</b> calls to make.</p></p class="citation"></blockquote><h3 id=1843--18309-transformers-provably-learn-feature-position-correlations-in-masked-image-modeling-yu-huang-et-al-2024>(18/43 | 18/309) Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling (Yu Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Huang, Zixin Wen, Yuejie Chi, Yingbin Liang. (2024)<br><strong>Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling</strong><br><button class=copy-to-clipboard title="Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 30<br>Keywords: Self-supervised Learning, Self-supervised Pre-training, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02233v1.pdf filename=2403.02233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Masked image modeling (MIM), which predicts randomly masked patches from unmasked ones, has emerged as a promising approach in <b>self-supervised</b> <b>vision</b> pretraining. However, the theoretical understanding of MIM is rather limited, especially with the foundational architecture of <b>transformers.</b> In this paper, to the best of our knowledge, we provide the first end-to-end theory of learning one-layer <b>transformers</b> with softmax attention in MIM <b>self-supervised</b> <b>pretraining.</b> On the conceptual side, we posit a theoretical mechanism of how <b>transformers,</b> pretrained with MIM, produce empirically observed local and diverse attention patterns on data distributions with spatial structures that highlight feature-position correlations. On the technical side, our end-to-end analysis of the training dynamics of softmax-based <b>transformers</b> accommodates both input and position embeddings simultaneously, which is developed based on a novel approach to track the interplay between the attention of feature-position and position-wise correlations.</p></p class="citation"></blockquote><h3 id=1943--19309-dyce-dynamic-configurable-exiting-for-deep-learning-compression-and-scaling-qingyuan-wang-et-al-2024>(19/43 | 19/309) DyCE: Dynamic Configurable Exiting for Deep Learning Compression and Scaling (Qingyuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingyuan Wang, Barry Cardiff, Antoine Frappé, Benoit Larras, Deepu John. (2024)<br><strong>DyCE: Dynamic Configurable Exiting for Deep Learning Compression and Scaling</strong><br><button class=copy-to-clipboard title="DyCE: Dynamic Configurable Exiting for Deep Learning Compression and Scaling" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Pruning, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01695v1.pdf filename=2403.01695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern deep learning (DL) models necessitate the employment of scaling and compression techniques for effective deployment in resource-constrained environments. Most existing techniques, such as <b>pruning</b> and <b>quantization</b> are generally static. On the other hand, dynamic compression methods, such as early exits, reduce complexity by recognizing the difficulty of input samples and allocating computation as needed. Dynamic methods, despite their superior flexibility and potential for co-existing with static methods, pose significant challenges in terms of implementation due to any changes in dynamic parts will influence subsequent processes. Moreover, most current dynamic compression designs are monolithic and tightly integrated with base models, thereby complicating the adaptation to novel base models. This paper introduces DyCE, an dynamic configurable early-exit framework that decouples design considerations from each other and from the base model. Utilizing this framework, various types and positions of exits can be organized according to predefined configurations, which can be dynamically switched in real-time to accommodate evolving performance-complexity requirements. We also propose techniques for generating optimized configurations based on any desired trade-off between performance and computational complexity. This empowers future researchers to focus on the improvement of individual exits without latent compromise of overall system performance. The efficacy of this approach is demonstrated through image classification tasks with deep <b>CNNs.</b> DyCE significantly reduces the computational complexity by 23.5% of ResNet152 and 25.9% of ConvNextv2-tiny on ImageNet, with accuracy reductions of less than 0.5%. Furthermore, DyCE offers advantages over existing dynamic methods in terms of real-time configuration and fine-grained performance tuning.</p></p class="citation"></blockquote><h3 id=2043--20309-quantifying-and-predicting-residential-building-flexibility-using-machine-learning-methods-patrick-salter-et-al-2024>(20/43 | 20/309) Quantifying and Predicting Residential Building Flexibility Using Machine Learning Methods (Patrick Salter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patrick Salter, Qiuhua Huang, Paulo Cesar Tabares-Velasco. (2024)<br><strong>Quantifying and Predicting Residential Building Flexibility Using Machine Learning Methods</strong><br><button class=copy-to-clipboard title="Quantifying and Predicting Residential Building Flexibility Using Machine Learning Methods" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01669v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01669v1.pdf filename=2403.01669v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Residential buildings account for a significant portion (35%) of the total electricity consumption in the U.S. as of 2022. As more distributed energy resources are installed in buildings, their potential to provide flexibility to the grid increases. To tap into that flexibility provided by buildings, aggregators or system operators need to quantify and forecast flexibility. Previous works in this area primarily focused on commercial buildings, with little work on residential buildings. To address the gap, this paper first proposes two complementary flexibility metrics (i.e., power and energy flexibility) and then investigates several mainstream machine learning-based models for predicting the time-variant and sporadic flexibility of residential buildings at four-hour and 24-hour forecast horizons. The <b>long-short-term-memory</b> <b>(LSTM)</b> <b>model</b> <b>achieves</b> the best performance and can predict power flexibility for up to 24 hours ahead with the average error around 0.7 kW. However, for energy flexibility, the <b>LSTM</b> model is only successful for loads with consistent operational patterns throughout the year and faces challenges when predicting energy flexibility associated with HVAC systems.</p></p class="citation"></blockquote><h3 id=2143--21309-mitigating-label-noise-on-graph-via-topological-sample-selection-yuhao-wu-et-al-2024>(21/43 | 21/309) Mitigating Label Noise on Graph via Topological Sample Selection (Yuhao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhao Wu, Jiangchao Yao, Xiaobo Xia, Jun Yu, Ruxin Wang, Bo Han, Tongliang Liu. (2024)<br><strong>Mitigating Label Noise on Graph via Topological Sample Selection</strong><br><button class=copy-to-clipboard title="Mitigating Label Noise on Graph via Topological Sample Selection" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01942v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01942v1.pdf filename=2403.01942v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the success of the carefully-annotated <b>benchmarks,</b> the effectiveness of existing <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> can be considerably impaired in practice when the real-world <b>graph</b> <b>data</b> <b>is</b> noisily labeled. Previous explorations in sample selection have been demonstrated as an effective way for robust learning with noisy labels, however, the conventional studies focus on i.i.d data, and when moving to non-iid <b>graph</b> <b>data</b> <b>and</b> <b>GNNs,</b> two notable challenges remain: (1) nodes located near topological class boundaries are very informative for classification but cannot be successfully distinguished by the heuristic sample selection. (2) there is no available measure that considers the <b>graph</b> <b>topological</b> <b>information</b> to promote sample selection in a <b>graph.</b> <b>To</b> <b>address</b> this dilemma, we propose a $\textit{Topological Sample Selection}$ (TSS) method that boosts the informative sample selection process in a <b>graph</b> <b>by</b> <b>utilising</b> topological information. We theoretically prove that our procedure minimizes an upper bound of the expected risk under target clean distribution, and experimentally show the superiority of our method compared with state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=2243--22309-geometry-and-stability-of-supervised-learning-problems-facundo-mémoli-et-al-2024>(22/43 | 22/309) Geometry and Stability of Supervised Learning Problems (Facundo Mémoli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Facundo Mémoli, Brantley Vose, Robert C. Williamson. (2024)<br><strong>Geometry and Stability of Supervised Learning Problems</strong><br><button class=copy-to-clipboard title="Geometry and Stability of Supervised Learning Problems" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-MG<br>Keyword Score: 25<br>Keywords: Geometry, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01660v1.pdf filename=2403.01660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a notion of distance between <b>supervised</b> <b>learning</b> problems, which we call the Risk distance. This optimal-transport-inspired distance facilitates stability results; one can quantify how seriously issues like sampling bias, noise, limited data, and approximations might change a given problem by bounding how much these modifications can move the problem under the Risk distance. With the distance established, we explore the <b>geometry</b> of the resulting space of <b>supervised</b> <b>learning</b> problems, providing explicit geodesics and proving that the set of classification problems is dense in a larger class of problems. We also provide two variants of the Risk distance: one that incorporates specified weights on a problem&rsquo;s predictors, and one that is more sensitive to the contours of a problem&rsquo;s risk landscape.</p></p class="citation"></blockquote><h3 id=2343--23309-encodings-for-prediction-based-neural-architecture-search-yash-akhauri-et-al-2024>(23/43 | 23/309) Encodings for Prediction-based Neural Architecture Search (Yash Akhauri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yash Akhauri, Mohamed S. Abdelfattah. (2024)<br><strong>Encodings for Prediction-based Neural Architecture Search</strong><br><button class=copy-to-clipboard title="Encodings for Prediction-based Neural Architecture Search" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs-NE, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Transfer Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02484v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02484v1.pdf filename=2403.02484v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predictor-based methods have substantially enhanced Neural Architecture Search (NAS) optimization. The efficacy of these predictors is largely influenced by the method of encoding neural network architectures. While traditional encodings used an adjacency matrix describing the <b>graph</b> structure of a neural network, novel encodings embrace a variety of approaches from <b>unsupervised</b> pretraining of latent representations to vectors of zero-cost proxies. In this paper, we categorize and investigate neural encodings from three main types: structural, learned, and score-based. Furthermore, we extend these encodings and introduce \textit{unified encodings}, that extend NAS predictors to multiple search spaces. Our analysis draws from experiments conducted on over 1.5 million neural network architectures on NAS spaces such as NASBench-101 (NB101), NB201, NB301, Network Design Spaces (NDS), and TransNASBench-101. Building on our study, we present our predictor \textbf{FLAN}: \textbf{Fl}ow \textbf{A}ttention for \textbf{N}AS. FLAN integrates critical insights on predictor design, <b>transfer</b> <b>learning,</b> and \textit{unified encodings} to enable more than an order of magnitude cost reduction for training NAS accuracy predictors. Our implementation and encodings for all neural networks are open-sourced at \href{https://github.com/abdelfattah-lab/flan_nas}{https://github.com/abdelfattah-lab/flan_nas}.</p></p class="citation"></blockquote><h3 id=2443--24309-addressing-long-tail-noisy-label-learning-problems-a-two-stage-solution-with-label-refurbishment-considering-label-rarity-ying-hsuan-wu-et-al-2024>(24/43 | 24/309) Addressing Long-Tail Noisy Label Learning Problems: a Two-Stage Solution with Label Refurbishment Considering Label Rarity (Ying-Hsuan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying-Hsuan Wu, Jun-Wei Hsieh, Li Xin, Shin-You Teng, Yi-Kuan Hsieh, Ming-Ching Chang. (2024)<br><strong>Addressing Long-Tail Noisy Label Learning Problems: a Two-Stage Solution with Label Refurbishment Considering Label Rarity</strong><br><button class=copy-to-clipboard title="Addressing Long-Tail Noisy Label Learning Problems: a Two-Stage Solution with Label Refurbishment Considering Label Rarity" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Contrastive Learning, Noise-tolerant<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02363v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02363v1.pdf filename=2403.02363v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world datasets commonly exhibit noisy labels and class imbalance, such as long-tailed distributions. While previous research addresses this issue by differentiating noisy and clean samples, reliance on information from predictions based on noisy long-tailed data introduces potential errors. To overcome the limitations of prior works, we introduce an effective two-stage approach by combining soft-label refurbishing with multi-expert ensemble learning. In the first stage of robust soft label refurbishing, we acquire unbiased features through <b>contrastive</b> <b>learning,</b> making preliminary predictions using a classifier trained with a carefully designed BAlanced <b>Noise-tolerant</b> Cross-entropy (BANC) loss. In the second stage, our label refurbishment method is applied to obtain soft labels for multi-expert ensemble learning, providing a principled solution to the long-tail noisy label problem. Experiments conducted across multiple <b>benchmarks</b> validate the superiority of our approach, Label Refurbishment considering Label Rarity (LR^2), achieving remarkable accuracies of 94.19% and 77.05% on simulated noisy CIFAR-10 and CIFAR-100 long-tail datasets, as well as 77.74% and 81.40% on real-noise long-tail datasets, Food-101N and Animal-10N, surpassing existing state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2543--25309-improving-out-of-distribution-generalization-in-graphs-via-hierarchical-semantic-environments-yinhua-piao-et-al-2024>(25/43 | 25/309) Improving out-of-distribution generalization in graphs via hierarchical semantic environments (Yinhua Piao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinhua Piao, Sangseon Lee, Yijingxiu Lu, Sun Kim. (2024)<br><strong>Improving out-of-distribution generalization in graphs via hierarchical semantic environments</strong><br><button class=copy-to-clipboard title="Improving out-of-distribution generalization in graphs via hierarchical semantic environments" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Distribution Shift, Distribution Shift, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01773v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01773v1.pdf filename=2403.01773v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-distribution</b> (OOD) generalization in the <b>graph</b> domain is challenging due to complex <b>distribution</b> <b>shifts</b> and a lack of environmental contexts. Recent methods attempt to enhance <b>graph</b> OOD generalization by generating flat environments. However, such flat environments come with inherent limitations to capture more complex data <b>distributions.</b> <b>Considering</b> the DrugOOD dataset, which contains diverse training environments (e.g., scaffold, size, etc.), flat contexts cannot sufficiently address its high heterogeneity. Thus, a new challenge is posed to generate more semantically enriched environments to enhance <b>graph</b> invariant learning for handling <b>distribution</b> <b>shifts.</b> In this paper, we propose a novel approach to generate hierarchical semantic environments for each <b>graph.</b> Firstly, given an input <b>graph,</b> we explicitly extract variant subgraphs from the input <b>graph</b> to generate proxy predictions on local environments. Then, stochastic attention mechanisms are employed to re-extract the subgraphs for regenerating global environments in a hierarchical manner. In addition, we introduce a new learning objective that guides our model to learn the diversity of environments within the same hierarchy while maintaining consistency across different hierarchies. This approach enables our model to consider the relationships between environments and facilitates robust <b>graph</b> invariant learning. Extensive experiments on real-world <b>graph</b> data have demonstrated the effectiveness of our framework. Particularly, in the challenging dataset DrugOOD, our method achieves up to 1.29% and 2.83% improvement over the best baselines on IC50 and EC50 prediction tasks, respectively.</p></p class="citation"></blockquote><h3 id=2643--26309-density-based-isometric-mapping-bardia-yousefi-et-al-2024>(26/43 | 26/309) Density-based Isometric Mapping (Bardia Yousefi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bardia Yousefi, Mélina Khansari, Ryan Trask, Patrick Tallon, Carina Carino, Arman Afrasiyabi, Vikas Kundra, Lan Ma, Lei Ren, Keyvan Farahani, Michelle Hershman. (2024)<br><strong>Density-based Isometric Mapping</strong><br><button class=copy-to-clipboard title="Density-based Isometric Mapping" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 22<br>Keywords: MNIST, Graph, Benchmarking, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02531v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02531v1.pdf filename=2403.02531v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The isometric mapping method employs the shortest path algorithm to estimate the Euclidean distance between points on High dimensional (HD) manifolds. This may not be sufficient for weakly uniformed HD data as it could lead to overestimating distances between far neighboring points, resulting in inconsistencies between the intrinsic (local) and extrinsic (global) distances during the projection. To address this issue, we modify the shortest path algorithm by adding a novel constraint inspired by the Parzen-Rosenblatt (PR) window, which helps to maintain the uniformity of the constructed shortest-path <b>graph</b> in Isomap. Multiple imaging datasets overall of 72,236 cases, 70,000 MINST data, 1596 from multiple Chest-XRay pneumonia datasets, and three NSCLC CT/PET datasets with a total of 640 lung cancer patients, were used to <b>benchmark</b> and validate PR-Isomap. 431 imaging biomarkers were extracted from each modality. Our results indicate that PR-Isomap projects HD attributes into a lower-dimensional (LD) space while preserving information, visualized by the <b>MNIST</b> dataset indicating the maintaining local and global distances. PR-Isomap achieved the highest comparative accuracies of 80.9% (STD:5.8) for pneumonia and 78.5% (STD:4.4), 88.4% (STD:1.4), and 61.4% (STD:11.4) for three NSCLC datasets, with a confidence interval of 95% for outcome prediction. Similarly, the multivariate Cox model showed higher overall survival, measured with c-statistics and log-likelihood test, of PR-Isomap compared to other dimensionality reduction methods. Kaplan Meier survival curve also signifies the notable ability of PR-Isomap to distinguish between high-risk and low-risk patients using <b>multimodal</b> imaging biomarkers preserving HD imaging characteristics for precision medicine.</p></p class="citation"></blockquote><h3 id=2743--27309-on-latency-predictors-for-neural-architecture-search-yash-akhauri-et-al-2024>(27/43 | 27/309) On Latency Predictors for Neural Architecture Search (Yash Akhauri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yash Akhauri, Mohamed S. Abdelfattah. (2024)<br><strong>On Latency Predictors for Neural Architecture Search</strong><br><button class=copy-to-clipboard title="On Latency Predictors for Neural Architecture Search" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AR, cs-CV, cs-LG, cs-PF, cs.LG<br>Keyword Score: 20<br>Keywords: Meta Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02446v1.pdf filename=2403.02446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient deployment of neural networks (NN) requires the co-optimization of accuracy and latency. For example, hardware-aware neural architecture search has been used to automatically find NN architectures that satisfy a latency constraint on a specific hardware device. Central to these search algorithms is a prediction model that is designed to provide a hardware latency estimate for a candidate NN architecture. Recent research has shown that the sample efficiency of these predictive models can be greatly improved through pre-training on some \textit{training} devices with many samples, and then transferring the predictor on the \textit{test} (target) device. <b>Transfer</b> <b>learning</b> and <b>meta-learning</b> <b>methods</b> have been used for this, but often exhibit significant performance variability. Additionally, the evaluation of existing latency predictors has been largely done on hand-crafted training/test device sets, making it difficult to ascertain design features that compose a robust and general latency predictor. To address these issues, we introduce a comprehensive suite of latency prediction tasks obtained in a principled way through automated partitioning of hardware device sets. We then design a general latency predictor to comprehensively study (1) the predictor architecture, (2) NN sample selection methods, (3) hardware device representations, and (4) NN operation encoding schemes. Building on conclusions from our study, we present an end-to-end latency predictor training strategy that outperforms existing methods on 11 out of 12 difficult latency prediction tasks, improving latency prediction by 22.5% on average, and up to to 87.6% on the hardest tasks. Focusing on latency prediction, our HW-Aware NAS reports a $5.8\times$ speedup in wall-clock time. Our code is available on \href{https://github.com/abdelfattah-lab/nasflat_latency}{https://github.com/abdelfattah-lab/nasflat_latency}.</p></p class="citation"></blockquote><h3 id=2843--28309-joint-parameter-and-parameterization-inference-with-uncertainty-quantification-through-differentiable-programming-yongquan-qu-et-al-2024>(28/43 | 28/309) Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming (Yongquan Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongquan Qu, Mohamed Aziz Bhouri, Pierre Gentine. (2024)<br><strong>Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming</strong><br><button class=copy-to-clipboard title="Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-DS, nlin-CD, physics-ao-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02215v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02215v1.pdf filename=2403.02215v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate representations of unknown and sub-grid physical processes through parameterizations (or closure) in numerical <b>simulations</b> with quantified uncertainty are critical for resolving the coarse-grained partial differential equations that govern many problems ranging from weather and climate prediction to turbulence <b>simulations.</b> Recent advances have seen machine learning (ML) increasingly applied to model these subgrid processes, resulting in the development of hybrid physics-ML models through the integration with numerical solvers. In this work, we introduce a novel framework for the joint estimation and uncertainty quantification of physical parameters and machine learning parameterizations in tandem, leveraging differentiable programming. Achieved through online training and efficient Bayesian inference within a high-dimensional parameter space, this approach is enabled by the capabilities of differentiable programming. This proof of concept underscores the substantial potential of differentiable programming in synergistically combining machine learning with differential equations, thereby enhancing the capabilities of hybrid physics-ML modeling.</p></p class="citation"></blockquote><h3 id=2943--29309-flowprecision-advancing-fpga-based-real-time-fluid-flow-estimation-with-linear-quantization-tianheng-ling-et-al-2024>(29/43 | 29/309) FlowPrecision: Advancing FPGA-Based Real-Time Fluid Flow Estimation with Linear Quantization (Tianheng Ling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianheng Ling, Julian Hoever, Chao Qian, Gregor Schiele. (2024)<br><strong>FlowPrecision: Advancing FPGA-Based Real-Time Fluid Flow Estimation with Linear Quantization</strong><br><button class=copy-to-clipboard title="FlowPrecision: Advancing FPGA-Based Real-Time Fluid Flow Estimation with Linear Quantization" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-flu-dyn<br>Keyword Score: 20<br>Keywords: Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01922v1.pdf filename=2403.01922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In industrial and environmental monitoring, achieving real-time and precise fluid flow measurement remains a critical challenge. This study applies linear <b>quantization</b> in FPGA-based soft sensors for fluid flow estimation, significantly enhancing Neural Network model precision by overcoming the limitations of traditional fixed-point <b>quantization.</b> Our approach achieves up to a 10.10% reduction in Mean Squared Error and a notable 9.39% improvement in inference speed through targeted hardware optimizations. Validated across multiple data sets, our findings demonstrate that the optimized FPGA-based <b>quantized</b> models can provide efficient, accurate real-time inference, offering a viable alternative to cloud-based processing in pervasive autonomous systems.</p></p class="citation"></blockquote><h3 id=3043--30309-a-survey-on-evaluation-of-out-of-distribution-generalization-han-yu-et-al-2024>(30/43 | 30/309) A Survey on Evaluation of Out-of-Distribution Generalization (Han Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Yu, Jiashuo Liu, Xingxuan Zhang, Jiayun Wu, Peng Cui. (2024)<br><strong>A Survey on Evaluation of Out-of-Distribution Generalization</strong><br><button class=copy-to-clipboard title="A Survey on Evaluation of Out-of-Distribution Generalization" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Distribution Shift, Distribution Shift, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01874v1.pdf filename=2403.01874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning models, while progressively advanced, rely heavily on the IID assumption, which is often unfulfilled in practice due to inevitable <b>distribution</b> <b>shifts.</b> This renders them susceptible and untrustworthy for deployment in risk-sensitive applications. Such a significant problem has consequently spawned various branches of works dedicated to developing algorithms capable of <b>Out-of-Distribution</b> (OOD) generalization. Despite these efforts, much less attention has been paid to the evaluation of OOD generalization, which is also a complex and fundamental problem. Its goal is not only to assess whether a model&rsquo;s OOD generalization capability is strong or not, but also to evaluate where a model generalizes well or poorly. This entails characterizing the types of <b>distribution</b> <b>shifts</b> that a model can effectively address, and identifying the safe and risky input regions given a model. This paper serves as the first effort to conduct a comprehensive review of OOD evaluation. We categorize existing research into three paradigms: OOD performance testing, OOD performance prediction, and OOD intrinsic property characterization, according to the availability of test data. Additionally, we briefly discuss OOD evaluation in the context of pretrained models. In closing, we propose several promising directions for future research in OOD evaluation.</p></p class="citation"></blockquote><h3 id=3143--31309-nash-neural-architecture-search-for-hardware-optimized-machine-learning-models-mengfei-ji-et-al-2024>(31/43 | 31/309) NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models (Mengfei Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengfei Ji, Zaid Al-Ars. (2024)<br><strong>NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models</strong><br><button class=copy-to-clipboard title="NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01845v1.pdf filename=2403.01845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As machine learning (ML) algorithms get deployed in an ever-increasing number of applications, these algorithms need to achieve better trade-offs between high accuracy, high throughput and low latency. This paper introduces NASH, a novel approach that applies neural architecture search to machine learning hardware. Using NASH, hardware designs can achieve not only high throughput and low latency but also superior accuracy performance. We present four versions of the NASH strategy in this paper, all of which show higher accuracy than the original models. The strategy can be applied to various <b>convolutional</b> <b>neural</b> <b>networks,</b> selecting specific model operations among many to guide the training process toward higher accuracy. Experimental results show that applying NASH on ResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top 5 accuracy increase of up to 2.2% compared to the non-NASH version when tested on the ImageNet data set. We also integrated this approach into the FINN hardware model synthesis tool to automate the application of our approach and the generation of the hardware model. Results show that using FINN can achieve a maximum throughput of 324.5 fps. In addition, NASH models can also result in a better trade-off between accuracy and hardware resource utilization. The accuracy-hardware (HW) Pareto curve shows that the models with the four NASH versions represent the best trade-offs achieving the highest accuracy for a given HW utilization. The code for our implementation is open-source and publicly available on GitHub at <a href=https://github.com/MFJI/NASH>https://github.com/MFJI/NASH</a>.</p></p class="citation"></blockquote><h3 id=3243--32309-diffusion-ts-interpretable-diffusion-for-general-time-series-generation-xinyu-yuan-et-al-2024>(32/43 | 32/309) Diffusion-TS: Interpretable Diffusion for General Time Series Generation (Xinyu Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Yuan, Yan Qiao. (2024)<br><strong>Diffusion-TS: Interpretable Diffusion for General Time Series Generation</strong><br><button class=copy-to-clipboard title="Diffusion-TS: Interpretable Diffusion for General Time Series Generation" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Probabilistic Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01742v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01742v1.pdf filename=2403.01742v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Denoising diffusion <b>probabilistic</b> <b>models</b> (DDPMs) are becoming the leading paradigm for generative models. It has recently shown breakthroughs in audio synthesis, time series imputation and forecasting. In this paper, we propose Diffusion-TS, a novel diffusion-based framework that generates multivariate time series samples of high quality by using an encoder-decoder <b>transformer</b> with disentangled temporal representations, in which the decomposition technique guides Diffusion-TS to capture the semantic meaning of time series while <b>transformers</b> mine detailed sequential information from the noisy model input. Different from existing diffusion-based approaches, we train the model to directly reconstruct the sample instead of the noise in each diffusion step, combining a Fourier-based loss term. Diffusion-TS is expected to generate time series satisfying both interpretablity and realness. In addition, it is shown that the proposed Diffusion-TS can be easily extended to conditional generation tasks, such as forecasting and imputation, without any model changes. This also motivates us to further explore the performance of Diffusion-TS under irregular settings. Finally, through qualitative and quantitative experiments, results show that Diffusion-TS achieves the state-of-the-art results on various realistic analyses of time series.</p></p class="citation"></blockquote><h3 id=3343--33309-day-ahead-regional-solar-power-forecasting-with-hierarchical-temporal-convolutional-neural-networks-using-historical-power-generation-and-weather-data-maneesha-perera-et-al-2024>(33/43 | 33/309) Day-ahead regional solar power forecasting with hierarchical temporal convolutional neural networks using historical power generation and weather data (Maneesha Perera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maneesha Perera, Julian De Hoog, Kasun Bandara, Damith Senanayake, Saman Halgamuge. (2024)<br><strong>Day-ahead regional solar power forecasting with hierarchical temporal convolutional neural networks using historical power generation and weather data</strong><br><button class=copy-to-clipboard title="Day-ahead regional solar power forecasting with hierarchical temporal convolutional neural networks using historical power generation and weather data" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01653v1.pdf filename=2403.01653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Regional solar power forecasting, which involves predicting the total power generation from all rooftop photovoltaic systems in a region holds significant importance for various stakeholders in the energy sector. However, the vast amount of solar power generation and weather time series from geographically dispersed locations that need to be considered in the forecasting process makes accurate regional forecasting challenging. Therefore, previous work has limited the focus to either forecasting a single time series (i.e., aggregated time series) which is the addition of all solar generation time series in a region, disregarding the location-specific weather effects or forecasting solar generation time series of each PV site (i.e., individual time series) independently using location-specific weather data, resulting in a large number of forecasting models. In this work, we propose two deep-learning-based regional forecasting methods that can effectively leverage both types of time series (aggregated and individual) with weather data in a region. We propose two hierarchical temporal <b>convolutional</b> <b>neural</b> <b>network</b> architectures (HTCNN) and two strategies to adapt HTCNNs for regional solar power forecasting. At first, we explore generating a regional forecast using a single HTCNN. Next, we divide the region into multiple sub-regions based on weather information and train separate HTCNNs for each sub-region; the forecasts of each sub-region are then added to generate a regional forecast. The proposed work is evaluated using a large dataset collected over a year from 101 locations across Western Australia to provide a day ahead forecast. We compare our approaches with well-known alternative methods and show that the sub-region HTCNN requires fewer individual networks and achieves a forecast skill score of 40.2% reducing a statistically significant error by 6.5% compared to the best counterpart.</p></p class="citation"></blockquote><h3 id=3443--34309-inf2guard-an-information-theoretic-framework-for-learning-privacy-preserving-representations-against-inference-attacks-sayedeh-leila-noorbakhsh-et-al-2024>(34/43 | 34/309) Inf2Guard: An Information-Theoretic Framework for Learning Privacy-Preserving Representations against Inference Attacks (Sayedeh Leila Noorbakhsh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayedeh Leila Noorbakhsh, Binghui Zhang, Yuan Hong, Binghui Wang. (2024)<br><strong>Inf2Guard: An Information-Theoretic Framework for Learning Privacy-Preserving Representations against Inference Attacks</strong><br><button class=copy-to-clipboard title="Inf2Guard: An Information-Theoretic Framework for Learning Privacy-Preserving Representations against Inference Attacks" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Mutual Information, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02116v1.pdf filename=2403.02116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning (ML) is vulnerable to inference (e.g., membership inference, property inference, and data reconstruction) attacks that aim to infer the private information of training data or dataset. Existing defenses are only designed for one specific type of attack and sacrifice significant utility or are soon broken by adaptive attacks. We address these limitations by proposing an information-theoretic defense framework, called Inf2Guard, against the three major types of inference attacks. Our framework, inspired by the success of <b>representation</b> <b>learning,</b> posits that learning shared <b>representations</b> <b>not</b> only saves time/costs but also benefits numerous downstream tasks. Generally, Inf2Guard involves two <b>mutual</b> <b>information</b> objectives, for privacy protection and utility preservation, respectively. Inf2Guard exhibits many merits: it facilitates the design of customized objectives against the specific inference attack; it provides a general defense framework which can treat certain existing defenses as special cases; and importantly, it aids in deriving theoretical results, e.g., inherent utility-privacy tradeoff and guaranteed privacy leakage. Extensive evaluations validate the effectiveness of Inf2Guard for learning privacy-preserving <b>representations</b> <b>against</b> inference attacks and demonstrate the superiority over the baselines.</p></p class="citation"></blockquote><h3 id=3543--35309-towards-optimal-customized-architecture-for-heterogeneous-federated-learning-with-contrastive-cloud-edge-model-decoupling-xingyan-chen-et-al-2024>(35/43 | 35/309) Towards Optimal Customized Architecture for Heterogeneous Federated Learning with Contrastive Cloud-Edge Model Decoupling (Xingyan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyan Chen, Tian Du, Mu Wang, Tiancheng Gu, Yu Zhao, Gang Kou, Changqiao Xu, Dapeng Oliver Wu. (2024)<br><strong>Towards Optimal Customized Architecture for Heterogeneous Federated Learning with Contrastive Cloud-Edge Model Decoupling</strong><br><button class=copy-to-clipboard title="Towards Optimal Customized Architecture for Heterogeneous Federated Learning with Contrastive Cloud-Edge Model Decoupling" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02360v1.pdf filename=2403.02360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning,</b> as a promising distributed learning paradigm, enables collaborative training of a global model across multiple network edge clients without the need for central data collecting. However, the heterogeneity of edge data distribution drags the model towards the local minima, which can be distant from the global optimum. Such heterogeneity often leads to slow convergence and substantial communication overhead. To address these issues, we propose a novel <b>federated</b> <b>learning</b> framework called FedCMD, a model decoupling tailored to the Cloud-edge supported <b>federated</b> <b>learning</b> that separates deep neural networks into a body for capturing shared representations in Cloud and a personalized head for migrating data heterogeneity. Our motivation is that, by the deep investigation of the performance of selecting different neural network layers as the personalized head, we found rigidly assigning the last layer as the personalized head in current studies is not always optimal. Instead, it is necessary to dynamically select the personalized layer that maximizes the training performance by taking the representation difference between neighbor layers into account. To find the optimal personalized layer, we utilize the low-dimensional representation of each layer to contrast feature distribution transfer and introduce a Wasserstein-based layer selection method, aimed at identifying the best-match layer for personalization. Additionally, a weighted global aggregation algorithm is proposed based on the selected personalized layer for the practical application of FedCMD. Extensive experiments on ten <b>benchmarks</b> demonstrate the efficiency and superior performance of our solution compared with nine state-of-the-art solutions. All code and results are available at <a href=https://github.com/elegy112138/FedCMD>https://github.com/elegy112138/FedCMD</a>.</p></p class="citation"></blockquote><h3 id=3643--36309-root-causing-prediction-anomalies-using-explainable-ai-ramanathan-vishnampet-et-al-2024>(36/43 | 36/309) Root Causing Prediction Anomalies Using Explainable AI (Ramanathan Vishnampet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ramanathan Vishnampet, Rajesh Shenoy, Jianhui Chen, Anuj Gupta. (2024)<br><strong>Root Causing Prediction Anomalies Using Explainable AI</strong><br><button class=copy-to-clipboard title="Root Causing Prediction Anomalies Using Explainable AI" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02439v1.pdf filename=2403.02439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel application of <b>explainable</b> <b>AI</b> (XAI) for root-causing performance degradation in machine learning models that learn continuously from user engagement data. In such systems a single feature corruption can cause cascading feature, label and concept drifts. We have successfully applied this technique to improve the reliability of models used in personalized advertising. Performance degradation in such systems manifest as prediction anomalies in the models. These models are typically trained continuously using features that are produced by hundreds of real time data processing pipelines or derived from other upstream models. A failure in any of these pipelines or an instability in any of the upstream models can cause feature corruption, causing the model&rsquo;s predicted output to deviate from the actual output and the training data to become corrupted. The causal relationship between the features and the predicted output is complex, and root-causing is challenging due to the scale and dynamism of the system. We demonstrate how temporal shifts in the global feature importance distribution can effectively isolate the cause of a prediction anomaly, with better recall than model-to-feature correlation methods. The technique appears to be effective even when approximating the local feature importance using a simple perturbation-based method, and aggregating over a few thousand examples. We have found this technique to be a model-agnostic, cheap and effective way to monitor complex data pipelines in production and have deployed a system for continuously analyzing the global feature importance distribution of continuously trained models.</p></p class="citation"></blockquote><h3 id=3743--37309-gradient-correlation-subspace-learning-against-catastrophic-forgetting-tammuz-dubnov-et-al-2024>(37/43 | 37/309) Gradient Correlation Subspace Learning against Catastrophic Forgetting (Tammuz Dubnov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tammuz Dubnov, Vishal Thengane. (2024)<br><strong>Gradient Correlation Subspace Learning against Catastrophic Forgetting</strong><br><button class=copy-to-clipboard title="Gradient Correlation Subspace Learning against Catastrophic Forgetting" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02334v1.pdf filename=2403.02334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient <b>continual</b> <b>learning</b> techniques have been a topic of significant research over the last few years. A fundamental problem with such learning is severe degradation of performance on previously learned tasks, known also as catastrophic forgetting. This paper introduces a novel method to reduce catastrophic forgetting in the context of incremental class learning called Gradient Correlation Subspace Learning (GCSL). The method detects a subspace of the weights that is least affected by previous tasks and projects the weights to train for the new task into said subspace. The method can be applied to one or more layers of a given network architectures and the size of the subspace used can be altered from layer to layer and task to task. Code will be available at \href{https://github.com/vgthengane/GCSL}{https://github.com/vgthengane/GCSL}</p></p class="citation"></blockquote><h3 id=3843--38309-neural-redshift-random-networks-are-not-random-functions-damien-teney-et-al-2024>(38/43 | 38/309) Neural Redshift: Random Networks are not Random Functions (Damien Teney et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Damien Teney, Armand Nicolicioiu, Valentin Hartmann, Ehsan Abbasnejad. (2024)<br><strong>Neural Redshift: Random Networks are not Random Functions</strong><br><button class=copy-to-clipboard title="Neural Redshift: Random Networks are not Random Functions" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02241v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02241v2.pdf filename=2403.02241v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs. Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent &ldquo;simplicity bias&rdquo;. This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. <b>Transformers</b> also inherit all these properties from their building blocks. Implications. We provide a fresh explanation for the success of deep learning independent from gradient-based training. It points at promising avenues for controlling the solutions implemented by trained models.</p></p class="citation"></blockquote><h3 id=3943--39309-mutual-information-estimation-via-normalizing-flows-ivan-butakov-et-al-2024>(39/43 | 39/309) Mutual Information Estimation via Normalizing Flows (Ivan Butakov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ivan Butakov, Alexander Tolmachev, Sofia Malanchuk, Anna Neopryatnaya, Alexey Frolov. (2024)<br><strong>Mutual Information Estimation via Normalizing Flows</strong><br><button class=copy-to-clipboard title="Mutual Information Estimation via Normalizing Flows" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT, stat-ML<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02187v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02187v2.pdf filename=2403.02187v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel approach to the problem of <b>mutual</b> <b>information</b> (MI) estimation via introducing normalizing flows-based estimator. The estimator maps original data to the target distribution with known closed-form expression for MI. We demonstrate that our approach yields MI estimates for the original data. Experiments with high-dimensional data are provided to show the advantages of the proposed estimator.</p></p class="citation"></blockquote><h3 id=4043--40309-iterated-q-network-beyond-the-one-step-bellman-operator-théo-vincent-et-al-2024>(40/43 | 40/309) Iterated $Q$-Network: Beyond the One-Step Bellman Operator (Théo Vincent et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Théo Vincent, Daniel Palenicek, Boris Belousov, Jan Peters, Carlo D&rsquo;Eramo. (2024)<br><strong>Iterated $Q$-Network: Beyond the One-Step Bellman Operator</strong><br><button class=copy-to-clipboard title="Iterated $Q$-Network: Beyond the One-Step Bellman Operator" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02107v1.pdf filename=2403.02107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Value-based <b>Reinforcement</b> <b>Learning</b> (RL) methods rely on the application of the Bellman operator, which needs to be approximated from samples. Most approaches consist of an iterative scheme alternating the application of the Bellman operator and a subsequent projection step onto a considered function space. However, we observe that these algorithms can be improved by considering multiple iterations of the Bellman operator at once. Thus, we introduce iterated $Q$-Networks (iQN), a novel approach that learns a sequence of $Q$-function approximations where each $Q$-function serves as the target for the next one in a chain of consecutive Bellman iterations. We demonstrate that iQN is theoretically sound and show how it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate its advantages on Atari $2600$ games and in continuous-control MuJoCo environments.</p></p class="citation"></blockquote><h3 id=4143--41309-matrix-completion-with-convex-optimization-and-column-subset-selection-antonina-krajewska-et-al-2024>(41/43 | 41/309) Matrix Completion with Convex Optimization and Column Subset Selection (Antonina Krajewska et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonina Krajewska, Ewa Niewiadomska-Szynkiewicz. (2024)<br><strong>Matrix Completion with Convex Optimization and Column Subset Selection</strong><br><button class=copy-to-clipboard title="Matrix Completion with Convex Optimization and Column Subset Selection" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: G-1-3; G-1-6, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01919v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01919v2.pdf filename=2403.01919v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a two-step method for the matrix recovery problem. Our approach combines the theoretical foundations of the Column Subset Selection and Low-rank Matrix Completion problems. The proposed method, in each step, solves a convex optimization task. We present two algorithms that implement our Columns Selected Matrix Completion (CSMC) method, each dedicated to a different size problem. We performed a formal analysis of the presented method, in which we formulated the necessary assumptions and the probability of finding a correct solution. In the second part of the paper, we present the results of the experimental work. Numerical experiments verified the correctness and performance of the algorithms. To study the influence of the matrix size, rank, and the proportion of missing elements on the quality of the solution and the computation time, we performed experiments on synthetic data. The presented method was applied to two real-life problems problems: prediction of movie rates in a <b>recommendation</b> system and image inpainting. Our thorough analysis shows that CSMC provides solutions of comparable quality to matrix completion algorithms, which are based on convex optimization. However, CSMC offers notable savings in terms of runtime.</p></p class="citation"></blockquote><h3 id=4243--42309-robustness-bounds-on-the-successful-adversarial-examples-theory-and-practice-hiroaki-maeshima-et-al-2024>(42/43 | 42/309) Robustness Bounds on the Successful Adversarial Examples: Theory and Practice (Hiroaki Maeshima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hiroaki Maeshima, Akira Otsuka. (2024)<br><strong>Robustness Bounds on the Successful Adversarial Examples: Theory and Practice</strong><br><button class=copy-to-clipboard title="Robustness Bounds on the Successful Adversarial Examples: Theory and Practice" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01896v1.pdf filename=2403.01896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adversarial example (AE) is an attack method for machine learning, which is crafted by adding imperceptible perturbation to the data inducing misclassification. In the current paper, we investigated the upper bound of the probability of successful AEs based on the <b>Gaussian</b> <b>Process</b> (GP) classification. We proved a new upper bound that depends on AE&rsquo;s perturbation norm, the kernel function used in GP, and the distance of the closest pair with different labels in the training dataset. Surprisingly, the upper bound is determined regardless of the distribution of the sample dataset. We showed that our theoretical result was confirmed through the experiment using ImageNet. In addition, we showed that changing the parameters of the kernel function induces a change of the upper bound of the probability of successful AEs.</p></p class="citation"></blockquote><h3 id=4343--43309-permutation-invariant-functions-statistical-tests-dimension-reduction-in-metric-entropy-and-estimation-wee-chaimanowong-et-al-2024>(43/43 | 43/309) Permutation invariant functions: statistical tests, dimension reduction in metric entropy and estimation (Wee Chaimanowong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wee Chaimanowong, Ying Zhu. (2024)<br><strong>Permutation invariant functions: statistical tests, dimension reduction in metric entropy and estimation</strong><br><button class=copy-to-clipboard title="Permutation invariant functions: statistical tests, dimension reduction in metric entropy and estimation" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 62G07 62G10 62G07 (Primary), 62G08, 62G10 (Secondary), cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01671v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01671v1.pdf filename=2403.01671v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Permutation invariance is among the most common symmetry that can be exploited to simplify complex problems in machine learning (ML). There has been a tremendous surge of research activities in building permutation invariant ML architectures. However, less attention is given to how to statistically test for permutation invariance of variables in a multivariate probability distribution where the dimension is allowed to grow with the <b>sample</b> <b>size.</b> Also, in terms of a statistical theory, little is known about how permutation invariance helps with estimation in reducing dimensions. In this paper, we take a step back and examine these questions in several fundamental problems: (i) testing the assumption of permutation invariance of multivariate distributions; (ii) estimating permutation invariant densities; (iii) analyzing the metric entropy of smooth permutation invariant function classes and compare them with their counterparts without imposing permutation invariance; (iv) kernel ridge regression of permutation invariant functions in reproducing kernel Hilbert space. In particular, our methods for (i) and (iv) are based on a sorting trick and (ii) is based on an averaging trick. These tricks substantially simplify the exploitation of permutation invariance.</p></p class="citation"></blockquote><h2 id=cscl-64>cs.CL (64)</h2><h3 id=164--44309-leveraging-weakly-annotated-data-for-hate-speech-detection-in-code-mixed-hinglish-a-feasibility-driven-transfer-learning-approach-with-large-language-models-sargam-yadav-et-al-2024>(1/64 | 44/309) Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language Models (Sargam Yadav et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sargam Yadav, Abhishek Kaushik, Kevin McDaid. (2024)<br><strong>Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language Models</strong><br><button class=copy-to-clipboard title="Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language Models" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 133<br>Keywords: Benchmarking, Few-shot, Few-shot Learning, Low-Resource, Transfer Learning, Zero-shot, BART, ChatGPT, Transformer, Hate Speech Detection, Large Language Model, Large Language Model, Prompt, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02121v1.pdf filename=2403.02121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has advanced the <b>benchmark</b> in various Natural Language Processing (NLP) tasks. However, <b>large</b> <b>amounts</b> <b>of</b> labelled training data are required to train <b>LLMs.</b> Furthermore, data annotation and training are computationally expensive and time-consuming. Zero and <b>few-shot</b> <b>learning</b> have recently emerged as viable options for labelling data using <b>large</b> <b>pre-trained</b> <b>models.</b> <b>Hate</b> <b>speech</b> <b>detection</b> in mix-code <b>low-resource</b> languages is an active problem area where the use of <b>LLMs</b> has proven beneficial. In this study, we have compiled a dataset of 100 YouTube comments, and weakly labelled them for coarse and fine-grained misogyny classification in mix-code Hinglish. Weak annotation was applied due to the labor-intensive annotation process. <b>Zero-shot</b> <b>learning,</b> one-shot learning, and <b>few-shot</b> <b>learning</b> and <b>prompting</b> approaches have then been applied to assign labels to the comments and compare them to human-assigned labels. Out of all the approaches, <b>zero-shot</b> <b>classification</b> using the Bidirectional Auto-Regressive <b>Transformers</b> <b>(BART)</b> <b>large</b> <b>model</b> <b>and</b> <b>few-shot</b> <b>prompting</b> using Generative Pre-trained Transformer- 3 <b>(ChatGPT-3)</b> achieve the best results</p></p class="citation"></blockquote><h3 id=264--45309-analyzing-and-adapting-large-language-models-for-few-shot-multilingual-nlu-are-we-there-yet-evgeniia-razumovskaia-et-al-2024>(2/64 | 45/309) Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet? (Evgeniia Razumovskaia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Evgeniia Razumovskaia, Ivan Vulić, Anna Korhonen. (2024)<br><strong>Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?</strong><br><button class=copy-to-clipboard title="Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 120<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Low-Resource, Supervised Learning, Language Generation, In-context Learning, In-context Learning, In-context Learning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01929v1.pdf filename=2403.01929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Supervised</b> <b>fine-tuning</b> (SFT), <b>supervised</b> <b>instruction</b> <b>tuning</b> (SIT) and <b>in-context</b> <b>learning</b> <b>(ICL)</b> are three alternative, de facto standard approaches to <b>few-shot</b> <b>learning.</b> <b>ICL</b> has gained popularity recently with the advent of <b>LLMs</b> due to its simplicity and sample efficiency. Prior research has conducted only limited investigation into how these approaches work for multilingual <b>few-shot</b> <b>learning,</b> and the focus so far has been mostly on their performance. In this work, we present an extensive and systematic comparison of the three approaches, testing them on 6 high- and <b>low-resource</b> <b>languages,</b> <b>three</b> different NLU tasks, and a myriad of <b>language</b> <b>and</b> domain setups. Importantly, performance is only one aspect of the comparison, where we also analyse the approaches through the optics of their computational, inference and financial costs. Our observations show that <b>supervised</b> <b>instruction</b> <b>tuning</b> has the best trade-off between performance and resource requirements. As another contribution, we analyse the impact of target <b>language</b> <b>adaptation</b> of pretrained <b>LLMs</b> and find that the standard adaptation approaches can (superficially) improve target <b>language</b> <b>generation</b> capabilities, but <b>language</b> <b>understanding</b> elicited through <b>ICL</b> does not improve and remains limited, with low scores especially for <b>low-resource</b> languages.</p></p class="citation"></blockquote><h3 id=364--46309-balancing-enhancement-harmlessness-and-general-capabilities-enhancing-conversational-llms-with-direct-rlhf-chen-zheng-et-al-2024>(3/64 | 46/309) Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF (Chen Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Zheng, Ke Sun, Hang Wu, Chenguang Xi, Xun Zhou. (2024)<br><strong>Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF</strong><br><button class=copy-to-clipboard title="Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Fine-tuning, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Supervised Learning, Mistral, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02513v1.pdf filename=2403.02513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent advancements in Conversational <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> a concerning trend has emerged, showing that many new base <b>LLMs</b> experience a knowledge reduction in their foundational capabilities following <b>Supervised</b> <b>Fine-Tuning</b> (SFT). This process often leads to issues such as forgetting or a decrease in the base model&rsquo;s abilities. Moreover, <b>fine-tuned</b> models struggle to align with user preferences, inadvertently increasing the generation of toxic outputs when specifically <b>prompted.</b> To overcome these challenges, we adopted an innovative approach by completely bypassing SFT and directly implementing Harmless <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF).</b> Our method not only preserves the base model&rsquo;s general capabilities but also significantly enhances its conversational abilities, while notably reducing the generation of toxic outputs. Our approach holds significant implications for fields that demand a nuanced understanding and generation of responses, such as customer service. We applied this methodology to <b>Mistral,</b> the most popular base model, thereby creating <b>Mistral-Plus.</b> Our validation across 11 general tasks demonstrates that <b>Mistral-Plus</b> outperforms similarly sized open-source base models and their corresponding instruct versions. Importantly, the conversational abilities of <b>Mistral-Plus</b> were significantly improved, indicating a substantial advancement over traditional SFT models in both safety and user preference alignment.</p></p class="citation"></blockquote><h3 id=464--47309-differentially-private-synthetic-data-via-foundation-model-apis-2-text-chulin-xie-et-al-2024>(4/64 | 47/309) Differentially Private Synthetic Data via Foundation Model APIs 2: Text (Chulin Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chulin Xie, Zinan Lin, Arturs Backurs, Sivakanth Gopi, Da Yu, Huseyin A Inan, Harsha Nori, Haotian Jiang, Huishuai Zhang, Yin Tat Lee, Bo Li, Sergey Yekhanin. (2024)<br><strong>Differentially Private Synthetic Data via Foundation Model APIs 2: Text</strong><br><button class=copy-to-clipboard title="Differentially Private Synthetic Data via Foundation Model APIs 2: Text" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 93<br>Keywords: Diffusion Model, Benchmarking, Fine-tuning, Foundation Model, GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01749v1.pdf filename=2403.01749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., <b>differential</b> <b>privacy</b> (DP), offers a promising and scalable solution. However, existing methods necessitate DP <b>finetuning</b> of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on private data to generate DP synthetic data. This approach is not viable for proprietary <b>LLMs</b> (e.g., <b>GPT-3.5)</b> and also demands considerable computational resources for open-source <b>LLMs.</b> Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to <b>diffusion</b> <b>models.</b> In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an <b>LLM</b> and generate DP synthetic text without any model training. We conduct comprehensive experiments on three <b>benchmark</b> datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP <b>finetuning</b> baselines. This underscores the feasibility of relying solely on API access of <b>LLMs</b> to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving <b>LLM</b> applications. Our code and data are available at <a href=https://github.com/AI-secure/aug-pe>https://github.com/AI-secure/aug-pe</a>.</p></p class="citation"></blockquote><h3 id=564--48309-sciassess-benchmarking-llm-proficiency-in-scientific-literature-analysis-hengxing-cai-et-al-2024>(5/64 | 48/309) SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis (Hengxing Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun Wang, Yuqi Yin, Yaqi Li, Linfeng Zhang, Guolin Ke. (2024)<br><strong>SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis</strong><br><button class=copy-to-clipboard title="SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 92<br>Keywords: Benchmarking, Benchmarking, Multi-modal, Multi-modal, GPT, GPT-3, GPT-3.5, GPT-4, Gemini, Natural Language Understanding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01976v1.pdf filename=2403.01976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent breakthroughs in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have revolutionized <b>natural</b> <b>language</b> <b>understanding</b> and generation, igniting a surge of interest in leveraging these technologies for the nuanced field of scientific literature analysis. Existing <b>benchmarks,</b> however, inadequately evaluate the proficiency of <b>LLMs</b> in the scientific domain, especially in scenarios involving complex comprehension and <b>multimodal</b> data. In response, we introduced SciAssess, a <b>benchmark</b> tailored for the in-depth analysis of scientific literature, crafted to provide a thorough assessment of <b>LLMs&rsquo;</b> efficacy. SciAssess focuses on evaluating <b>LLMs&rsquo;</b> abilities in memorization, comprehension, and analysis within scientific contexts. It includes representative tasks from diverse scientific fields, such as general chemistry, organic materials, and alloy materials. And rigorous quality control measures ensure its reliability in terms of correctness, anonymization, and copyright compliance. SciAssess evaluates leading <b>LLMs,</b> including <b>GPT-4,</b> <b>GPT-3.5-turbo,</b> and <b>Gemini,</b> identifying their strengths and areas for improvement and supporting the ongoing development of <b>LLM</b> applications in scientific literature analysis. SciAssess and its resources are made available at <a href=https://sci-assess.github.io>https://sci-assess.github.io</a>, offering a valuable tool for advancing <b>LLM</b> capabilities in scientific literature analysis.</p></p class="citation"></blockquote><h3 id=664--49309-phantom-personality-has-an-effect-on-theory-of-mind-reasoning-in-large-language-models-fiona-anting-tan-et-al-2024>(6/64 | 49/309) PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models (Fiona Anting Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fiona Anting Tan, Gerard Christopher Yeo, Fanyou Wu, Weijie Xu, Vinija Jain, Aman Chadha, Kokil Jaidka, Yang Liu, See-Kiong Ng. (2024)<br><strong>PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models</strong><br><button class=copy-to-clipboard title="PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: GPT, GPT-3, GPT-3.5, LLaMA, Mistral, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02246v1.pdf filename=2403.02246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> demonstrate that their capabilities are comparable, or even superior, to humans in many tasks in natural language processing. Despite this progress, <b>LLMs</b> are still inadequate at social-cognitive <b>reasoning,</b> which humans are naturally good at. Drawing inspiration from psychological research on the links between certain personality traits and Theory-of-Mind (ToM) <b>reasoning,</b> and from <b>prompt</b> engineering research on the hyper-sensitivity of <b>prompts</b> in affecting <b>LLMs</b> capabilities, this study investigates how inducing personalities in <b>LLMs</b> using <b>prompts</b> affects their ToM <b>reasoning</b> capabilities. Our findings show that certain induced personalities can significantly affect the <b>LLMs&rsquo;</b> <b>reasoning</b> capabilities in three different ToM tasks. In particular, traits from the Dark Triad have a larger variable effect on <b>LLMs</b> like <b>GPT-3.5,</b> <b>Llama</b> 2, and <b>Mistral</b> across the different ToM tasks. We find that <b>LLMs</b> that exhibit a higher variance across personality <b>prompts</b> in ToM also tends to be more controllable in personality tests: personality traits in <b>LLMs</b> like <b>GPT-3.5,</b> <b>Llama</b> 2 and <b>Mistral</b> can be controllably adjusted through our personality <b>prompts.</b> In today&rsquo;s landscape where role-play is a common strategy when using <b>LLMs,</b> our research highlights the need for caution, as models that adopt specific personas with personalities potentially also alter their <b>reasoning</b> abilities in an unexpected manner.</p></p class="citation"></blockquote><h3 id=764--50309-human-evaluation-of-english--irish-transformer-based-nmt-séamus-lankford-et-al-2024>(7/64 | 50/309) Human Evaluation of English&ndash;Irish Transformer-Based NMT (Séamus Lankford et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Séamus Lankford, Haithem Afli, Andy Way. (2024)<br><strong>Human Evaluation of English&ndash;Irish Transformer-Based NMT</strong><br><button class=copy-to-clipboard title="Human Evaluation of English--Irish Transformer-Based NMT" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Low-Resource, Recurrent Neural Network, Recurrent Neural Network, Transformer, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02366v1.pdf filename=2403.02366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, a human evaluation is carried out on how hyperparameter settings impact the quality of <b>Transformer-based</b> <b>Neural</b> <b>Machine</b> <b>Translation</b> <b>(NMT)</b> for the <b>low-resourced</b> English&ndash;Irish pair. SentencePiece models using both Byte Pair Encoding (BPE) and unigram approaches were appraised. Variations in model architectures included modifying the number of layers, evaluating the optimal number of heads for attention and testing various regularisation techniques. The greatest performance improvement was recorded for a <b>Transformer-optimized</b> model with a 16k BPE subword model. Compared with a baseline <b>Recurrent</b> <b>Neural</b> <b>Network</b> <b>(RNN)</b> model, a <b>Transformer-optimized</b> model demonstrated a <b>BLEU</b> score improvement of 7.8 points. When <b>benchmarked</b> against Google Translate, our translation engines demonstrated significant improvements. Furthermore, a quantitative fine-grained manual evaluation was conducted which compared the performance of <b>machine</b> <b>translation</b> systems. Using the Multidimensional Quality Metrics (MQM) error taxonomy, a human evaluation of the error types generated by an <b>RNN-based</b> system and a <b>Transformer-based</b> system was explored. Our findings show the best-performing <b>Transformer</b> system significantly reduces both accuracy and fluency errors when compared with an <b>RNN-based</b> model.</p></p class="citation"></blockquote><h3 id=864--51309-key-point-driven-data-synthesis-with-its-enhancement-on-mathematical-reasoning-yiming-huang-et-al-2024>(8/64 | 51/309) Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning (Yiming Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, Weizhu Chen. (2024)<br><strong>Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning</strong><br><button class=copy-to-clipboard title="Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, Zero-shot, Mistral, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02333v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02333v1.pdf filename=2403.02333v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown great potential in complex <b>reasoning</b> tasks, yet their performance is often hampered by the scarcity of high-quality, <b>reasoning-focused</b> training datasets. Addressing this challenge, we propose Key-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar pairs from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, the most extensive synthetic dataset tailored for <b>mathematical</b> <b>reasoning</b> to date, comprising over one million question-answer pairs. Utilizing KPMath and augmenting it with additional <b>reasoning-intensive</b> corpora, we create the comprehensive KPMath-Plus dataset. <b>Fine-tuning</b> the <b>Mistral-7B</b> model on KPMath-Plus yields a <b>zero-shot</b> PASS@1 accuracy of 39.3% on the MATH test set, a performance that not only outpaces other <b>finetuned</b> 7B models but also exceeds that of certain 34B models. Our ablation studies further confirm the substantial enhancement in <b>mathematical</b> <b>reasoning</b> across various subtopics, marking a significant stride in <b>LLMs&rsquo;</b> <b>reasoning</b> capabilities.</p></p class="citation"></blockquote><h3 id=964--52309-using-llms-for-the-extraction-and-normalization-of-product-attribute-values-nick-baumann-et-al-2024>(9/64 | 52/309) Using LLMs for the Extraction and Normalization of Product Attribute Values (Nick Baumann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nick Baumann, Alexander Brinkmann, Christian Bizer. (2024)<br><strong>Using LLMs for the Extraction and Normalization of Product Attribute Values</strong><br><button class=copy-to-clipboard title="Using LLMs for the Extraction and Normalization of Product Attribute Values" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Recommendation, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02130v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02130v2.pdf filename=2403.02130v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Product offers on e-commerce websites often consist of a textual product title and a textual product description. In order to provide features such as faceted product filtering or content-based product <b>recommendation,</b> the websites need to extract attribute-value pairs from the unstructured product descriptions. This paper explores the potential of using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> such as OpenAI&rsquo;s <b>GPT-3.5</b> and <b>GPT-4,</b> to extract and normalize attribute values from product titles and product descriptions. For our experiments, we introduce the WDC Product Attribute-Value Extraction (WDC PAVE) dataset. WDC PAVE consists of product offers from 87 websites that provide schema$.$org annotations. The offers belong to five different categories, each featuring a specific set of attributes. The dataset provides manually verified attribute-value pairs in two forms: (i) directly extracted values and (ii) normalized attribute values. The normalization of the attribute values requires systems to perform the following types of operations: name expansion, generalization, unit of measurement normalization, and string wrangling. Our experiments demonstrate that <b>GPT-4</b> outperforms <b>PLM-based</b> extraction methods by 10%, achieving an F1-Score of 91%. For the extraction and normalization of product attribute values, <b>GPT-4</b> achieves a similar performance to the extraction scenario, while being particularly strong at string wrangling and name expansion.</p></p class="citation"></blockquote><h3 id=1064--53309-adaptmllm-fine-tuning-multilingual-language-models-on-low-resource-languages-with-integrated-llm-playgrounds-séamus-lankford-et-al-2024>(10/64 | 53/309) adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds (Séamus Lankford et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Séamus Lankford, Haithem Afli, Andy Way. (2024)<br><strong>adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds</strong><br><button class=copy-to-clipboard title="adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, Low-Resource, Neural Machine Translation, Neural Machine Translation, BLEU, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02370v1.pdf filename=2403.02370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of Multilingual Language Models (MLLMs) and <b>Large</b> <b>Language</b> <b>Models</b> has spawned innovation in many areas of natural language processing. Despite the exciting potential of this technology, its impact on developing high-quality <b>Machine</b> <b>Translation</b> <b>(MT)</b> outputs for <b>low-resource</b> languages remains relatively under-explored. Furthermore, an open-source application, dedicated to both <b>fine-tuning</b> MLLMs and managing the complete <b>MT</b> workflow for <b>low-resources</b> languages, remains unavailable. We aim to address these imbalances through the development of adaptMLLM, which streamlines all processes involved in the <b>fine-tuning</b> of MLLMs for <b>MT.</b> This open-source application is tailored for developers, translators, and users who are engaged in <b>MT.</b> An intuitive interface allows for easy customisation of hyperparameters, and the application offers a range of metrics for model evaluation and the capability to deploy models as a translation service directly within the application. As a multilingual tool, we used adaptMLLM to <b>fine-tune</b> models for two <b>low-resource</b> language pairs: English to Irish (EN$\leftrightarrow$GA) and English to Marathi (EN$\leftrightarrow$MR). Compared with baselines from the LoResMT2021 Shared Task, the adaptMLLM system demonstrated significant improvements. In the EN$\rightarrow$GA direction, an improvement of 5.2 <b>BLEU</b> points was observed and an increase of 40.5 <b>BLEU</b> points was recorded in the GA$\rightarrow$EN direction. Significant improvements in the translation performance of the EN$\leftrightarrow$MR pair were also observed notably in the MR$\rightarrow$EN direction with an increase of 21.3 <b>BLEU</b> points. Finally, a fine-grained human evaluation of the MLLM output on the EN$\rightarrow$GA pair was conducted using the Multidimensional Quality Metrics and Scalar Quality Metrics error taxonomies. The application and models are freely available.</p></p class="citation"></blockquote><h3 id=1164--54309-llm-oriented-retrieval-tuner-si-sun-et-al-2024>(11/64 | 54/309) LLM-Oriented Retrieval Tuner (Si Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Si Sun, Hanqing Zhang, Zhiyuan Liu, Jie Bao, Dawei Song. (2024)<br><strong>LLM-Oriented Retrieval Tuner</strong><br><button class=copy-to-clipboard title="LLM-Oriented Retrieval Tuner" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Dense Retrieval, Zero-shot, GPT, GPT-3, GPT-4, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01999v1.pdf filename=2403.01999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Dense</b> <b>Retrieval</b> (DR) is now considered as a promising tool to enhance the memorization capacity of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> such as <b>GPT3</b> and <b>GPT-4</b> by incorporating external memories. However, due to the paradigm discrepancy between <b>text</b> <b>generation</b> of <b>LLM</b> and DR, it is still an open challenge to integrate the retrieval and generation tasks in a shared <b>LLM.</b> In this paper, we propose an efficient <b>LLM-Oriented</b> Retrieval Tuner, namely LMORT, which decouples DR capacity from base <b>LLM</b> and non-invasively coordinates the optimally aligned and uniform layers of the <b>LLM</b> towards a unified DR space, achieving an efficient and effective DR without tuning the <b>LLM</b> itself. The extensive experiments on six BEIR datasets show that our approach could achieve competitive <b>zero-shot</b> retrieval performance compared to a range of strong DR models while maintaining the generation ability of <b>LLM.</b></p></p class="citation"></blockquote><h3 id=1264--55309-daco-towards-application-driven-and-comprehensive-data-analysis-via-code-generation-xueqing-wu-et-al-2024>(12/64 | 55/309) DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation (Xueqing Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xueqing Wu, Rui Zheng, Jingzhen Sha, Te-Lin Wu, Hanyu Zhou, Mohan Tang, Kai-Wei Chang, Nanyun Peng, Haoran Huang. (2024)<br><strong>DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation</strong><br><button class=copy-to-clipboard title="DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Reinforcement Learning, Supervised Learning, Code Generation, Large Language Model, Prompt, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02528v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02528v1.pdf filename=2403.02528v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. In this work, we aim to propose new resources and <b>benchmarks</b> to inspire future research on this crucial yet challenging and under-explored task. However, collecting data analysis annotations curated by experts can be prohibitively expensive. We propose to automatically generate high-quality answer annotations leveraging the <b>code-generation</b> <b>capabilities</b> of <b>LLMs</b> with a multi-turn <b>prompting</b> technique. We construct the DACO dataset, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as <b>weak</b> <b>supervision</b> for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation <b>benchmark.</b> We train a 6B <b>supervised</b> <b>fine-tuning</b> (SFT) model on DACO dataset, and find that the SFT model learns reasonable data analysis capabilities. To further align the models with human preference, we use <b>reinforcement</b> <b>learning</b> to encourage generating analysis perceived by human as helpful, and design a set of dense rewards to propagate the sparse human preference reward to intermediate <b>code</b> <b>generation</b> steps. Our DACO-RL algorithm is evaluated by human annotators to produce more helpful answers than SFT model in 57.72% cases, validating the effectiveness of our proposed algorithm. Data and <b>code</b> <b>are</b> released at <a href=https://github.com/shirley-wu/daco>https://github.com/shirley-wu/daco</a></p></p class="citation"></blockquote><h3 id=1364--56309-riff-learning-to-rephrase-inputs-for-few-shot-fine-tuning-of-language-models-saeed-najafi-et-al-2024>(13/64 | 56/309) RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models (Saeed Najafi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saeed Najafi, Alona Fyshe. (2024)<br><strong>RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models</strong><br><button class=copy-to-clipboard title="RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Fine-tuning, Fine-tuning, Text Classification, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02271v1.pdf filename=2403.02271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> can be accurately <b>fine-tuned</b> for downstream <b>text</b> <b>processing</b> tasks. Recently, researchers have introduced several parameter-efficient <b>fine-tuning</b> methods that optimize input <b>prompts</b> or adjust a small number of model parameters (e.g LoRA). In this study, we explore the impact of altering the input <b>text</b> <b>of</b> the original task in conjunction with parameter-efficient <b>fine-tuning</b> methods. To most effectively rewrite the input <b>text,</b> <b>we</b> train a <b>few-shot</b> paraphrase model with a Maximum-Marginal Likelihood objective. Using six <b>few-shot</b> <b>text</b> <b>classification</b> datasets, we show that enriching data with paraphrases at train and test time enhances the performance beyond what can be achieved with parameter-efficient <b>fine-tuning</b> alone.</p></p class="citation"></blockquote><h3 id=1464--57309-protrix-building-models-for-planning-and-reasoning-over-tables-with-sentence-context-zirui-wu-et-al-2024>(14/64 | 57/309) ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context (Zirui Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zirui Wu, Yansong Feng. (2024)<br><strong>ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context</strong><br><button class=copy-to-clipboard title="ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, GPT, GPT-3, GPT-3.5, LLaMA, Reasoning, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02177v1.pdf filename=2403.02177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tables play a crucial role in conveying information in various domains, serving as indispensable tools for organizing and presenting data in a structured manner. We propose a Plan-then-Reason framework to answer different types of user queries over tables with sentence context. The framework first plans the <b>reasoning</b> paths over the context, then assigns each step to program-based or textual <b>reasoning</b> to reach the final answer. We construct an <b>instruction</b> <b>tuning</b> set TrixInstruct following the framework. Our dataset cover queries that are program-unsolvable or need combining information from tables and sentences to obtain planning and <b>reasoning</b> abilities. We present ProTrix by <b>finetuning</b> <b>Llama-2-7B</b> on TrixInstruct. Our experiments show that ProTrix generalizes to diverse tabular tasks and achieves comparable performance to <b>GPT-3.5-turbo.</b> We further demonstrate that ProTrix can generate accurate and faithful explanations to answer complex free-form questions. Our work underscores the importance of the planning and <b>reasoning</b> abilities towards a model over tabular tasks with generalizability and interpretability. We will release our dataset and model at <a href=https://github.com/WilliamZR/ProTrix>https://github.com/WilliamZR/ProTrix</a>.</p></p class="citation"></blockquote><h3 id=1564--58309-to-generate-or-to-retrieve-on-the-effectiveness-of-artificial-contexts-for-medical-open-domain-question-answering-giacomo-frisoni-et-al-2024>(15/64 | 58/309) To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering (Giacomo Frisoni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giacomo Frisoni, Alessio Cocchieri, Alex Presepi, Gianluca Moro, Zaiqiao Meng. (2024)<br><strong>To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering</strong><br><button class=copy-to-clipboard title="To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Low-Resource, Zero-shot, Massive Multitask Language Understanding (MMLU), Open-Domain Question Answering, Question Answering, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01924v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01924v1.pdf filename=2403.01924v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical <b>open-domain</b> <b>question</b> <b>answering</b> demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common <b>low-resource</b> hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific <b>large</b> <b>language</b> <b>models,</b> entails constructing artificial contexts through <b>prompting.</b> As a result, &ldquo;to generate or to retrieve&rdquo; is the modern equivalent of Hamlet&rsquo;s dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice <b>question</b> <b>answering</b> in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and <b>MMLU,</b> incorporating a practical perspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new state-of-the-art (SOTA) in the open-book setting of each testbed, even allowing a small-scale reader to outcompete <b>zero-shot</b> closed-book 175B baselines while using up to 706$\times$ fewer parameters. Overall, our findings reveal that generated passages are more effective than retrieved counterparts in attaining higher accuracy.</p></p class="citation"></blockquote><h3 id=1664--59309-fenice-factuality-evaluation-of-summarization-based-on-natural-language-inference-and-claim-extraction-alessandro-scirè-et-al-2024>(16/64 | 59/309) FENICE: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction (Alessandro Scirè et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Scirè, Karim Ghonim, Roberto Navigli. (2024)<br><strong>FENICE: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction</strong><br><button class=copy-to-clipboard title="FENICE: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Natural Language Inference, Natural Language Inference, Text Summarization, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02270v1.pdf filename=2403.02270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>text</b> <b>summarization,</b> particularly with the advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> have shown remarkable performance. However, a notable challenge persists as a substantial number of automatically-generated summaries exhibit factual inconsistencies, such as hallucinations. In response to this issue, various approaches for the evaluation of consistency for <b>summarization</b> have emerged. Yet, these newly-introduced metrics face several limitations, including lack of interpretability, focus on short document summaries (e.g., news articles), and computational impracticality, especially for <b>LLM-based</b> metrics. To address these shortcomings, we propose Factuality Evaluation of <b>summarization</b> based on <b>Natural</b> <b>language</b> <b>Inference</b> and Claim Extraction (FENICE), a more interpretable and efficient factuality-oriented metric. FENICE leverages an <b>NLI-based</b> alignment between information in the source document and a set of atomic facts, referred to as claims, extracted from the summary. Our metric sets a new state of the art on AGGREFACT, the de-facto <b>benchmark</b> for factuality evaluation. Moreover, we extend our evaluation to a more challenging setting by conducting a human annotation process of long-form <b>summarization.</b></p></p class="citation"></blockquote><h3 id=1764--60309-multi-perspective-improvement-of-knowledge-graph-completion-with-large-language-models-derong-xu-et-al-2024>(17/64 | 60/309) Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models (Derong Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Derong Xu, Ziheng Zhang, Zhenxi Lin, Xian Wu, Zhihong Zhu, Tong Xu, Xiangyu Zhao, Yefeng Zheng, Enhong Chen. (2024)<br><strong>Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models</strong><br><button class=copy-to-clipboard title="Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Reasoning, Large Language Model, Large Language Model, Pre-trained Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01972v1.pdf filename=2403.01972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>graph</b> completion (KGC) is a widely used method to tackle incompleteness in <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> by making predictions for missing links. Description-based KGC leverages <b>pre-trained</b> <b>language</b> <b>models</b> to learn entity and relation representations with their names or descriptions, which shows promising results. However, the performance of description-based KGC is still limited by the quality of text and the incomplete structure, as it lacks sufficient entity descriptions and relies solely on relation names, leading to sub-optimal results. To address this issue, we propose MPIKGC, a general framework to compensate for the deficiency of contextualized <b>knowledge</b> <b>and</b> improve KGC by querying <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> from various perspectives, which involves leveraging the <b>reasoning,</b> explanation, and <b>summarization</b> capabilities of <b>LLMs</b> to expand entity descriptions, understand relations, and extract structures, respectively. We conducted extensive evaluation of the effectiveness and improvement of our framework based on four description-based KGC models and four datasets, for both link prediction and triplet classification tasks.</p></p class="citation"></blockquote><h3 id=1864--61309-how-does-architecture-influence-the-base-capabilities-of-pre-trained-language-models-a-case-study-based-on-ffn-wider-transformer-models-xin-lu-et-al-2024>(18/64 | 61/309) How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models (Xin Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Lu, Yanyan Zhao, Bing Qin. (2024)<br><strong>How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models</strong><br><button class=copy-to-clipboard title="How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, Few-shot Learning, Out-of-distribution, Transfer Learning, Transformer, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02436v1.pdf filename=2403.02436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pre-trained</b> <b>language</b> <b>models</b> have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in <b>out-of-distribution</b> language modeling, <b>transfer</b> <b>learning</b> and <b>few-shot</b> <b>learning.</b> Unlike existing work focusing on the influence of scale on base capabilities, our work examines the influence of architecture on those. Specifically, our concern is: How does architecture influence the base capabilities of <b>pre-trained</b> <b>language</b> <b>models?</b> In this work, we attempt to explain and reverse the decline in base capabilities caused by the architecture of FFN-Wider <b>Transformers,</b> seeking to provide some insights. Through analysis, we found the contribution ratio of Multi-Head Attention (a combination function) to <b>pre-trained</b> <b>language</b> <b>modeling</b> is a key factor affecting base capabilities. FFN-Wider <b>Transformers</b> reduce the contribution ratio of this combination function, leading to a decline in base capabilities. We confirmed this by experiments and proposed Combination Enhancement Architecture (CEA) to address the decline in base capabilities of such models. Significantly, we extended our explanation and CEA to Mixture of Experts (MoE) architecture <b>Transformers,</b> which also alleviated their decline in base capabilities to some extent, proving our work can offer useful guidance for architecture analysis, architecture improvement and architecture design.</p></p class="citation"></blockquote><h3 id=1964--62309-masked-thought-simply-masking-partial-reasoning-steps-can-improve-mathematical-reasoning-learning-of-language-models-changyu-chen-et-al-2024>(19/64 | 62/309) Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models (Changyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changyu Chen, Xiting Wang, Ting-En Lin, Ang Lv, Yuchuan Wu, Xin Gao, Ji-Rong Wen, Rui Yan, Yongbin Li. (2024)<br><strong>Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models</strong><br><button class=copy-to-clipboard title="Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Data Augmentation, Fine-tuning, Supervised Learning, Mathematical Reasoning, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02178v1.pdf filename=2403.02178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>reasoning</b> tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of <b>large</b> <b>language</b> <b>models</b> in such domains. Earlier <b>fine-tuning</b> approaches sought to mitigate this by leveraging more precise supervisory signals from human labeling, larger models, or self-sampling, although at a high cost. Conversely, we develop a method that avoids external resources, relying instead on introducing perturbations to the input. Our training approach randomly masks certain tokens within the chain of thought, a technique we found to be particularly effective for <b>reasoning</b> tasks. When applied to <b>fine-tuning</b> with GSM8K, this method achieved a 5% improvement in accuracy over standard <b>supervised</b> <b>fine-tuning</b> with a few codes modified and no additional labeling effort. Furthermore, it is complementary to existing methods. When integrated with related <b>data</b> <b>augmentation</b> methods, it leads to an average improvement of 3% improvement in GSM8K accuracy and 1% improvement in MATH accuracy across five datasets of various quality and size, as well as two base models. We further investigate the mechanisms behind this improvement through case studies and quantitative analysis, suggesting that our approach may provide superior support for the model in capturing long-distance dependencies, especially those related to questions. This enhancement could deepen understanding of premises in questions and prior steps. Our code is available at Github.</p></p class="citation"></blockquote><h3 id=2064--63309-as-es-learning-towards-efficient-cot-learning-in-small-models-nuwa-xi-et-al-2024>(20/64 | 63/309) AS-ES Learning: Towards Efficient CoT Learning in Small Models (Nuwa Xi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nuwa Xi, Yuhan Chen, Sendong Zhao, Haochun Wang, Bing Qin, Ting Liu. (2024)<br><strong>AS-ES Learning: Towards Efficient CoT Learning in Small Models</strong><br><button class=copy-to-clipboard title="AS-ES Learning: Towards Efficient CoT Learning in Small Models" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Data Augmentation, Knowledge Distillation, Reasoning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01969v1.pdf filename=2403.01969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chain-of-Thought (CoT) serves as a critical emerging ability in <b>LLMs,</b> especially when it comes to logical <b>reasoning.</b> Attempts have been made to induce such ability in small models as well by <b>distilling</b> from the <b>data</b> <b>with</b> CoT generated by <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> However, existing methods often simply generate and incorporate more <b>data</b> <b>from</b> <b>LLMs</b> and fail to note the importance of efficiently utilizing existing CoT <b>data.</b> <b>We</b> here propose a new training paradigm AS-ES (Abstractive Segments - Extractive Segments) learning, which exploits the inherent information in CoT for iterative generation. Experiments show that our methods surpass the direct seq2seq training on CoT-extensive tasks like MWP and PET <b>summarization,</b> without <b>data</b> <b>augmentation</b> or altering the model itself. Furthermore, we explore the reason behind the inefficiency of small models in learning CoT and provide an explanation of why AS-ES learning works, giving insights into the underlying mechanism of CoT.</p></p class="citation"></blockquote><h3 id=2164--64309-fakenewsgpt4-advancing-multimodal-fake-news-detection-through-knowledge-augmented-lvlms-xuannan-liu-et-al-2024>(21/64 | 64/309) FakeNewsGPT4: Advancing Multimodal Fake News Detection through Knowledge-Augmented LVLMs (Xuannan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuannan Liu, Peipei Li, Huaibo Huang, Zekun Li, Xing Cui, Jiahao Liang, Lixiong Qin, Weihong Deng, Zhaofeng He. (2024)<br><strong>FakeNewsGPT4: Advancing Multimodal Fake News Detection through Knowledge-Augmented LVLMs</strong><br><button class=copy-to-clipboard title="FakeNewsGPT4: Advancing Multimodal Fake News Detection through Knowledge-Augmented LVLMs" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 59<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Fake News Detection, Reasoning, Fake News Detection, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01988v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01988v1.pdf filename=2403.01988v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The massive generation of <b>multimodal</b> <b>fake</b> <b>news</b> <b>exhibits</b> substantial distribution discrepancies, <b>prompting</b> the need for generalized detectors. However, the insulated nature of training within specific domains restricts the capability of classical detectors to obtain open-world facts. In this paper, we propose FakeNewsGPT4, a novel framework that augments Large <b>Vision-Language</b> Models (LVLMs) with forgery-specific knowledge for manipulation <b>reasoning</b> while inheriting extensive world knowledge as complementary. Knowledge augmentation in FakeNewsGPT4 involves acquiring two types of forgery-specific knowledge, i.e., semantic correlation and artifact trace, and merging them into LVLMs. Specifically, we design a multi-level cross-modal <b>reasoning</b> module that establishes interactions across modalities for extracting semantic correlations. Concurrently, a dual-branch fine-grained verification module is presented to comprehend localized details to encode artifact traces. The generated knowledge is translated into refined embeddings compatible with LVLMs. We also incorporate candidate answer heuristics and soft <b>prompts</b> to enhance input informativeness. Extensive experiments on the public <b>benchmark</b> demonstrate that FakeNewsGPT4 achieves superior cross-domain performance compared to previous methods. Code will be available.</p></p class="citation"></blockquote><h3 id=2264--65309-nphardeval4v-a-dynamic-reasoning-benchmark-of-multimodal-large-language-models-lizhou-fan-et-al-2024>(22/64 | 65/309) NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models (Lizhou Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lizhou Fan, Wenyue Hua, Xiang Li, Kaijie Zhu, Mingyu Jin, Lingyao Li, Haoyang Ling, Jinkui Chi, Jindong Wang, Xin Ma, Yongfeng Zhang. (2024)<br><strong>NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 59<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Instruction Following, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01777v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01777v2.pdf filename=2403.01777v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the <b>reasoning</b> capabilities of <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) is an important area of research. In this study, we introduce a dynamic <b>benchmark,</b> NPHardEval4V, aimed at addressing the existing gaps in evaluating the pure <b>reasoning</b> abilities of MLLMs. Our <b>benchmark</b> aims to provide a venue to disentangle the effect of various factors such as image recognition and <b>instruction</b> <b>following,</b> from the overall performance of the models, allowing us to focus solely on evaluating their <b>reasoning</b> abilities. It is built by converting textual description of questions from NPHardEval to image representations. Our findings reveal significant discrepancies in <b>reasoning</b> abilities across different models and highlight the relatively weak performance of MLLMs compared to <b>LLMs</b> in terms of <b>reasoning.</b> We also investigate the impact of different <b>prompting</b> styles, including visual, text, and combined visual and text <b>prompts,</b> on the <b>reasoning</b> abilities of MLLMs, demonstrating the different impacts of <b>multimodal</b> inputs in model performance. Unlike traditional <b>benchmarks,</b> which focus primarily on static evaluations, our <b>benchmark</b> will be updated monthly to prevent overfitting and ensure a more authentic and fine-grained evaluation of the models. We believe that this <b>benchmark</b> can aid in understanding and guide the further development of <b>reasoning</b> abilities in MLLMs. The <b>benchmark</b> dataset and code are available at <a href=https://github.com/lizhouf/NPHardEval4V>https://github.com/lizhouf/NPHardEval4V</a></p></p class="citation"></blockquote><h3 id=2364--66309-an-improved-traditional-chinese-evaluation-suite-for-foundation-model-zhi-rui-tam-et-al-2024>(23/64 | 66/309) An Improved Traditional Chinese Evaluation Suite for Foundation Model (Zhi-Rui Tam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhi-Rui Tam, Ya-Ting Pai, Yen-Wei Lee, Sega Cheng, Hong-Han Shuai. (2024)<br><strong>An Improved Traditional Chinese Evaluation Suite for Foundation Model</strong><br><button class=copy-to-clipboard title="An Improved Traditional Chinese Evaluation Suite for Foundation Model" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Foundation Model, Massive Multitask Language Understanding (MMLU), Massive Multitask Language Understanding (MMLU), Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01858v1.pdf filename=2403.01858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present TMMLU+, a comprehensive dataset designed for the Traditional Chinese <b>massive</b> <b>multitask</b> <b>language</b> <b>understanding</b> dataset. TMMLU+ is a multiple-choice <b>question-answering</b> <b>dataset</b> with 66 subjects from elementary to professional level. Compared to its predecessor, TMMLU, TMMLU+ is six times larger and boasts a more balanced subject distribution. We included <b>benchmark</b> results in TMMLU+ from closed-source models and 24 open-weight Chinese <b>large</b> <b>language</b> <b>models</b> of parameters ranging from 1.8B to 72B. Our findings reveal that Traditional Chinese models still trail behind their Simplified Chinese counterparts. Additionally, current <b>large</b> <b>language</b> <b>models</b> have yet to outperform human performance in average scores. We publicly release our dataset and the corresponding <b>benchmark</b> source code.</p></p class="citation"></blockquote><h3 id=2464--67309-birbal-an-efficient-7b-instruct-model-fine-tuned-with-curated-datasets-ashvini-kumar-jindal-et-al-2024>(24/64 | 67/309) Birbal: An efficient 7B instruct-model fine-tuned with curated datasets (Ashvini Kumar Jindal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashvini Kumar Jindal, Pawan Kumar Rajpoot, Ankur Parikh. (2024)<br><strong>Birbal: An efficient 7B instruct-model fine-tuned with curated datasets</strong><br><button class=copy-to-clipboard title="Birbal: An efficient 7B instruct-model fine-tuned with curated datasets" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model, Mistral, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02247v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02247v1.pdf filename=2403.02247v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>LLMOps incur significant costs due to hardware requirements, hindering their widespread accessibility. Additionally, a lack of transparency in model training methods and data contributes to the majority of models being non-reproducible. To tackle these challenges, the <b>LLM</b> Efficiency Challenge was introduced at NeurIPS Workshop, aiming to adapt <b>foundation</b> <b>models</b> on a diverse set of tasks via <b>fine-tuning</b> on a single GPU (RTX 4090 or A100 with 40GB) within a 24-hour timeframe. In this system description paper, we introduce Birbal, our <b>Mistral-7B</b> based winning model, <b>fine-tuned</b> on a single RTX 4090 for 16 hours. Birbal&rsquo;s success lies in curating high-quality instructions covering diverse tasks, resulting in a 35% performance improvement over second-best Qwen-14B based submission.</p></p class="citation"></blockquote><h3 id=2564--68309-transformers-for-low-resource-languagesis-féidir-linn-séamus-lankford-et-al-2024>(25/64 | 68/309) Transformers for Low-Resource Languages:Is Féidir Linn! (Séamus Lankford et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Séamus Lankford, Haithem Afli, Andy Way. (2024)<br><strong>Transformers for Low-Resource Languages:Is Féidir Linn!</strong><br><button class=copy-to-clipboard title="Transformers for Low-Resource Languages:Is Féidir Linn!" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Low-Resource, Recurrent Neural Network, Transformer, Neural Machine Translation, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01985v1.pdf filename=2403.01985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>Transformer</b> model is the state-of-the-art in <b>Machine</b> <b>Translation.</b> However, in general, neural translation models often under perform on language pairs with insufficient training data. As a consequence, relatively few experiments have been carried out using this architecture on <b>low-resource</b> language pairs. In this study, hyperparameter optimization of <b>Transformer</b> models in translating the <b>low-resource</b> English-Irish language pair is evaluated. We demonstrate that choosing appropriate parameters leads to considerable performance improvements. Most importantly, the correct choice of subword model is shown to be the biggest driver of translation performance. SentencePiece models using both unigram and BPE approaches were appraised. Variations on model architectures included modifying the number of layers, testing various regularisation techniques and evaluating the optimal number of heads for attention. A generic 55k DGT corpus and an in-domain 88k public admin corpus were used for evaluation. A <b>Transformer</b> optimized model demonstrated a <b>BLEU</b> score improvement of 7.8 points when compared with a baseline <b>RNN</b> model. Improvements were observed across a range of metrics, including TER, indicating a substantially reduced post editing effort for <b>Transformer</b> optimized models with 16k BPE subword models. Bench-marked against Google Translate, our translation engines demonstrated significant improvements. The question of whether or not <b>Transformers</b> can be used effectively in a <b>low-resource</b> setting of English-Irish translation has been addressed. Is f'eidir linn - yes we can.</p></p class="citation"></blockquote><h3 id=2664--69309-eee-qa-exploring-effective-and-efficient-question-answer-representations-zhanghao-hu-et-al-2024>(26/64 | 69/309) EEE-QA: Exploring Effective and Efficient Question-Answer Representations (Zhanghao Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanghao Hu, Yijun Yang, Junjie Xu, Yifu Qiu, Pinzhen Chen. (2024)<br><strong>EEE-QA: Exploring Effective and Efficient Question-Answer Representations</strong><br><button class=copy-to-clipboard title="EEE-QA: Exploring Effective and Efficient Question-Answer Representations" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 48<br>Keywords: Graph, Knowledge Graph, RoBERTa, Question Answering, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02176v1.pdf filename=2403.02176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current approaches to <b>question</b> <b>answering</b> rely on <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> like <b>RoBERTa.</b> This work challenges the existing <b>question-answer</b> <b>encoding</b> convention and explores finer representations. We begin with testing various pooling methods compared to using the begin-of-sentence token as a <b>question</b> <b>representation</b> for better quality. Next, we explore opportunities to simultaneously embed all answer candidates with the <b>question.</b> <b>This</b> enables cross-reference between answer choices and improves inference throughput via reduced memory usage. Despite their simplicity and effectiveness, these methods have yet to be widely studied in current frameworks. We experiment with different <b>PLMs,</b> and with and without the integration of <b>knowledge</b> <b>graphs.</b> Results prove that the memory efficacy of the proposed techniques with little sacrifice in performance. Practically, our work enhances 38-100% throughput with 26-65% speedups on consumer-grade GPUs by allowing for considerably larger batch sizes. Our work sends a message to the community with promising directions in both representation quality and efficiency for the <b>question-answering</b> <b>task</b> in natural language processing.</p></p class="citation"></blockquote><h3 id=2764--70309-adaptnmt-an-open-source-language-agnostic-development-environment-for-neural-machine-translation-séamus-lankford-et-al-2024>(27/64 | 70/309) adaptNMT: an open-source, language-agnostic development environment for Neural Machine Translation (Séamus Lankford et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Séamus Lankford, Haithem Afli, Andy Way. (2024)<br><strong>adaptNMT: an open-source, language-agnostic development environment for Neural Machine Translation</strong><br><button class=copy-to-clipboard title="adaptNMT: an open-source, language-agnostic development environment for Neural Machine Translation" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Graph, Recurrent Neural Network, Transformer, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02367v1.pdf filename=2403.02367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>adaptNMT streamlines all processes involved in the development and deployment of <b>RNN</b> and <b>Transformer</b> <b>neural</b> <b>translation</b> <b>models.</b> As an open-source application, it is designed for both technical and non-technical users who work in the field of <b>machine</b> <b>translation.</b> Built upon the widely-adopted OpenNMT ecosystem, the application is particularly useful for new entrants to the field since the setup of the development environment and creation of train, validation and test splits is greatly simplified. <b>Graphing,</b> embedded within the application, illustrates the progress of model training, and SentencePiece is used for creating subword segmentation models. Hyperparameter customization is facilitated through an intuitive user interface, and a single-click model development approach has been implemented. Models developed by adaptNMT can be evaluated using a range of metrics, and deployed as a translation service within the application. To support eco-friendly research in the NLP space, a green report also flags the power consumption and kgCO$_{2}$ emissions generated during model development. The application is freely available.</p></p class="citation"></blockquote><h3 id=2864--71309-offlandat-a-community-based-implicit-offensive-language-dataset-generated-by-large-language-model-through-prompt-engineering-amit-das-et-al-2024>(28/64 | 71/309) OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering (Amit Das et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amit Das, Mostafa Rahgouy, Dongji Feng, Zheng Zhang, Tathagata Bhattacharya, Nilanjana Raychawdhary, Mary Sandage, Lauramarie Pope, Gerry Dozier, Cheryl Seals. (2024)<br><strong>OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering</strong><br><button class=copy-to-clipboard title="OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Zero-shot, ChatGPT, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02472v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02472v3.pdf filename=2403.02472v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread presence of offensive languages on social media has resulted in adverse effects on societal well-being. As a result, it has become very important to address this issue with high priority. Offensive languages exist in both explicit and implicit forms, with the latter being more challenging to detect. Current research in this domain encounters several challenges. Firstly, the existing datasets primarily rely on the collection of texts containing explicit offensive keywords, making it challenging to capture implicitly offensive contents that are devoid of these keywords. Secondly, usual methodologies tend to focus solely on textual analysis, neglecting the valuable insights that community information can provide. In this research paper, we introduce a novel dataset OffLanDat, a community based implicit offensive language dataset generated by <b>ChatGPT</b> containing data for 38 different target groups. Despite limitations in generating offensive texts using <b>ChatGPT</b> due to ethical constraints, we present a <b>prompt-based</b> approach that effectively generates implicit offensive languages. To ensure data quality, we evaluate our data with human. Additionally, we employ a <b>prompt-based</b> <b>Zero-Shot</b> method with <b>ChatGPT</b> and compare the detection results between human annotation and <b>ChatGPT</b> annotation. We utilize existing state-of-the-art models to see how effective they are in detecting such languages. We will make our code and dataset public for other researchers.</p></p class="citation"></blockquote><h3 id=2964--72309-not-all-layers-of-llms-are-necessary-during-inference-siqi-fan-et-al-2024>(29/64 | 72/309) Not all Layers of LLMs are Necessary during Inference (Siqi Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang, Zhongyuan Wang. (2024)<br><strong>Not all Layers of LLMs are Necessary during Inference</strong><br><button class=copy-to-clipboard title="Not all Layers of LLMs are Necessary during Inference" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02181v1.pdf filename=2403.02181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The inference phase of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is very expensive. An ideal inference stage of <b>LLMs</b> could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and <b>in-context</b> <b>learning</b> ability). In this paper, we try to answer the question, &ldquo;During <b>LLM</b> inference, can we use shallow layers for easy instances; and deep layers for hard ones?&rdquo; To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter <b>LLM</b> parameters and maintains generalizability across tasks. Experiments on well-known <b>LLMs</b> (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment tasks, while maintaining comparable performance. Additionally, this method is orthogonal to other model acceleration techniques, potentially boosting inference efficiency further.</p></p class="citation"></blockquote><h3 id=3064--73309-large-language-models-in-fire-engineering-an-examination-of-technical-questions-against-domain-knowledge-haley-hostetter-et-al-2024>(30/64 | 73/309) Large Language Models in Fire Engineering: An Examination of Technical Questions Against Domain Knowledge (Haley Hostetter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haley Hostetter, M. Z. Naser, Xinyan Huang, John Gales. (2024)<br><strong>Large Language Models in Fire Engineering: An Examination of Technical Questions Against Domain Knowledge</strong><br><button class=copy-to-clipboard title="Large Language Models in Fire Engineering: An Examination of Technical Questions Against Domain Knowledge" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Bard, ChatGPT, Chatbot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04795v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04795v1.pdf filename=2403.04795v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This communication presents preliminary findings from comparing two recent <b>chatbots,</b> OpenAI&rsquo;s <b>ChatGPT</b> and Google&rsquo;s <b>Bard,</b> in the context of fire engineering by evaluating their responses in handling fire safety related queries. A diverse range of fire engineering questions and scenarios were created and examined, including structural fire design, fire prevention strategies, evacuation, building code compliance, and fire suppression systems (some of which resemble those commonly present in the Fire Protection exam (FPE)). The results reveal some key differences in the performance of the <b>chatbots,</b> with <b>ChatGPT</b> demonstrating a relatively superior performance. Then, this communication highlights the potential for <b>chatbot</b> technology to revolutionize fire engineering practices by providing instant access to critical information while outlining areas for further improvement and research. Evidently, and when it matures, this technology will likely be elemental to our engineers&rsquo; practice and education.</p></p class="citation"></blockquote><h3 id=3164--74309-automated-generation-of-multiple-choice-cloze-questions-for-assessing-english-vocabulary-using-gpt-turbo-35-qiao-wang-et-al-2024>(31/64 | 74/309) Automated Generation of Multiple-Choice Cloze Questions for Assessing English Vocabulary Using GPT-turbo 3.5 (Qiao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiao Wang, Ralph Rose, Naho Orita, Ayaka Sugawara. (2024)<br><strong>Automated Generation of Multiple-Choice Cloze Questions for Assessing English Vocabulary Using GPT-turbo 3.5</strong><br><button class=copy-to-clipboard title="Automated Generation of Multiple-Choice Cloze Questions for Assessing English Vocabulary Using GPT-turbo 3.5" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: GPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02078v1.pdf filename=2403.02078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A common way of assessing language learners&rsquo; mastery of vocabulary is via multiple-choice cloze (i.e., fill-in-the-blank) questions. But the creation of test items can be laborious for individual teachers or in <b>large-scale</b> <b>language</b> <b>programs.</b> In this paper, we evaluate a new method for automatically generating these types of questions using <b>large</b> <b>language</b> <b>models</b> <b>(LLM).</b> The VocaTT (vocabulary teaching and training) engine is written in Python and comprises three basic steps: pre-processing target word lists, generating sentences and candidate word options using <b>GPT,</b> and finally selecting suitable word options. To test the efficiency of this system, 60 questions were generated targeting academic words. The generated items were reviewed by expert reviewers who judged the well-formedness of the sentences and word options, adding comments to items judged not well-formed. Results showed a 75% rate of well-formedness for sentences and 66.85% rate for suitable word options. This is a marked improvement over the generator used earlier in our research which did not take advantage of <b>GPT&rsquo;s</b> capabilities. Post-hoc qualitative analysis reveals several points for improvement in future work including cross-referencing part-of-speech tagging, better sentence validation, and improving <b>GPT</b> <b>prompts.</b></p></p class="citation"></blockquote><h3 id=3264--75309-decider-a-rule-controllable-decoding-strategy-for-language-generation-by-imitating-dual-system-cognitive-theory-chen-xu-et-al-2024>(32/64 | 75/309) DECIDER: A Rule-Controllable Decoding Strategy for Language Generation by Imitating Dual-System Cognitive Theory (Chen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Xu, Tian Lan, Changlong Yu, Wei Wang, Jun Gao, Yu Ji, Qunxi Dong, Kun Qian, Piji Li, Wei Bi, Bin Hu. (2024)<br><strong>DECIDER: A Rule-Controllable Decoding Strategy for Language Generation by Imitating Dual-System Cognitive Theory</strong><br><button class=copy-to-clipboard title="DECIDER: A Rule-Controllable Decoding Strategy for Language Generation by Imitating Dual-System Cognitive Theory" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LO, cs.CL<br>Keyword Score: 40<br>Keywords: Language Generation, Reasoning, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01954v1.pdf filename=2403.01954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lexicon-based constrained decoding approaches aim to control the meaning or style of the generated text through certain target concepts. Existing approaches over-focus the targets themselves, leading to a lack of high-level <b>reasoning</b> about how to achieve them. However, human usually tackles tasks by following certain rules that not only focuses on the targets but also on semantically relevant concepts that induce the occurrence of targets. In this work, we present DECIDER, a rule-controllable decoding strategy for constrained <b>language</b> <b>generation</b> inspired by dual-system cognitive theory. Specifically, in DECIDER, a <b>pre-trained</b> <b>language</b> <b>model</b> <b>(PLM)</b> is equiped with a logic reasoner that takes high-level rules as input. Then, the DECIDER allows rule signals to flow into the <b>PLM</b> at each decoding step. Extensive experimental results demonstrate that DECIDER can effectively follow given rules to guide generation direction toward the targets in a more human-like manner.</p></p class="citation"></blockquote><h3 id=3364--76309-llm-vs-lawyers-identifying-a-subset-of-summary-judgments-in-a-large-uk-case-law-dataset-ahmed-izzidien-et-al-2024>(33/64 | 76/309) LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset (Ahmed Izzidien et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmed Izzidien, Holli Sargeant, Felix Steffek. (2024)<br><strong>LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset</strong><br><button class=copy-to-clipboard title="LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Claude, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04791v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04791v1.pdf filename=2403.04791v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To undertake computational research of the law, efficiently identifying datasets of court decisions that relate to a specific legal issue is a crucial yet challenging endeavour. This study addresses the gap in the literature working with <b>large</b> <b>legal</b> <b>corpora</b> about how to isolate cases, in our case summary judgments, from a <b>large</b> <b>corpus</b> <b>of</b> UK court decisions. We introduce a comparative analysis of two computational methods: (1) a traditional natural language processing-based approach leveraging expert-generated keywords and logical operators and (2) an innovative application of the <b>Claude</b> 2 <b>large</b> <b>language</b> <b>model</b> to classify cases based on content-specific <b>prompts.</b> We use the Cambridge Law Corpus of 356,011 UK court decisions and determine that the <b>large</b> <b>language</b> <b>model</b> achieves a weighted F1 score of 0.94 versus 0.78 for keywords. Despite iterative refinement, the search logic based on keywords fails to capture nuances in legal language. We identify and extract 3,102 summary judgment cases, enabling us to map their distribution across various UK courts over a temporal span. The paper marks a pioneering step in employing advanced natural language processing to tackle core legal research tasks, demonstrating how these technologies can bridge systemic gaps and enhance the accessibility of legal information. We share the extracted dataset metrics to support further research on summary judgments.</p></p class="citation"></blockquote><h3 id=3464--77309-rethinking-llm-language-adaptation-a-case-study-on-chinese-mixtral-yiming-cui-et-al-2024>(34/64 | 77/309) Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral (Yiming Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Cui, Xin Yao. (2024)<br><strong>Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral</strong><br><button class=copy-to-clipboard title="Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Foundation Model, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01851v1.pdf filename=2403.01851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mixtral, a representative sparse mixture of experts (SMoE) language model, has received significant attention due to its unique model design and superior performance. Based on Mixtral-8x7B-v0.1, in this paper, we propose Chinese-Mixtral and Chinese-Mixtral-Instruct with improved Chinese language abilities by adopting further pre-training and instruction <b>fine-tuning.</b> Experimental results show that our Chinese-Mixtral and Chinese-Mixtral-Instruct successfully improve Chinese understanding and generation performance while retaining the original English abilities. Then, we discuss several key questions when performing language adaptation on <b>large</b> <b>language</b> <b>models,</b> including the necessity of extending the language-specific vocabulary and the choice of the initialization model <b>(foundation</b> <b>model</b> v.s. instruction model), by providing empirical results and analysis. We also present the visualizations of each expert to examine their importance on downstream tasks. Our resources are publicly available through \url{https://github.com/ymcui/Chinese-Mixtral}.</p></p class="citation"></blockquote><h3 id=3564--78309-enhancing-multi-domain-automatic-short-answer-grading-through-an-explainable-neuro-symbolic-pipeline-felix-künnecke-et-al-2024>(35/64 | 78/309) Enhancing Multi-Domain Automatic Short Answer Grading through an Explainable Neuro-Symbolic Pipeline (Felix Künnecke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felix Künnecke, Anna Filighera, Colin Leong, Tim Steuer. (2024)<br><strong>Enhancing Multi-Domain Automatic Short Answer Grading through an Explainable Neuro-Symbolic Pipeline</strong><br><button class=copy-to-clipboard title="Enhancing Multi-Domain Automatic Short Answer Grading through an Explainable Neuro-Symbolic Pipeline" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Supervised Learning, Weakly-supervised Learning, Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01811v1.pdf filename=2403.01811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Grading short answer questions automatically with interpretable <b>reasoning</b> behind the grading decision is a challenging goal for current <b>transformer</b> approaches. Justification cue detection, in combination with logical reasoners, has shown a promising direction for neuro-symbolic architectures in ASAG. But, one of the main challenges is the requirement of annotated justification cues in the students&rsquo; responses, which only exist for a few ASAG datasets. To overcome this challenge, we contribute (1) a weakly <b>supervised</b> annotation procedure for justification cues in ASAG datasets, and (2) a neuro-symbolic model for explainable ASAG based on justification cues. Our approach improves upon the RMSE by 0.24 to 0.3 compared to the state-of-the-art on the Short Answer Feedback dataset in a bilingual, multi-domain, and multi-question training setup. This result shows that our approach provides a promising direction for generating high-quality grades and accompanying explanations for future research in ASAG and educational NLP.</p></p class="citation"></blockquote><h3 id=3664--79309-kenetknowledge-enhanced-doc-label-attention-network-for-multi-label-text-classification-bo-li-et-al-2024>(36/64 | 79/309) KeNet:Knowledge-enhanced Doc-Label Attention Network for Multi-label text classification (Bo Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Li, Yuyan Chen, Liang Zeng. (2024)<br><strong>KeNet:Knowledge-enhanced Doc-Label Attention Network for Multi-label text classification</strong><br><button class=copy-to-clipboard title="KeNet:Knowledge-enhanced Doc-Label Attention Network for Multi-label text classification" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Recommendation, Information Retrieval, Sentiment Analysis, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01767v1.pdf filename=2403.01767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-Label <b>Text</b> <b>Classification</b> (MLTC) is a fundamental task in the field of Natural Language Processing (NLP) that involves the assignment of multiple labels to a given <b>text.</b> <b>MLTC</b> has gained significant importance and has been widely applied in various domains such as topic recognition, <b>recommendation</b> systems, <b>sentiment</b> <b>analysis,</b> and <b>information</b> <b>retrieval.</b> However, traditional machine learning and Deep neural network have not yet addressed certain issues, such as the fact that some documents are brief but have a large number of labels and how to establish relationships between the labels. It is imperative to additionally acknowledge that the significance of knowledge is substantiated in the realm of MLTC. To address this issue, we provide a novel approach known as Knowledge-enhanced Doc-Label Attention Network (KeNet). Specifically, we design an Attention Network that incorporates external knowledge, label embedding, and a comprehensive attention mechanism. In contrast to conventional methods, we use comprehensive representation of documents, knowledge and labels to predict all labels for each single <b>text.</b> <b>Our</b> approach has been validated by comprehensive research conducted on three multi-label datasets. Experimental results demonstrate that our method outperforms state-of-the-art MLTC method. Additionally, a case study is undertaken to illustrate the practical implementation of KeNet.</p></p class="citation"></blockquote><h3 id=3764--80309-topicdiff-a-topic-enriched-diffusion-approach-for-multimodal-conversational-emotion-detection-jiamin-luo-et-al-2024>(37/64 | 80/309) TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection (Jiamin Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiamin Luo, Jingjing Wang, Guodong Zhou. (2024)<br><strong>TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection</strong><br><button class=copy-to-clipboard title="TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 36<br>Keywords: Diffusion Model, Multi-modal, Multi-modal, Topic Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04789v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04789v1.pdf filename=2403.04789v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> Conversational Emotion (MCE) detection, generally spanning across the acoustic, vision and language modalities, has attracted increasing interest in the multimedia community. Previous studies predominantly focus on learning contextual information in conversations with only a few considering the <b>topic</b> <b>information</b> in single language modality, while always neglecting the acoustic and vision <b>topic</b> <b>information.</b> On this basis, we propose a model-agnostic <b>Topic-enriched</b> <b>Diffusion</b> <b>(TopicDiff)</b> approach for capturing <b>multimodal</b> <b>topic</b> <b>information</b> in MCE tasks. Particularly, we integrate the <b>diffusion</b> <b>model</b> into neural <b>topic</b> <b>model</b> to alleviate the diversity deficiency problem of neural <b>topic</b> <b>model</b> in capturing <b>topic</b> <b>information.</b> Detailed evaluations demonstrate the significant improvements of TopicDiff over the state-of-the-art MCE baselines, justifying the importance of <b>multimodal</b> <b>topic</b> <b>information</b> to MCE and the effectiveness of TopicDiff in capturing such information. Furthermore, we observe an interesting finding that the <b>topic</b> <b>information</b> in acoustic and vision is more discriminative and robust compared to the language.</p></p class="citation"></blockquote><h3 id=3864--81309-breaking-the-language-barrier-can-direct-inference-outperform-pre-translation-in-multilingual-llm-applications-yotam-intrator-et-al-2024>(38/64 | 81/309) Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications? (Yotam Intrator et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yotam Intrator, Matan Halfon, Roman Goldenberg, Reut Tsarfaty, Matan Eyal, Ehud Rivlin, Yossi Matias, Natalia Aizenberg. (2024)<br><strong>Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?</strong><br><button class=copy-to-clipboard title="Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Stemming, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04792v1.pdf filename=2403.04792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> hold significant promise in multilingual applications. However, inherent biases <b>stemming</b> from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss. This study re-evaluates the need for pre-translation in the context of PaLM2 models (Anil et al., 2023), which have been established as highly performant in multilingual tasks. We offer a comprehensive investigation across 108 languages and 6 diverse <b>benchmarks,</b> including open-end generative tasks, which were excluded from previous similar studies. Our findings challenge the pre-translation paradigm established in prior research, highlighting the advantages of direct inference in PaLM2. Specifically, PaLM2-L consistently outperforms pre-translation in 94 out of 108 languages. These findings pave the way for more efficient and effective multilingual applications, alleviating the limitations associated with pre-translation and unlocking linguistic authenticity.</p></p class="citation"></blockquote><h3 id=3964--82309-varierr-nli-separating-annotation-error-from-human-label-variation-leon-weber-genzel-et-al-2024>(39/64 | 82/309) VariErr NLI: Separating Annotation Error from Human Label Variation (Leon Weber-Genzel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leon Weber-Genzel, Siyao Peng, Marie-Catherine de Marneffe, Barbara Plank. (2024)<br><strong>VariErr NLI: Separating Annotation Error from Human Label Variation</strong><br><button class=copy-to-clipboard title="VariErr NLI: Separating Annotation Error from Human Label Variation" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, GPT, GPT-4, Natural Language Inference<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01931v1.pdf filename=2403.01931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human label variation arises when annotators assign different labels to the same item for valid reasons, while annotation errors occur when labels are assigned for invalid reasons. These two issues are prevalent in NLP <b>benchmarks,</b> yet existing research has studied them in isolation. To the best of our knowledge, there exists no prior work that focuses on teasing apart error from signal, especially in cases where signal is beyond black-and-white. To fill this gap, we introduce a systematic methodology and a new dataset, VariErr (variation versus error), focusing on the <b>NLI</b> task in English. We propose a 2-round annotation scheme with annotators explaining each label and subsequently judging the validity of label-explanation pairs. \name{} contains 7,574 validity judgments on 1,933 explanations for 500 re-annotated <b>NLI</b> items. We assess the effectiveness of various automatic error detection (AED) methods and <b>GPTs</b> in uncovering errors versus human label variation. We find that state-of-the-art AED methods significantly underperform compared to <b>GPTs</b> and humans. While <b>GPT-4</b> is the best system, it still falls short of human performance. Our methodology is applicable beyond <b>NLI,</b> offering fertile ground for future research on error versus plausible variation, which in turn can yield better and more trustworthy NLP systems.</p></p class="citation"></blockquote><h3 id=4064--83309-fostering-the-ecosystem-of-open-neural-encoders-for-portuguese-with-albertina-pt-family-rodrigo-santos-et-al-2024>(40/64 | 83/309) Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina PT* Family (Rodrigo Santos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rodrigo Santos, João Rodrigues, Luís Gomes, João Silva, António Branco, Henrique Lopes Cardoso, Tomás Freitas Osório, Bernardo Leite. (2024)<br><em><em>Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina PT</em> Family</em>*<br><button class=copy-to-clipboard title="Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina PT* Family" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Low-Resource, Large Language Model, SuperGLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01897v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01897v2.pdf filename=2403.01897v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To foster the neural encoding of Portuguese, this paper contributes foundation encoder models that represent an expansion of the still very scarce ecosystem of <b>large</b> <b>language</b> <b>models</b> specifically developed for this language that are fully open, in the sense that they are open source and openly distributed for free under an open license for any purpose, thus including research and commercial usages. Like most languages other than English, Portuguese is <b>low-resourced</b> in terms of these foundational language resources, there being the inaugural 900 million parameter Albertina and 335 million Bertimbau. Taking this couple of models as an inaugural set, we present the extension of the ecosystem of state-of-the-art open encoders for Portuguese with a larger, top performance-driven model with 1.5 billion parameters, and a smaller, efficiency-driven model with 100 million parameters. While achieving this primary goal, further results that are relevant for this ecosystem were obtained as well, namely new datasets for Portuguese based on the <b>SuperGLUE</b> <b>benchmark,</b> which we also distribute openly.</p></p class="citation"></blockquote><h3 id=4164--84309-nusabert-teaching-indobert-to-be-multilingual-and-multicultural-wilson-wongso-et-al-2024>(41/64 | 84/309) NusaBERT: Teaching IndoBERT to be Multilingual and Multicultural (Wilson Wongso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wilson Wongso, David Samuel Setiawan, Steven Limcorn, Ananto Joyoadikusumo. (2024)<br><strong>NusaBERT: Teaching IndoBERT to be Multilingual and Multicultural</strong><br><button class=copy-to-clipboard title="NusaBERT: Teaching IndoBERT to be Multilingual and Multicultural" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Low-Resource, Natural Language Understanding, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01817v1.pdf filename=2403.01817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Indonesia&rsquo;s linguistic landscape is remarkably diverse, encompassing over 700 languages and dialects, making it one of the world&rsquo;s most linguistically rich nations. This diversity, coupled with the widespread practice of code-switching and the presence of <b>low-resource</b> regional languages, presents unique challenges for modern <b>pre-trained</b> <b>language</b> <b>models.</b> In response to these challenges, we developed NusaBERT, building upon IndoBERT by incorporating vocabulary expansion and leveraging a diverse multilingual corpus that includes regional languages and dialects. Through rigorous evaluation across a range of <b>benchmarks,</b> NusaBERT demonstrates state-of-the-art performance in tasks involving multiple languages of Indonesia, paving the way for future <b>natural</b> <b>language</b> <b>understanding</b> research for under-represented languages.</p></p class="citation"></blockquote><h3 id=4264--85309-spuq-perturbation-based-uncertainty-quantification-for-large-language-models-xiang-gao-et-al-2024>(42/64 | 85/309) SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models (Xiang Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Gao, Jiaxin Zhang, Lalla Mouatadid, Kamalika Das. (2024)<br><strong>SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models</strong><br><button class=copy-to-clipboard title="SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02509v1.pdf filename=2403.02509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have become increasingly prevalent, offering remarkable <b>text</b> <b>generation</b> capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in <b>LLMs.</b> While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for <b>LLM</b> inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for <b>text</b> <b>generation</b> tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggregation techniques. Our findings show a substantial improvement in model uncertainty calibration, with a reduction in Expected Calibration Error (ECE) by 50% on average. Our findings suggest that our proposed UQ method offers promising steps toward enhancing the reliability and trustworthiness of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=4364--86309-trial-and-error-exploration-based-trajectory-optimization-for-llm-agents-yifan-song-et-al-2024>(43/64 | 86/309) Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents (Yifan Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, Bill Yuchen Lin. (2024)<br><strong>Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents</strong><br><button class=copy-to-clipboard title="Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02502v1.pdf filename=2403.02502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open <b>LLM</b> agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create <b>contrastive</b> <b>trajectory</b> pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using <b>contrastive</b> <b>learning</b> methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a <b>large</b> <b>margin.</b> <b>Furthermore,</b> an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=4464--87309-what-has-lebenchmark-learnt-about-french-syntax-zdravko-dugonjić-et-al-2024>(44/64 | 87/309) What has LeBenchmark Learnt about French Syntax? (Zdravko Dugonjić et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zdravko Dugonjić, Adrien Pupier, Benjamin Lecouteux, Maximin Coavoux. (2024)<br><strong>What has LeBenchmark Learnt about French Syntax?</strong><br><button class=copy-to-clipboard title="What has LeBenchmark Learnt about French Syntax?" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Speech-to-Speech Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02173v1.pdf filename=2403.02173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper reports on a series of experiments aiming at probing LeBenchmark, a pretrained acoustic model trained on 7k hours of spoken French, for syntactic information. Pretrained acoustic models are increasingly used for downstream <b>speech</b> <b>tasks</b> such as <b>automatic</b> <b>speech</b> <b>recognition,</b> <b>speech</b> <b>translation,</b> spoken language understanding or <b>speech</b> <b>parsing.</b> They are trained on very low level information (the raw <b>speech</b> <b>signal),</b> and do not have explicit lexical knowledge. Despite that, they obtained reasonable results on tasks that requires higher level linguistic knowledge. As a result, an emerging question is whether these models encode syntactic information. We probe each representation layer of LeBenchmark for syntax, using the Orf'eo treebank, and observe that it has learnt some syntactic information. Our results show that syntactic information is more easily extractable from the middle layers of the network, after which a very sharp decrease is observed.</p></p class="citation"></blockquote><h3 id=4564--88309-topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic-vasudevan-nedumpozhimana-et-al-2024>(45/64 | 88/309) Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic? (Vasudevan Nedumpozhimana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vasudevan Nedumpozhimana, John D. Kelleher. (2024)<br><strong>Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?</strong><br><button class=copy-to-clipboard title="Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: BERT, RoBERTa, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02009v1.pdf filename=2403.02009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> Neural Language Models achieve state-of-the-art performance on various natural language processing tasks. However, an open question is the extent to which these models rely on word-order/syntactic or word co-occurrence/topic-based information when processing natural language. This work contributes to this debate by addressing the question of whether these models primarily use topic as a signal, by exploring the relationship between <b>Transformer-based</b> models&rsquo; <b>(BERT</b> and <b>RoBERTa&rsquo;s)</b> performance on a range of probing tasks in English, from simple lexical tasks such as sentence length prediction to complex semantic tasks such as idiom token identification, and the sensitivity of these tasks to the topic information. To this end, we propose a novel probing method which we call topic-aware probing. Our initial results indicate that <b>Transformer-based</b> models encode both topic and non-topic information in their intermediate layers, but also that the facility of these models to distinguish idiomatic usage is primarily based on their ability to identify and encode topic. Furthermore, our analysis of these models&rsquo; performance on other standard probing tasks suggests that tasks that are relatively insensitive to the topic information are also tasks that are relatively difficult for these models.</p></p class="citation"></blockquote><h3 id=4664--89309-vanilla-transformers-are-transfer-capability-teachers-xin-lu-et-al-2024>(46/64 | 89/309) Vanilla Transformers are Transfer Capability Teachers (Xin Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Lu, Yanyan Zhao, Bing Qin. (2024)<br><strong>Vanilla Transformers are Transfer Capability Teachers</strong><br><button class=copy-to-clipboard title="Vanilla Transformers are Transfer Capability Teachers" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, BERT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01994v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01994v1.pdf filename=2403.01994v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, Mixture of Experts (MoE) <b>Transformers</b> have garnered increasing attention due to their advantages in model capacity and computational efficiency. However, studies have indicated that MoE <b>Transformers</b> underperform vanilla <b>Transformers</b> in many downstream tasks, significantly diminishing the practical value of MoE models. To explain this issue, we propose that the pre-training performance and transfer capability of a model are joint determinants of its downstream task performance. MoE models, in comparison to vanilla models, have poorer transfer capability, leading to their subpar performance in downstream tasks. To address this issue, we introduce the concept of transfer capability <b>distillation,</b> positing that although vanilla models have weaker performance, they are effective teachers of transfer capability. The MoE models guided by vanilla models can achieve both strong pre-training performance and transfer capability, ultimately enhancing their performance in downstream tasks. We design a specific <b>distillation</b> method and conduct experiments on the <b>BERT</b> architecture. Experimental results show a significant improvement in downstream performance of MoE models, and many further evidences also strongly support the concept of transfer capability <b>distillation.</b> Finally, we attempt to interpret transfer capability <b>distillation</b> and provide some insights from the perspective of model feature.</p></p class="citation"></blockquote><h3 id=4764--90309-language-and-speech-technology-for-central-kurdish-varieties-sina-ahmadi-et-al-2024>(47/64 | 90/309) Language and Speech Technology for Central Kurdish Varieties (Sina Ahmadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sina Ahmadi, Daban Q. Jaff, Md Mahfuz Ibn Alam, Antonios Anastasopoulos. (2024)<br><strong>Language and Speech Technology for Central Kurdish Varieties</strong><br><button class=copy-to-clipboard title="Language and Speech Technology for Central Kurdish Varieties" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01983v1.pdf filename=2403.01983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Kurdish, an Indo-European language spoken by over 30 million speakers, is considered a dialect continuum and known for its diversity in language varieties. Previous studies addressing language and <b>speech</b> <b>technology</b> for Kurdish handle it in a monolithic way as a macro-language, resulting in disparities for dialects and varieties for which there are few resources and tools available. In this paper, we take a step towards developing resources for language and <b>speech</b> <b>technology</b> for varieties of Central Kurdish, creating a corpus by transcribing movies and TV series as an alternative to fieldwork. Additionally, we report the performance of <b>machine</b> <b>translation,</b> <b>automatic</b> <b>speech</b> <b>recognition,</b> and language identification as downstream tasks evaluated on Central Kurdish varieties. Data and models are publicly available under an open license at <a href=https://github.com/sinaahmadi/CORDI>https://github.com/sinaahmadi/CORDI</a>.</p></p class="citation"></blockquote><h3 id=4864--91309-webcites-attributed-query-focused-summarization-on-chinese-web-search-results-with-citations-haolin-deng-et-al-2024>(48/64 | 91/309) WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations (Haolin Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haolin Deng, Chang Wang, Xin Li, Dezhang Yuan, Junlang Zhan, Tianhua Zhou, Jin Ma, Jun Gao, Ruifeng Xu. (2024)<br><strong>WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations</strong><br><button class=copy-to-clipboard title="WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01774v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01774v1.pdf filename=2403.01774v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enhancing the attribution in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is a crucial task. One feasible approach is to enable <b>LLMs</b> to cite external sources that support their generations. However, existing datasets and evaluation methods in this domain still exhibit notable limitations. In this work, we formulate the task of attributed query-focused <b>summarization</b> (AQFS) and present WebCiteS, a Chinese dataset featuring 7k human-annotated summaries with citations. WebCiteS derives from real-world user queries and web search results, offering a valuable resource for model training and evaluation. Prior works in attribution evaluation do not differentiate between groundedness errors and citation errors. They also fall short in automatically verifying sentences that draw partial support from multiple sources. We tackle these issues by developing detailed metrics and enabling the automatic evaluator to decompose the sentences into sub-claims for fine-grained verification. Our comprehensive evaluation of both open-source and proprietary models on WebCiteS highlights the challenge <b>LLMs</b> face in correctly citing sources, underscoring the necessity for further improvement. The dataset and code will be open-sourced to facilitate further research in this crucial field.</p></p class="citation"></blockquote><h3 id=4964--92309-derivative-free-optimization-for-low-rank-adaptation-in-large-language-models-feihu-jin-et-al-2024>(49/64 | 92/309) Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models (Feihu Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feihu Jin, Yin Liu, Ying Tan. (2024)<br><strong>Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models</strong><br><button class=copy-to-clipboard title="Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Few-shot, Large Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01754v1.pdf filename=2403.01754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient tuning methods such as LoRA could achieve comparable performance to model tuning by tuning a small portion of the parameters. However, substantial computational resources are still required, as this process involves calculating gradients and performing back-propagation throughout the model. Much effort has recently been devoted to utilizing the derivative-free optimization method to eschew the computation of gradients and showcase an augmented level of robustness in <b>few-shot</b> settings. In this paper, we prepend the low-rank modules into each <b>self-attention</b> layer of the model and employ two derivative-free optimization methods to optimize these low-rank modules at each layer alternately. Extensive results on various tasks and language models demonstrate that our proposed method achieves substantial improvement and exhibits clear advantages in memory usage and convergence speed compared to existing gradient-based parameter-efficient tuning and derivative-free optimization methods in <b>few-shot</b> settings.</p></p class="citation"></blockquote><h3 id=5064--93309-decode-neural-signal-as-speech-yiqian-yang-et-al-2024>(50/64 | 93/309) Decode Neural signal as Speech (Yiqian Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiqian Yang, Yiqun Duan, Qiang Zhang, Renjing Xu, Hui Xiong. (2024)<br><strong>Decode Neural signal as Speech</strong><br><button class=copy-to-clipboard title="Decode Neural signal as Speech" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: BLEU, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01748v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01748v1.pdf filename=2403.01748v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decoding language from brain dynamics is an important open direction in the realm of brain-computer interface (BCI), especially considering the rapid growth of <b>large</b> <b>language</b> <b>models.</b> Compared to invasive-based signals which require electrode implantation surgery, non-invasive neural signals (e.g. EEG, MEG) have attracted increasing attention considering their safety and generality. However, the exploration is not adequate in three aspects: 1) previous methods mainly focus on EEG but none of the previous works address this problem on MEG with better signal quality; 2) prior works have predominantly used <code>teacher-forcing" during generative decoding, which is impractical; 3) prior works are mostly </code>BART-based" not fully auto-regressive, which performs better in other sequence tasks. In this paper, we explore the brain-to-text translation of MEG signals in a speech-decoding formation. Here we are the first to investigate a cross-attention-based ``whisper" model for generating text directly from MEG signals without teacher forcing. Our model achieves impressive <b>BLEU-1</b> scores of 60.30 and 52.89 without pretraining & teacher-forcing on two major datasets (\textit{GWilliams} and \textit{Schoffelen}). This paper conducts a comprehensive review to understand how speech decoding formation performs on the neural decoding tasks, including pretraining initialization, training & evaluation set splitting, augmentation, and <b>scaling</b> <b>law.</b></p></p class="citation"></blockquote><h3 id=5164--94309-hypertext-entity-extraction-in-webpage-yifei-yang-et-al-2024>(51/64 | 94/309) Hypertext Entity Extraction in Webpage (Yifei Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Yang, Tianqiao Liu, Bo Shao, Hai Zhao, Linjun Shou, Ming Gong, Daxin Jiang. (2024)<br><strong>Hypertext Entity Extraction in Webpage</strong><br><button class=copy-to-clipboard title="Hypertext Entity Extraction in Webpage" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: GPT, GPT-3, GPT-3.5<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01698v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01698v1.pdf filename=2403.01698v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Webpage entity extraction is a fundamental natural language processing task in both research and applications. Nowadays, the majority of webpage entity extraction models are trained on structured datasets which strive to retain textual content and its structure information. However, existing datasets all overlook the rich hypertext features (e.g., font color, font size) which show their effectiveness in previous works. To this end, we first collect a \textbf{H}ypertext \textbf{E}ntity \textbf{E}xtraction \textbf{D}ataset (\textit{HEED}) from the e-commerce domains, scraping both the text and the corresponding explicit hypertext features with high-quality manual entity annotations. Furthermore, we present the \textbf{Mo}E-based \textbf{E}ntity \textbf{E}xtraction \textbf{F}ramework (\textit{MoEEF}), which efficiently integrates multiple features to enhance model performance by Mixture of Experts and outperforms strong baselines, including the state-of-the-art small-scale models and <b>GPT-3.5-turbo.</b> Moreover, the effectiveness of hypertext features in \textit{HEED} and several model components in \textit{MoEEF} are analyzed.</p></p class="citation"></blockquote><h3 id=5264--95309-fcds-fusing-constituency-and-dependency-syntax-into-document-level-relation-extraction-xudong-zhu-et-al-2024>(52/64 | 95/309) FCDS: Fusing Constituency and Dependency Syntax into Document-Level Relation Extraction (Xudong Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xudong Zhu, Zhao Kang, Bei Hui. (2024)<br><strong>FCDS: Fusing Constituency and Dependency Syntax into Document-Level Relation Extraction</strong><br><button class=copy-to-clipboard title="FCDS: Fusing Constituency and Dependency Syntax into Document-Level Relation Extraction" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Graph, Reasoning, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01886v1.pdf filename=2403.01886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Document-level <b>Relation</b> <b>Extraction</b> (DocRE) aims to identify <b>relation</b> <b>labels</b> between entities within a single document. It requires handling several sentences and <b>reasoning</b> over them. State-of-the-art DocRE methods use a <b>graph</b> structure to connect entities across the document to capture dependency syntax information. However, this is insufficient to fully exploit the rich syntax information in the document. In this work, we propose to fuse constituency and dependency syntax into DocRE. It uses constituency syntax to aggregate the whole sentence information and select the instructive sentences for the pairs of targets. It exploits the dependency syntax in a <b>graph</b> structure with constituency syntax enhancement and chooses the path between entity pairs based on the dependency <b>graph.</b> The experimental results on datasets from various domains demonstrate the effectiveness of the proposed method. The code is publicly available at this url.</p></p class="citation"></blockquote><h3 id=5364--96309-brilla-ai-ai-contestant-for-the-national-science-and-maths-quiz-george-boateng-et-al-2024>(53/64 | 96/309) Brilla AI: AI Contestant for the National Science and Maths Quiz (George Boateng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>George Boateng, Jonathan Abrefah Mensah, Kevin Takyi Yeboah, William Edor, Andrew Kojo Mensah-Onumah, Naafi Dasana Ibrahim, Nana Sam Yeboah. (2024)<br><strong>Brilla AI: AI Contestant for the National Science and Maths Quiz</strong><br><button class=copy-to-clipboard title="Brilla AI: AI Contestant for the National Science and Maths Quiz" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs-SD, cs.CL, eess-AS<br>Keyword Score: 23<br>Keywords: Benchmarking, Question Answering, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01699v1.pdf filename=2403.01699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The African continent lacks enough qualified teachers which hampers the provision of adequate learning support. An AI could potentially augment the efforts of the limited number of teachers, leading to better learning outcomes. Towards that end, this work describes and evaluates the first key output for the NSMQ AI Grand Challenge, which proposes a robust, real-world <b>benchmark</b> for such an AI: &ldquo;Build an AI to compete live in Ghana&rsquo;s National Science and Maths Quiz (NSMQ) competition and win - performing better than the best contestants in all rounds and stages of the competition&rdquo;. The NSMQ is an annual live science and mathematics competition for senior secondary school students in Ghana in which 3 teams of 2 students compete by answering <b>questions</b> <b>across</b> biology, chemistry, physics, and math in 5 rounds over 5 progressive stages until a winning team is crowned for that year. In this work, we built Brilla AI, an AI contestant that we deployed to unofficially compete remotely and live in the Riddles round of the 2023 NSMQ Grand Finale, the first of its kind in the 30-year history of the competition. Brilla AI is currently available as a web app that livestreams the Riddles round of the contest, and runs 4 machine learning systems: (1) speech to text (2) <b>question</b> <b>extraction</b> (3) <b>question</b> <b>answering</b> and (4) text to speech that work together in real-time to quickly and accurately provide an answer, and then say it with a Ghanaian accent. In its debut, our AI answered one of the 4 riddles ahead of the 3 human contesting teams, unofficially placing second (tied). Improvements and extensions of this AI could potentially be deployed to offer science tutoring to students and eventually enable millions across Africa to have one-on-one learning interactions, democratizing science education.</p></p class="citation"></blockquote><h3 id=5464--97309-a-tutorial-on-the-pretrain-finetune-paradigm-for-natural-language-processing-yu-wang-2024>(54/64 | 97/309) A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing (Yu Wang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Wang. (2024)<br><strong>A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing</strong><br><button class=copy-to-clipboard title="A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02504v1.pdf filename=2403.02504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP). This paradigm distinguishes itself through the use of large <b>pretrained</b> <b>language</b> <b>models,</b> demonstrating remarkable efficiency in <b>finetuning</b> tasks, even with limited training data. This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. We first delve into the fundamental concepts of pretraining and <b>finetuning,</b> followed by practical exercises using real-world applications. We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. To this end, we have provided open access to all our code and datasets. The tutorial is particularly valuable for quantitative researchers in psychology, offering them an insightful guide into this innovative approach.</p></p class="citation"></blockquote><h3 id=5564--98309-subjective-textitisms-on-the-danger-of-conflating-hate-and-offence-in-abusive-language-detection-amanda-cercas-curry-et-al-2024>(55/64 | 98/309) Subjective $\textit{Isms}$? On the Danger of Conflating Hate and Offence in Abusive Language Detection (Amanda Cercas Curry et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amanda Cercas Curry, Gavin Abercrombie, Zeerak Talat. (2024)<br><strong>Subjective $\textit{Isms}$? On the Danger of Conflating Hate and Offence in Abusive Language Detection</strong><br><button class=copy-to-clipboard title="Subjective $\textit{Isms}$? On the Danger of Conflating Hate and Offence in Abusive Language Detection" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs.CL<br>Keyword Score: 20<br>Keywords: Hate Speech Detection, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02268v1.pdf filename=2403.02268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural language processing research has begun to embrace the notion of annotator subjectivity, motivated by variations in labelling. This approach understands each annotator&rsquo;s view as valid, which can be highly suitable for tasks that embed subjectivity, e.g., <b>sentiment</b> <b>analysis.</b> However, this construction may be inappropriate for tasks such as <b>hate</b> <b>speech</b> <b>detection,</b> as it affords equal validity to all positions on e.g., sexism or racism. We argue that the conflation of <b>hate</b> <b>and</b> <b>offence</b> can invalidate findings on <b>hate</b> <b>speech,</b> <b>and</b> call for future work to be situated in theory, disentangling <b>hate</b> <b>from</b> <b>its</b> orthogonal concept, offence.</p></p class="citation"></blockquote><h3 id=5664--99309-indicvoices-towards-building-an-inclusive-multilingual-speech-dataset-for-indian-languages-tahir-javed-et-al-2024>(56/64 | 99/309) IndicVoices: Towards building an Inclusive Multilingual Speech Dataset for Indian Languages (Tahir Javed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tahir Javed, Janki Atul Nawale, Eldho Ittan George, Sakshi Joshi, Kaushal Santosh Bhogale, Deovrat Mehendale, Ishvinder Virender Sethi, Aparna Ananthanarayanan, Hafsah Faquih, Pratiti Palit, Sneha Ravishankar, Saranya Sukumaran, Tripura Panchagnula, Sunjay Murali, Kunal Sharad Gandhi, Ambujavalli R, Manickam K M, C Venkata Vaijayanthi, Krishnan Srinivasa Raghavan Karunganni, Pratyush Kumar, Mitesh M Khapra. (2024)<br><strong>IndicVoices: Towards building an Inclusive Multilingual Speech Dataset for Indian Languages</strong><br><button class=copy-to-clipboard title="IndicVoices: Towards building an Inclusive Multilingual Speech Dataset for Indian Languages" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Automatic Speech Recognition, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01926v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01926v1.pdf filename=2403.01926v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present INDICVOICES, a dataset of natural and spontaneous speech containing a total of 7348 hours of read (9%), extempore (74%) and conversational (17%) audio from 16237 speakers covering 145 Indian districts and 22 languages. Of these 7348 hours, 1639 hours have already been transcribed, with a median of 73 hours per language. Through this paper, we share our journey of capturing the cultural, linguistic and demographic diversity of India to create a one-of-its-kind inclusive and representative dataset. More specifically, we share an open-source blueprint for data collection at scale comprising of standardised protocols, centralised tools, a repository of engaging questions, <b>prompts</b> and conversation scenarios spanning multiple domains and topics of interest, quality control mechanisms, comprehensive transcription guidelines and transcription tools. We hope that this open source blueprint will serve as a comprehensive starter kit for data collection efforts in other multilingual regions of the world. Using INDICVOICES, we build IndicASR, the first <b>ASR</b> model to support all the 22 languages listed in the 8th schedule of the Constitution of India. All the data, tools, guidelines, models and other materials developed as a part of this work will be made publicly available</p></p class="citation"></blockquote><h3 id=5764--100309-arabic-text-sentiment-analysis-reinforcing-human-performed-surveys-with-wider-topic-analysis-latifah-almurqren-et-al-2024>(57/64 | 100/309) Arabic Text Sentiment Analysis: Reinforcing Human-Performed Surveys with Wider Topic Analysis (Latifah Almurqren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Latifah Almurqren, Ryan Hodgson, Alexandra Cristea. (2024)<br><strong>Arabic Text Sentiment Analysis: Reinforcing Human-Performed Surveys with Wider Topic Analysis</strong><br><button class=copy-to-clipboard title="Arabic Text Sentiment Analysis: Reinforcing Human-Performed Surveys with Wider Topic Analysis" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: LSTM, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01921v1.pdf filename=2403.01921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Sentiment</b> <b>analysis</b> (SA) has been, and is still, a thriving research area. However, the task of Arabic <b>sentiment</b> <b>analysis</b> (ASA) is still underrepresented in the body of research. This study offers the first in-depth and in-breadth analysis of existing ASA studies of textual content and identifies their common themes, domains of application, methods, approaches, technologies and algorithms used. The in-depth study manually analyses 133 ASA papers published in the English language between 2002 and 2020 from four academic databases (SAGE, IEEE, Springer, WILEY) and from Google Scholar. The in-breadth study uses modern, automatic machine learning techniques, such as topic modelling and temporal analysis, on Open Access resources, to reinforce themes and trends identified by the prior study, on 2297 ASA publications between 2010-2020. The main findings show the different approaches used for ASA: machine learning, lexicon-based and hybrid approaches. Other findings include ASA &lsquo;winning&rsquo; algorithms (SVM, NB, hybrid methods). Deep learning methods, such as <b>LSTM</b> can provide higher accuracy, but for ASA sometimes the corpora are not large enough to support them. Additionally, whilst there are some ASA corpora and lexicons, more are required. Specifically, Arabic tweets corpora and datasets are currently only moderately sized. Moreover, Arabic lexicons that have high coverage contain only Modern Standard Arabic (MSA) words, and those with Arabic dialects are quite small. Thus, new corpora need to be created. On the other hand, ASA tools are stringently lacking. There is a need to develop ASA tools that can be used in industry, as well as in academia, for Arabic text SA. Hence, our study offers insights into the challenges associated with ASA research and provides suggestions for ways to move the field forward such as lack of Dialectical Arabic resource, Arabic tweets, corpora and data sets for SA.</p></p class="citation"></blockquote><h3 id=5864--101309-online-training-of-large-language-models-learn-while-chatting-juhao-liang-et-al-2024>(58/64 | 101/309) Online Training of Large Language Models: Learn while chatting (Juhao Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang, Xiangbo Wu, Benyou Wang. (2024)<br><strong>Online Training of Large Language Models: Learn while chatting</strong><br><button class=copy-to-clipboard title="Online Training of Large Language Models: Learn while chatting" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04790v1.pdf filename=2403.04790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models(LLMs)</b> have dramatically revolutionized the field of Natural Language Processing(NLP), offering remarkable capabilities that have garnered widespread usage. However, existing interaction paradigms between <b>LLMs</b> and users are constrained by either inflexibility, limitations in customization, or a lack of persistent learning. This inflexibility is particularly evident as users, especially those without programming skills, have restricted avenues to enhance or personalize the model. Existing frameworks further complicate the model training and deployment process due to their computational inefficiencies and lack of user-friendly interfaces. To overcome these challenges, this paper introduces a novel interaction paradigm-&lsquo;Online Training using External Interactions&rsquo;-that merges the benefits of persistent, real-time model updates with the flexibility for individual customization through external interactions such as AI agents or online/offline knowledge bases.</p></p class="citation"></blockquote><h3 id=5964--102309-making-pre-trained-language-models-great-on-tabular-prediction-jiahuan-yan-et-al-2024>(59/64 | 102/309) Making Pre-trained Language Models Great on Tabular Prediction (Jiahuan Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun, Jian Wu, Jintai Chen. (2024)<br><strong>Making Pre-trained Language Models Great on Tabular Prediction</strong><br><button class=copy-to-clipboard title="Making Pre-trained Language Models Great on Tabular Prediction" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Tokenization, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01841v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01841v1.pdf filename=2403.01841v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically <b>pre-trained</b> <b>LM</b> <b>model</b> for tabular data prediction. Concretely, a novel relative magnitude <b>tokenization</b> converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates feature values with the corresponding feature names. Comprehensive experiments demonstrate that our <b>pre-trained</b> <b>TP-BERTa</b> <b>leads</b> the performance among tabular DNNs and is competitive with Gradient Boosted Decision Tree models in typical tabular data regime.</p></p class="citation"></blockquote><h3 id=6064--103309-topic-modeling-analysis-of-aviation-accident-reports-a-comparative-study-between-lda-and-nmf-models-aziida-nanyonga-et-al-2024>(60/64 | 103/309) Topic Modeling Analysis of Aviation Accident Reports: A Comparative Study between LDA and NMF Models (Aziida Nanyonga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aziida Nanyonga, Hassan Wasswa, Graham Wild. (2024)<br><strong>Topic Modeling Analysis of Aviation Accident Reports: A Comparative Study between LDA and NMF Models</strong><br><button class=copy-to-clipboard title="Topic Modeling Analysis of Aviation Accident Reports: A Comparative Study between LDA and NMF Models" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: Topic Modeling, Aviation Safety, Aviation Accident Reports, Machine
Learning, LDA, NMF, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Topic Model, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04788v1.pdf filename=2403.04788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aviation safety is paramount in the modern world, with a continuous commitment to reducing accidents and improving safety standards. Central to this endeavor is the analysis of aviation accident reports, rich textual resources that hold insights into the causes and contributing factors behind aviation mishaps. This paper compares two prominent <b>topic</b> <b>modeling</b> techniques, Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), in the context of aviation accident report analysis. The study leverages the National Transportation Safety Board (NTSB) Dataset with the primary objective of automating and streamlining the process of identifying latent themes and patterns within accident reports. The Coherence Value (C_v) metric was used to evaluate the quality of generated <b>topics.</b> <b>LDA</b> demonstrates higher <b>topic</b> <b>coherence,</b> indicating stronger semantic relevance among words within <b>topics.</b> <b>At</b> the same time, NMF excelled in producing distinct and granular <b>topics,</b> <b>enabling</b> a more focused analysis of specific aspects of aviation accidents.</p></p class="citation"></blockquote><h3 id=6164--104309-cet2-modelling-topic-transitions-for-coherent-and-engaging-knowledge-grounded-conversations-lin-xu-et-al-2024>(61/64 | 104/309) CET2: Modelling Topic Transitions for Coherent and Engaging Knowledge-Grounded Conversations (Lin Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Xu, Qixian Zhou, Jinlan Fu, See-Kiong Ng. (2024)<br><strong>CET2: Modelling Topic Transitions for Coherent and Engaging Knowledge-Grounded Conversations</strong><br><button class=copy-to-clipboard title="CET2: Modelling Topic Transitions for Coherent and Engaging Knowledge-Grounded Conversations" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Dialogue System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01848v1.pdf filename=2403.01848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge-grounded <b>dialogue</b> <b>systems</b> aim to generate coherent and engaging responses based on the <b>dialogue</b> <b>contexts</b> and selected external knowledge. Previous knowledge selection methods tend to rely too heavily on the <b>dialogue</b> <b>contexts</b> or over-emphasize the new information in the selected knowledge, resulting in the selection of repetitious or incongruous knowledge and further generating repetitive or incoherent responses, as the generation of the response depends on the chosen knowledge. To address these shortcomings, we introduce a Coherent and Engaging Topic Transition (CET2) framework to model topic transitions for selecting knowledge that is coherent to the context of the conversations while providing adequate knowledge diversity for topic development. Our CET2 framework considers multiple factors for knowledge selection, including valid transition logic from <b>dialogue</b> <b>contexts</b> to the following topics and systematic comparisons between available knowledge candidates. Extensive experiments on two public <b>benchmarks</b> demonstrate the superiority and the better generalization ability of CET2 on knowledge selection. This is due to our well-designed transition features and comparative knowledge selection strategy, which are more transferable to conversations about unseen topics. Analysis of fine-grained knowledge selection accuracy also shows that CET2 can better balance topic entailment (contextual coherence) and development (knowledge diversity) in <b>dialogue</b> <b>than</b> existing approaches.</p></p class="citation"></blockquote><h3 id=6264--105309-choose-your-own-adventure-interactive-e-books-to-improve-word-knowledge-and-comprehension-skills-stephanie-day-et-al-2024>(62/64 | 105/309) Choose Your Own Adventure: Interactive E-Books to Improve Word Knowledge and Comprehension Skills (Stephanie Day et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephanie Day, Jin K. Hwang, Tracy Arner, Danielle McNamara, Carol Connor. (2024)<br><strong>Choose Your Own Adventure: Interactive E-Books to Improve Word Knowledge and Comprehension Skills</strong><br><button class=copy-to-clipboard title="Choose Your Own Adventure: Interactive E-Books to Improve Word Knowledge and Comprehension Skills" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02496v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02496v1.pdf filename=2403.02496v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The purpose of this feasibility study was to examine the potential impact of reading digital interactive e-books on essential skills that support reading comprehension with third-fifth grade students. Students read two e-Books that taught word learning and comprehension monitoring strategies in the service of learning difficult vocabulary and targeted science concepts about hurricanes. We investigated whether specific comprehension strategies including word learning and strategies that supported general reading comprehension, <b>summarization,</b> and question generation, show promise of effectiveness in building vocabulary knowledge and comprehension skills in the e-Books. Students were assigned to read one of three versions of each of the e-Books, each version implemented one strategy. The books employed a choose-your-adventure format with embedded comprehension questions that provided students with immediate feedback on their responses. Paired samples t-tests were run to examine pre-to-post differences in learning the targeted vocabulary and science concepts taught in both e-Books. For both e-Books, students demonstrated significant gains in word learning and on the targeted hurricane concepts. Additionally, Hierarchical Linear Modeling (HLM) revealed that no one strategy was more associated with larger gains than the other. Performance on the embedded questions in the books was also associated with greater posttest outcomes for both e-Books. This work discusses important considerations for implementation and future development of e-books that can enhance student engagement and improve reading comprehension.</p></p class="citation"></blockquote><h3 id=6364--106309-detection-of-non-recorded-word-senses-in-english-and-swedish-jonathan-lautenschlager-et-al-2024>(63/64 | 106/309) Detection of Non-recorded Word Senses in English and Swedish (Jonathan Lautenschlager et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Lautenschlager, Emma Sköldberg, Simon Hengchen, Dominik Schlechtweg. (2024)<br><strong>Detection of Non-recorded Word Senses in English and Swedish</strong><br><button class=copy-to-clipboard title="Detection of Non-recorded Word Senses in English and Swedish" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02285v1.pdf filename=2403.02285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study addresses the task of Unknown Sense Detection in English and Swedish. The primary objective of this task is to determine whether the meaning of a particular word usage is documented in a dictionary or not. For this purpose, sense entries are compared with word usages from modern and historical corpora using a pre-trained Word-in-Context embedder that allows us to model this task in a <b>few-shot</b> scenario. Additionally, we use human annotations to adapt and evaluate our models. Compared to a random sample from a corpus, our model is able to considerably increase the detected number of word usages with non-recorded senses.</p></p class="citation"></blockquote><h3 id=6464--107309-views-are-my-own-but-also-yours-benchmarking-theory-of-mind-using-common-ground-adil-soubki-et-al-2024>(64/64 | 107/309) Views Are My Own, But Also Yours: Benchmarking Theory of Mind using Common Ground (Adil Soubki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adil Soubki, John Murzaku, Arash Yousefi Jordehi, Peter Zeng, Magdalena Markowska, Seyed Abolghasem Mirroshandel, Owen Rambow. (2024)<br><strong>Views Are My Own, But Also Yours: Benchmarking Theory of Mind using Common Ground</strong><br><button class=copy-to-clipboard title="Views Are My Own, But Also Yours: Benchmarking Theory of Mind using Common Ground" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02451v1.pdf filename=2403.02451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluating the theory of mind (ToM) capabilities of language models (LMs) has recently received much attention. However, many existing <b>benchmarks</b> rely on synthetic data which risks misaligning the resulting experiments with human behavior. We introduce the first ToM dataset based on naturally occurring spoken dialogs, Common-ToM, and show that LMs struggle to demonstrate ToM. We then show that integrating a simple, explicit representation of beliefs improves LM performance on Common-ToM.</p></p class="citation"></blockquote><h2 id=csse-2>cs.SE (2)</h2><h3 id=12--108309-can-llms-generate-architectural-design-decisions--an-exploratory-empirical-study-rudra-dhar-et-al-2024>(1/2 | 108/309) Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study (Rudra Dhar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rudra Dhar, Karthik Vaidhyanathan, Vasudeva Varma. (2024)<br><strong>Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study</strong><br><button class=copy-to-clipboard title="Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-LG, cs-SE, cs.SE<br>Keyword Score: 90<br>Keywords: Few-shot, Fine-tuning, GPT, GPT-3, GPT-3.5, GPT-4, T5, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01709v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01709v1.pdf filename=2403.01709v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Architectural Knowledge Management (AKM) involves the organized handling of information related to architectural decisions and design within a project or organization. An essential artifact of AKM is the Architecture Decision Records (ADR), which documents key design decisions. ADRs are documents that capture decision context, decision made and various aspects related to a design decision, thereby promoting transparency, collaboration, and understanding. Despite their benefits, ADR adoption in software development has been slow due to challenges like time constraints and inconsistent uptake. Recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> may help bridge this adoption gap by facilitating ADR generation. However, the effectiveness of <b>LLM</b> for ADR generation or understanding is something that has not been explored. To this end, in this work, we perform an exploratory study that aims to investigate the feasibility of using <b>LLM</b> for the generation of ADRs given the decision context. In our exploratory study, we utilize <b>GPT</b> and <b>T5-based</b> models with 0-shot, <b>few-shot,</b> and <b>fine-tuning</b> approaches to generate the Decision of an ADR given its Context. Our results indicate that in a 0-shot setting, state-of-the-art models such as <b>GPT-4</b> generate relevant and accurate Design Decisions, although they fall short of human-level performance. Additionally, we observe that more cost-effective models like <b>GPT-3.5</b> can achieve similar outcomes in a <b>few-shot</b> setting, and smaller models such as Flan-T5 can yield comparable results after <b>fine-tuning.</b> To conclude, this exploratory study suggests that <b>LLM</b> can generate Design Decisions, but further research is required to attain human-level generation and establish standardized widespread adoption.</p></p class="citation"></blockquote><h3 id=22--109309-contrastrepair-enhancing-conversation-based-automated-program-repair-via-contrastive-test-case-pairs-jiaolong-kong-et-al-2024>(2/2 | 109/309) ContrastRepair: Enhancing Conversation-Based Automated Program Repair via Contrastive Test Case Pairs (Jiaolong Kong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaolong Kong, Mingfei Cheng, Xiaofei Xie, Shangqing Liu, Xiaoning Du, Qi Guo. (2024)<br><strong>ContrastRepair: Enhancing Conversation-Based Automated Program Repair via Contrastive Test Case Pairs</strong><br><button class=copy-to-clipboard title="ContrastRepair: Enhancing Conversation-Based Automated Program Repair via Contrastive Test Case Pairs" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 33<br>Keywords: Benchmarking, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01971v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01971v2.pdf filename=2403.01971v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated Program Repair (APR) aims to automatically generate patches for rectifying software bugs. Recent strides in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM),</b> such as <b>ChatGPT,</b> have yielded encouraging outcomes in APR, especially within the conversation-driven APR framework. Nevertheless, the efficacy of conversation-driven APR is contingent on the quality of the feedback information. In this paper, we propose ContrastRepair, a novel conversation-based APR approach that augments conversation-driven APR by providing <b>LLMs</b> with contrastive test pairs. A test pair consists of a failing test and a passing test, which offer contrastive feedback to the <b>LLM.</b> Our key insight is to minimize the difference between the generated passing test and the given failing test, which can better isolate the root causes of bugs. By providing informative and specific feedback, ContrastRepair enables the <b>LLM</b> to produce effective bug fixes. The implementation of ContrastRepair is based on the state-of-the-art <b>LLM,</b> <b>ChatGPT,</b> and it iteratively interacts with <b>ChatGPT</b> until plausible patches are generated. We evaluate ContrastRepair on multiple <b>benchmark</b> datasets, including Defects4j, QuixBugs, and HumanEval-Java. The results demonstrate that ContrastRepair significantly outperforms existing methods, achieving a new state-of-the-art in program repair. For instance, among Defects4j 1.2 and 2.0, ContrastRepair correctly repairs 143 out of all 337 bug cases, while the best-performing baseline fixes 124 bugs.</p></p class="citation"></blockquote><h2 id=csir-5>cs.IR (5)</h2><h3 id=15--110309-notellm-a-retrievable-large-language-model-for-note-recommendation-chao-zhang-et-al-2024>(1/5 | 110/309) NoteLLM: A Retrievable Large Language Model for Note Recommendation (Chao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Zhang, Shiwei Wu, Haoxin Zhang, Tong Xu, Yan Gao, Yao Hu, Enhong Chen. (2024)<br><strong>NoteLLM: A Retrievable Large Language Model for Note Recommendation</strong><br><button class=copy-to-clipboard title="NoteLLM: A Retrievable Large Language Model for Note Recommendation" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 80<br>Keywords: Contrastive Learning, Recommendation, BERT, Instruction Tuning, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01744v1.pdf filename=2403.01744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>People enjoy sharing &ldquo;notes&rdquo; including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into <b>BERT-based</b> models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings, both of which compress key note information into limited content. Besides, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have significantly outperformed <b>BERT</b> in understanding natural languages. It is promising to introduce <b>LLMs</b> into note <b>recommendation.</b> In this paper, we propose a novel unified framework called NoteLLM, which leverages <b>LLMs</b> to address the item-to-item (I2I) note <b>recommendation.</b> Specifically, we utilize Note Compression <b>Prompt</b> to compress a note into a single special token, and further learn the potentially related notes&rsquo; embeddings via a <b>contrastive</b> <b>learning</b> approach. Moreover, we use NoteLLM to <b>summarize</b> the note and generate the hashtag/category automatically through <b>instruction</b> <b>tuning.</b> Extensive validations on real scenarios demonstrate the effectiveness of our proposed method compared with the online baseline and show major improvements in the <b>recommendation</b> system of Xiaohongshu.</p></p class="citation"></blockquote><h3 id=25--111309-evaluating-the-explainability-of-neural-rankers-saran-pandian-et-al-2024>(2/5 | 111/309) Evaluating the Explainability of Neural Rankers (Saran Pandian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saran Pandian, Debasis Ganguly, Sean MacAvaney. (2024)<br><strong>Evaluating the Explainability of Neural Rankers</strong><br><button class=copy-to-clipboard title="Evaluating the Explainability of Neural Rankers" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Supervised Learning, Unsupervised Learning, Information Retrieval, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01981v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01981v1.pdf filename=2403.01981v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Information</b> <b>retrieval</b> models have witnessed a paradigm shift from <b>unsupervised</b> statistical approaches to feature-based <b>supervised</b> approaches to completely data-driven ones that make use of the pre-training of <b>large</b> <b>language</b> <b>models.</b> While the increasing complexity of the search models have been able to demonstrate improvements in effectiveness (measured in terms of relevance of top-retrieved results), a question worthy of a thorough inspection is - &ldquo;how explainable are these models?&rdquo;, which is what this paper aims to evaluate. In particular, we propose a common evaluation platform to systematically evaluate the explainability of any ranking model (the explanation algorithm being identical for all the models that are to be evaluated). In our proposed framework, each model, in addition to returning a ranked list of documents, also requires to return a list of explanation units or rationales for each document. This meta-information from each document is then used to measure how locally consistent these rationales are as an intrinsic measure of interpretability - one that does not require manual relevance assessments. Additionally, as an extrinsic measure, we compute how relevant these rationales are by leveraging sub-document level relevance assessments. Our findings show a number of interesting observations, such as sentence-level rationales are more consistent, an increase in complexity mostly leads to less consistent explanations, and that interpretability measures offer a complementary dimension of evaluation of IR systems because consistency is not well-correlated with nDCG at top ranks.</p></p class="citation"></blockquote><h3 id=35--112309-code-accord-a-corpus-of-building-regulatory-data-for-rule-generation-towards-automatic-compliance-checking-hansi-hettiarachchi-et-al-2024>(3/5 | 112/309) CODE-ACCORD: A Corpus of Building Regulatory Data for Rule Generation towards Automatic Compliance Checking (Hansi Hettiarachchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hansi Hettiarachchi, Amna Dridi, Mohamed Medhat Gaber, Pouyan Parsafard, Nicoleta Bocaneala, Katja Breitenfelder, Gonçal Costa, Maria Hedblom, Mihaela Juganaru-Mathieu, Thamer Mecharnia, Sumee Park, He Tan, Abdel-Rahman H. Tawil, Edlira Vakaj. (2024)<br><strong>CODE-ACCORD: A Corpus of Building Regulatory Data for Rule Generation towards Automatic Compliance Checking</strong><br><button class=copy-to-clipboard title="CODE-ACCORD: A Corpus of Building Regulatory Data for Rule Generation towards Automatic Compliance Checking" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Information Retrieval, Relation Extraction, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02231v1.pdf filename=2403.02231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic Compliance Checking (ACC) within the Architecture, Engineering, and Construction (AEC) sector necessitates automating the interpretation of building regulations to achieve its full potential. However, extracting <b>information</b> <b>from</b> textual rules to convert them to a machine-readable format has been a challenge due to the complexities associated with natural language and the limited resources that can support advanced machine-learning techniques. To address this challenge, we introduce CODE-ACCORD, a unique dataset compiled under the EU Horizon ACCORD project. CODE-ACCORD comprises 862 self-contained sentences extracted from the building regulations of England and Finland. Aligned with our core objective of facilitating <b>information</b> <b>extraction</b> from <b>text</b> <b>for</b> machine-readable rule generation, each sentence was annotated with entities and <b>relations.</b> <b>Entities</b> represent specific components such as &ldquo;window&rdquo; and &ldquo;smoke detectors&rdquo;, while <b>relations</b> <b>denote</b> semantic associations between these entities, collectively capturing the conveyed ideas in natural language. We manually annotated all the sentences using a group of 12 annotators. Each sentence underwent annotations by multiple annotators and subsequently careful data curation to finalise annotations, ensuring their accuracy and reliability, thereby establishing the dataset as a solid ground truth. CODE-ACCORD offers a rich resource for diverse machine learning and natural language processing (NLP) related tasks in ACC, including <b>text</b> <b>classification,</b> entity recognition and <b>relation</b> <b>extraction.</b> To the best of our knowledge, this is the first entity and <b>relation-annotated</b> <b>dataset</b> in compliance checking, which is also publicly available.</p></p class="citation"></blockquote><h3 id=45--113309-magnetic-localization-for-in-body-nano-communication-medical-systems-krzysztof-skos-et-al-2024>(4/5 | 113/309) Magnetic Localization for In-body Nano-communication Medical Systems (Krzysztof Skos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Krzysztof Skos, Josep Miquel Jornet, Pawel Kulakowski. (2024)<br><strong>Magnetic Localization for In-body Nano-communication Medical Systems</strong><br><button class=copy-to-clipboard title="Magnetic Localization for In-body Nano-communication Medical Systems" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-NI, cs.IR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02497v1.pdf filename=2403.02497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nano-machines circulating inside the human body, collecting data on tissue conditions, represent a vital part of next-generation medical diagnostic systems. However, for these devices to operate effectively, they need to relay not only their medical measurements but also their positions. This paper introduces a novel localization method for in-body nano-machines based on the magnetic field, leveraging the advantageous magnetic permeability of all human tissues. The entire proposed localization system is described, starting from 10x10 ${\mu}m^2$ magnetometers to be integrated into the nano-machines, to a set of external wires generating the magnetic field. Mathematical equations for the localization algorithm are also provided, assuming the nano-machines do not execute the computations themselves, but transmit their magnetic field measurements together with medical data outside of the body. The whole system is validated with computer <b>simulations</b> that capture the measurement error of the magnetometers, the error induced by the Earth magnetic field, and a human body model assuming different possible positions of nano-machines. The results show a very high system accuracy with localization errors even below 1 cm.</p></p class="citation"></blockquote><h3 id=55--114309-recommending-missed-citations-identified-by-reviewers-a-new-task-dataset-and-baselines-kehan-long-et-al-2024>(5/5 | 114/309) Recommending Missed Citations Identified by Reviewers: A New Task, Dataset and Baselines (Kehan Long et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kehan Long, Shasha Li, Pancheng Wang, Chenlong Bao, Jintao Tang, Ting Wang. (2024)<br><strong>Recommending Missed Citations Identified by Reviewers: A New Task, Dataset and Baselines</strong><br><button class=copy-to-clipboard title="Recommending Missed Citations Identified by Reviewers: A New Task, Dataset and Baselines" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 13<br>Keywords: Benchmarking, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01873v1.pdf filename=2403.01873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Citing comprehensively and appropriately has become a challenging task with the explosive growth of scientific publications. Current citation <b>recommendation</b> systems aim to recommend a list of scientific papers for a given text context or a draft paper. However, none of the existing work focuses on already included citations of full papers, which are imperfect and still have much room for improvement. In the scenario of peer reviewing, it is a common phenomenon that submissions are identified as missing vital citations by reviewers. This may lead to a negative impact on the credibility and validity of the research presented. To help improve citations of full papers, we first define a novel task of Recommending Missed Citations Identified by Reviewers (RMC) and construct a corresponding expert-labeled dataset called CitationR. We conduct an extensive evaluation of several state-of-the-art methods on CitationR. Furthermore, we propose a new framework RMCNet with an Attentive Reference Encoder module mining the relevance between papers, already-made citations, and missed citations. Empirical results prove that RMC is challenging, with the proposed architecture outperforming previous methods in all metrics. We release our dataset and <b>benchmark</b> models to motivate future research on this challenging new task.</p></p class="citation"></blockquote><h2 id=cscy-3>cs.CY (3)</h2><h3 id=13--115309-ai-language-models-could-both-help-and-harm-equity-in-marine-policymaking-the-case-study-of-the-bbnj-question-answering-bot-matt-ziegler-et-al-2024>(1/3 | 115/309) AI Language Models Could Both Help and Harm Equity in Marine Policymaking: The Case Study of the BBNJ Question-Answering Bot (Matt Ziegler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matt Ziegler, Sarah Lothian, Brian O&rsquo;Neill, Richard Anderson, Yoshitaka Ota. (2024)<br><strong>AI Language Models Could Both Help and Harm Equity in Marine Policymaking: The Case Study of the BBNJ Question-Answering Bot</strong><br><button class=copy-to-clipboard title="AI Language Models Could Both Help and Harm Equity in Marine Policymaking: The Case Study of the BBNJ Question-Answering Bot" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 70<br>Keywords: Fairness, Generative AI, ChatGPT, Chatbot, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01755v1.pdf filename=2403.01755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>ChatGPT</b> are set to reshape some aspects of policymaking processes. Policy practitioners are already using <b>ChatGPT</b> for help with a variety of tasks: from drafting statements, submissions, and presentations, to conducting background research. We are cautiously hopeful that <b>LLMs</b> could be used to promote a marginally more balanced footing among decision makers in policy negotiations by assisting with certain tedious work, particularly benefiting developing countries who face capacity constraints that put them at a disadvantage in negotiations. However, the risks are particularly concerning for environmental and marine policy uses, due to the urgency of crises like climate change, high uncertainty, and trans-boundary impact. To explore the realistic potentials, limitations, and equity risks for <b>LLMs</b> in marine policymaking, we present a case study of an AI <b>chatbot</b> for the recently adopted Biodiversity Beyond National Jurisdiction Agreement (BBNJ), and critique its answers to key policy <b>questions.</b> <b>Our</b> case study demonstrates the dangers of <b>LLMs</b> in marine policymaking via their potential bias towards generating text that favors the perspectives of mainly Western economic centers of power, while neglecting developing countries&rsquo; viewpoints. We describe several ways these biases can enter the system, including: (1) biases in the underlying foundational language models; (2) biases arising from the <b>chatbot&rsquo;s</b> connection to UN negotiation documents, and (3) biases arising from the application design. We urge caution in the use of <b>generative</b> <b>AI</b> in ocean policy processes and call for more research on its equity and <b>fairness</b> implications. Our work also underscores the need for developing countries&rsquo; policymakers to develop the technical capacity to engage with AI on their own terms.</p></p class="citation"></blockquote><h3 id=23--116309-towards-implicit-prompt-for-text-to-image-models-yue-yang-et-al-2024>(2/3 | 116/309) Towards Implicit Prompt For Text-To-Image Models (Yue Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Yang, Yuqi lin, Hong Liu, Wenqi Shao, Runjian Chen, Hailong Shang, Yu Wang, Yu Qiao, Kaipeng Zhang, Ping Luo. (2024)<br><strong>Towards Implicit Prompt For Text-To-Image Models</strong><br><button class=copy-to-clipboard title="Towards Implicit Prompt For Text-To-Image Models" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CV, cs-CY, cs.CY<br>Keyword Score: 23<br>Keywords: Benchmarking, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02118v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02118v3.pdf filename=2403.02118v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>text-to-image</b> (T2I) models have had great success, and many <b>benchmarks</b> have been proposed to evaluate their performance and safety. However, they only consider explicit <b>prompts</b> while neglecting implicit <b>prompts</b> (hint at a target without explicitly mentioning it). These <b>prompts</b> may get rid of safety constraints and pose potential threats to the applications of these models. This position paper highlights the current state of T2I models toward implicit <b>prompts.</b> We present a <b>benchmark</b> named ImplicitBench and conduct an investigation on the performance and impacts of implicit <b>prompts</b> with popular T2I models. Specifically, we design and collect more than 2,000 implicit <b>prompts</b> of three aspects: General Symbols, Celebrity Privacy, and Not-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models&rsquo; capabilities under these implicit <b>prompts.</b> Experiment results show that (1) T2I models are able to accurately create various target symbols indicated by implicit <b>prompts;</b> (2) Implicit <b>prompts</b> bring potential risks of privacy leakage for T2I models. (3) Constraints of NSFW in most of the evaluated T2I models can be bypassed with implicit <b>prompts.</b> We call for increased attention to the potential and risks of implicit <b>prompts</b> in the T2I community and further investigation into the capabilities and impacts of implicit <b>prompts,</b> advocating for a balanced approach that harnesses their benefits while mitigating their risks.</p></p class="citation"></blockquote><h3 id=33--117309-recommendations-for-government-development-and-use-of-advanced-automated-systems-to-make-decisions-about-individuals-susan-landau-et-al-2024>(3/3 | 117/309) Recommendations for Government Development and Use of Advanced Automated Systems to Make Decisions about Individuals (Susan Landau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Susan Landau, James X. Dempsey, Ece Kamar, Steven M. Bellovin. (2024)<br><strong>Recommendations for Government Development and Use of Advanced Automated Systems to Make Decisions about Individuals</strong><br><button class=copy-to-clipboard title="Recommendations for Government Development and Use of Advanced Automated Systems to Make Decisions about Individuals" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: K-4; K-5; I-2, cs-AI, cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Fairness, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01649v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01649v1.pdf filename=2403.01649v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contestability &ndash; the ability to effectively challenge a decision &ndash; is critical to the implementation of <b>fairness.</b> In the context of governmental decision making about individuals, contestability is often constitutionally required as an element of due process; specific procedures may be required by state or federal law relevant to a particular program. In addition, contestability can be a valuable way to discover systemic errors, contributing to ongoing assessments and system improvement. On January 24-25, 2024, with support from the National Science Foundation and the William and Flora Hewlett Foundation, we convened a diverse group of government officials, representatives of leading technology companies, technology and policy experts from academia and the non-profit sector, advocates, and stakeholders for a workshop on advanced automated decision making, contestability, and the law. Informed by the workshop&rsquo;s rich and wide-ranging discussion, we offer these <b>recommendations.</b> A full report summarizing the discussion is in preparation.</p></p class="citation"></blockquote><h2 id=eessiv-7>eess.IV (7)</h2><h3 id=17--118309-afbt-gan-enhanced-explainability-and-diagnostic-performance-for-cognitive-decline-by-counterfactual-generative-adversarial-network-xiongri-shen-et-al-2024>(1/7 | 118/309) AFBT GAN: enhanced explainability and diagnostic performance for cognitive decline by counterfactual generative adversarial network (Xiongri Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiongri Shen, Zhenxi Song, Zhiguo Zhang. (2024)<br><strong>AFBT GAN: enhanced explainability and diagnostic performance for cognitive decline by counterfactual generative adversarial network</strong><br><button class=copy-to-clipboard title="AFBT GAN: enhanced explainability and diagnostic performance for cognitive decline by counterfactual generative adversarial network" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 65<br>Keywords: Black Box, Counter-factual, Generative Adversarial Network, Generative Adversarial Network, Transformer, Counterfactual Reasoning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01758v1.pdf filename=2403.01758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing explanation results of functional connectivity (FC) are normally generated by using classification result labels and correlation analysis methods such as Pearson&rsquo;s correlation or gradient backward. However, the diagnostic model is still trained on the <b>black</b> <b>box</b> model and might lack the attention of FCs in important regions during the training. To enhance the explainability and improve diagnostic performance, providing prior knowledge on neurodegeneration-related regions when healthy subjects (HC) develop into subject cognitive decline (SCD) and mild cognitive impairment (MCI) for the diagnostic model is a key step. To better determine the neurodegeneration-related regions, we employ <b>counterfactual</b> <b>reasoning</b> to generate the target label FC matrices derived from source label FC and then subtract source label FC with target label FC. The <b>counterfactual</b> <b>reasoning</b> architecture is constructed by adaptive forward and backward <b>transformer</b> <b>generative</b> <b>adversarial</b> <b>network</b> (AFBT <b>GAN),</b> which is specifically designed by network property in FC and inverse patch embedding operation in the <b>transformer.</b> The specific design can make the model focus more on the current network correlation and employ the global insight of the <b>transformer</b> to reconstruct FC, which both help the generation of high-quality target label FC. The validation experiments are conducted on both clinical and public datasets, the generated attention map are both vital correlated to cognitive function and the diagnostic performance is also significant. The code is available at <a href=https://github.com/SXR3015/AFBT-GAN>https://github.com/SXR3015/AFBT-GAN</a>.</p></p class="citation"></blockquote><h3 id=27--119309-harnessing-intra-group-variations-via-a-population-level-context-for-pathology-detection-p-bilha-githinji-et-al-2024>(2/7 | 119/309) Harnessing Intra-group Variations Via a Population-Level Context for Pathology Detection (P. Bilha Githinji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>P. Bilha Githinji, Xi Yuan, Zhenglin Chen, Ijaz Gul, Dingqi Shang, Wen Liang, Jianming Deng, Dan Zeng, Dongmei yu, Chenggang Yan, Peiwu Qin. (2024)<br><strong>Harnessing Intra-group Variations Via a Population-Level Context for Pathology Detection</strong><br><button class=copy-to-clipboard title="Harnessing Intra-group Variations Via a Population-Level Context for Pathology Detection" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 23<br>Keywords: Graph, Autoencoder, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02307v1.pdf filename=2403.02307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Realizing sufficient separability between the distributions of healthy and pathological samples is a critical obstacle for pathology detection <b>convolutional</b> models. Moreover, these models exhibit a bias for contrast-based images, with diminished performance on texture-based medical images. This study introduces the notion of a population-level context for pathology detection and employs a <b>graph</b> theoretic approach to model and incorporate it into the latent code of an <b>autoencoder</b> via a refinement module we term PopuSense. PopuSense seeks to capture additional intra-group variations inherent in biomedical data that a local or global context of the <b>convolutional</b> model might miss or smooth out. Experiments on contrast-based and texture-based images, with minimal adaptation, encounter the existing preference for intensity-based input. Nevertheless, PopuSense demonstrates improved separability in contrast-based images, presenting an additional avenue for refining representations learned by a model.</p></p class="citation"></blockquote><h3 id=37--120309-bayesian-uncertainty-estimation-by-hamiltonian-monte-carlo-applications-to-cardiac-mri-segmentation-yidong-zhao-et-al-2024>(3/7 | 120/309) Bayesian Uncertainty Estimation by Hamiltonian Monte Carlo: Applications to Cardiac MRI Segmentation (Yidong Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yidong Zhao, Joao Tourais, Iain Pierce, Christian Nitsche, Thomas A. Treibel, Sebastian Weingärtner, Artur M. Schweidtmann, Qian Tao. (2024)<br><strong>Bayesian Uncertainty Estimation by Hamiltonian Monte Carlo: Applications to Cardiac MRI Segmentation</strong><br><button class=copy-to-clipboard title="Bayesian Uncertainty Estimation by Hamiltonian Monte Carlo: Applications to Cardiac MRI Segmentation" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Data Augmentation, Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02311v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02311v1.pdf filename=2403.02311v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning (DL)-based methods have achieved state-of-the-art performance for a wide range of medical image segmentation tasks. Nevertheless, recent studies show that deep neural networks (DNNs) can be miscalibrated and overconfident, leading to &ldquo;silent failures&rdquo; that are risky} for clinical applications. Bayesian statistics provide an intuitive approach to DL failure detection, based on posterior probability estimation. However, Bayesian DL, and in particular the posterior estimation, is intractable for large medical image segmentation DNNs. To tackle this challenge, we propose a Bayesian learning framework by Hamiltonian Monte Carlo (HMC), tempered by cold posterior (CP) to accommodate medical <b>data</b> <b>augmentation,</b> named HMC-CP. For HMC computation, we further propose a cyclical annealing strategy, which captures both local and global geometries of the posterior distribution, enabling highly efficient Bayesian DNN training with the same computational budget requirements as training a single DNN. The resulting Bayesian DNN outputs an ensemble segmentation along with the segmentation uncertainty. We evaluate the proposed HMC-CP extensively on cardiac magnetic resonance image (MRI) segmentation, using in-domain steady-state free precession (SSFP) cine images as well as <b>out-of-domain</b> datasets of quantitative $T_1$ and $T_2$ mapping.</p></p class="citation"></blockquote><h3 id=47--121309-domain-adaptation-explainability--fairness-in-ai-for-medical-image-analysis-diagnosis-of-covid-19-based-on-3-d-chest-ct-scans-dimitrios-kollias-et-al-2024>(4/7 | 121/309) Domain adaptation, Explainability & Fairness in AI for Medical Image Analysis: Diagnosis of COVID-19 based on 3-D Chest CT-scans (Dimitrios Kollias et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitrios Kollias, Anastasios Arsenos, Stefanos Kollias. (2024)<br><strong>Domain adaptation, Explainability & Fairness in AI for Medical Image Analysis: Diagnosis of COVID-19 based on 3-D Chest CT-scans</strong><br><button class=copy-to-clipboard title="Domain adaptation, Explainability & Fairness in AI for Medical Image Analysis: Diagnosis of COVID-19 based on 3-D Chest CT-scans" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Fairness, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02192v1.pdf filename=2403.02192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper presents the DEF-AI-MIA COV19D Competition, which is organized in the framework of the <b>&lsquo;Domain</b> <b>adaptation,</b> Explainability, <b>Fairness</b> in AI for Medical Image Analysis (DEF-AI-MIA)&rsquo; Workshop of the 2024 Computer Vision and Pattern Recognition (CVPR) Conference. The Competition is the 4th in the series, following the first three Competitions held in the framework of ICCV 2021, ECCV 2022 and ICASSP 2023 International Conferences respectively. It includes two Challenges on: i) Covid-19 Detection and ii) Covid-19 <b>Domain</b> <b>Adaptation.</b> The Competition use data from COV19-CT-DB database, which is described in the paper and includes a large number of chest CT scan series. Each chest CT scan series consists of a sequence of 2-D CT slices, the number of which is between 50 and 700. Training, validation and test datasets have been extracted from COV19-CT-DB and provided to the participants in both Challenges. The paper presents the baseline models used in the Challenges and the performance which was obtained respectively.</p></p class="citation"></blockquote><h3 id=57--122309-a-spatio-temporal-aligned-sunet-model-for-low-light-video-enhancement-ruirui-lin-et-al-2024>(5/7 | 122/309) A Spatio-temporal Aligned SUNet Model for Low-light Video Enhancement (Ruirui Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruirui Lin, Nantheera Anantrasirichai, Alexandra Malyugina, David Bull. (2024)<br><strong>A Spatio-temporal Aligned SUNet Model for Low-light Video Enhancement</strong><br><button class=copy-to-clipboard title="A Spatio-temporal Aligned SUNet Model for Low-light Video Enhancement" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02408v1.pdf filename=2403.02408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distortions caused by low-light conditions are not only visually unpleasant but also degrade the performance of computer vision tasks. The restoration and enhancement have proven to be highly beneficial. However, there are only a limited number of enhancement methods explicitly designed for videos acquired in low-light conditions. We propose a Spatio-Temporal Aligned SUNet (STA-SUNet) model using a Swin <b>Transformer</b> as a backbone to capture low light video features and exploit their spatio-temporal correlations. The STA-SUNet model is trained on a novel, fully registered dataset (BVI), which comprises dynamic scenes captured under varying light conditions. It is further analysed comparatively against various other models over three test datasets. The model demonstrates superior adaptivity across all datasets, obtaining the highest PSNR and SSIM values. It is particularly effective in extreme low-light conditions, yielding fairly good visualisation results.</p></p class="citation"></blockquote><h3 id=67--123309-real-colon-a-dataset-for-developing-real-world-ai-applications-in-colonoscopy-carlo-biffi-et-al-2024>(6/7 | 123/309) REAL-Colon: A dataset for developing real-world AI applications in colonoscopy (Carlo Biffi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlo Biffi, Giulio Antonelli, Sebastian Bernhofer, Cesare Hassan, Daizen Hirata, Mineo Iwatate, Andreas Maieron, Pietro Salvagnini, Andrea Cherubini. (2024)<br><strong>REAL-Colon: A dataset for developing real-world AI applications in colonoscopy</strong><br><button class=copy-to-clipboard title="REAL-Colon: A dataset for developing real-world AI applications in colonoscopy" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02163v1.pdf filename=2403.02163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detection and diagnosis of colon polyps are key to preventing colorectal cancer. Recent evidence suggests that AI-based computer-aided detection (CADe) and computer-aided diagnosis (CADx) systems can enhance endoscopists&rsquo; performance and boost colonoscopy effectiveness. However, most available public datasets primarily consist of still images or video clips, often at a down-sampled resolution, and do not accurately represent real-world colonoscopy procedures. We introduce the REAL-Colon (Real-world multi-center Endoscopy Annotated video Library) dataset: a compilation of 2.7M native video frames from sixty full-resolution, real-world colonoscopy recordings across multiple centers. The dataset contains 350k bounding-box annotations, each created under the supervision of expert gastroenterologists. Comprehensive patient clinical data, colonoscopy acquisition information, and polyp histopathological information are also included in each video. With its unprecedented size, quality, and heterogeneity, the REAL-Colon dataset is a unique resource for researchers and developers aiming to advance AI research in colonoscopy. Its openness and transparency facilitate rigorous and reproducible research, fostering the development and <b>benchmarking</b> of more accurate and reliable colonoscopy-related algorithms and models.</p></p class="citation"></blockquote><h3 id=77--124309-iterative-occlusion-aware-light-field-depth-estimation-using-4d-geometrical-cues-rui-lourenço-et-al-2024>(7/7 | 124/309) Iterative Occlusion-Aware Light Field Depth Estimation using 4D Geometrical Cues (Rui Lourenço et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Lourenço, Lucas Thomaz, Eduardo A. B. Silva, Sergio M. M. Faria. (2024)<br><strong>Iterative Occlusion-Aware Light Field Depth Estimation using 4D Geometrical Cues</strong><br><button class=copy-to-clipboard title="Iterative Occlusion-Aware Light Field Depth Estimation using 4D Geometrical Cues" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02043v1.pdf filename=2403.02043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Light field cameras and multi-camera arrays have emerged as promising solutions for accurately estimating depth by passively capturing light information. This is possible because the 3D information of a scene is embedded in the 4D light field <b>geometry.</b> Commonly, depth estimation methods extract this information relying on gradient information, heuristic-based optimisation models, or learning-based approaches. This paper focuses mainly on explicitly understanding and exploiting 4D geometrical cues for light field depth estimation. Thus, a novel method is proposed, based on a non-learning-based optimisation approach for depth estimation that explicitly considers surface normal accuracy and occlusion regions by utilising a fully explainable 4D geometric model of the light field. The 4D model performs depth/disparity estimation by determining the orientations and analysing the intersections of key 2D planes in 4D space, which are the images of 3D-space points in the 4D light field. Experimental results show that the proposed method outperforms both learning-based and non-learning-based state-of-the-art methods in terms of surface normal angle accuracy, achieving a Median Angle Error on planar surfaces, on average, 26.3% lower than the state-of-the-art, and still being competitive with state-of-the-art methods in terms of Mean Squared Error $\vc{\times}$ 100 and Badpix 0.07.</p></p class="citation"></blockquote><h2 id=cscv-62>cs.CV (62)</h2><h3 id=162--125309-regiongpt-towards-region-understanding-vision-language-model-qiushan-guo-et-al-2024>(1/62 | 125/309) RegionGPT: Towards Region Understanding Vision Language Model (Qiushan Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, Sifei Liu. (2024)<br><strong>RegionGPT: Towards Region Understanding Vision Language Model</strong><br><button class=copy-to-clipboard title="RegionGPT: Towards Region Understanding Vision Language Model" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Image2text, Reasoning, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02330v1.pdf filename=2403.02330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vision language models (VLMs) have experienced rapid advancements through the integration of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with <b>image-text</b> pairs, yet they struggle with detailed regional visual understanding due to limited spatial awareness of the vision encoder, and the use of coarse-grained training data that lacks detailed, region-specific captions. To address this, we introduce RegionGPT (short as RGPT), a novel framework designed for complex region-level captioning and understanding. RGPT enhances the spatial awareness of regional representation with simple yet effective modifications to existing visual encoders in VLMs. We further improve performance on tasks requiring a specific output scope by integrating task-guided instruction <b>prompts</b> during both training and inference phases, while maintaining the model&rsquo;s versatility for general-purpose tasks. Additionally, we develop an automated region caption data generation pipeline, enriching the training set with detailed region-level captions. We demonstrate that a universal RGPT model can be effectively applied and significantly enhancing performance across a range of region-level tasks, including but not limited to complex region descriptions, <b>reasoning,</b> object classification, and referring expressions comprehension.</p></p class="citation"></blockquote><h3 id=262--126309-perceptive-self-supervised-learning-network-for-noisy-image-watermark-removal-chunwei-tian-et-al-2024>(2/62 | 126/309) Perceptive self-supervised learning network for noisy image watermark removal (Chunwei Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunwei Tian, Menghua Zheng, Bo Li, Yanning Zhang, Shichao Zhang, David Zhang. (2024)<br><strong>Perceptive self-supervised learning network for noisy image watermark removal</strong><br><button class=copy-to-clipboard title="Perceptive self-supervised learning network for noisy image watermark removal" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Self-supervised Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02211v1.pdf filename=2403.02211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Popular methods usually use a degradation model in a <b>supervised</b> way to learn a watermark removal model. However, it is true that reference images are difficult to obtain in the real world, as well as collected images by cameras suffer from noise. To overcome these drawbacks, we propose a perceptive <b>self-supervised</b> <b>learning</b> network for noisy image watermark removal (PSLNet) in this paper. PSLNet depends on a parallel network to remove noise and watermarks. The upper network uses task decomposition ideas to remove noise and watermarks in sequence. The lower network utilizes the degradation model idea to simultaneously remove noise and watermarks. Specifically, mentioned paired watermark images are obtained in a self <b>supervised</b> way, and paired noisy images (i.e., noisy and reference images) are obtained in a <b>supervised</b> way. To enhance the clarity of obtained images, interacting two sub-networks and fusing obtained clean images are used to improve the effects of image watermark removal in terms of structural information and pixel enhancement. Taking into texture information account, a mixed loss uses obtained images and features to achieve a robust model of noisy image watermark removal. Comprehensive experiments show that our proposed method is very effective in comparison with popular <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> for noisy image watermark removal. Codes can be obtained at <a href=https://github.com/hellloxiaotian/PSLNet>https://github.com/hellloxiaotian/PSLNet</a>.</p></p class="citation"></blockquote><h3 id=362--127309-locr-location-guided-transformer-for-optical-character-recognition-yu-sun-et-al-2024>(3/62 | 127/309) LOCR: Location-Guided Transformer for Optical Character Recognition (Yu Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Sun, Dongzhan Zhou, Chen Lin, Conghui He, Wanli Ouyang, Han-Sen Zhong. (2024)<br><strong>LOCR: Location-Guided Transformer for Optical Character Recognition</strong><br><button class=copy-to-clipboard title="LOCR: Location-Guided Transformer for Optical Character Recognition" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Optical Character Recognition, Optical Character Recognition, Out-of-domain, Transformer, BLEU, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02127v1.pdf filename=2403.02127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Academic documents are packed with texts, equations, tables, and figures, requiring comprehensive understanding for accurate <b>Optical</b> <b>Character</b> <b>Recognition</b> <b>(OCR).</b> While end-to-end <b>OCR</b> methods offer improved accuracy over layout-based approaches, they often grapple with significant repetition issues, especially with complex layouts in <b>Out-Of-Domain</b> (OOD) documents.To tackle this issue, we propose LOCR, a model that integrates location guiding into the <b>transformer</b> architecture during autoregression. We train the model on a dataset comprising over 77M text-location pairs from 125K academic document pages, including bounding boxes for words, tables and mathematical symbols. LOCR adeptly handles various formatting elements and generates content in Markdown language. It outperforms all existing methods in our test set constructed from arXiv, as measured by edit distance, <b>BLEU,</b> METEOR and F-measure.LOCR also reduces repetition frequency from 4.4% of pages to 0.5% in the arXiv dataset, from 13.2% to 1.3% in OOD quantum physics documents and from 8.1% to 1.8% in OOD marketing documents. Additionally, LOCR features an interactive <b>OCR</b> mode, facilitating the generation of complex documents through a few location <b>prompts</b> from human.</p></p class="citation"></blockquote><h3 id=462--128309-vtg-gpt-tuning-free-zero-shot-video-temporal-grounding-with-gpt-yifang-xu-et-al-2024>(4/62 | 128/309) VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT (Yifang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifang Xu, Yunzhuo Sun, Zien Xie, Benxiang Zhai, Sidan Du. (2024)<br><strong>VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT</strong><br><button class=copy-to-clipboard title="VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Fine-tuning, Supervised Learning, Unsupervised Learning, Zero-shot, GPT, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02076v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02076v1.pdf filename=2403.02076v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video temporal <b>grounding</b> (VTG) aims to locate specific temporal segments from an untrimmed video based on a linguistic query. Most existing VTG models are trained on extensive annotated video-text pairs, a process that not only introduces human biases from the queries but also incurs significant computational costs. To tackle these challenges, we propose VTG-GPT, a <b>GPT-based</b> method for <b>zero-shot</b> VTG without training or <b>fine-tuning.</b> To reduce prejudice in the original query, we employ Baichuan2 to generate debiased queries. To lessen redundant information in videos, we apply MiniGPT-v2 to transform visual content into more precise captions. Finally, we devise the proposal generator and post-processing to produce accurate segments from debiased queries and image captions. Extensive experiments demonstrate that VTG-GPT significantly outperforms SOTA methods in <b>zero-shot</b> settings and surpasses <b>unsupervised</b> approaches. More notably, it achieves competitive performance comparable to <b>supervised</b> methods. The code is available on <a href=https://github.com/YoucanBaby/VTG-GPT>https://github.com/YoucanBaby/VTG-GPT</a></p></p class="citation"></blockquote><h3 id=562--129309-lightweight-object-detection-a-study-based-on-yolov7-integrated-with-shufflenetv2-and-vision-transformer-wenkai-gong-2024>(5/62 | 129/309) Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer (Wenkai Gong, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenkai Gong. (2024)<br><strong>Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer</strong><br><button class=copy-to-clipboard title="Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Vision Transformer, Yolo, Object Detection, Convolution, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01736v1.pdf filename=2403.01736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As mobile computing technology rapidly evolves, deploying efficient <b>object</b> <b>detection</b> algorithms on mobile devices emerges as a pivotal research area in computer <b>vision.</b> <b>This</b> study zeroes in on optimizing the YOLOv7 algorithm to boost its operational efficiency and speed on mobile platforms while ensuring high accuracy. Leveraging a synergy of advanced techniques such as Group <b>Convolution,</b> ShuffleNetV2, and <b>Vision</b> <b>Transformer,</b> this research has effectively minimized the model&rsquo;s parameter count and memory usage, streamlined the network architecture, and fortified the real-time <b>object</b> <b>detection</b> proficiency on resource-constrained devices. The experimental outcomes reveal that the refined <b>YOLO</b> model demonstrates exceptional performance, markedly enhancing processing velocity while sustaining superior detection accuracy.</p></p class="citation"></blockquote><h3 id=662--130309-beyond-specialization-assessing-the-capabilities-of-mllms-in-age-and-gender-estimation-maksim-kuprashevich-et-al-2024>(6/62 | 130/309) Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation (Maksim Kuprashevich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh. (2024)<br><strong>Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation</strong><br><button class=copy-to-clipboard title="Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2-0; I-4-0; I-4-9, cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 56<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, ChatGPT, Gemini, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02302v1.pdf filename=2403.02302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have recently gained immense popularity. Powerful commercial models like <b>ChatGPT-4V</b> and <b>Gemini,</b> as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and <b>reasoning</b> abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, <b>ChatGPT,</b> LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to <b>fine-tune</b> the ShareGPT4V model for this specific task, aiming to achieve state-of-the-art results in this particular challenge. Although such a model would not be practical in production, as it is incredibly expensive compared to a specialized model like MiVOLO, it could be very useful in some tasks, like data annotation.</p></p class="citation"></blockquote><h3 id=762--131309-contrastive-region-guidance-improving-grounding-in-vision-language-models-without-training-david-wan-et-al-2024>(7/62 | 131/309) Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training (David Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Wan, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal. (2024)<br><strong>Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training</strong><br><button class=copy-to-clipboard title="Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Grounding, Image2text, Reasoning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02325v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02325v1.pdf filename=2403.02325v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Highlighting particularly relevant regions of an image can improve the performance of <b>vision-language</b> models (VLMs) on various <b>vision-language</b> (VL) tasks by guiding the model to attend more closely to these regions of interest. For example, VLMs can be given a &ldquo;visual <b>prompt&rdquo;,</b> where visual markers such as bounding boxes delineate key image regions. However, current VLMs that can incorporate visual guidance are either proprietary and expensive or require costly training on curated data that includes visual <b>prompts.</b> We introduce Contrastive Region Guidance (CRG), a training-free guidance method that enables open-source VLMs to respond to visual <b>prompts.</b> CRG contrasts model outputs produced with and without visual <b>prompts,</b> factoring out biases revealed by the model when answering without the information required to produce a correct answer (i.e., the model&rsquo;s prior). CRG achieves substantial improvements in a wide variety of VL tasks: When region annotations are provided, CRG increases absolute accuracy by up to 11.1% on ViP-Bench, a collection of six diverse region-based tasks such as recognition, math, and object relationship <b>reasoning.</b> We also show CRG&rsquo;s applicability to spatial <b>reasoning,</b> with 10% improvement on What&rsquo;sUp, as well as to compositional generalization &ndash; improving accuracy by 11.5% and 7.5% on two challenging splits from SugarCrepe &ndash; and to <b>image-text</b> alignment for generated images, where we improve by up to 8.4 AUROC and 6.8 F1 points on SeeTRUE. When reference regions are absent, CRG allows us to re-rank proposed regions in referring expression comprehension and phrase <b>grounding</b> <b>benchmarks</b> like RefCOCO/+/g and Flickr30K Entities, with an average gain of 3.2% in accuracy. Our analysis explores alternative masking strategies for CRG, quantifies CRG&rsquo;s probability shift, and evaluates the role of region guidance strength, empirically validating CRG&rsquo;s design choices.</p></p class="citation"></blockquote><h3 id=862--132309-ninformer-a-network-in-network-transformer-with-token-mixing-generated-gating-function-abdullah-nazhat-abdullah-et-al-2024>(8/62 | 132/309) NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function (Abdullah Nazhat Abdullah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdullah Nazhat Abdullah, Tarkan Aydin. (2024)<br><strong>NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function</strong><br><button class=copy-to-clipboard title="NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Object Detection, Graph Attention Networks, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02411v1.pdf filename=2403.02411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Attention mechanism is the main component of the <b>Transformer</b> architecture, and since its introduction, it has led to significant advancements in Deep Learning that span many domains and multiple tasks. The Attention Mechanism was utilized in Computer <b>Vision</b> <b>as</b> the <b>Vision</b> <b>Transformer</b> ViT, and its usage has expanded into many tasks in the <b>vision</b> <b>domain,</b> such as classification, segmentation, <b>object</b> <b>detection,</b> and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the <b>vision</b> <b>domain</b> are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as an alternative to the standard ViT block that reduces the compute burdens by replacing the normal Attention layers with a Network in Network structure that enhances the static approach of the MLP Mixer with a dynamic system of learning an element-wise <b>gating</b> function by a token mixing process. Extensive experimentation shows that the proposed design provides better performance than the baseline architectures on multiple datasets applied in the image classification task of the <b>vision</b> <b>domain.</b></p></p class="citation"></blockquote><h3 id=962--133309-self-supervised-facial-representation-learning-with-facial-region-awareness-zheng-gao-et-al-2024>(9/62 | 133/309) Self-Supervised Facial Representation Learning with Facial Region Awareness (Zheng Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Gao, Ioannis Patras. (2024)<br><strong>Self-Supervised Facial Representation Learning with Facial Region Awareness</strong><br><button class=copy-to-clipboard title="Self-Supervised Facial Representation Learning with Facial Region Awareness" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 48<br>Keywords: Clustering, Representation Learning, Self-supervised Learning, Self-supervised Pre-training, Supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02138v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02138v1.pdf filename=2403.02138v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>pre-training</b> has been proved to be effective in learning transferable <b>representations</b> <b>that</b> benefit various visual tasks. This paper asks this question: can <b>self-supervised</b> <b>pre-training</b> learn general facial <b>representations</b> <b>for</b> various facial analysis tasks? Recent efforts toward this goal are limited to treating each face image as a whole, i.e., learning consistent facial <b>representations</b> <b>at</b> the image-level, which overlooks the consistency of local facial <b>representations</b> <b>(i.e.,</b> facial regions like eyes, nose, etc). In this work, we make a first attempt to propose a novel <b>self-supervised</b> <b>facial</b> <b>representation</b> <b>learning</b> framework to learn consistent global and local facial <b>representations,</b> <b>Facial</b> Region Awareness (FRA). Specifically, we explicitly enforce the consistency of facial regions by matching the local facial <b>representations</b> <b>across</b> views, which are extracted with learned heatmaps highlighting the facial regions. Inspired by the mask prediction in <b>supervised</b> semantic segmentation, we obtain the heatmaps via cosine similarity between the per-pixel projection of feature maps and facial mask embeddings computed from learnable positional embeddings, which leverage the attention mechanism to globally look up the facial image for facial regions. To learn such heatmaps, we formulate the learning of facial mask embeddings as a deep <b>clustering</b> problem by assigning the pixel features from the feature maps to them. The <b>transfer</b> <b>learning</b> results on facial classification and regression tasks show that our FRA outperforms previous pre-trained models and more importantly, using ResNet as the unified backbone for various tasks, our FRA achieves comparable or even better performance compared with SOTA methods in facial analysis tasks.</p></p class="citation"></blockquote><h3 id=1062--134309-vision-language-models-for-medical-report-generation-and-visual-question-answering-a-review-iryna-hartsock-et-al-2024>(10/62 | 134/309) Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review (Iryna Hartsock et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Iryna Hartsock, Ghulam Rasool. (2024)<br><strong>Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review</strong><br><button class=copy-to-clipboard title="Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, Question Answering, Visual Question Answering, Summarization, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02469v1.pdf filename=2403.02469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical <b>vision-language</b> models (VLMs) combine computer vision and natural language processing to analyze <b>visual</b> <b>and</b> <b>textual</b> medical data. Our paper reviews recent advancements in developing VLMs specialized for healthcare, focusing on models designed for medical report generation and <b>visual</b> <b>question</b> <b>answering.</b> We provide background on natural language processing and computer vision, explaining how techniques from both fields are integrated into VLMs to enable learning from <b>multimodal</b> data. Key areas we address include the exploration of medical <b>vision-language</b> datasets, in-depth analyses of architectures and pre-training strategies employed in recent noteworthy medical VLMs, and comprehensive discussion on evaluation metrics for assessing VLMs&rsquo; performance in medical report generation and <b>visual</b> <b>question</b> <b>answering.</b> We also highlight current challenges and propose future directions, including enhancing clinical validity and addressing patient privacy concerns. Overall, our review <b>summarizes</b> recent progress in developing VLMs to harness <b>multimodal</b> medical data for improved healthcare applications.</p></p class="citation"></blockquote><h3 id=1162--135309-superpixel-graph-contrastive-clustering-with-semantic-invariant-augmentations-for-hyperspectral-images-jianhan-qi-et-al-2024>(11/62 | 135/309) Superpixel Graph Contrastive Clustering with Semantic-Invariant Augmentations for Hyperspectral Images (Jianhan Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhan Qi, Yuheng Jia, Hui Liu, Junhui Hou. (2024)<br><strong>Superpixel Graph Contrastive Clustering with Semantic-Invariant Augmentations for Hyperspectral Images</strong><br><button class=copy-to-clipboard title="Superpixel Graph Contrastive Clustering with Semantic-Invariant Augmentations for Hyperspectral Images" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Graph, Clustering, Contrastive Learning, Convolution, Convolutional Neural Network, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01799v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01799v1.pdf filename=2403.01799v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyperspectral images (HSI) <b>clustering</b> is an important but challenging task. The state-of-the-art (SOTA) methods usually rely on superpixels, however, they do not fully utilize the spatial and spectral information in HSI 3-D structure, and their optimization targets are not <b>clustering-oriented.</b> In this work, we first use 3-D and 2-D hybrid <b>convolutional</b> <b>neural</b> <b>networks</b> to extract the high-order spatial and spectral features of HSI through pre-training, and then design a superpixel <b>graph</b> <b>contrastive</b> <b>clustering</b> (SPGCC) model to learn discriminative superpixel representations. Reasonable augmented views are crucial for <b>contrastive</b> <b>clustering,</b> and conventional <b>contrastive</b> <b>learning</b> may hurt the cluster structure since different samples are pushed away in the embedding space even if they belong to the same class. In SPGCC, we design two semantic-invariant <b>data</b> <b>augmentations</b> for HSI superpixels: pixel sampling augmentation and model weight augmentation. Then sample-level alignment and <b>clustering-center-level</b> contrast are performed for better intra-class similarity and inter-class dissimilarity of superpixel embeddings. We perform <b>clustering</b> and network optimization alternatively. Experimental results on several HSI datasets verify the advantages of the proposed method, e.g., on India Pines, our model improves the <b>clustering</b> accuracy from 58.79% to 67.59% compared to the SOTA method.</p></p class="citation"></blockquote><h3 id=1262--136309-explicit-motion-handling-and-interactive-prompting-for-video-camouflaged-object-detection-xin-zhang-et-al-2024>(12/62 | 136/309) Explicit Motion Handling and Interactive Prompting for Video Camouflaged Object Detection (Xin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Zhang, Tao Xiao, Gepeng Ji, Xuan Wu, Keren Fu, Qijun Zhao. (2024)<br><strong>Explicit Motion Handling and Interactive Prompting for Video Camouflaged Object Detection</strong><br><button class=copy-to-clipboard title="Explicit Motion Handling and Interactive Prompting for Video Camouflaged Object Detection" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Object Detection, Benchmarking, Self-supervised Learning, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01968v1.pdf filename=2403.01968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Camouflage poses challenges in distinguishing a static target, whereas any movement of the target can break this disguise. Existing video camouflaged <b>object</b> <b>detection</b> (VCOD) approaches take noisy motion estimation as input or model motion implicitly, restricting detection performance in complex dynamic scenes. In this paper, we propose a novel Explicit Motion handling and Interactive <b>Prompting</b> <b>framework</b> for VCOD, dubbed EMIP, which handles motion cues explicitly using a frozen pre-trained optical flow fundamental model. EMIP is characterized by a two-stream architecture for simultaneously conducting camouflaged segmentation and optical flow estimation. Interactions across the dual streams are realized in an interactive <b>prompting</b> <b>way</b> that is inspired by emerging visual <b>prompt</b> <b>learning.</b> Two learnable modules, i.e. the camouflaged feeder and motion collector, are designed to incorporate segmentation-to-motion and motion-to-segmentation <b>prompts,</b> <b>respectively,</b> and enhance outputs of the both streams. The <b>prompt</b> <b>fed</b> to the motion stream is learned by supervising optical flow in a <b>self-supervised</b> manner. Furthermore, we show that long-term historical information can also be incorporated as a <b>prompt</b> <b>into</b> EMIP and achieve more robust results with temporal consistency. Experimental results demonstrate that our EMIP achieves new state-of-the-art records on popular VCOD <b>benchmarks.</b> The code will be publicly available.</p></p class="citation"></blockquote><h3 id=1362--137309-xt-nested-tokenization-for-larger-context-in-large-images-ritwik-gupta-et-al-2024>(13/62 | 137/309) xT: Nested Tokenization for Larger Context in Large Images (Ritwik Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ritwik Gupta, Shufan Li, Tyler Zhu, Jitendra Malik, Trevor Darrell, Karttikeya Mangalam. (2024)<br><strong>xT: Nested Tokenization for Larger Context in Large Images</strong><br><button class=copy-to-clipboard title="xT: Nested Tokenization for Larger Context in Large Images" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Vision Transformer, Benchmarking, Transformer, Tokenization, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01915v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01915v1.pdf filename=2403.01915v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern computer <b>vision</b> <b>pipelines</b> handle large images in one of two sub-optimal ways: down-sampling or cropping. These two methods incur significant losses in the amount of information and context present in an image. There are many downstream applications in which global context matters as much as high frequency details, such as in real-world satellite imagery; in such cases researchers have to make the uncomfortable choice of which information to discard. We introduce xT, a simple framework for <b>vision</b> <b>transformers</b> which effectively aggregates global context with local details and can model large images end-to-end on contemporary GPUs. We select a set of <b>benchmark</b> datasets across classic <b>vision</b> <b>tasks</b> which accurately reflect a <b>vision</b> <b>model&rsquo;s</b> ability to understand truly large images and incorporate fine details over large scales and assess our method&rsquo;s improvement on them. By introducing a nested <b>tokenization</b> scheme for large images in conjunction with long-sequence length models normally used for natural language processing, we are able to increase accuracy by up to 8.6% on challenging classification tasks and $F_1$ score by 11.6 on context-dependent segmentation in large images.</p></p class="citation"></blockquote><h3 id=1462--138309-one-prompt-word-is-enough-to-boost-adversarial-robustness-for-pre-trained-vision-language-models-lin-li-et-al-2024>(14/62 | 138/309) One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models (Lin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Li, Haoyan Guan, Jianing Qiu, Michael Spratling. (2024)<br><strong>One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models</strong><br><button class=copy-to-clipboard title="One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, Prompt, Vision-and-Language, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01849v1.pdf filename=2403.01849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large pre-trained <b>Vision-Language</b> Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to <b>adversarial</b> <b>examples.</b> This work studies the <b>adversarial</b> <b>robustness</b> of VLMs from the novel perspective of the text <b>prompt</b> instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both <b>adversarial</b> <b>attack</b> and defense are sensitive to the used text <b>prompt.</b> Inspired by this, we propose a method to improve resilience to <b>adversarial</b> <b>attacks</b> by learning a robust text <b>prompt</b> for VLMs. The proposed method, named <b>Adversarial</b> <b>Prompt</b> Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT&rsquo;s superiority over hand-engineered <b>prompts</b> and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input <b>distribution</b> <b>shift</b> and across datasets. Surprisingly, by simply adding one learned word to the <b>prompts,</b> APT can significantly boost the accuracy and robustness (epsilon=4/255) over the hand-engineered <b>prompts</b> by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at <a href=https://github.com/TreeLLi/APT>https://github.com/TreeLLi/APT</a>.</p></p class="citation"></blockquote><h3 id=1562--139309-tnf-tri-branch-neural-fusion-for-multimodal-medical-data-classification-tong-zheng-et-al-2024>(15/62 | 139/309) TNF: Tri-branch Neural Fusion for Multimodal Medical Data Classification (Tong Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tong Zheng, Shusaku Sone, Yoshitaka Ushiku, Yuki Oba, Jiaxin Ma. (2024)<br><strong>TNF: Tri-branch Neural Fusion for Multimodal Medical Data Classification</strong><br><button class=copy-to-clipboard title="TNF: Tri-branch Neural Fusion for Multimodal Medical Data Classification" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Convolution, Convolutional Neural Network, Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01802v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01802v2.pdf filename=2403.01802v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a Tri-branch Neural Fusion (TNF) approach designed for classifying <b>multimodal</b> medical images and tabular data. It also introduces two solutions to address the challenge of label inconsistency in <b>multimodal</b> classification. Traditional methods in multi-modality medical data classification often rely on single-label approaches, typically merging features from two distinct input modalities. This becomes problematic when features are mutually exclusive or labels differ across modalities, leading to reduced accuracy. To overcome this, our TNF approach implements a tri-branch framework that manages three separate outputs: one for image modality, another for tabular modality, and a third hybrid output that fuses both image and tabular data. The final decision is made through an ensemble method that integrates likelihoods from all three branches. We validate the effectiveness of TNF through extensive experiments, which illustrate its superiority over traditional fusion and ensemble methods in various <b>convolutional</b> <b>neural</b> <b>networks</b> and <b>transformer-based</b> architectures across multiple datasets.</p></p class="citation"></blockquote><h3 id=1662--140309-when-do-convolutional-neural-networks-stop-learning-sahan-ahmad-et-al-2024>(16/62 | 140/309) When do Convolutional Neural Networks Stop Learning? (Sahan Ahmad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sahan Ahmad, Gabriel Trahan, Aminul Islam. (2024)<br><strong>When do Convolutional Neural Networks Stop Learning?</strong><br><button class=copy-to-clipboard title="When do Convolutional Neural Networks Stop Learning?" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02473v1.pdf filename=2403.02473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> have demonstrated outstanding performance in computer vision tasks such as image classification, detection, segmentation, and medical image analysis. In general, an arbitrary number of epochs is used to train such neural networks. In a single epoch, the entire training data &ndash; divided by batch size &ndash; are fed to the network. In practice, validation error with training loss is used to estimate the neural network&rsquo;s generalization, which indicates the optimal learning capacity of the network. Current practice is to stop training when the training loss decreases and the gap between training and validation error increases (i.e., the generalization gap) to avoid overfitting. However, this is a trial-and-error-based approach which raises a critical question: Is it possible to estimate when neural networks stop learning based on training data? This research work introduces a hypothesis that analyzes the data variation across all the layers of a <b>CNN</b> variant to anticipate its near-optimal learning capacity. In the training phase, we use our hypothesis to anticipate the near-optimal learning capacity of a <b>CNN</b> variant without using any validation data. Our hypothesis can be deployed as a plug-and-play to any existing <b>CNN</b> variant without introducing additional trainable parameters to the network. We test our hypothesis on six different <b>CNN</b> variants and three different general image datasets (CIFAR10, CIFAR100, and SVHN). The result based on these <b>CNN</b> variants and datasets shows that our hypothesis saves 58.49% of computational time (on average) in training. We further conduct our hypothesis on ten medical image datasets and compared with the MedMNIST-V2 <b>benchmark.</b> Based on our experimental result, we save $\approx$ 44.1% of computational time without losing accuracy against the MedMNIST-V2 <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=1762--141309-enhancing-information-maximization-with-distance-aware-contrastive-learning-for-source-free-cross-domain-few-shot-learning-huali-xu-et-al-2024>(17/62 | 141/309) Enhancing Information Maximization with Distance-Aware Contrastive Learning for Source-Free Cross-Domain Few-Shot Learning (Huali Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huali Xu, Li Liu, Shuaifeng Zhi, Shaojing Fu, Zhuo Su, Ming-Ming Cheng, Yongxiang Liu. (2024)<br><strong>Enhancing Information Maximization with Distance-Aware Contrastive Learning for Source-Free Cross-Domain Few-Shot Learning</strong><br><button class=copy-to-clipboard title="Enhancing Information Maximization with Distance-Aware Contrastive Learning for Source-Free Cross-Domain Few-Shot Learning" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Contrastive Learning, Few-shot, Few-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01966v1.pdf filename=2403.01966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing Cross-Domain <b>Few-Shot</b> <b>Learning</b> (CDFSL) methods require access to source domain data to train a model in the pre-training phase. However, due to increasing concerns about data privacy and the desire to reduce data transmission and training costs, it is necessary to develop a CDFSL solution without accessing source data. For this reason, this paper explores a Source-Free CDFSL (SF-CDFSL) problem, in which CDFSL is addressed through the use of existing pretrained models instead of training a model with source data, avoiding accessing source data. This paper proposes an Enhanced Information Maximization with Distance-Aware <b>Contrastive</b> <b>Learning</b> (IM-DCL) method to address these challenges. Firstly, we introduce the transductive mechanism for learning the query set. Secondly, information maximization (IM) is explored to map target samples into both individual certainty and global diversity predictions, helping the source model better fit the target data distribution. However, IM fails to learn the decision boundary of the target task. This motivates us to introduce a novel approach called Distance-Aware <b>Contrastive</b> <b>Learning</b> (DCL), in which we consider the entire feature set as both positive and negative sets, akin to Schrodinger&rsquo;s concept of a dual state. Instead of a rigid separation between positive and negative sets, we employ a weighted distance calculation among features to establish a soft classification of the positive and negative sets for the entire feature set. Furthermore, we address issues related to IM by incorporating <b>contrastive</b> <b>constraints</b> between object features and their corresponding positive and negative sets. Evaluations of the 4 datasets in the BSCD-FSL <b>benchmark</b> indicate that the proposed IM-DCL, without accessing the source domain, demonstrates superiority over existing methods, especially in the distant domain task.</p></p class="citation"></blockquote><h3 id=1862--142309-freea-human-object-interaction-detection-using-free-annotation-labels-yuxiao-wang-et-al-2024>(18/62 | 142/309) FreeA: Human-object Interaction Detection using Free Annotation Labels (Yuxiao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxiao Wang, Zhenao Wei, Xinyu Jiang, Yu Lei, Weiying Xue, Jinxiu Liu, Qi Liu. (2024)<br><strong>FreeA: Human-object Interaction Detection using Free Annotation Labels</strong><br><button class=copy-to-clipboard title="FreeA: Human-object Interaction Detection using Free Annotation Labels" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Self-adaption, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01840v1.pdf filename=2403.01840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent human-object interaction (HOI) detection approaches rely on high cost of manpower and require comprehensive annotated image datasets. In this paper, we propose a novel <b>self-adaption</b> language-driven HOI detection method, termed as FreeA, without labeling by leveraging the adaptability of CLIP to generate latent HOI labels. To be specific, FreeA matches image features of human-object pairs with HOI text templates, and a priori knowledge-based mask method is developed to suppress improbable interactions. In addition, FreeA utilizes the proposed interaction correlation matching method to enhance the likelihood of actions related to a specified action, further refine the generated HOI labels. Experiments on two <b>benchmark</b> datasets show that FreeA achieves state-of-the-art performance among weakly <b>supervised</b> HOI models. Our approach is +8.58 mean Average Precision (mAP) on HICO-DET and +1.23 mAP on V-COCO more accurate in localizing and classifying the interactive actions than the newest weakly model, and +1.68 mAP and +7.28 mAP than the latest weakly+ model, respectively. Code will be available at <a href=https://drliuqi.github.io/>https://drliuqi.github.io/</a>.</p></p class="citation"></blockquote><h3 id=1962--143309-unictrl-improving-the-spatiotemporal-consistency-of-text-to-video-diffusion-models-via-training-free-unified-attention-control-xuweiyi-chen-et-al-2024>(19/62 | 143/309) UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control (Xuweiyi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuweiyi Chen, Tian Xia, Sihan Xu. (2024)<br><strong>UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control</strong><br><button class=copy-to-clipboard title="UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02332v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02332v3.pdf filename=2403.02332v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video <b>Diffusion</b> <b>Models</b> have been developed for video generation, usually integrating text and image conditioning to enhance control over the generated content. Despite the progress, ensuring consistency across frames remains a challenge, particularly when using text <b>prompts</b> as control conditions. To address this problem, we introduce UniCtrl, a novel, plug-and-play method that is universally applicable to improve the spatiotemporal consistency and motion diversity of videos generated by text-to-video models without additional training. UniCtrl ensures semantic consistency across different frames through cross-frame <b>self-attention</b> control, and meanwhile, enhances the motion quality and spatiotemporal consistency through motion injection and spatiotemporal synchronization. Our experimental results demonstrate UniCtrl&rsquo;s efficacy in enhancing various text-to-video models, confirming its effectiveness and universality.</p></p class="citation"></blockquote><h3 id=2062--144309-vision-rwkv-efficient-and-scalable-visual-perception-with-rwkv-like-architectures-yuchen-duan-et-al-2024>(20/62 | 144/309) Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures (Yuchen Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Hongsheng Li, Jifeng Dai, Wenhai Wang. (2024)<br><strong>Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures</strong><br><button class=copy-to-clipboard title="Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02308v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02308v2.pdf filename=2403.02308v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have revolutionized computer <b>vision</b> <b>and</b> natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces <b>Vision-RWKV</b> <b>(VRWKV),</b> a model adapted from the RWKV model used in the NLP field with necessary modifications for <b>vision</b> <b>tasks.</b> Similar to the <b>Vision</b> <b>Transformer</b> (ViT), our model is designed to efficiently handle sparse inputs and demonstrate robust global processing capabilities, while also scaling up effectively, accommodating both large-scale parameters and extensive datasets. Its distinctive advantage lies in its reduced spatial aggregation complexity, which renders it exceptionally adept at processing high-resolution images seamlessly, eliminating the necessity for windowing operations. Our evaluations demonstrate that VRWKV surpasses ViT&rsquo;s performance in image classification and has significantly faster speeds and lower memory usage processing high-resolution inputs. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV&rsquo;s potential as a more efficient alternative for visual perception tasks. Code is released at \url{https://github.com/OpenGVLab/Vision-RWKV}.</p></p class="citation"></blockquote><h3 id=2162--145309-3dtopia-large-text-to-3d-generation-model-with-hybrid-diffusion-priors-fangzhou-hong-et-al-2024>(21/62 | 145/309) 3DTopia: Large Text-to-3D Generation Model with Hybrid Diffusion Priors (Fangzhou Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi Chen, Tengfei Wang, Liang Pan, Dahua Lin, Ziwei Liu. (2024)<br><strong>3DTopia: Large Text-to-3D Generation Model with Hybrid Diffusion Priors</strong><br><button class=copy-to-clipboard title="3DTopia: Large Text-to-3D Generation Model with Hybrid Diffusion Priors" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02234v1.pdf filename=2403.02234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a two-stage text-to-3D generation system, namely 3DTopia, which generates high-quality general 3D assets within 5 minutes using hybrid <b>diffusion</b> <b>priors.</b> The first stage samples from a 3D <b>diffusion</b> <b>prior</b> directly learned from 3D data. Specifically, it is powered by a text-conditioned tri-plane latent <b>diffusion</b> <b>model,</b> which quickly generates coarse 3D samples for fast prototyping. The second stage utilizes 2D <b>diffusion</b> <b>priors</b> to further refine the texture of coarse 3D models from the first stage. The refinement consists of both latent and pixel space optimization for high-quality texture generation. To facilitate the training of the proposed system, we clean and caption the largest open-source 3D dataset, Objaverse, by combining the power of vision language models and <b>large</b> <b>language</b> <b>models.</b> Experiment results are reported qualitatively and quantitatively to show the performance of the proposed system. Our codes and models are available at <a href=https://github.com/3DTopia/3DTopia>https://github.com/3DTopia/3DTopia</a></p></p class="citation"></blockquote><h3 id=2262--146309-mim-istd-mamba-in-mamba-for-efficient-infrared-small-target-detection-tianxiang-chen-et-al-2024>(22/62 | 146/309) MiM-ISTD: Mamba-in-Mamba for Efficient Infrared Small Target Detection (Tianxiang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianxiang Chen, Zhentao Tan, Tao Gong, Qi Chu, Yue Wu, Bin Liu, Jieping Ye, Nenghai Yu. (2024)<br><strong>MiM-ISTD: Mamba-in-Mamba for Efficient Infrared Small Target Detection</strong><br><button class=copy-to-clipboard title="MiM-ISTD: Mamba-in-Mamba for Efficient Infrared Small Target Detection" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02148v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02148v2.pdf filename=2403.02148v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, infrared small target detection (ISTD) has made significant progress, thanks to the development of basic models. Specifically, the structures combining <b>convolutional</b> <b>networks</b> with <b>transformers</b> can successfully extract both local and global features. However, the disadvantage of the <b>transformer</b> is also inherited, i.e., the quadratic computational complexity to the length of the sequence. Inspired by the recent basic model with linear complexity for long-distance modeling, called Mamba, we explore the potential of this state space model for ISTD task in terms of effectiveness and efficiency in the paper. However, directly applying Mamba achieves poor performance since local features, which are critical to detecting small targets, cannot be fully exploited. Instead, we tailor a Mamba-in-Mamba (MiM-ISTD) structure for efficient ISTD. Specifically, we treat the local patches as &ldquo;visual sentences&rdquo; and use the Outer Mamba to explore the global information. We then decompose each visual sentence into sub-patches as &ldquo;visual words&rdquo; and use the Inner Mamba to further explore the local information among words in the visual sentence with negligible computational costs. By aggregating the word and sentence features, the MiM-ISTD can effectively explore both global and local information. Experiments on NUAA-SIRST and IRSTD-1k show the superior accuracy and efficiency of our method. Specifically, MiM-ISTD is $10 \times$ faster than the SOTA method and reduces GPU memory usage by 73.4$%$ when testing on $2048 \times 2048$ image, overcoming the computation and memory constraints on high-resolution infrared images. Source code is available at <a href=https://github.com/txchen-USTC/MiM-ISTD>https://github.com/txchen-USTC/MiM-ISTD</a>.</p></p class="citation"></blockquote><h3 id=2362--147309-ub-finenet-urban-building-fine-grained-classification-network-for-open-access-satellite-images-zhiyi-he-et-al-2024>(23/62 | 147/309) UB-FineNet: Urban Building Fine-grained Classification Network for Open-access Satellite Images (Zhiyi He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyi He, Wei Yao, Jie Shao, Puzuo Wang. (2024)<br><strong>UB-FineNet: Urban Building Fine-grained Classification Network for Open-access Satellite Images</strong><br><button class=copy-to-clipboard title="UB-FineNet: Urban Building Fine-grained Classification Network for Open-access Satellite Images" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02132v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02132v1.pdf filename=2403.02132v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fine classification of city-scale buildings from satellite remote sensing imagery is a crucial research area with significant implications for urban planning, infrastructure development, and population distribution analysis. However, the task faces big challenges due to low-resolution overhead images acquired from high altitude space-borne platforms and the long-tail sample distribution of fine-grained urban building categories, leading to severe class imbalance problem. To address these issues, we propose a deep network approach to fine-grained classification of urban buildings using open-access satellite images. A Denoising Diffusion <b>Probabilistic</b> <b>Model</b> (DDPM) based super-resolution method is first introduced to enhance the spatial resolution of satellite images, which benefits from domain-adaptive <b>knowledge</b> <b>distillation.</b> Then, a new fine-grained classification network with Category Information Balancing Module (CIBM) and Contrastive Supervision (CS) technique is proposed to mitigate the problem of class imbalance and improve the classification robustness and accuracy. Experiments on Hong Kong data set with 11 fine building types revealed promising classification results with a mean Top-1 accuracy of 60.45%, which is on par with street-view image based approaches. Extensive ablation study shows that CIBM and CS improve Top-1 accuracy by 2.6% and 3.5% compared to the baseline method, respectively. And both modules can be easily inserted into other classification networks and similar enhancements have been achieved. Our research contributes to the field of urban analysis by providing a practical solution for fine classification of buildings in challenging mega city scenarios solely using open-access satellite images. The proposed method can serve as a valuable tool for urban planners, aiding in the understanding of economic, industrial, and population distribution.</p></p class="citation"></blockquote><h3 id=2462--148309-resadapter-domain-consistent-resolution-adapter-for-diffusion-models-jiaxiang-cheng-et-al-2024>(24/62 | 148/309) ResAdapter: Domain Consistent Resolution Adapter for Diffusion Models (Jiaxiang Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxiang Cheng, Pan Xie, Xin Xia, Jiashi Li, Jie Wu, Yuxi Ren, Huixia Li, Xuefeng Xiao, Min Zheng, Lean Fu. (2024)<br><strong>ResAdapter: Domain Consistent Resolution Adapter for Diffusion Models</strong><br><button class=copy-to-clipboard title="ResAdapter: Domain Consistent Resolution Adapter for Diffusion Models" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: ControlNet, Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02084v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02084v1.pdf filename=2403.02084v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancement in <b>text-to-image</b> models (e.g., Stable <b>Diffusion)</b> <b>and</b> corresponding personalized technologies (e.g., DreamBooth and LoRA) enables individuals to generate high-quality and imaginative images. However, they often suffer from limitations when generating images with resolutions outside of their trained domain. To overcome this limitation, we present the Resolution Adapter (ResAdapter), a domain-consistent adapter designed for <b>diffusion</b> <b>models</b> to generate images with unrestricted resolutions and aspect ratios. Unlike other multi-resolution generation methods that process images of static resolution with complex post-process operations, ResAdapter directly generates images with the dynamical resolution. Especially, after learning a deep understanding of pure resolution priors, ResAdapter trained on the general dataset, generates resolution-free images with personalized <b>diffusion</b> <b>models</b> while preserving their original style domain. Comprehensive experiments demonstrate that ResAdapter with only 0.5M can process images with flexible resolutions for arbitrary <b>diffusion</b> <b>models.</b> More extended experiments demonstrate that ResAdapter is compatible with other modules (e.g., <b>ControlNet,</b> IP-Adapter and LCM-LoRA) for image generation across a broad range of resolutions, and can be integrated into other multi-resolution model (e.g., ElasticDiffusion) for efficiently generating higher-resolution images. Project link is <a href=https://res-adapter.github.io>https://res-adapter.github.io</a></p></p class="citation"></blockquote><h3 id=2562--149309-cse-surface-anomaly-detection-with-contrastively-selected-embedding-simon-thomine-et-al-2024>(25/62 | 149/309) CSE: Surface Anomaly Detection with Contrastively Selected Embedding (Simon Thomine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Thomine, Hichem Snoussi. (2024)<br><strong>CSE: Surface Anomaly Detection with Contrastively Selected Embedding</strong><br><button class=copy-to-clipboard title="CSE: Surface Anomaly Detection with Contrastively Selected Embedding" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Anomaly Detection, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01859v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01859v1.pdf filename=2403.01859v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting surface anomalies of industrial materials poses a significant challenge within a myriad of industrial manufacturing processes. In recent times, various methodologies have emerged, capitalizing on the advantages of employing a network pre-trained on natural images for the extraction of representative features. Subsequently, these features are subjected to processing through a diverse range of techniques including memory banks, normalizing flow, and <b>knowledge</b> <b>distillation,</b> which have exhibited exceptional accuracy. This paper revisits approaches based on pre-trained features by introducing a novel method centered on target-specific embedding. To capture the most representative features of the texture under consideration, we employ a variant of a contrastive training procedure that incorporates both artificially generated defective samples and <b>anomaly-free</b> <b>samples</b> during training. Exploiting the intrinsic properties of surfaces, we derived a meaningful representation from the defect-free samples during training, facilitating a straightforward yet effective calculation of <b>anomaly</b> <b>scores.</b> The experiments conducted on the MVTEC AD and TILDA datasets demonstrate the competitiveness of our approach compared to state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2662--150309-viewdiff-3d-consistent-image-generation-with-text-to-image-models-lukas-höllein-et-al-2024>(26/62 | 150/309) ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models (Lukas Höllein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas Höllein, Aljaž Božič, Norman Müller, David Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollhöfer, Matthias Nießner. (2024)<br><strong>ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models</strong><br><button class=copy-to-clipboard title="ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01807v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01807v1.pdf filename=2403.01807v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D asset generation is getting massive amounts of attention, inspired by the recent success of text-guided 2D content creation. Existing text-to-3D methods use pretrained <b>text-to-image</b> <b>diffusion</b> <b>models</b> in an optimization problem or <b>fine-tune</b> them on synthetic data, which often results in non-photorealistic 3D objects without backgrounds. In this paper, we present a method that leverages pretrained <b>text-to-image</b> models as a prior, and learn to generate multi-view images in a single denoising process from real-world data. Concretely, we propose to integrate 3D volume-rendering and cross-frame-attention layers into each block of the existing U-Net network of the <b>text-to-image</b> model. Moreover, we design an autoregressive generation that renders more 3D-consistent images at any viewpoint. We train our model on real-world datasets of objects and showcase its capabilities to generate instances with a variety of high-quality shapes and textures in authentic surroundings. Compared to the existing methods, the results generated by our method are consistent, and have favorable visual quality (-30% FID, -37% KID).</p></p class="citation"></blockquote><h3 id=2762--151309-handiffuser-text-to-image-generation-with-realistic-hand-appearances-supreeth-narasimhaswamy-et-al-2024>(27/62 | 151/309) HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances (Supreeth Narasimhaswamy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Supreeth Narasimhaswamy, Uttaran Bhattacharya, Xiang Chen, Ishita Dasgupta, Saayan Mitra, Minh Hoai. (2024)<br><strong>HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances</strong><br><button class=copy-to-clipboard title="HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01693v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01693v1.pdf filename=2403.01693v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> generative models can generate high-quality humans, but realism is lost when generating hands. Common artifacts include irregular hand poses, shapes, incorrect numbers of fingers, and physically implausible finger orientations. To generate images with realistic hands, we propose a novel <b>diffusion-based</b> <b>architecture</b> called HanDiffuser that achieves realism by injecting hand embeddings in the generative process. HanDiffuser consists of two components: a Text-to-Hand-Params <b>diffusion</b> <b>model</b> to generate SMPL-Body and MANO-Hand parameters from input text <b>prompts,</b> and a Text-Guided Hand-Params-to-Image <b>diffusion</b> <b>model</b> to synthesize images by conditioning on the <b>prompts</b> and hand parameters generated by the previous component. We incorporate multiple aspects of hand representation, including 3D shapes and joint-level finger positions, orientations and articulations, for robust learning and reliable performance during inference. We conduct extensive quantitative and qualitative experiments and perform user studies to demonstrate the efficacy of our method in generating images with high-quality hands.</p></p class="citation"></blockquote><h3 id=2862--152309-zero-shot-generalizable-incremental-learning-for-vision-language-object-detection-jieren-deng-et-al-2024>(28/62 | 152/309) Zero-shot Generalizable Incremental Learning for Vision-Language Object Detection (Jieren Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jieren Deng, Haojian Zhang, Kun Ding, Jianhua Hu, Xingxuan Zhang, Yunkuan Wang. (2024)<br><strong>Zero-shot Generalizable Incremental Learning for Vision-Language Object Detection</strong><br><button class=copy-to-clipboard title="Zero-shot Generalizable Incremental Learning for Vision-Language Object Detection" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01680v1.pdf filename=2403.01680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents Incremental <b>Vision-Language</b> <b>Object</b> <b>Detection</b> (IVLOD), a novel learning task designed to incrementally adapt pre-trained <b>Vision-Language</b> <b>Object</b> <b>Detection</b> Models (VLODMs) to various specialized domains, while simultaneously preserving their <b>zero-shot</b> generalization capabilities for the generalized domain. To address this new challenge, we present the Zero-interference Reparameterizable Adaptation (ZiRa), a novel method that introduces Zero-interference Loss and reparameterization techniques to tackle IVLOD without incurring additional inference costs or a significant increase in memory usage. Comprehensive experiments on COCO and ODinW-13 datasets demonstrate that ZiRa effectively safeguards the <b>zero-shot</b> generalization ability of VLODMs while continuously adapting to new tasks. Specifically, after training on ODinW-13 datasets, ZiRa exhibits superior performance compared to CL-DETR and iDETR, boosting <b>zero-shot</b> generalizability by substantial 13.91 and 8.71 AP, respectively.</p></p class="citation"></blockquote><h3 id=2962--153309-modeling-multimodal-social-interactions-new-challenges-and-baselines-with-densely-aligned-representations-sangmin-lee-et-al-2024>(29/62 | 153/309) Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations (Sangmin Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sangmin Lee, Bolin Lai, Fiona Ryan, Bikram Boote, James M. Rehg. (2024)<br><strong>Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations</strong><br><button class=copy-to-clipboard title="Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 29<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Coreference Resolution, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02090v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02090v1.pdf filename=2403.02090v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding social interactions involving both verbal and non-verbal cues is essential to effectively interpret social situations. However, most prior works on <b>multimodal</b> social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not densely aligned to utterances in multi-party environments. They are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun <b>coreference</b> <b>resolution,</b> and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel <b>multimodal</b> baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates concurrently capturing verbal and non-verbal cues pertinent to social <b>reasoning.</b> Experiments demonstrate the effectiveness of the proposed approach with densely aligned <b>multimodal</b> representations in modeling social interactions. We will release our <b>benchmarks</b> and source code to facilitate further research.</p></p class="citation"></blockquote><h3 id=3062--154309-scalable-vision-based-3d-object-detection-and-monocular-depth-estimation-for-autonomous-driving-yuxuan-liu-2024>(30/62 | 154/309) Scalable Vision-Based 3D Object Detection and Monocular Depth Estimation for Autonomous Driving (Yuxuan Liu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Liu. (2024)<br><strong>Scalable Vision-Based 3D Object Detection and Monocular Depth Estimation for Autonomous Driving</strong><br><button class=copy-to-clipboard title="Scalable Vision-Based 3D Object Detection and Monocular Depth Estimation for Autonomous Driving" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02037v1.pdf filename=2403.02037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This dissertation is a multifaceted contribution to the advancement of vision-based 3D perception technologies. In the first segment, the thesis introduces structural enhancements to both monocular and stereo 3D <b>object</b> <b>detection</b> algorithms. By integrating ground-referenced geometric priors into monocular detection models, this research achieves unparalleled accuracy in <b>benchmark</b> evaluations for monocular 3D detection. Concurrently, the work refines stereo 3D detection paradigms by incorporating insights and inferential structures gleaned from monocular networks, thereby augmenting the operational efficiency of stereo detection systems. The second segment is devoted to data-driven strategies and their real-world applications in 3D vision detection. A novel training regimen is introduced that amalgamates datasets annotated with either 2D or 3D labels. This approach not only augments the detection models through the utilization of a substantially expanded dataset but also facilitates economical model deployment in real-world scenarios where only 2D annotations are readily available. Lastly, the dissertation presents an innovative pipeline tailored for <b>unsupervised</b> depth estimation in autonomous driving contexts. Extensive empirical analyses affirm the robustness and efficacy of this newly proposed pipeline. Collectively, these contributions lay a robust foundation for the widespread adoption of vision-based 3D perception technologies in autonomous driving applications.</p></p class="citation"></blockquote><h3 id=3162--155309-differentially-private-representation-learning-via-image-captioning-tom-sander-et-al-2024>(31/62 | 155/309) Differentially Private Representation Learning via Image Captioning (Tom Sander et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tom Sander, Yaodong Yu, Maziar Sanjabi, Alain Durmus, Yi Ma, Kamalika Chaudhuri, Chuan Guo. (2024)<br><strong>Differentially Private Representation Learning via Image Captioning</strong><br><button class=copy-to-clipboard title="Differentially Private Representation Learning via Image Captioning" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 21<br>Keywords: Multi-modal, Multi-modal, Representation Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02506v1.pdf filename=2403.02506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Differentially private (DP) machine learning is considered the gold-standard solution for training a model from sensitive data while still preserving privacy. However, a major barrier to achieving this ideal is its sub-optimal privacy-accuracy trade-off, which is particularly visible in DP <b>representation</b> <b>learning.</b> Specifically, it has been shown that under modest privacy budgets, most models learn <b>representations</b> <b>that</b> are not significantly better than hand-crafted features. In this work, we show that effective DP <b>representation</b> <b>learning</b> can be done via image captioning and scaling up to internet-scale <b>multimodal</b> datasets. Through a series of engineering tricks, we successfully train a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratch using a reasonable amount of computation, and obtaining unprecedented high-quality image features that can be used in a variety of downstream vision and <b>vision-language</b> tasks. For example, under a privacy budget of $\varepsilon=8$, a linear classifier trained on top of learned DP-Cap features attains 65.8% accuracy on ImageNet-1K, considerably improving the previous SOTA of 56.5%. Our work challenges the prevailing sentiment that high-utility DP <b>representation</b> <b>learning</b> cannot be achieved by training from scratch.</p></p class="citation"></blockquote><h3 id=3262--156309-dragtex-generative-point-based-texture-editing-on-3d-mesh-yudi-zhang-et-al-2024>(32/62 | 156/309) DragTex: Generative Point-Based Texture Editing on 3D Mesh (Yudi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yudi Zhang, Qi Xu, Lei Zhang. (2024)<br><strong>DragTex: Generative Point-Based Texture Editing on 3D Mesh</strong><br><button class=copy-to-clipboard title="DragTex: Generative Point-Based Texture Editing on 3D Mesh" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02217v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02217v1.pdf filename=2403.02217v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Creating 3D textured meshes using generative artificial intelligence has garnered significant attention recently. While existing methods support text-based generative texture generation or editing on 3D meshes, they often struggle to precisely control pixels of texture images through more intuitive interaction. While 2D images can be edited generatively using drag interaction, applying this type of methods directly to 3D mesh textures still leads to issues such as the lack of local consistency among multiple views, error accumulation and long training times. To address these challenges, we propose a generative point-based 3D mesh texture editing method called DragTex. This method utilizes a <b>diffusion</b> <b>model</b> to blend locally inconsistent textures in the region near the deformed silhouette between different views, enabling locally consistent texture editing. Besides, we <b>fine-tune</b> a decoder to reduce reconstruction errors in the non-drag region, thereby mitigating overall error accumulation. Moreover, we train LoRA using multi-view images instead of training each view individually, which significantly shortens the training time. The experimental results show that our method effectively achieves dragging textures on 3D meshes and generates plausible textures that align with the desired intent of drag interaction.</p></p class="citation"></blockquote><h3 id=3362--157309-triposr-fast-3d-object-reconstruction-from-a-single-image-dmitry-tochilkin-et-al-2024>(33/62 | 157/309) TripoSR: Fast 3D Object Reconstruction from a Single Image (Dmitry Tochilkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, Yan-Pei Cao. (2024)<br><strong>TripoSR: Fast 3D Object Reconstruction from a Single Image</strong><br><button class=copy-to-clipboard title="TripoSR: Fast 3D Object Reconstruction from a Single Image" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Generative AI, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02151v1.pdf filename=2403.02151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This technical report introduces TripoSR, a 3D reconstruction model leveraging <b>transformer</b> architecture for fast feed-forward 3D generation, producing 3D mesh from a single image in under 0.5 seconds. Building upon the LRM network architecture, TripoSR integrates substantial improvements in data processing, model design, and training techniques. Evaluations on public datasets show that TripoSR exhibits superior performance, both quantitatively and qualitatively, compared to other open-source alternatives. Released under the MIT license, TripoSR is intended to empower researchers, developers, and creatives with the latest advancements in 3D <b>generative</b> <b>AI.</b></p></p class="citation"></blockquote><h3 id=3462--158309-multi-spectral-remote-sensing-image-retrieval-using-geospatial-foundation-models-benedikt-blumenstiel-et-al-2024>(34/62 | 158/309) Multi-Spectral Remote Sensing Image Retrieval Using Geospatial Foundation Models (Benedikt Blumenstiel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benedikt Blumenstiel, Viktoria Moor, Romeo Kienzler, Thomas Brunschwiler. (2024)<br><strong>Multi-Spectral Remote Sensing Image Retrieval Using Geospatial Foundation Models</strong><br><button class=copy-to-clipboard title="Multi-Spectral Remote Sensing Image Retrieval Using Geospatial Foundation Models" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02059v1.pdf filename=2403.02059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image retrieval enables an efficient search through vast amounts of satellite imagery and returns similar images to a query. Deep learning models can identify images across various semantic concepts without the need for annotations. This work proposes to use Geospatial <b>Foundation</b> <b>Models,</b> like Prithvi, for remote sensing image retrieval with multiple benefits: i) the models encode multi-spectral satellite data and ii) generalize without further <b>fine-tuning.</b> We introduce two datasets to the retrieval task and observe a strong performance: Prithvi processes six bands and achieves a mean Average Precision of 97.62% on BigEarthNet-43 and 44.51% on ForestNet-12, outperforming other RGB-based models. Further, we evaluate three compression methods with binarized embeddings balancing retrieval speed and accuracy. They match the retrieval speed of much shorter hash codes while maintaining the same accuracy as floating-point embeddings but with a 32-fold compression. The code is available at <a href=https://github.com/IBM/remote-sensing-image-retrieval>https://github.com/IBM/remote-sensing-image-retrieval</a>.</p></p class="citation"></blockquote><h3 id=3562--159309-physics-informed-learning-for-time-resolved-angiographic-contrast-agent-concentration-reconstruction-noah-maul-et-al-2024>(35/62 | 159/309) Physics-Informed Learning for Time-Resolved Angiographic Contrast Agent Concentration Reconstruction (Noah Maul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Noah Maul, Annette Birkhold, Fabian Wagner, Mareike Thies, Maximilian Rohleder, Philipp Berg, Markus Kowarschik, Andreas Maier. (2024)<br><strong>Physics-Informed Learning for Time-Resolved Angiographic Contrast Agent Concentration Reconstruction</strong><br><button class=copy-to-clipboard title="Physics-Informed Learning for Time-Resolved Angiographic Contrast Agent Concentration Reconstruction" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01993v1.pdf filename=2403.01993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Three-dimensional Digital Subtraction Angiography (3D-DSA) is a well-established X-ray-based technique for visualizing vascular anatomy. Recently, four-dimensional DSA (4D-DSA) reconstruction algorithms have been developed to enable the visualization of volumetric contrast flow dynamics through time-series of volumes. . This reconstruction problem is ill-posed mainly due to vessel overlap in the projection direction and geometric vessel foreshortening, which leads to information loss in the recorded projection images. However, knowledge about the underlying fluid dynamics can be leveraged to constrain the solution space. In our work, we implicitly include this information in a neural network-based model that is trained on a dataset of image-based blood flow <b>simulations.</b> The model predicts the spatially averaged contrast agent concentration for each centerline point of the vasculature over time, lowering the overall computational demand. The trained network enables the reconstruction of relative contrast agent concentrations with a mean absolute error of 0.02 $\pm$ 0.02 and a mean absolute percentage error of 5.31 % $\pm$ 9.25 %. Moreover, the network is robust to varying degrees of vessel overlap and vessel foreshortening. Our approach demonstrates the potential of the integration of machine learning and blood flow <b>simulations</b> in time-resolved angiographic flow reconstruction.</p></p class="citation"></blockquote><h3 id=3662--160309-place-adaptive-layout-semantic-fusion-for-semantic-image-synthesis-zhengyao-lv-et-al-2024>(36/62 | 160/309) PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis (Zhengyao Lv et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyao Lv, Yuxiang Wei, Wangmeng Zuo, Kwan-Yee K. Wong. (2024)<br><strong>PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis</strong><br><button class=copy-to-clipboard title="PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01852v1.pdf filename=2403.01852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in large-scale pre-trained <b>text-to-image</b> models have led to remarkable progress in semantic image synthesis. Nevertheless, synthesizing high-quality images with consistent semantics and layout remains a challenge. In this paper, we propose the adaPtive LAyout-semantiC fusion modulE (PLACE) that harnesses pre-trained models to alleviate the aforementioned issues. Specifically, we first employ the layout control map to faithfully represent layouts in the feature space. Subsequently, we combine the layout and semantic features in a timestep-adaptive manner to synthesize images with realistic details. During <b>fine-tuning,</b> we propose the Semantic Alignment (SA) loss to further enhance layout alignment. Additionally, we introduce the Layout-Free Prior Preservation (LFP) loss, which leverages unlabeled data to maintain the priors of pre-trained models, thereby improving the visual quality and semantic consistency of synthesized images. Extensive experiments demonstrate that our approach performs favorably in terms of visual quality, semantic consistency, and layout alignment. The source code and model are available at <a href=https://github.com/cszy98/PLACE/tree/main>https://github.com/cszy98/PLACE/tree/main</a>.</p></p class="citation"></blockquote><h3 id=3762--161309-pointcore-efficient-unsupervised-point-cloud-anomaly-detector-using-local-global-features-baozhu-zhao-et-al-2024>(37/62 | 161/309) PointCore: Efficient Unsupervised Point Cloud Anomaly Detector Using Local-Global Features (Baozhu Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baozhu Zhao, Qiwei Xiong, Xiaohan Zhang, Jingfeng Guo, Qi Liu, Xiaofen Xing, Xiangmin Xu. (2024)<br><strong>PointCore: Efficient Unsupervised Point Cloud Anomaly Detector Using Local-Global Features</strong><br><button class=copy-to-clipboard title="PointCore: Efficient Unsupervised Point Cloud Anomaly Detector Using Local-Global Features" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01804v1.pdf filename=2403.01804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Three-dimensional point cloud <b>anomaly</b> <b>detection</b> that aims to detect <b>anomaly</b> <b>data</b> points from a training set serves as the foundation for a variety of applications, including industrial inspection and autonomous driving. However, existing point cloud <b>anomaly</b> <b>detection</b> methods often incorporate multiple feature memory banks to fully preserve local and global representations, which comes at the high cost of computational complexity and mismatches between features. To address that, we propose an <b>unsupervised</b> point cloud <b>anomaly</b> <b>detection</b> framework based on joint local-global features, termed PointCore. To be specific, PointCore only requires a single memory bank to store local (coordinate) and global (PointMAE) representations and different priorities are assigned to these local-global features, thereby reducing the computational cost and mismatching disturbance in inference. Furthermore, to robust against the outliers, a normalization ranking method is introduced to not only adjust values of different scales to a notionally common scale, but also transform densely-distributed data into a uniform distribution. Extensive experiments on Real3D-AD dataset demonstrate that PointCore achieves competitive inference time and the best performance in both detection and localization as compared to the state-of-the-art Reg3D-AD approach and several competitors.</p></p class="citation"></blockquote><h3 id=3862--162309-ootdiffusion-outfitting-fusion-based-latent-diffusion-for-controllable-virtual-try-on-yuhao-xu-et-al-2024>(38/62 | 162/309) OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on (Yuhao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhao Xu, Tao Gu, Weifeng Chen, Chengcai Chen. (2024)<br><strong>OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on</strong><br><button class=copy-to-clipboard title="OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01779v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01779v2.pdf filename=2403.01779v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present OOTDiffusion, a novel network architecture for realistic and controllable image-based virtual try-on (VTON). We leverage the power of pretrained latent <b>diffusion</b> <b>models,</b> designing an outfitting UNet to learn the garment detail features. Without a redundant warping process, the garment features are precisely aligned with the target human body via the proposed outfitting fusion in the <b>self-attention</b> layers of the denoising UNet. In order to further enhance the controllability, we introduce outfitting dropout to the training process, which enables us to adjust the strength of the garment features through classifier-free guidance. Our comprehensive experiments on the VITON-HD and Dress Code datasets demonstrate that OOTDiffusion efficiently generates high-quality try-on results for arbitrary human and garment images, which outperforms other VTON methods in both realism and controllability, indicating an impressive breakthrough in virtual try-on. Our source code is available at <a href=https://github.com/levihsu/OOTDiffusion>https://github.com/levihsu/OOTDiffusion</a>.</p></p class="citation"></blockquote><h3 id=3962--163309-mca-moment-channel-attention-networks-yangbo-jiang-et-al-2024>(39/62 | 163/309) MCA: Moment Channel Attention Networks (Yangbo Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yangbo Jiang, Zhiwei Jiang, Le Han, Zenan Huang, Nenggan Zheng. (2024)<br><strong>MCA: Moment Channel Attention Networks</strong><br><button class=copy-to-clipboard title="MCA: Moment Channel Attention Networks" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01713v1.pdf filename=2403.01713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Channel attention mechanisms endeavor to recalibrate channel weights to enhance representation abilities of networks. However, mainstream methods often rely solely on global average pooling as the feature squeezer, which significantly limits the overall potential of models. In this paper, we investigate the statistical moments of feature maps within a neural network. Our findings highlight the critical role of high-order moments in enhancing model capacity. Consequently, we introduce a flexible and comprehensive mechanism termed Extensive Moment Aggregation (EMA) to capture the global spatial context. Building upon this mechanism, we propose the Moment Channel Attention (MCA) framework, which efficiently incorporates multiple levels of moment-based information while minimizing additional computation costs through our Cross Moment <b>Convolution</b> (CMC) module. The CMC module via channel-wise <b>convolution</b> layer to capture multiple order moment information as well as cross channel features. The MCA block is designed to be lightweight and easily integrated into a variety of neural network architectures. Experimental results on classical image classification, <b>object</b> <b>detection,</b> and instance segmentation tasks demonstrate that our proposed method achieves state-of-the-art results, outperforming existing channel attention methods.</p></p class="citation"></blockquote><h3 id=4062--164309-a-new-perspective-on-smiling-and-laughter-detection-intensity-levels-matter-hugo-bohy-et-al-2024>(40/62 | 164/309) A New Perspective on Smiling and Laughter Detection: Intensity Levels Matter (Hugo Bohy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hugo Bohy, Kevin El Haddad, Thierry Dutoit. (2024)<br><strong>A New Perspective on Smiling and Laughter Detection: Intensity Levels Matter</strong><br><button class=copy-to-clipboard title="A New Perspective on Smiling and Laughter Detection: Intensity Levels Matter" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02112v1.pdf filename=2403.02112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Smiles and laughs detection systems have attracted a lot of attention in the past decade contributing to the improvement of human-agent interaction systems. But very few considered these expressions as distinct, although no prior work clearly proves them to belong to the same category or not. In this work, we present a deep learning-based <b>multimodal</b> smile and laugh classification system, considering them as two different entities. We compare the use of audio and vision-based models as well as a fusion approach. We show that, as expected, the fusion leads to a better generalization on unseen data. We also present an in-depth analysis of the behavior of these models on the smiles and laughs intensity levels. The analyses on the intensity levels show that the relationship between smiles and laughs might not be as simple as a binary one or even grouping them in a single category, and so, a more complex approach should be taken when dealing with them. We also tackle the problem of limited resources by showing that <b>transfer</b> <b>learning</b> allows the models to improve the detection of confusing intensity levels.</p></p class="citation"></blockquote><h3 id=4162--165309-facechain-imagineid-freely-crafting-high-fidelity-diverse-talking-faces-from-disentangled-audio-chao-xu-et-al-2024>(41/62 | 165/309) FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio (Chao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun. (2024)<br><strong>FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio</strong><br><button class=copy-to-clipboard title="FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Diffusion Model, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01901v1.pdf filename=2403.01901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial <b>geometry</b> and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent <b>Diffusion</b> <b>Models</b> (LDMs) to focus on maintaining facial <b>geometry</b> and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at <a href=https://github.com/modelscope/facechain>https://github.com/modelscope/facechain</a>.</p></p class="citation"></blockquote><h3 id=4262--166309-modality-aware-and-shift-mixer-for-multi-modal-brain-tumor-segmentation-zhongzhen-huang-et-al-2024>(42/62 | 166/309) Modality-Aware and Shift Mixer for Multi-modal Brain Tumor Segmentation (Zhongzhen Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongzhen Huang, Linda Wei, Shaoting Zhang, Xiaofan Zhang. (2024)<br><strong>Modality-Aware and Shift Mixer for Multi-modal Brain Tumor Segmentation</strong><br><button class=copy-to-clipboard title="Modality-Aware and Shift Mixer for Multi-modal Brain Tumor Segmentation" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Multi-modal, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02074v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02074v1.pdf filename=2403.02074v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Combining images from multi-modalities is beneficial to explore various information in computer vision, especially in the medical domain. As an essential part of clinical diagnosis, <b>multi-modal</b> brain tumor segmentation aims to delineate the malignant entity involving multiple modalities. Although existing methods have shown remarkable performance in the task, the information exchange for cross-scale and high-level representations fusion in spatial and modality are limited in these methods. In this paper, we present a novel Modality Aware and Shift Mixer that integrates intra-modality and inter-modality dependencies of <b>multi-modal</b> images for effective and robust brain tumor segmentation. Specifically, we introduce a Modality-Aware module according to neuroimaging studies for modeling the specific modality pair relationships at low levels, and a Modality-Shift module with specific mosaic patterns is developed to explore the complex relationships across modalities at high levels via the <b>self-attention.</b> Experimentally, we outperform previous state-of-the-art approaches on the public Brain Tumor Segmentation (BraTS 2021 segmentation) dataset. Further qualitative experiments demonstrate the efficacy and robustness of MASM.</p></p class="citation"></blockquote><h3 id=4362--167309-allspark-reborn-labeled-features-from-unlabeled-in-transformer-for-semi-supervised-semantic-segmentation-haonan-wang-et-al-2024>(43/62 | 167/309) AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation (Haonan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haonan Wang, Qixiang Zhang, Yi Li, Xiaomeng Li. (2024)<br><strong>AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01818v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01818v2.pdf filename=2403.01818v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate the burden of time-consuming pixel-level manual labeling, which leverages limited labeled data along with larger amounts of unlabeled data. Current state-of-the-art methods train the labeled data with ground truths and unlabeled data with pseudo labels. However, the two training flows are separate, which allows labeled data to dominate the training process, resulting in low-quality pseudo labels and, consequently, sub-optimal results. To alleviate this issue, we present AllSpark, which reborns the labeled features from unlabeled ones with the channel-wise cross-attention mechanism. We further introduce a Semantic Memory along with a Channel Semantic Grouping strategy to ensure that unlabeled features adequately represent labeled features. The AllSpark shed new light on the architecture level designs of SSSS rather than framework level, which avoids increasingly complicated training pipeline designs. It can also be regarded as a flexible bottleneck module that can be seamlessly integrated into a general <b>transformer-based</b> segmentation model. The proposed AllSpark outperforms existing methods across all evaluation protocols on Pascal, Cityscapes and COCO <b>benchmarks</b> without bells-and-whistles. Code and model weights are available at: <a href=https://github.com/xmed-lab/AllSpark>https://github.com/xmed-lab/AllSpark</a>.</p></p class="citation"></blockquote><h3 id=4462--168309-exposing-the-deception-uncovering-more-forgery-clues-for-deepfake-detection-zhongjie-ba-et-al-2024>(44/62 | 168/309) Exposing the Deception: Uncovering More Forgery Clues for Deepfake Detection (Zhongjie Ba et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongjie Ba, Qingyu Liu, Zhenguang Liu, Shuang Wu, Feng Lin, Li Lu, Kui Ren. (2024)<br><strong>Exposing the Deception: Uncovering More Forgery Clues for Deepfake Detection</strong><br><button class=copy-to-clipboard title="Exposing the Deception: Uncovering More Forgery Clues for Deepfake Detection" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-IT, cs.CV, math-IT<br>Keyword Score: 13<br>Keywords: Benchmarking, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01786v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01786v1.pdf filename=2403.01786v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deepfake technology has given rise to a spectrum of novel and compelling applications. Unfortunately, the widespread proliferation of high-fidelity fake videos has led to pervasive confusion and deception, shattering our faith that seeing is believing. One aspect that has been overlooked so far is that current deepfake detection approaches may easily fall into the trap of overfitting, focusing only on forgery clues within one or a few local regions. Moreover, existing works heavily rely on neural networks to extract forgery features, lacking theoretical constraints guaranteeing that sufficient forgery clues are extracted and superfluous features are eliminated. These deficiencies culminate in unsatisfactory accuracy and limited generalizability in real-life scenarios. In this paper, we try to tackle these challenges through three designs: (1) We present a novel framework to capture broader forgery clues by extracting multiple non-overlapping local representations and fusing them into a global semantic-rich feature. (2) Based on the information bottleneck theory, we derive Local Information Loss to guarantee the orthogonality of local representations while preserving comprehensive task-relevant information. (3) Further, to fuse the local representations and remove task-irrelevant information, we arrive at a Global Information Loss through the theoretical analysis of <b>mutual</b> <b>information.</b> Empirically, our method achieves state-of-the-art performance on five <b>benchmark</b> datasets.Our code is available at \url{https://github.com/QingyuLiu/Exposing-the-Deception}, hoping to inspire researchers.</p></p class="citation"></blockquote><h3 id=4562--169309-optimizing-illuminant-estimation-in-dual-exposure-hdr-imaging-mahmoud-afifi-et-al-2024>(45/62 | 169/309) Optimizing Illuminant Estimation in Dual-Exposure HDR Imaging (Mahmoud Afifi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahmoud Afifi, Zhenhua Hu, Liang Liang. (2024)<br><strong>Optimizing Illuminant Estimation in Dual-Exposure HDR Imaging</strong><br><button class=copy-to-clipboard title="Optimizing Illuminant Estimation in Dual-Exposure HDR Imaging" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02449v1.pdf filename=2403.02449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High dynamic range (HDR) imaging involves capturing a series of frames of the same scene, each with different exposure settings, to broaden the dynamic range of light. This can be achieved through burst capturing or using staggered HDR sensors that capture long and short exposures simultaneously in the camera image signal processor (ISP). Within camera ISP pipeline, illuminant estimation is a crucial step aiming to estimate the color of the global illuminant in the scene. This estimation is used in camera ISP white-balance module to remove undesirable color cast in the final image. Despite the multiple frames captured in the HDR pipeline, conventional illuminant estimation methods often rely only on a single frame of the scene. In this paper, we explore leveraging information from frames captured with different exposure times. Specifically, we introduce a simple feature extracted from dual-exposure images to guide illuminant estimators, referred to as the dual-exposure feature (DEF). To validate the efficiency of DEF, we employed two illuminant estimators using the proposed DEF: 1) a multilayer perceptron network (MLP), referred to as exposure-based MLP (EMLP), and 2) a modified version of the <b>convolutional</b> color constancy (CCC) to integrate our DEF, that we call ECCC. Both EMLP and ECCC achieve promising results, in some cases surpassing prior methods that require hundreds of thousands or millions of parameters, with only a few hundred parameters for EMLP and a few thousand parameters for ECCC.</p></p class="citation"></blockquote><h3 id=4662--170309-brand-visibility-in-packaging-a-deep-learning-approach-for-logo-detection-saliency-map-prediction-and-logo-placement-analysis-alireza-hosseini-et-al-2024>(46/62 | 170/309) Brand Visibility in Packaging: A Deep Learning Approach for Logo Detection, Saliency-Map Prediction, and Logo Placement Analysis (Alireza Hosseini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alireza Hosseini, Kiana Hooshanfar, Pouria Omrani, Reza Toosi, Ramin Toosi, Zahra Ebrahimian, Mohammad Ali Akhaee. (2024)<br><strong>Brand Visibility in Packaging: A Deep Learning Approach for Logo Detection, Saliency-Map Prediction, and Logo Placement Analysis</strong><br><button class=copy-to-clipboard title="Brand Visibility in Packaging: A Deep Learning Approach for Logo Detection, Saliency-Map Prediction, and Logo Placement Analysis" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02336v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02336v1.pdf filename=2403.02336v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the highly competitive area of product marketing, the visibility of brand logos on packaging plays a crucial role in shaping consumer perception, directly influencing the success of the product. This paper introduces a comprehensive framework to measure the brand logo&rsquo;s attention on a packaging design. The proposed method consists of three steps. The first step leverages YOLOv8 for precise logo detection across prominent datasets, FoodLogoDet-1500 and LogoDet-3K. The second step involves modeling the user&rsquo;s visual attention with a novel saliency prediction model tailored for the packaging context. The proposed saliency model combines the visual elements with text maps employing a <b>transformers-based</b> architecture to predict user attention maps. In the third step, by integrating logo detection with a saliency map generation, the framework provides a comprehensive brand attention score. The effectiveness of the proposed method is assessed module by module, ensuring a thorough evaluation of each component. Comparing logo detection and saliency map prediction with state-of-the-art models shows the superiority of the proposed methods. To investigate the robustness of the proposed brand attention score, we collected a unique dataset to examine previous psychophysical hypotheses related to brand visibility. the results show that the brand attention score is in line with all previous studies. Also, we introduced seven new hypotheses to check the impact of position, orientation, presence of person, and other visual elements on brand attention. This research marks a significant stride in the intersection of cognitive psychology, computer vision, and marketing, paving the way for advanced, consumer-centric packaging designs.</p></p class="citation"></blockquote><h3 id=4762--171309-non-autoregressive-sequence-to-sequence-vision-language-models-kunyu-shi-et-al-2024>(47/62 | 171/309) Non-autoregressive Sequence-to-Sequence Vision-Language Models (Kunyu Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kunyu Shi, Qi Dong, Luis Goncalves, Zhuowen Tu, Stefano Soatto. (2024)<br><strong>Non-autoregressive Sequence-to-Sequence Vision-Language Models</strong><br><button class=copy-to-clipboard title="Non-autoregressive Sequence-to-Sequence Vision-Language Models" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02249v1.pdf filename=2403.02249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequence-to-sequence <b>vision-language</b> models are showing promise, but their applicability is limited by their inference latency due to their autoregressive way of generating predictions. We propose a parallel decoding sequence-to-sequence <b>vision-language</b> model, trained with a Query-CTC loss, that marginalizes over multiple inference paths in the decoder. This allows us to model the joint distribution of tokens, rather than restricting to conditional distribution as in an autoregressive model. The resulting model, NARVL, achieves performance on-par with its state-of-the-art autoregressive counterpart, but is faster at inference time, reducing from the linear complexity associated with the sequential generation of tokens to a paradigm of constant time joint inference.</p></p class="citation"></blockquote><h3 id=4862--172309-diffmot-a-real-time-diffusion-based-multiple-object-tracker-with-non-linear-prediction-weiyi-lv-et-al-2024>(48/62 | 172/309) DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction (Weiyi Lv et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiyi Lv, Yuhang Huang, Ning Zhang, Ruei-Sung Lin, Mei Han, Dan Zeng. (2024)<br><strong>DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction</strong><br><button class=copy-to-clipboard title="DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02075v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02075v1.pdf filename=2403.02075v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Multiple Object Tracking, objects often exhibit non-linear motion of acceleration and deceleration, with irregular direction changes. Tacking-by-detection (TBD) with Kalman Filter motion prediction works well in pedestrian-dominant scenarios but falls short in complex situations when multiple objects perform non-linear and diverse motion simultaneously. To tackle the complex non-linear motion, we propose a real-time diffusion-based MOT approach named DiffMOT. Specifically, for the motion predictor component, we propose a novel Decoupled Diffusion-based Motion Predictor (D MP). It models the entire distribution of various motion presented by the data as a whole. It also predicts an individual object&rsquo;s motion conditioning on an individual&rsquo;s historical motion information. Furthermore, it optimizes the diffusion process with much less sampling steps. As a MOT tracker, the DiffMOT is real-time at 22.7FPS, and also outperforms the state-of-the-art on DanceTrack and SportsMOT datasets with 63.4 and 76.2 in HOTA metrics, respectively. To the best of our knowledge, DiffMOT is the first to introduce a diffusion <b>probabilistic</b> <b>model</b> into the MOT to tackle non-linear motion prediction.</p></p class="citation"></blockquote><h3 id=4962--173309-depth-guided-robust-and-fast-point-cloud-fusion-nerf-for-sparse-input-views-shuai-guo-et-al-2024>(49/62 | 173/309) Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input Views (Shuai Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Guo, Qiuwen Wang, Yijie Gao, Rong Xie, Li Song. (2024)<br><strong>Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input Views</strong><br><button class=copy-to-clipboard title="Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input Views" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02063v1.pdf filename=2403.02063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Novel-view synthesis with sparse input views is important for real-world applications like AR/VR and autonomous driving. Recent methods have integrated depth information into NeRFs for sparse input synthesis, leveraging depth prior for geometric and spatial understanding. However, most existing works tend to overlook inaccuracies within depth maps and have low time efficiency. To address these issues, we propose a depth-guided robust and fast point cloud fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel grid of features. A point cloud is constructed for each input view, characterized within the voxel grid using matrices and vectors. We accumulate the point cloud of each input view to construct the fused point cloud of the entire scene. Each voxel determines its density and appearance by referring to the point cloud of the entire scene. Through point cloud fusion and voxel grid <b>fine-tuning,</b> inaccuracies in depth values are refined or substituted by those from other views. Moreover, our method can achieve faster reconstruction and greater compactness through effective vector-matrix decomposition. Experimental results underline the superior performance and time efficiency of our approach compared to state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=5062--174309-leveraging-anchor-based-lidar-3d-object-detection-via-point-assisted-sample-selection-shitao-chen-et-al-2024>(50/62 | 174/309) Leveraging Anchor-based LiDAR 3D Object Detection via Point Assisted Sample Selection (Shitao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shitao Chen, Haolin Zhang, Nanning Zheng. (2024)<br><strong>Leveraging Anchor-based LiDAR 3D Object Detection via Point Assisted Sample Selection</strong><br><button class=copy-to-clipboard title="Leveraging Anchor-based LiDAR 3D Object Detection via Point Assisted Sample Selection" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01978v1.pdf filename=2403.01978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D <b>object</b> <b>detection</b> based on LiDAR point cloud and prior anchor boxes is a critical technology for autonomous driving environment perception and understanding. Nevertheless, an overlooked practical issue in existing methods is the ambiguity in training sample allocation based on box Intersection over Union (IoU_box). This problem impedes further enhancements in the performance of anchor-based LiDAR 3D <b>object</b> <b>detectors.</b> To tackle this challenge, this paper introduces a new training sample selection method that utilizes point cloud distribution for anchor sample quality measurement, named Point Assisted Sample Selection (PASS). This method has undergone rigorous evaluation on two widely utilized datasets. Experimental results demonstrate that the application of PASS elevates the average precision of anchor-based LiDAR 3D <b>object</b> <b>detectors</b> to a novel state-of-the-art, thereby proving the effectiveness of the proposed approach. The codes will be made available at <a href=https://github.com/XJTU-Haolin/Point_Assisted_Sample_Selection>https://github.com/XJTU-Haolin/Point_Assisted_Sample_Selection</a>.</p></p class="citation"></blockquote><h3 id=5162--175309-fourier-basis-functions-to-bridge-augmentation-gap-rethinking-frequency-augmentation-in-image-classification-puru-vaish-et-al-2024>(51/62 | 175/309) Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification (Puru Vaish et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Puru Vaish, Shunxin Wang, Nicola Strisciuglio. (2024)<br><strong>Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification</strong><br><button class=copy-to-clipboard title="Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01944v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01944v2.pdf filename=2403.01944v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computer vision models normally witness degraded performance when deployed in real-world scenarios, due to unexpected changes in inputs that were not accounted for during training. <b>Data</b> <b>augmentation</b> is commonly used to address this issue, as it aims to increase <b>data</b> <b>variety</b> and reduce the distribution gap between training and test <b>data.</b> <b>However,</b> common visual augmentations might not guarantee extensive robustness of computer vision models. In this paper, we propose Auxiliary Fourier-basis Augmentation (AFA), a complementary technique targeting augmentation in the frequency domain and filling the augmentation gap left by visual augmentations. We demonstrate the utility of augmentation via Fourier-basis additive noise in a straightforward and efficient adversarial setting. Our results show that AFA benefits the robustness of models against common corruptions, OOD generalization, and consistency of performance of models against increasing perturbations, with negligible deficit to the standard performance of models. It can be seamlessly integrated with other augmentation techniques to further boost performance. Code and models can be found at: <a href=https://github.com/nis-research/afa-augment>https://github.com/nis-research/afa-augment</a></p></p class="citation"></blockquote><h3 id=5262--176309-semi-supervised-semantic-segmentation-based-on-pseudo-labels-a-survey-lingyan-ran-et-al-2024>(52/62 | 176/309) Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey (Lingyan Ran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingyan Ran, Yali Li, Guoqiang Liang, Yanning Zhang. (2024)<br><strong>Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey</strong><br><button class=copy-to-clipboard title="Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01909v1.pdf filename=2403.01909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic segmentation is an important and popular research area in computer vision that focuses on classifying pixels in an image based on their semantics. However, <b>supervised</b> deep learning requires large amounts of data to train models and the process of labeling images pixel by pixel is time-consuming and laborious. This review aims to provide a first comprehensive and organized overview of the state-of-the-art research results on pseudo-label methods in the field of semi-supervised semantic segmentation, which we categorize from different perspectives and present specific methods for specific application areas. In addition, we explore the application of pseudo-label technology in medical and remote-sensing image segmentation. Finally, we also propose some feasible future research directions to address the existing challenges.</p></p class="citation"></blockquote><h3 id=5362--177309-atomovideo-high-fidelity-image-to-video-generation-litong-gong-et-al-2024>(53/62 | 177/309) AtomoVideo: High Fidelity Image-to-Video Generation (Litong Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Litong Gong, Yiran Zhu, Weijie Li, Xiaoyang Kang, Biao Wang, Tiezheng Ge, Bo Zheng. (2024)<br><strong>AtomoVideo: High Fidelity Image-to-Video Generation</strong><br><button class=copy-to-clipboard title="AtomoVideo: High Fidelity Image-to-Video Generation" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01800v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01800v2.pdf filename=2403.01800v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, video generation has achieved significant rapid development based on superior <b>text-to-image</b> generation techniques. In this work, we propose a high fidelity framework for image-to-video generation, named AtomoVideo. Based on multi-granularity image injection, we achieve higher fidelity of the generated video to the given image. In addition, thanks to high quality datasets and training strategies, we achieve greater motion intensity while maintaining superior temporal consistency and stability. Our architecture extends flexibly to the video frame prediction task, enabling long sequence prediction through iterative generation. Furthermore, due to the design of adapter training, our approach can be well combined with existing personalized models and controllable modules. By quantitatively and qualitatively evaluation, AtomoVideo achieves superior results compared to popular methods, more examples can be found on our project website: <a href=https://atomo-video.github.io/>https://atomo-video.github.io/</a>.</p></p class="citation"></blockquote><h3 id=5462--178309-integrating-efficient-optimal-transport-and-functional-maps-for-unsupervised-shape-correspondence-learning-tung-le-et-al-2024>(54/62 | 178/309) Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning (Tung Le et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tung Le, Khai Nguyen, Shanlin Sun, Nhat Ho, Xiaohui Xie. (2024)<br><strong>Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning</strong><br><button class=copy-to-clipboard title="Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01781v1.pdf filename=2403.01781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of computer vision and graphics, accurately establishing correspondences between geometric 3D shapes is pivotal for applications like object tracking, registration, texture transfer, and statistical shape analysis. Moving beyond traditional hand-crafted and data-driven feature learning methods, we incorporate spectral methods with deep learning, focusing on functional maps (FMs) and optimal transport (OT). Traditional OT-based approaches, often reliant on entropy regularization OT in learning-based framework, face computational challenges due to their quadratic cost. Our key contribution is to employ the sliced Wasserstein distance (SWD) for OT, which is a valid fast optimal transport metric in an <b>unsupervised</b> shape matching framework. This <b>unsupervised</b> framework integrates functional map regularizers with a novel OT-based loss derived from SWD, enhancing feature alignment between shapes treated as discrete probability measures. We also introduce an adaptive refinement process utilizing entropy regularized OT, further refining feature alignments for accurate point-to-point correspondences. Our method demonstrates superior performance in non-rigid shape matching, including near-isometric and non-isometric scenarios, and excels in downstream tasks like segmentation transfer. The empirical results on diverse datasets highlight our framework&rsquo;s effectiveness and generalization capabilities, setting new standards in non-rigid shape matching with efficient OT metrics and an adaptive refinement module.</p></p class="citation"></blockquote><h3 id=5562--179309-attention-guidance-mechanism-for-handwritten-mathematical-expression-recognition-yutian-liu-et-al-2024>(55/62 | 179/309) Attention Guidance Mechanism for Handwritten Mathematical Expression Recognition (Yutian Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutian Liu, Wenjun Ke, Jianguo Wei. (2024)<br><strong>Attention Guidance Mechanism for Handwritten Mathematical Expression Recognition</strong><br><button class=copy-to-clipboard title="Attention Guidance Mechanism for Handwritten Mathematical Expression Recognition" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01756v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01756v2.pdf filename=2403.01756v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Handwritten mathematical expression recognition (HMER) is challenging in <b>image-to-text</b> tasks due to the complex layouts of mathematical expressions and suffers from problems including over-parsing and under-parsing. To solve these, previous HMER methods improve the attention mechanism by utilizing historical alignment information. However, this approach has limitations in addressing under-parsing since it cannot correct the erroneous attention on image areas that should be parsed at subsequent decoding steps. This faulty attention causes the attention module to incorporate future context into the current decoding step, thereby confusing the alignment process. To address this issue, we propose an attention guidance mechanism to explicitly suppress attention weights in irrelevant areas and enhance the appropriate ones, thereby inhibiting access to information outside the intended context. Depending on the type of attention guidance, we devise two complementary approaches to refine attention weights: self-guidance that coordinates attention of multiple heads and neighbor-guidance that integrates attention from adjacent time steps. Experiments show that our method outperforms existing state-of-the-art methods, achieving expression recognition rates of 60.75% / 61.81% / 63.30% on the CROHME 2014/ 2016/ 2019 datasets.</p></p class="citation"></blockquote><h3 id=5662--180309-training-free-pretrained-model-merging-zhengqi-xu-et-al-2024>(56/62 | 180/309) Training-Free Pretrained Model Merging (Zhengqi Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengqi Xu, Ke Yuan, Huiqiong Wang, Yong Wang, Mingli Song, Jie Song. (2024)<br><strong>Training-Free Pretrained Model Merging</strong><br><button class=copy-to-clipboard title="Training-Free Pretrained Model Merging" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01753v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01753v2.pdf filename=2403.01753v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, model merging techniques have surfaced as a solution to combine multiple single-talent models into a single multi-talent model. However, previous endeavors in this field have either necessitated additional training or <b>fine-tuning</b> processes, or require that the models possess the same pre-trained initialization. In this work, we identify a common drawback in prior works w.r.t. the inconsistency of unit similarity in the weight space and the activation space. To address this inconsistency, we propose an innovative model merging framework, coined as merging under dual-space constraints (MuDSC). Specifically, instead of solely maximizing the objective of a single space, we advocate for the exploration of permutation matrices situated in a region with a unified high similarity in the dual space, achieved through the linear combination of activation and weight similarity matrices. In order to enhance usability, we have also incorporated adaptations for group structure, including Multi-Head Attention and Group Normalization. Comprehensive experimental comparisons demonstrate that MuDSC can significantly boost the performance of merged models with various task combinations and architectures. Furthermore, the visualization of the merged model within the multi-task loss landscape reveals that MuDSC enables the merged model to reside in the overlapping segment, featuring a unified lower loss for each task. Our code is publicly available at <a href=https://github.com/zju-vipa/training_free_model_merging>https://github.com/zju-vipa/training_free_model_merging</a>.</p></p class="citation"></blockquote><h3 id=5762--181309-pillargen-enhancing-radar-point-cloud-density-and-quality-via-pillar-based-point-generation-network-jisong-kim-et-al-2024>(57/62 | 181/309) PillarGen: Enhancing Radar Point Cloud Density and Quality via Pillar-based Point Generation Network (Jisong Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jisong Kim, Geonho Bang, Kwangjin Choi, Minjae Seong, Jaechang Yoo, Eunjong Pyo, Jun Won Choi. (2024)<br><strong>PillarGen: Enhancing Radar Point Cloud Density and Quality via Pillar-based Point Generation Network</strong><br><button class=copy-to-clipboard title="PillarGen: Enhancing Radar Point Cloud Density and Quality via Pillar-based Point Generation Network" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01663v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01663v2.pdf filename=2403.01663v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a novel point generation model, referred to as Pillar-based Point Generation Network (PillarGen), which facilitates the transformation of point clouds from one domain into another. PillarGen can produce synthetic point clouds with enhanced density and quality based on the provided input point clouds. The PillarGen model performs the following three steps: 1) pillar encoding, 2) Occupied Pillar Prediction (OPP), and 3) Pillar to Point Generation (PPG). The input point clouds are encoded using a pillar grid structure to generate pillar features. Then, OPP determines the active pillars used for point generation and predicts the center of points and the number of points to be generated for each active pillar. PPG generates the synthetic points for each active pillar based on the information provided by OPP. We evaluate the performance of PillarGen using our proprietary radar dataset, focusing on enhancing the density and quality of short-range radar data using the long-range radar data as supervision. Our experiments demonstrate that PillarGen outperforms traditional point upsampling methods in quantitative and qualitative measures. We also confirm that when PillarGen is incorporated into bird&rsquo;s eye view <b>object</b> <b>detection,</b> a significant improvement in detection accuracy is achieved.</p></p class="citation"></blockquote><h3 id=5862--182309-neural-network-assisted-lifting-steps-for-improved-fully-scalable-lossy-image-compression-in-jpeg-2000-xinyue-li-et-al-2024>(58/62 | 182/309) Neural Network Assisted Lifting Steps For Improved Fully Scalable Lossy Image Compression in JPEG 2000 (Xinyue Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyue Li, Aous Naman, David Taubman. (2024)<br><strong>Neural Network Assisted Lifting Steps For Improved Fully Scalable Lossy Image Compression in JPEG 2000</strong><br><button class=copy-to-clipboard title="Neural Network Assisted Lifting Steps For Improved Fully Scalable Lossy Image Compression in JPEG 2000" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01647v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01647v1.pdf filename=2403.01647v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work proposes to augment the lifting steps of the conventional wavelet transform with additional neural network assisted lifting steps. These additional steps reduce residual redundancy (notably aliasing information) amongst the wavelet subbands, and also improve the visual quality of reconstructed images at reduced resolutions. The proposed approach involves two steps, a high-to-low step followed by a low-to-high step. The high-to-low step suppresses aliasing in the low-pass band by using the detail bands at the same resolution, while the low-to-high step aims to further remove redundancy from detail bands, so as to achieve higher energy compaction. The proposed two lifting steps are trained in an end-to-end fashion; we employ a backward annealing approach to overcome the non-differentiability of the <b>quantization</b> and cost functions during back-propagation. Importantly, the networks employed in this paper are compact and with limited non-linearities, allowing a fully scalable system; one pair of trained network parameters are applied for all levels of decomposition and for all bit-rates of interest. By employing the proposed approach within the JPEG 2000 image coding standard, our method can achieve up to 17.4% average BD bit-rate saving over a wide range of bit-rates, while retaining quality and resolution scalability features of JPEG 2000.</p></p class="citation"></blockquote><h3 id=5962--183309-3d-hand-reconstruction-via-aggregating-intra-and-inter-graphs-guided-by-prior-knowledge-for-hand-object-interaction-scenario-feng-shuang-et-al-2024>(59/62 | 183/309) 3D Hand Reconstruction via Aggregating Intra and Inter Graphs Guided by Prior Knowledge for Hand-Object Interaction Scenario (Feng Shuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Shuang, Wenbo He, Shaodong Li. (2024)<br><strong>3D Hand Reconstruction via Aggregating Intra and Inter Graphs Guided by Prior Knowledge for Hand-Object Interaction Scenario</strong><br><button class=copy-to-clipboard title="3D Hand Reconstruction via Aggregating Intra and Inter Graphs Guided by Prior Knowledge for Hand-Object Interaction Scenario" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01733v1.pdf filename=2403.01733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, 3D hand reconstruction has gained more attention in human-computer cooperation, especially for hand-object interaction scenario. However, it still remains huge challenge due to severe hand-occlusion caused by interaction, which contain the balance of accuracy and physical plausibility, highly nonlinear mapping of model parameters and occlusion feature enhancement. To overcome these issues, we propose a 3D hand reconstruction network combining the benefits of model-based and model-free approaches to balance accuracy and physical plausibility for hand-object interaction scenario. Firstly, we present a novel MANO pose parameters regression module from 2D joints directly, which avoids the process of highly nonlinear mapping from abstract image feature and no longer depends on accurate 3D joints. Moreover, we further propose a vertex-joint mutual <b>graph-attention</b> model guided by MANO to jointly refine hand meshes and joints, which model the dependencies of vertex-vertex and joint-joint and capture the correlation of vertex-joint for aggregating intra-graph and inter-graph node features respectively. The experimental results demonstrate that our method achieves a competitive performance on recently <b>benchmark</b> datasets HO3DV2 and Dex-YCB, and outperforms all only model-base approaches and model-free approaches.</p></p class="citation"></blockquote><h3 id=6062--184309-a-generative-approach-for-wikipedia-scale-visual-entity-recognition-mathilde-caron-et-al-2024>(60/62 | 184/309) A Generative Approach for Wikipedia-Scale Visual Entity Recognition (Mathilde Caron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathilde Caron, Ahmet Iscen, Alireza Fathi, Cordelia Schmid. (2024)<br><strong>A Generative Approach for Wikipedia-Scale Visual Entity Recognition</strong><br><button class=copy-to-clipboard title="A Generative Approach for Wikipedia-Scale Visual Entity Recognition" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02041v1.pdf filename=2403.02041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address web-scale visual entity recognition, specifically the task of mapping a given query image to one of the 6 million existing entities in Wikipedia. One way of approaching a problem of such scale is using dual-encoder models (eg CLIP), where all the entity names and query images are embedded into a unified space, paving the way for an approximate k-NN search. Alternatively, it is also possible to re-purpose a captioning model to directly generate the entity names for a given image. In contrast, we introduce a novel Generative Entity Recognition (GER) framework, which given an input image learns to auto-regressively decode a semantic and discriminative ``code&rsquo;&rsquo; identifying the target entity. Our experiments demonstrate the efficacy of this GER paradigm, showcasing state-of-the-art performance on the challenging OVEN <b>benchmark.</b> GER surpasses strong captioning, dual-encoder, visual matching and hierarchical classification baselines, affirming its advantage in tackling the complexities of web-scale recognition.</p></p class="citation"></blockquote><h3 id=6162--185309-towards-calibrated-deep-clustering-network-yuheng-jia-et-al-2024>(61/62 | 185/309) Towards Calibrated Deep Clustering Network (Yuheng Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuheng Jia, Jianhong Cheng, Hui Liu, Junhui Hou. (2024)<br><strong>Towards Calibrated Deep Clustering Network</strong><br><button class=copy-to-clipboard title="Towards Calibrated Deep Clustering Network" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02998v1.pdf filename=2403.02998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>clustering</b> has exhibited remarkable performance; however, the overconfidence problem, i.e., the estimated confidence for a sample belonging to a particular cluster greatly exceeds its actual prediction accuracy, has been overlooked in prior research. To tackle this critical issue, we pioneer the development of a calibrated deep <b>clustering</b> framework. Specifically, we propose a novel dual-head deep <b>clustering</b> pipeline that can effectively calibrate the estimated confidence and the actual accuracy. The calibration head adjusts the overconfident predictions of the <b>clustering</b> head using regularization methods, generating prediction confidence and pseudo-labels that match the model learning status. This calibration process also guides the <b>clustering</b> head in dynamically selecting reliable high-confidence samples for training. Additionally, we introduce an effective network initialization strategy that enhances both training speed and network robustness. Extensive experiments demonstrate the proposed calibrated deep <b>clustering</b> framework not only surpasses state-of-the-art deep <b>clustering</b> methods by approximately 10 times in terms of expected calibration error but also significantly outperforms them in terms of <b>clustering</b> accuracy.</p></p class="citation"></blockquote><h3 id=6262--186309-tree-counting-by-bridging-3d-point-clouds-with-imagery-lei-li-et-al-2024>(62/62 | 186/309) Tree Counting by Bridging 3D Point Clouds with Imagery (Lei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Li, Tianfang Zhang, Zhongyu Jiang, Cheng-Yen Yang, Jenq-Neng Hwang, Stefan Oehmcke, Dimitri Pierre Johannes Gominski, Fabian Gieseke, Christian Igel. (2024)<br><strong>Tree Counting by Bridging 3D Point Clouds with Imagery</strong><br><button class=copy-to-clipboard title="Tree Counting by Bridging 3D Point Clouds with Imagery" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01932v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01932v2.pdf filename=2403.01932v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate and consistent methods for counting trees based on remote sensing data are needed to support sustainable forest management, assess climate change mitigation strategies, and build trust in tree carbon credits. Two-dimensional remote sensing imagery primarily shows overstory canopy, and it does not facilitate easy differentiation of individual trees in areas with a dense canopy and does not allow for easy separation of trees when the canopy is dense. We leverage the fusion of three-dimensional LiDAR measurements and 2D imagery to facilitate the accurate counting of trees. We compare a deep learning approach to counting trees in forests using 3D airborne LiDAR data and 2D imagery. The approach is compared with state-of-the-art algorithms, like operating on 3D point cloud and 2D imagery. We empirically evaluate the different methods on the NeonTreeCount data set, which we use to define a tree-counting <b>benchmark.</b> The experiments show that FuseCountNet yields more accurate tree counts.</p></p class="citation"></blockquote><h2 id=csro-20>cs.RO (20)</h2><h3 id=120--187309-zsl-rppo-zero-shot-learning-for-quadrupedal-locomotion-in-challenging-terrains-using-recurrent-proximal-policy-optimization-yao-zhao-et-al-2024>(1/20 | 187/309) ZSL-RPPO: Zero-Shot Learning for Quadrupedal Locomotion in Challenging Terrains using Recurrent Proximal Policy Optimization (Yao Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Zhao, Tao Wu, Yijie Zhu, Xiang Lu, Jun Wang, Haitham Bou-Ammar, Xinyu Zhang, Peng Du. (2024)<br><strong>ZSL-RPPO: Zero-Shot Learning for Quadrupedal Locomotion in Challenging Terrains using Recurrent Proximal Policy Optimization</strong><br><button class=copy-to-clipboard title="ZSL-RPPO: Zero-Shot Learning for Quadrupedal Locomotion in Challenging Terrains using Recurrent Proximal Policy Optimization" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Fine-tuning, Simulation, Simulator, Zero-shot, Recurrent Neural Network, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01928v1.pdf filename=2403.01928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present ZSL-RPPO, an improved <b>zero-shot</b> <b>learning</b> architecture that overcomes the limitations of teacher-student neural networks and enables generating robust, reliable, and versatile locomotion for quadrupedal robots in challenging terrains. We propose a new algorithm RPPO <b>(Recurrent</b> <b>Proximal</b> <b>Policy</b> Optimization) that directly trains <b>recurrent</b> <b>neural</b> <b>network</b> in partially observable environments and results in more robust training using domain randomization. Our locomotion controller supports extensive perturbation across <b>simulation-to-reality</b> transfer for both intrinsic and extrinsic physical parameters without further <b>fine-tuning.</b> This can avoid the significant decline of student&rsquo;s performance during <b>simulation-to-reality</b> transfer and therefore enhance the robustness and generalization of the locomotion controller. We deployed our controller on the Unitree A1 and Aliengo robots in real environment and exteroceptive perception is provided by either a solid-state Lidar or a depth camera. Our locomotion controller was tested in various challenging terrains like slippery surfaces, Grassy Terrain, and stairs. Our experiment results and comparison show that our approach significantly outperforms the state-of-the-art.</p></p class="citation"></blockquote><h3 id=220--188309-offline-goal-conditioned-reinforcement-learning-for-safety-critical-tasks-with-recovery-policy-chenyang-cao-et-al-2024>(2/20 | 188/309) Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy (Chenyang Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyang Cao, Zichen Yan, Renhao Lu, Junbo Tan, Xueqian Wang. (2024)<br><strong>Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy</strong><br><button class=copy-to-clipboard title="Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: 68T40, cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Benchmarking, Reinforcement Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01734v1.pdf filename=2403.01734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Offline goal-conditioned <b>reinforcement</b> <b>learning</b> (GCRL) aims at solving goal-reaching tasks with sparse rewards from an offline dataset. While prior work has demonstrated various approaches for agents to learn near-optimal policies, these methods encounter limitations when dealing with diverse constraints in complex environments, such as safety constraints. Some of these approaches prioritize goal attainment without considering safety, while others excessively focus on safety at the expense of training efficiency. In this paper, we study the problem of constrained offline GCRL and propose a new method called Recovery-based <b>Supervised</b> <b>Learning</b> (RbSL) to accomplish safety-critical tasks with various goals. To evaluate the method performance, we build a <b>benchmark</b> based on the robot-fetching environment with a randomly positioned obstacle and use expert or random policies to generate an offline dataset. We compare RbSL with three offline GCRL algorithms and one offline safe RL algorithm. As a result, our method outperforms the existing state-of-the-art methods to a large extent. Furthermore, we validate the practicality and effectiveness of RbSL by deploying it on a real Panda manipulator. Code is available at <a href=https://github.com/Sunlighted/RbSL.git>https://github.com/Sunlighted/RbSL.git</a>.</p></p class="citation"></blockquote><h3 id=320--189309-aspire-an-informative-trajectory-planner-with-mutual-information-approximation-for-target-search-and-tracking-kangjie-zhou-et-al-2024>(3/20 | 189/309) ASPIRe: An Informative Trajectory Planner with Mutual Information Approximation for Target Search and Tracking (Kangjie Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kangjie Zhou, Pengying Wu, Yao Su, Han Gao, Ji Ma, Hangxin Liu, Chang Liu. (2024)<br><strong>ASPIRe: An Informative Trajectory Planner with Mutual Information Approximation for Target Search and Tracking</strong><br><button class=copy-to-clipboard title="ASPIRe: An Informative Trajectory Planner with Mutual Information Approximation for Target Search and Tracking" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Benchmarking, Mutual Information, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01674v1.pdf filename=2403.01674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes an informative trajectory planning approach, namely, \textit{adaptive particle filter tree with sigma point-based <b>mutual</b> <b>information</b> reward approximation} (ASPIRe), for mobile target search and tracking (SAT) in cluttered environments with limited sensing field of view. We develop a novel sigma point-based approximation to accurately estimate <b>mutual</b> <b>information</b> (MI) for general, non-Gaussian distributions utilizing particle representation of the belief state, while simultaneously maintaining high computational efficiency. Building upon the MI approximation, we develop the Adaptive Particle Filter Tree (APFT) approach with MI as the reward, which features belief state tree nodes for informative trajectory planning in continuous state and measurement spaces. An adaptive criterion is proposed in APFT to adjust the planning horizon based on the expected information gain. <b>Simulations</b> and physical experiments demonstrate that ASPIRe achieves real-time computation and outperforms <b>benchmark</b> methods in terms of both search efficiency and estimation accuracy.</p></p class="citation"></blockquote><h3 id=420--190309-pseudo-labeling-and-contextual-curriculum-learning-for-online-grasp-learning-in-robotic-bin-picking-huy-le-et-al-2024>(4/20 | 190/309) Pseudo-Labeling and Contextual Curriculum Learning for Online Grasp Learning in Robotic Bin Picking (Huy Le et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huy Le, Philipp Schillinger, Miroslav Gabriel, Alexander Qualmann, Ngo Anh Vien. (2024)<br><strong>Pseudo-Labeling and Contextual Curriculum Learning for Online Grasp Learning in Robotic Bin Picking</strong><br><button class=copy-to-clipboard title="Pseudo-Labeling and Contextual Curriculum Learning for Online Grasp Learning in Robotic Bin Picking" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Curriculum Learning, Reinforcement Learning, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02495v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02495v1.pdf filename=2403.02495v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The prevailing grasp prediction methods predominantly rely on offline learning, overlooking the dynamic grasp learning that occurs during real-time adaptation to novel picking scenarios. These scenarios may involve previously unseen objects, variations in camera perspectives, and bin configurations, among other factors. In this paper, we introduce a novel approach, SSL-ConvSAC, that combines <b>semi-supervised</b> <b>learning</b> and <b>reinforcement</b> <b>learning</b> for online grasp learning. By treating pixels with reward feedback as labeled data and others as unlabeled, it efficiently exploits unlabeled data to enhance learning. In addition, we address the imbalance between labeled and unlabeled data by proposing a contextual <b>curriculum-based</b> <b>method.</b> We ablate the proposed approach on real-world evaluation data and demonstrate promise for improving online grasp learning on bin picking tasks using a physical 7-DoF Franka Emika robot arm with a suction gripper. Video: <a href=https://youtu.be/OAro5pg8I9U>https://youtu.be/OAro5pg8I9U</a></p></p class="citation"></blockquote><h3 id=520--191309-twisting-lids-off-with-two-hands-toru-lin-et-al-2024>(5/20 | 191/309) Twisting Lids Off with Two Hands (Toru Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Toru Lin, Zhao-Heng Yin, Haozhi Qi, Pieter Abbeel, Jitendra Malik. (2024)<br><strong>Twisting Lids Off with Two Hands</strong><br><button class=copy-to-clipboard title="Twisting Lids Off with Two Hands" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02338v1.pdf filename=2403.02338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, attributed to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we consider the problem of twisting lids of various bottle-like objects with two hands, and demonstrate that policies trained in <b>simulation</b> using deep <b>reinforcement</b> <b>learning</b> can be effectively transferred to the real world. With novel engineering insights into physical modeling, real-time perception, and reward design, the policy demonstrates generalization capabilities across a diverse set of unseen objects, showcasing dynamic and dexterous behaviors. Our findings serve as compelling evidence that deep <b>reinforcement</b> <b>learning</b> combined with sim-to-real transfer remains a promising approach for addressing manipulation problems of unprecedented complexity.</p></p class="citation"></blockquote><h3 id=620--192309-an-efficient-model-based-approach-on-learning-agile-motor-skills-without-reinforcement-haojie-shi-et-al-2024>(6/20 | 192/309) An Efficient Model-Based Approach on Learning Agile Motor Skills without Reinforcement (Haojie Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haojie Shi, Tingguang Li, Qingxu Zhu, Jiapeng Sheng, Lei Han, Max Q. -H. Meng. (2024)<br><strong>An Efficient Model-Based Approach on Learning Agile Motor Skills without Reinforcement</strong><br><button class=copy-to-clipboard title="An Efficient Model-Based Approach on Learning Agile Motor Skills without Reinforcement" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Autoencoder, Reinforcement Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01962v1.pdf filename=2403.01962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning-based methods have improved locomotion skills of quadruped robots through deep <b>reinforcement</b> <b>learning.</b> However, the sim-to-real gap and low sample efficiency still limit the skill transfer. To address this issue, we propose an efficient model-based learning framework that combines a world model with a policy network. We train a differentiable world model to predict future states and use it to directly supervise a <b>Variational</b> <b>Autoencoder</b> (VAE)-based policy network to imitate real animal behaviors. This significantly reduces the need for real interaction data and allows for rapid policy updates. We also develop a high-level network to track diverse commands and trajectories. Our simulated results show a tenfold sample efficiency increase compared to <b>reinforcement</b> <b>learning</b> methods such as PPO. In real-world testing, our policy achieves proficient command-following performance with only a two-minute data collection period and generalizes well to new speeds and paths.</p></p class="citation"></blockquote><h3 id=720--193309-sensor-based-multi-robot-search-and-coverage-with-spatial-separation-in-unstructured-environments-xinyi-wang-et-al-2024>(7/20 | 193/309) Sensor-based Multi-Robot Search and Coverage with Spatial Separation in Unstructured Environments (Xinyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyi Wang, Jiwen Xu, Chuanxiang Gao, Yizhou Chen, Jihan Zhang, Chenggang Wang, Ben M. Chen. (2024)<br><strong>Sensor-based Multi-Robot Search and Coverage with Spatial Separation in Unstructured Environments</strong><br><button class=copy-to-clipboard title="Sensor-based Multi-Robot Search and Coverage with Spatial Separation in Unstructured Environments" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01710v1.pdf filename=2403.01710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-robot systems have increasingly become instrumental in tackling search and coverage problems. However, the challenge of optimizing task efficiency without compromising task success still persists, particularly in expansive, unstructured environments with dense obstacles. This paper presents an innovative, decentralized Voronoi-based approach for search and coverage to reactively navigate these complexities while maintaining safety. This approach leverages the active sensing capabilities of multi-robot systems to supplement GIS (Geographic Information System), offering a more comprehensive and real-time understanding of the environment. Based on point cloud data, which is inherently non-convex and unstructured, this method efficiently generates collision-free Voronoi regions using only local sensing information through spatial decomposition and spherical mirroring techniques. Then, deadlock-aware guided map integrated with a gradient-optimized, centroid Voronoi-based coverage control policy, is constructed to improve efficiency by avoiding exhaustive searches and local sensing pitfalls. The effectiveness of our algorithm has been validated through extensive numerical <b>simulations</b> in high-fidelity environments, demonstrating significant improvements in both task success rate, coverage ratio, and task execution time compared with others.</p></p class="citation"></blockquote><h3 id=820--194309-tac-man-tactile-informed-prior-free-manipulation-of-articulated-objects-zihang-zhao-et-al-2024>(8/20 | 194/309) Tac-Man: Tactile-Informed Prior-Free Manipulation of Articulated Objects (Zihang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihang Zhao, Yuyang Li, Wanlin Li, Zhenghao Qi, Lecheng Ruan, Yixin Zhu, Kaspar Althoefer. (2024)<br><strong>Tac-Man: Tactile-Informed Prior-Free Manipulation of Articulated Objects</strong><br><button class=copy-to-clipboard title="Tac-Man: Tactile-Informed Prior-Free Manipulation of Articulated Objects" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01694v1.pdf filename=2403.01694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrating robotics into human-centric environments such as homes, necessitates advanced manipulation skills as robotic devices will need to engage with articulated objects like doors and drawers. Key challenges in robotic manipulation are the unpredictability and diversity of these objects&rsquo; internal structures, which render models based on priors, both explicit and implicit, inadequate. Their reliability is significantly diminished by pre-interaction ambiguities, imperfect structural parameters, encounters with unknown objects, and unforeseen disturbances. Here, we present a prior-free strategy, Tac-Man, focusing on maintaining stable robot-object contact during manipulation. Utilizing tactile feedback, but independent of object priors, Tac-Man enables robots to proficiently handle a variety of articulated objects, including those with complex joints, even when influenced by unexpected disturbances. Demonstrated in both real-world experiments and extensive <b>simulations,</b> it consistently achieves near-perfect success in dynamic and varied settings, outperforming existing methods. Our results indicate that tactile sensing alone suffices for managing diverse articulated objects, offering greater robustness and generalization than prior-based approaches. This underscores the importance of detailed contact modeling in complex manipulation tasks, especially with articulated objects. Advancements in tactile sensors significantly expand the scope of robotic applications in human-centric environments, particularly where accurate models are difficult to obtain.</p></p class="citation"></blockquote><h3 id=920--195309-purpose-for-open-ended-learning-robots-a-computational-taxonomy-definition-and-operationalisation-gianluca-baldassarre-et-al-2024>(9/20 | 195/309) Purpose for Open-Ended Learning Robots: A Computational Taxonomy, Definition, and Operationalisation (Gianluca Baldassarre et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gianluca Baldassarre, Richard J. Duro, Emilio Cartoni, Mehdi Khamassi, Alejandro Romero, Vieri Giuliano Santucci. (2024)<br><strong>Purpose for Open-Ended Learning Robots: A Computational Taxonomy, Definition, and Operationalisation</strong><br><button class=copy-to-clipboard title="Purpose for Open-Ended Learning Robots: A Computational Taxonomy, Definition, and Operationalisation" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02514v1.pdf filename=2403.02514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous open-ended learning (OEL) robots are able to cumulatively acquire new skills and knowledge through direct interaction with the environment, for example relying on the guidance of intrinsic motivations and self-generated goals. OEL robots have a high relevance for applications as they can use the autonomously acquired knowledge to accomplish tasks relevant for their human users. OEL robots, however, encounter an important limitation: this may lead to the acquisition of knowledge that is not so much relevant to accomplish the users&rsquo; tasks. This work analyses a possible solution to this problem that pivots on the novel concept of <code>purpose'. Purposes indicate what the designers and/or users want from the robot. The robot should use internal representations of purposes, called here </code>desires&rsquo;, to focus its open-ended exploration towards the acquisition of knowledge relevant to accomplish them. This work contributes to develop a computational framework on purpose in two ways. First, it formalises a framework on purpose based on a three-level motivational hierarchy involving: (a) the purposes; (b) the desires, which are domain independent; (c) specific domain dependent state-goals. Second, the work highlights key challenges highlighted by the framework such as: the <code>purpose-desire alignment problem', the </code>purpose-goal <b>grounding</b> problem&rsquo;, and the `arbitration between desires&rsquo;. Overall, the approach enables OEL robots to learn in an autonomous way but also to focus on acquiring goals and skills that meet the purposes of the designers and users.</p></p class="citation"></blockquote><h3 id=1020--196309-cross-domain-policy-transfer-with-effect-cycle-consistency-ruiqi-zhu-et-al-2024>(10/20 | 196/309) Cross Domain Policy Transfer with Effect Cycle-Consistency (Ruiqi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiqi Zhu, Tianhong Dai, Oya Celiktutan. (2024)<br><strong>Cross Domain Policy Transfer with Effect Cycle-Consistency</strong><br><button class=copy-to-clipboard title="Cross Domain Policy Transfer with Effect Cycle-Consistency" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02018v1.pdf filename=2403.02018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training a robotic policy from scratch using deep <b>reinforcement</b> <b>learning</b> methods can be prohibitively expensive due to sample inefficiency. To address this challenge, transferring policies trained in the source domain to the target domain becomes an attractive paradigm. Previous research has typically focused on domains with similar state and action spaces but differing in other aspects. In this paper, our primary focus lies in domains with different state and action spaces, which has broader practical implications, i.e. transfer the policy from robot A to robot B. Unlike prior methods that rely on paired data, we propose a novel approach for learning the mapping functions between state and action spaces across domains using unpaired data. We propose effect cycle consistency, which aligns the effects of transitions across two domains through a symmetrical optimization structure for learning these mapping functions. Once the mapping functions are learned, we can seamlessly transfer the policy from the source domain to the target domain. Our approach has been tested on three locomotion tasks and two robotic manipulation tasks. The empirical results demonstrate that our method can reduce alignment errors significantly and achieve better performance compared to the state-of-the-art method.</p></p class="citation"></blockquote><h3 id=1120--197309-rt-h-action-hierarchies-using-language-suneel-belkhale-et-al-2024>(11/20 | 197/309) RT-H: Action Hierarchies Using Language (Suneel Belkhale et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quon Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, Dorsa Sadigh. (2024)<br><strong>RT-H: Action Hierarchies Using Language</strong><br><button class=copy-to-clipboard title="RT-H: Action Hierarchies Using Language" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01823v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01823v1.pdf filename=2403.01823v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language provides a way to break down complex concepts into digestible pieces. Recent works in robot imitation learning use language-conditioned policies that predict actions given visual observations and the high-level task specified in language. These methods leverage the structure of natural language to share data between semantically similar tasks (e.g., &ldquo;pick coke can&rdquo; and &ldquo;pick an apple&rdquo;) in multi-task datasets. However, as tasks become more semantically diverse (e.g., &ldquo;pick coke can&rdquo; and &ldquo;pour cup&rdquo;), sharing data between tasks becomes harder, so learning to map high-level tasks to actions requires much more demonstration data. To bridge tasks and actions, our insight is to teach the robot the language of actions, describing low-level motions with more fine-grained phrases like &ldquo;move arm forward&rdquo;. Predicting these language motions as an intermediate step between tasks and actions forces the policy to learn the shared structure of low-level motions across seemingly disparate tasks. Furthermore, a policy that is conditioned on language motions can easily be corrected during execution through <b>human-specified</b> <b>language</b> motions. This enables a new paradigm for flexible policies that can learn from <b>human</b> <b>intervention</b> in language. Our method RT-H builds an action hierarchy using language motions: it first learns to predict language motions, and conditioned on this and the high-level task, it predicts actions, using visual context at all stages. We show that RT-H leverages this language-action hierarchy to learn policies that are more robust and flexible by effectively tapping into multi-task datasets. We show that these policies not only allow for responding to language interventions, but can also learn from such interventions and outperform methods that learn from teleoperated interventions. Our website and videos are found at <a href=https://rt-hierarchy.github.io>https://rt-hierarchy.github.io</a>.</p></p class="citation"></blockquote><h3 id=1220--198309-natsgd-a-dataset-with-speech-gestures-and-demonstrations-for-robot-learning-in-natural-human-robot-interaction-snehesh-shrestha-et-al-2024>(12/20 | 198/309) NatSGD: A Dataset with Speech, Gestures, and Demonstrations for Robot Learning in Natural Human-Robot Interaction (Snehesh Shrestha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Snehesh Shrestha, Yantian Zha, Saketh Banagiri, Ge Gao, Yiannis Aloimonos, Cornelia Fermuller. (2024)<br><strong>NatSGD: A Dataset with Speech, Gestures, and Demonstrations for Robot Learning in Natural Human-Robot Interaction</strong><br><button class=copy-to-clipboard title="NatSGD: A Dataset with Speech, Gestures, and Demonstrations for Robot Learning in Natural Human-Robot Interaction" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02274v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02274v1.pdf filename=2403.02274v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>multimodal</b> Human-Robot Interaction (HRI) datasets have highlighted the fusion of speech and gesture, expanding robots&rsquo; capabilities to absorb explicit and implicit HRI insights. However, existing speech-gesture HRI datasets often focus on elementary tasks, like object pointing and pushing, revealing limitations in scaling to intricate domains and prioritizing human command data over robot behavior records. To bridge these gaps, we introduce NatSGD, a <b>multimodal</b> HRI dataset encompassing human commands through speech and gestures that are natural, synchronized with robot behavior demonstrations. NatSGD serves as a foundational resource at the intersection of machine learning and HRI research, and we demonstrate its effectiveness in training robots to understand tasks through <b>multimodal</b> human commands, emphasizing the significance of jointly considering speech and gestures. We have released our dataset, simulator, and code to facilitate future research in human-robot interaction system learning; access these resources at <a href=https://www.snehesh.com/natsgd/>https://www.snehesh.com/natsgd/</a></p></p class="citation"></blockquote><h3 id=1320--199309-saqiel-ultra-light-and-safe-manipulator-with-passive-3d-wire-alignment-mechanism-temma-suzuki-et-al-2024>(13/20 | 199/309) SAQIEL: Ultra-Light and Safe Manipulator with Passive 3D Wire Alignment Mechanism (Temma Suzuki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Temma Suzuki, Masahiro Bando, Kento Kawaharazuka, Kei Okada, Masayuki Inaba. (2024)<br><strong>SAQIEL: Ultra-Light and Safe Manipulator with Passive 3D Wire Alignment Mechanism</strong><br><button class=copy-to-clipboard title="SAQIEL: Ultra-Light and Safe Manipulator with Passive 3D Wire Alignment Mechanism" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01803v1.pdf filename=2403.01803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Improving the safety of collaborative manipulators necessitates the reduction of inertia in the moving part. Within this paper, we introduce a novel approach in the form of a passive 3D wire aligner, serving as a lightweight and low-friction power transmission mechanism, thus achieving the desired low inertia in the manipulator&rsquo;s operation. Through the utilization of this innovation, the consolidation of hefty actuators onto the root link becomes feasible, consequently enabling a supple drive characterized by minimal friction. To demonstrate the efficacy of this device, we fabricate an ultralight 7 degrees of freedom (DoF) manipulator named SAQIEL, boasting a mere 1.5 <b>kg</b> weight for its moving components. Notably, to mitigate friction within SAQIEL&rsquo;s actuation system, we employ a distinctive mechanism that directly winds wires using motors, obviating the need for traditional gear or belt-based speed reduction mechanisms. Through a series of empirical trials, we substantiate that SAQIEL adeptly strikes balance between lightweight design, substantial payload capacity, elevated velocity, precision, and adaptability.</p></p class="citation"></blockquote><h3 id=1420--200309-exposure-conscious-path-planning-for-equal-exposure-corridors-eugene-t-hamzezadeh-et-al-2024>(14/20 | 200/309) Exposure-Conscious Path Planning for Equal-Exposure Corridors (Eugene T. Hamzezadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eugene T. Hamzezadeh, John G. Rogers, Neil T. Dantam, Andrew J. Petruska. (2024)<br><strong>Exposure-Conscious Path Planning for Equal-Exposure Corridors</strong><br><button class=copy-to-clipboard title="Exposure-Conscious Path Planning for Equal-Exposure Corridors" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02450v1.pdf filename=2403.02450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While maximizing line-of-sight coverage of specific regions or agents in the environment is a well explored path planning objective, the converse problem of minimizing exposure to the entire environment during navigation is especially interesting in the context of minimizing detection risk. This work demonstrates that minimizing line-of-sight exposure to the environment is non-Markovian, which cannot be efficiently solved optimally with traditional path planning. The optimality gap of the <b>graph-search</b> algorithm A* and the trade-offs in optimality vs. computation time of several approximating heuristics is explored. Finally, the concept of equal-exposure corridors, which afford polynomial time determination of all paths that do not increase exposure, is presented.</p></p class="citation"></blockquote><h3 id=1520--201309-uncertainty-aware-prediction-and-application-in-planning-for-autonomous-driving-definitions-methods-and-comparison-wenbo-shao-et-al-2024>(15/20 | 201/309) Uncertainty-Aware Prediction and Application in Planning for Autonomous Driving: Definitions, Methods, and Comparison (Wenbo Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenbo Shao, Jiahui Xu, Zhong Cao, Hong Wang, Jun Li. (2024)<br><strong>Uncertainty-Aware Prediction and Application in Planning for Autonomous Driving: Definitions, Methods, and Comparison</strong><br><button class=copy-to-clipboard title="Uncertainty-Aware Prediction and Application in Planning for Autonomous Driving: Definitions, Methods, and Comparison" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02297v1.pdf filename=2403.02297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous driving systems face the formidable challenge of navigating intricate and dynamic environments with uncertainty. This study presents a unified prediction and planning framework that concurrently models short-term aleatoric uncertainty (SAU), long-term aleatoric uncertainty (LAU), and epistemic uncertainty (EU) to predict and establish a robust foundation for planning in dynamic contexts. The framework uses Gaussian mixture models and deep ensemble methods, to concurrently capture and assess SAU, LAU, and EU, where traditional methods do not integrate these uncertainties simultaneously. Additionally, uncertainty-aware planning is introduced, considering various uncertainties. The study&rsquo;s contributions include comparisons of uncertainty estimations, risk modeling, and planning methods in comparison to existing approaches. The proposed methods were rigorously evaluated using the CommonRoad <b>benchmark</b> and settings with limited perception. These experiments illuminated the advantages and roles of different uncertainty factors in autonomous driving processes. In addition, comparative assessments of various uncertainty modeling strategies underscore the benefits of modeling multiple types of uncertainties, thus enhancing planning accuracy and reliability. The proposed framework facilitates the development of methods for UAP and surpasses existing uncertainty-aware risk models, particularly when considering diverse traffic scenarios. Project page: <a href=https://swb19.github.io/UAP/>https://swb19.github.io/UAP/</a>.</p></p class="citation"></blockquote><h3 id=1620--202309-tightly-coupled-lidar-visual-inertial-slam-and-large-scale-volumetric-occupancy-mapping-simon-boche-et-al-2024>(16/20 | 202/309) Tightly-Coupled LiDAR-Visual-Inertial SLAM and Large-Scale Volumetric Occupancy Mapping (Simon Boche et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Boche, Sebastián Barbas Laina, Stefan Leutenegger. (2024)<br><strong>Tightly-Coupled LiDAR-Visual-Inertial SLAM and Large-Scale Volumetric Occupancy Mapping</strong><br><button class=copy-to-clipboard title="Tightly-Coupled LiDAR-Visual-Inertial SLAM and Large-Scale Volumetric Occupancy Mapping" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02280v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02280v1.pdf filename=2403.02280v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous navigation is one of the key requirements for every potential application of mobile robots in the real-world. Besides high-accuracy state estimation, a suitable and globally consistent representation of the 3D environment is indispensable. We present a fully tightly-coupled LiDAR-Visual-Inertial SLAM system and 3D mapping framework applying local submapping strategies to achieve scalability to large-scale environments. A novel and correspondence-free, inherently probabilistic, formulation of LiDAR residuals is introduced, expressed only in terms of the occupancy fields and its respective gradients. These residuals can be added to a factor <b>graph</b> optimisation problem, either as frame-to-map factors for the live estimates or as map-to-map factors aligning the submaps with respect to one another. Experimental validation demonstrates that the approach achieves state-of-the-art pose accuracy and furthermore produces globally consistent volumetric occupancy submaps which can be directly used in downstream tasks such as navigation or exploration.</p></p class="citation"></blockquote><h3 id=1720--203309-lista-geometric-object-based-change-detection-in-cluttered-environments-joseph-rowell-et-al-2024>(17/20 | 203/309) LiSTA: Geometric Object-Based Change Detection in Cluttered Environments (Joseph Rowell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joseph Rowell, Lintong Zhang, Maurice Fallon. (2024)<br><strong>LiSTA: Geometric Object-Based Change Detection in Cluttered Environments</strong><br><button class=copy-to-clipboard title="LiSTA: Geometric Object-Based Change Detection in Cluttered Environments" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02175v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02175v2.pdf filename=2403.02175v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present LiSTA (LiDAR Spatio-Temporal Analysis), a system to detect probabilistic object-level change over time using multi-mission SLAM. Many applications require such a system, including construction, robotic navigation, long-term autonomy, and environmental monitoring. We focus on the semi-static scenario where objects are added, subtracted, or changed in position over weeks or months. Our system combines multi-mission LiDAR SLAM, volumetric differencing, object instance description, and correspondence grouping using learned descriptors to keep track of an open set of objects. Object correspondences between missions are determined by <b>clustering</b> the object&rsquo;s learned descriptors. We demonstrate our approach using datasets collected in a simulated environment and a real-world dataset captured using a LiDAR system mounted on a quadruped robot monitoring an industrial facility containing static, semi-static, and dynamic objects. Our method demonstrates superior performance in detecting changes in semi-static environments compared to existing methods.</p></p class="citation"></blockquote><h3 id=1820--204309-skater-a-novel-bi-modal-bi-copter-robot-for-adaptive-locomotion-in-air-and-diverse-terrain-junxiao-lin-et-al-2024>(18/20 | 204/309) Skater: A Novel Bi-modal Bi-copter Robot for Adaptive Locomotion in Air and Diverse Terrain (Junxiao Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junxiao Lin, Ruibin Zhang, Neng Pan, Chao Xu, Fei Gao. (2024)<br><strong>Skater: A Novel Bi-modal Bi-copter Robot for Adaptive Locomotion in Air and Diverse Terrain</strong><br><button class=copy-to-clipboard title="Skater: A Novel Bi-modal Bi-copter Robot for Adaptive Locomotion in Air and Diverse Terrain" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01991v1.pdf filename=2403.01991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this letter, we present a novel bi-modal bi-copter robot called Skater, which is adaptable to air and various ground surfaces. Skater consists of a bi-copter moving along its longitudinal direction with two passive wheels on both sides. Using longitudinally arranged bi-copter as the unified actuation system for both aerial and ground modes, this robot not only keeps concise and lightweight mechanism, but also possesses exceptional terrain traversing capability and strong steering capacity. Moreover, leveraging the vectored thrust characteristic of bi-copters, Skater can actively generate the centripetal force needed for steering, enabling it to achieve stable movement even on slippery surfaces. Furthermore, we model the comprehensive dynamics of Skater, analyze its differential flatness and introduce a controller using nonlinear model predictive control for trajectory tracking. The outstanding performance of the system is verified by extensive real-world experiments and <b>benchmark</b> comparisons.</p></p class="citation"></blockquote><h3 id=1920--205309-tta-nav-test-time-adaptive-reconstruction-for-point-goal-navigation-under-visual-corruptions-maytus-piriyajitakonkij-et-al-2024>(19/20 | 205/309) TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation under Visual Corruptions (Maytus Piriyajitakonkij et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maytus Piriyajitakonkij, Mingfei Sun, Mengmi Zhang, Wei Pan. (2024)<br><strong>TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation under Visual Corruptions</strong><br><button class=copy-to-clipboard title="TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation under Visual Corruptions" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01977v1.pdf filename=2403.01977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robot navigation under visual corruption presents a formidable challenge. To address this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav, for point-goal navigation under visual corruptions. Our &ldquo;plug-and-play&rdquo; method incorporates a top-down decoder to a pre-trained navigation model. Firstly, the pre-trained navigation model gets a corrupted image and extracts features. Secondly, the top-down decoder produces the reconstruction given the high-level features extracted by the pre-trained model. Then, it feeds the reconstruction of a corrupted image back to the pre-trained model. Finally, the pre-trained model does forward pass again to output action. Despite being trained solely on clean images, the top-down decoder can reconstruct cleaner images from corrupted ones without the need for gradient-based adaptation. The pre-trained navigation model with our top-down decoder significantly enhances navigation performance across almost all visual corruptions in our <b>benchmarks.</b> Our method improves the success rate of point-goal navigation from the state-of-the-art result of 46% to 94% on the most severe corruption. This suggests its potential for broader application in robotic visual navigation.</p></p class="citation"></blockquote><h3 id=2020--206309-aerial-tensile-perching-and-disentangling-mechanism-for-long-term-environmental-monitoring-tian-lan-et-al-2024>(20/20 | 206/309) Aerial Tensile Perching and Disentangling Mechanism for Long-Term Environmental Monitoring (Tian Lan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Lan, Luca Romanello, Mirko Kovac, Sophie F. Armanini, Basaran Bahadir Kocer. (2024)<br><strong>Aerial Tensile Perching and Disentangling Mechanism for Long-Term Environmental Monitoring</strong><br><button class=copy-to-clipboard title="Aerial Tensile Perching and Disentangling Mechanism for Long-Term Environmental Monitoring" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01890v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01890v2.pdf filename=2403.01890v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aerial robots show significant potential for forest canopy research and environmental monitoring by providing data collection capabilities at high spatial and temporal resolutions. However, limited flight endurance hinders their application. Inspired by natural perching behaviours, we propose a <b>multi-modal</b> aerial robot system that integrates tensile perching for energy conservation and a suspended actuated pod for data collection. The system consists of a quadrotor drone, a slewing ring mechanism allowing 360{\deg} tether rotation, and a streamlined pod with two ducted propellers connected via a tether. Winding and unwinding the tether allows the pod to move within the canopy, and activating the propellers allows the tether to be wrapped around branches for perching or disentangling. We experimentally determined the minimum counterweights required for stable perching under various conditions. Building on this, we devised and evaluated multiple perching and disentangling strategies. Comparisons of perching and disentangling manoeuvres demonstrate energy savings that could be further maximized with the use of the pod or tether winding. These approaches can reduce energy consumption to only 22% and 1.5%, respectively, compared to a drone disentangling manoeuvre. We also calculated the minimum idle time required by the proposed system after the system perching and motor shut down to save energy on a mission, which is 48.9% of the operating time. Overall, the integrated system expands the operational capabilities and enhances the energy efficiency of aerial robots for long-term monitoring tasks.</p></p class="citation"></blockquote><h2 id=csai-11>cs.AI (11)</h2><h3 id=111--207309-large-language-model-based-evolutionary-optimizer-reasoning-with-elitism-shuvayan-brahmachary-et-al-2024>(1/11 | 207/309) Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism (Shuvayan Brahmachary et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuvayan Brahmachary, Subodh M. Joshi, Aniruddha Panda, Kaushik Koneripalli, Arun Kumar Sagotra, Harshil Patel, Ankush Sharma, Ameya D. Jagtap, Kaushic Kalyanaraman. (2024)<br><strong>Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism</strong><br><button class=copy-to-clipboard title="Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 58<br>Keywords: Benchmarking, Black Box, Zero-shot, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02054v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02054v1.pdf filename=2403.02054v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable <b>reasoning</b> abilities, <b>prompting</b> interest in their application as <b>black-box</b> <b>optimizers.</b> This paper asserts that <b>LLMs</b> possess the capability for <b>zero-shot</b> optimization across diverse scenarios, including multi-objective and high-dimensional problems. We introduce a novel population-based method for numerical optimization using <b>LLMs</b> called Language-Model-Based Evolutionary Optimizer (LEO). Our hypothesis is supported through numerical examples, spanning <b>benchmark</b> and industrial engineering problems such as supersonic nozzle shape optimization, heat transfer, and windfarm layout optimization. We compare our method to several gradient-based and gradient-free optimization approaches. While <b>LLMs</b> yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling. We provide practical guidelines for obtaining reliable answers from <b>LLMs</b> and discuss method limitations and potential research directions.</p></p class="citation"></blockquote><h3 id=211--208309-the-ink-splotch-effect-a-case-study-on-chatgpt-as-a-co-creative-game-designer-asad-anjum-et-al-2024>(2/11 | 208/309) The Ink Splotch Effect: A Case Study on ChatGPT as a Co-Creative Game Designer (Asad Anjum et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Asad Anjum, Yuting Li, Noelle Law, M Charity, Julian Togelius. (2024)<br><strong>The Ink Splotch Effect: A Case Study on ChatGPT as a Co-Creative Game Designer</strong><br><button class=copy-to-clipboard title="The Ink Splotch Effect: A Case Study on ChatGPT as a Co-Creative Game Designer" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 50<br>Keywords: ChatGPT, Chatbot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02454v1.pdf filename=2403.02454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies how <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can act as effective, high-level creative collaborators and ``muses&rsquo;&rsquo; for game design. We model the design of this study after the exercises artists use by looking at amorphous ink splotches for creative inspiration. Our goal is to determine whether AI-assistance can improve, hinder, or provide an alternative quality to games when compared to the creative intents implemented by human designers. The capabilities of <b>LLMs</b> as game designers are stress tested by placing it at the forefront of the decision making process. Three prototype games are designed across 3 different genres: (1) a minimalist base game, (2) a game with features and game feel elements added by a human game designer, and (3) a game with features and feel elements directly implemented from <b>prompted</b> outputs of the <b>LLM,</b> <b>ChatGPT.</b> A user study was conducted and participants were asked to blindly evaluate the quality and their preference of these games. We discuss both the development process of communicating creative intent to an AI <b>chatbot</b> and the synthesized open feedback of the participants. We use this data to determine both the benefits and shortcomings of AI in a more design-centric role.</p></p class="citation"></blockquote><h3 id=311--209309-catcode-a-comprehensive-evaluation-framework-for-llms-on-the-mixture-of-code-and-text-zhenru-lin-et-al-2024>(3/11 | 209/309) CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text (Zhenru Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenru Lin, Yiqun Yao, Yang Yuan. (2024)<br><strong>CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text</strong><br><button class=copy-to-clipboard title="CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-PL, cs.AI<br>Keyword Score: 50<br>Keywords: Automatic Evaluation, ChatGPT, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01784v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01784v1.pdf filename=2403.01784v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>ChatGPT</b> are increasingly proficient in understanding and generating a mixture of <b>code</b> <b>and</b> text. Evaluation based on such $\textit{mixture}$ can lead to a more comprehensive understanding of the models&rsquo; abilities in solving coding problems. However, in this context, current evaluation methods are either limited in task coverage or lack standardization. To address this issue, we propose using category theory as a framework for evaluation. Specifically, morphisms within a <b>code</b> <b>category</b> can represent <b>code</b> <b>debugging</b> and transformation, functors between two categories represent <b>code</b> <b>translation,</b> and functors between a <b>code</b> <b>category</b> and a natural language category represent <b>code</b> <b>generation,</b> explanation, and reproduction. We present an <b>automatic</b> <b>evaluation</b> framework called $\textbf{CatCode}$ ($\textbf{Cat}$egory $\textbf{Code}$) that can comprehensively assess the coding abilities of <b>LLMs,</b> including <b>ChatGPT,</b> Text-Davinci, and CodeGeeX.</p></p class="citation"></blockquote><h3 id=411--210309-how-multimodal-integration-boost-the-performance-of-llm-for-optimization-case-study-on-capacitated-vehicle-routing-problems-yuxiao-huang-et-al-2024>(4/11 | 210/309) How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems (Yuxiao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxiao Huang, Wenjie Zhang, Liang Feng, Xingyu Wu, Kay Chen Tan. (2024)<br><strong>How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems</strong><br><button class=copy-to-clipboard title="How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs-NE, cs.AI, math-OC<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01757v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01757v1.pdf filename=2403.01757v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have notably positioned them as capable tools for addressing complex optimization challenges. Despite this recognition, a predominant limitation of existing <b>LLM-based</b> optimization methods is their struggle to capture the relationships among decision variables when relying exclusively on numerical text <b>prompts,</b> especially in high-dimensional problems. Keeping this in mind, we first propose to enhance the optimization performance using <b>multimodal</b> <b>LLM</b> capable of processing both textual and visual <b>prompts</b> for deeper insights of the processed optimization problem. This integration allows for a more comprehensive understanding of optimization problems, akin to human cognitive processes. We have developed a <b>multimodal</b> <b>LLM-based</b> optimization framework that simulates human problem-solving workflows, thereby offering a more nuanced and effective analysis. The efficacy of this method is evaluated through extensive empirical studies focused on a well-known combinatorial optimization problem, i.e., capacitated vehicle routing problem. The results are compared against those obtained from the <b>LLM-based</b> optimization algorithms that rely solely on textual <b>prompts,</b> demonstrating the significant advantages of our <b>multimodal</b> approach.</p></p class="citation"></blockquote><h3 id=511--211309-koopman-assisted-reinforcement-learning-preston-rozwood-et-al-2024>(5/11 | 211/309) Koopman-Assisted Reinforcement Learning (Preston Rozwood et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Preston Rozwood, Edward Mehrez, Ludger Paehler, Wen Sun, Steven L. Brunton. (2024)<br><strong>Koopman-Assisted Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Koopman-Assisted Reinforcement Learning" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI, math-DS, math-OC<br>Keyword Score: 30<br>Keywords: Continuous Time, Continuous Time, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02290v1.pdf filename=2403.02290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Bellman equation and its <b>continuous</b> <b>form,</b> the Hamilton-Jacobi-Bellman (HJB) equation, are ubiquitous in <b>reinforcement</b> <b>learning</b> (RL) and control theory. However, these equations quickly become intractable for systems with high-dimensional states and nonlinearity. This paper explores the connection between the data-driven Koopman operator and Markov Decision Processes <b>(MDPs),</b> resulting in the development of two new RL algorithms to address these limitations. We leverage Koopman operator techniques to lift a nonlinear system into new coordinates where the dynamics become approximately linear, and where HJB-based methods are more tractable. In particular, the Koopman operator is able to capture the expectation of the time evolution of the value function of a given system via linear dynamics in the lifted coordinates. By parameterizing the Koopman operator with the control actions, we construct a ``Koopman tensor&rsquo;&rsquo; that facilitates the estimation of the optimal value function. Then, a transformation of Bellman&rsquo;s framework in terms of the Koopman tensor enables us to reformulate two max-entropy RL algorithms: soft value iteration and soft actor-critic (SAC). This highly flexible framework can be used for deterministic or stochastic systems as well as for discrete or <b>continuous-time</b> <b>dynamics.</b> Finally, we show that these Koopman Assisted <b>Reinforcement</b> <b>Learning</b> (KARL) algorithms attain state-of-the-art (SOTA) performance with respect to traditional neural network-based SAC and linear quadratic regulator (LQR) baselines on four controlled dynamical systems: a linear state-space system, the Lorenz system, fluid flow past a cylinder, and a double-well potential with non-isotropic stochastic forcing.</p></p class="citation"></blockquote><h3 id=611--212309-cognition-is-all-you-need----the-next-layer-of-ai-above-large-language-models-nova-spivack-et-al-2024>(6/11 | 212/309) Cognition is All You Need &ndash; The Next Layer of AI Above Large Language Models (Nova Spivack et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nova Spivack, Sam Douglas, Michelle Crames, Tim Connors. (2024)<br><strong>Cognition is All You Need &ndash; The Next Layer of AI Above Large Language Models</strong><br><button class=copy-to-clipboard title="Cognition is All You Need -- The Next Layer of AI Above Large Language Models" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-0, cs-AI, cs-MA, cs.AI<br>Keyword Score: 30<br>Keywords: Chatbot, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02164v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02164v2.pdf filename=2403.02164v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies of the applications of conversational AI tools, such as <b>chatbots</b> powered by <b>large</b> <b>language</b> <b>models,</b> to complex real-world knowledge work have shown limitations related to <b>reasoning</b> and multi-step problem solving. Specifically, while existing <b>chatbots</b> simulate shallow <b>reasoning</b> and understanding they are prone to errors as problem complexity increases. The failure of these systems to address complex knowledge work is due to the fact that they do not perform any actual cognition. In this position paper, we present Cognitive AI, a higher-level framework for implementing programmatically defined neuro-symbolic cognition above and outside of <b>large</b> <b>language</b> <b>models.</b> Specifically, we propose a dual-layer functional architecture for Cognitive AI that serves as a roadmap for AI systems that can perform complex multi-step knowledge work. We propose that Cognitive AI is a necessary precursor for the evolution of higher forms of AI, such as AGI, and specifically claim that AGI cannot be achieved by probabilistic approaches on their own. We conclude with a discussion of the implications for <b>large</b> <b>language</b> <b>models,</b> adoption cycles in AI, and commercial Cognitive AI development.</p></p class="citation"></blockquote><h3 id=711--213309-a-scoping-review-of-energy-efficient-driving-behaviors-and-applied-state-of-the-art-ai-methods-zhipeng-ma-et-al-2024>(7/11 | 213/309) A Scoping Review of Energy-Efficient Driving Behaviors and Applied State-of-the-Art AI Methods (Zhipeng Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhipeng Ma, Bo Nørregaard Jørgensen, Zheng Ma. (2024)<br><strong>A Scoping Review of Energy-Efficient Driving Behaviors and Applied State-of-the-Art AI Methods</strong><br><button class=copy-to-clipboard title="A Scoping Review of Energy-Efficient Driving Behaviors and Applied State-of-the-Art AI Methods" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Recommendation, Reinforcement Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02053v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02053v1.pdf filename=2403.02053v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The transportation sector remains a major contributor to greenhouse gas emissions. The understanding of energy-efficient driving behaviors and utilization of energy-efficient driving strategies are essential to reduce vehicles&rsquo; fuel consumption. However, there is no comprehensive investigation into energy-efficient driving behaviors and strategies. Furthermore, many state-of-the-art AI models have been applied for the analysis of eco-friendly driving styles, but no overview is available. To fill the gap, this paper conducts a thorough literature review on ecological driving behaviors and styles and analyzes the driving factors influencing energy consumption and state-of-the-art methodologies. With a thorough scoping review process, the methodological and related data are compared. The results show that the factors that impact driving behaviors can be <b>summarized</b> into eleven features including speed, acceleration, deceleration, pedal, and so on. This paper finds that supervised/unsupervised learning algorithms and <b>reinforcement</b> <b>learning</b> frameworks have been popularly used to model the vehicle&rsquo;s energy consumption with multi-dimensional data. Furthermore, the literature shows that the driving data are collected from either simulators or real-world experiments, and the real-world data are mainly stored and transmitted by meters, controller area networks, onboard data services, smartphones, and additional sensors installed in the vehicle. Based on driving behavior factors, driver characteristics, and safety rules, this paper recommends nine energy-efficient driving styles including four guidelines for the drivers&rsquo; selection and adjustment of the vehicle parameters, three <b>recommendations</b> for the energy-efficient driving styles in different driving scenarios, and two subjective suggestions for different types of drivers and employers.</p></p class="citation"></blockquote><h3 id=811--214309-transformer-for-times-series-an-application-to-the-sp500-pierre-brugiere-et-al-2024>(8/11 | 214/309) Transformer for Times Series: an Application to the S&amp;P500 (Pierre Brugiere et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Brugiere, Gabriel Turinici. (2024)<br><strong>Transformer for Times Series: an Application to the S&amp;P500</strong><br><button class=copy-to-clipboard title="Transformer for Times Series: an Application to the S&P500" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI, q-fin-PM, q-fin-ST, stat-ML<br>Keyword Score: 20<br>Keywords: Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02523v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02523v1.pdf filename=2403.02523v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>transformer</b> models have been extensively used with good results in a wide area of machine learning applications including <b>Large</b> <b>Language</b> <b>Models</b> and image generation. Here, we inquire on the applicability of this approach to financial time series. We first describe the dataset construction for two prototypical situations: a mean reverting synthetic Ornstein-Uhlenbeck process on one hand and real S&amp;P500 data on the other hand. Then, we present in detail the proposed <b>Transformer</b> architecture and finally we discuss some encouraging results. For the synthetic data we predict rather accurately the next move, and for the S&amp;P500 we get some interesting results related to quadratic variation and volatility prediction.</p></p class="citation"></blockquote><h3 id=911--215309-smaug-a-sliding-multidimensional-task-window-based-marl-framework-for-adaptive-real-time-subtask-recognition-wenjing-zhang-et-al-2024>(9/11 | 215/309) SMAUG: A Sliding Multidimensional Task Window-Based MARL Framework for Adaptive Real-Time Subtask Recognition (Wenjing Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenjing Zhang, Wei Zhang. (2024)<br><strong>SMAUG: A Sliding Multidimensional Task Window-Based MARL Framework for Adaptive Real-Time Subtask Recognition</strong><br><button class=copy-to-clipboard title="SMAUG: A Sliding Multidimensional Task Window-Based MARL Framework for Adaptive Real-Time Subtask Recognition" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-MA, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01816v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01816v1.pdf filename=2403.01816v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Instead of making behavioral decisions directly from the exponentially expanding joint observational-action space, subtask-based multi-agent <b>reinforcement</b> <b>learning</b> (MARL) methods enable agents to learn how to tackle different subtasks. Most existing subtask-based MARL methods are based on hierarchical <b>reinforcement</b> <b>learning</b> (HRL). However, these approaches often limit the number of subtasks, perform subtask recognition periodically, and can only identify and execute a specific subtask within the predefined fixed time period, which makes them inflexible and not suitable for diverse and dynamic scenarios with constantly changing subtasks. To break through above restrictions, a \textbf{S}liding \textbf{M}ultidimensional t\textbf{A}sk window based m\textbf{U}ti-agent <b>reinforcement</b> <b>learnin\textbf{G}</b> framework (SMAUG) is proposed for adaptive real-time subtask recognition. It leverages a sliding multidimensional task window to extract essential information of subtasks from trajectory segments concatenated based on observed and predicted trajectories in varying lengths. An inference network is designed to iteratively predict future trajectories with the subtask-oriented policy network. Furthermore, intrinsic motivation rewards are defined to promote subtask exploration and behavior diversity. SMAUG can be integrated with any Q-learning-based approach. Experiments on StarCraft II show that SMAUG not only demonstrates performance superiority in comparison with all baselines but also presents a more prominent and swift rise in rewards during the initial training stage.</p></p class="citation"></blockquote><h3 id=1011--216309-fast-benchmarking-of-asynchronous-multi-fidelity-optimization-on-zero-cost-benchmarks-shuhei-watanabe-et-al-2024>(10/11 | 216/309) Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks (Shuhei Watanabe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuhei Watanabe, Neeratyoy Mallik, Edward Bergman, Frank Hutter. (2024)<br><strong>Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks</strong><br><button class=copy-to-clipboard title="Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01888v1.pdf filename=2403.01888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While deep learning has celebrated many successes, its results often hinge on the meticulous selection of hyperparameters (HPs). However, the time-consuming nature of deep learning training makes HP optimization (HPO) a costly endeavor, slowing down the development of efficient HPO tools. While zero-cost <b>benchmarks,</b> which provide performance and runtime without actual training, offer a solution for non-parallel setups, they fall short in parallel setups as each worker must communicate its queried runtime to return its evaluation in the exact order. This work addresses this challenge by introducing a user-friendly Python package that facilitates efficient parallel HPO with zero-cost <b>benchmarks.</b> Our approach calculates the exact return order based on the information stored in file system, eliminating the need for long waiting times and enabling much faster HPO evaluations. We first verify the correctness of our approach through extensive testing and the experiments with 6 popular HPO libraries show its applicability to diverse libraries and its ability to achieve over 1000x speedup compared to a traditional approach. Our package can be installed via pip install mfhpo-simulator.</p></p class="citation"></blockquote><h3 id=1111--217309-morbdd-multiobjective-restricted-binary-decision-diagrams-by-learning-to-sparsify-rahul-patel-et-al-2024>(11/11 | 217/309) MORBDD: Multiobjective Restricted Binary Decision Diagrams by Learning to Sparsify (Rahul Patel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rahul Patel, Elias B. Khalil, David Bergman. (2024)<br><strong>MORBDD: Multiobjective Restricted Binary Decision Diagrams by Learning to Sparsify</strong><br><button class=copy-to-clipboard title="MORBDD: Multiobjective Restricted Binary Decision Diagrams by Learning to Sparsify" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02482v1.pdf filename=2403.02482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In multicriteria decision-making, a user seeks a set of non-dominated solutions to a (constrained) multiobjective optimization problem, the so-called Pareto frontier. In this work, we seek to bring a state-of-the-art method for exact multiobjective integer linear programming into the heuristic realm. We focus on binary decision diagrams (BDDs) which first construct a <b>graph</b> that represents all feasible solutions to the problem and then traverse the <b>graph</b> to extract the Pareto frontier. Because the Pareto frontier may be exponentially large, enumerating it over the BDD can be time-consuming. We explore how restricted BDDs, which have already been shown to be effective as heuristics for single-objective problems, can be adapted to multiobjective optimization through the use of machine learning (ML). MORBDD, our ML-based BDD sparsifier, first trains a binary classifier to eliminate BDD nodes that are unlikely to contribute to Pareto solutions, then post-processes the sparse BDD to ensure its connectivity via optimization. Experimental results on multiobjective knapsack problems show that MORBDD is highly effective at producing very small restricted BDDs with excellent approximation quality, outperforming width-limited restricted BDDs and the well-known evolutionary algorithm NSGA-II.</p></p class="citation"></blockquote><h2 id=cscr-8>cs.CR (8)</h2><h3 id=18--218309-unveiling-hidden-links-between-unseen-security-entities-daniel-alfasi-et-al-2024>(1/8 | 218/309) Unveiling Hidden Links Between Unseen Security Entities (Daniel Alfasi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Alfasi, Tal Shapira, Anat Bremler Barr. (2024)<br><strong>Unveiling Hidden Links Between Unseen Security Entities</strong><br><button class=copy-to-clipboard title="Unveiling Hidden Links Between Unseen Security Entities" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 51<br>Keywords: Graph, Foundation Model, Knowledge Graph, Knowledge Graph, Multi-modal, Representation Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02014v1.pdf filename=2403.02014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of software vulnerabilities poses a significant challenge for security databases and analysts tasked with their timely identification, classification, and remediation. With the National Vulnerability Database (NVD) reporting an ever-increasing number of vulnerabilities, the traditional manual analysis becomes untenably time-consuming and prone to errors. This paper introduces VulnScopper, an innovative approach that utilizes <b>multi-modal</b> <b>representation</b> <b>learning,</b> combining <b>Knowledge</b> <b>Graphs</b> <b>(KG)</b> and Natural Language Processing (NLP), to automate and enhance the analysis of software vulnerabilities. Leveraging ULTRA, a <b>knowledge</b> <b>graph</b> <b>foundation</b> <b>model,</b> combined with a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM),</b> VulnScopper effectively handles unseen entities, overcoming the limitations of previous <b>KG</b> approaches. We evaluate VulnScopper on two major security datasets, the NVD and the Red Hat CVE database. Our method significantly improves the link prediction accuracy between Common Vulnerabilities and Exposures (CVEs), Common Weakness Enumeration (CWEs), and Common Platform Enumerations (CPEs). Our results show that VulnScopper outperforms existing methods, achieving up to 78% Hits@10 accuracy in linking CVEs to CPEs and CWEs and presenting an 11.7% improvement over <b>large</b> <b>language</b> <b>models</b> in predicting CWE labels based on the Red Hat database. Based on the NVD, only 6.37% of the linked CPEs are being published during the first 30 days; many of them are related to critical and high-risk vulnerabilities which, according to multiple compliance frameworks (such as CISA and PCI), should be remediated within 15-30 days. Our model can uncover new products linked to vulnerabilities, reducing remediation time and improving vulnerability management. We analyzed several CVEs from 2023 to showcase this ability.</p></p class="citation"></blockquote><h3 id=28--219309-knowphish-large-language-models-meet-multimodal-knowledge-graphs-for-enhancing-reference-based-phishing-detection-yuexin-li-et-al-2024>(2/8 | 219/309) KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection (Yuexin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuexin Li, Chengyu Huang, Shumin Deng, Mei Lin Lock, Tri Cao, Nay Oo, Bryan Hooi, Hoon Wei Lim. (2024)<br><strong>KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection</strong><br><button class=copy-to-clipboard title="KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 34<br>Keywords: Graph, Knowledge Graph, Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02253v1.pdf filename=2403.02253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Phishing attacks have inflicted substantial losses on individuals and businesses alike, necessitating the development of robust and efficient automated phishing detection approaches. Reference-based phishing detectors (RBPDs), which compare the logos on a target webpage to a known set of logos, have emerged as the state-of-the-art approach. However, a major limitation of existing RBPDs is that they rely on a manually constructed brand <b>knowledge</b> <b>base,</b> making it infeasible to scale to a <b>large</b> <b>number</b> <b>of</b> brands, which results in false negative errors due to the insufficient brand coverage of the <b>knowledge</b> <b>base.</b> To address this issue, we propose an automated <b>knowledge</b> <b>collection</b> pipeline, using which we collect and release a <b>large-scale</b> <b>multimodal</b> <b>brand</b> <b>knowledge</b> <b>base,</b> KnowPhish, containing 20k brands with rich information about each brand. KnowPhish can be used to boost the performance of existing RBPDs in a plug-and-play manner. A second limitation of existing RBPDs is that they solely rely on the image modality, ignoring useful textual information present in the webpage HTML. To utilize this textual information, we propose a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)-based</b> approach to extract brand information of webpages from text. Our resulting <b>multimodal</b> phishing detection approach, KnowPhish Detector (KPD), can detect phishing webpages with or without logos. We evaluate KnowPhish and KPD on a manually validated dataset, and on a field study under Singapore&rsquo;s local context, showing substantial improvements in effectiveness and efficiency compared to state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=38--220309-malignnoma-gnn-based-malicious-circuit-classifier-for-secure-cloud-fpgas-lilas-alrahis-et-al-2024>(3/8 | 220/309) MaliGNNoma: GNN-Based Malicious Circuit Classifier for Secure Cloud FPGAs (Lilas Alrahis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lilas Alrahis, Hassan Nassar, Jonas Krautter, Dennis Gnad, Lars Bauer, Jorg Henkel, Mehdi Tahoori. (2024)<br><strong>MaliGNNoma: GNN-Based Malicious Circuit Classifier for Secure Cloud FPGAs</strong><br><button class=copy-to-clipboard title="MaliGNNoma: GNN-Based Malicious Circuit Classifier for Secure Cloud FPGAs" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01860v1.pdf filename=2403.01860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The security of cloud field-programmable gate arrays (FPGAs) faces challenges from untrusted users attempting fault and side-channel attacks through malicious circuit configurations. Fault injection attacks can result in denial of service, disrupting functionality or leaking secret information. This threat is further amplified in multi-tenancy scenarios. Detecting such threats before loading onto the FPGA is crucial, but existing methods face difficulty identifying sophisticated attacks. We present MaliGNNoma, a machine learning-based solution that accurately identifies malicious FPGA configurations. Serving as a netlist scanning mechanism, it can be employed by cloud service providers as an initial security layer within a necessary multi-tiered security system. By leveraging the inherent <b>graph</b> <b>representation</b> <b>of</b> FPGA netlists, MaliGNNoma employs a <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> to learn distinctive malicious features, surpassing current approaches. To enhance transparency, MaliGNNoma utilizes a parameterized explainer for the <b>GNN,</b> labeling the FPGA configuration and pinpointing the sub-circuit responsible for the malicious classification. Through extensive experimentation on the ZCU102 board with a Xilinx UltraScale+ FPGA, we validate the effectiveness of MaliGNNoma in detecting malicious configurations, including sophisticated attacks, such as those based on benign modules, like cryptography accelerators. MaliGNNoma achieves a classification accuracy and precision of 98.24% and 97.88%, respectively, surpassing state-of-the-art. We compare MaliGNNoma with five state-of-the-art scanning methods, revealing that not all attack vectors detected by MaliGNNoma are recognized by existing solutions, further emphasizing its effectiveness. Additionally, we make MaliGNNoma and its associated dataset publicly available.</p></p class="citation"></blockquote><h3 id=48--221309-k-stars-ldp-a-novel-framework-for-p-q-clique-enumeration-under-local-differential-privacy-henan-sun-et-al-2024>(4/8 | 221/309) K-stars LDP: A Novel Framework for (p, q)-clique Enumeration under Local Differential Privacy (Henan Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Henan Sun, Zhengyu Wu, Rong-Hua Li, Guoren Wang, Zening Li. (2024)<br><strong>K-stars LDP: A Novel Framework for (p, q)-clique Enumeration under Local Differential Privacy</strong><br><button class=copy-to-clipboard title="K-stars LDP: A Novel Framework for (p, q)-clique Enumeration under Local Differential Privacy" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CY, cs-DS, cs.CR<br>Keyword Score: 16<br>Keywords: Graph, Clustering, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01788v1.pdf filename=2403.01788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>(p,q)-clique enumeration on a bipartite <b>graph</b> is critical for calculating <b>clustering</b> coefficient and detecting densest subgraph. It is necessary to carry out subgraph enumeration while protecting users&rsquo; privacy from any potential attacker as the count of subgraph may contain sensitive information. Most recent studies focus on the privacy protection algorithms based on edge LDP (Local <b>Differential</b> <b>Privacy).</b> However, these algorithms suffer a large estimation error due to the great amount of required noise. In this paper, we propose a novel idea of k-stars LDP and a novel k-stars LDP algorithm for (p, q)-clique enumeration with a small estimation error, where a k-stars is a star-shaped <b>graph</b> with k nodes connecting to one node. The effectiveness of edge LDP relies on its capacity to obfuscate the existence of an edge between the user and his one-hop neighbors. This is based on the premise that a user should be aware of the existence of his one-hop neighbors. Similarly, we can apply this premise to k-stars as well, where an edge is a specific genre of 1-stars. Based on this fact, we first propose the k-stars neighboring list to enable our algorithm to obfuscate the existence of k-stars with Warner&rsquo; s RR. Then, we propose the absolute value correction technique and the k-stars sampling technique to further reduce the estimation error. Finally, with the two-round user-collector interaction mechanism, we propose our k-stars LDP algorithm to count the number of (p, q)-clique while successfully protecting users&rsquo; privacy. Both the theoretical analysis and experiments have showed the superiority of our algorithm over the algorithms based on edge LDP.</p></p class="citation"></blockquote><h3 id=58--222309-comprehensive-evaluation-of-mal-api-2019-dataset-by-machine-learning-in-malware-detection-zhenglin-li-et-al-2024>(5/8 | 222/309) Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection (Zhenglin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenglin Li, Haibei Zhu, Houze Liu, Jintong Song, Qishuo Cheng. (2024)<br><strong>Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection</strong><br><button class=copy-to-clipboard title="Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: TF-IDF<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02232v1.pdf filename=2403.02232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study conducts a thorough examination of malware detection using machine learning techniques, focusing on the evaluation of various classification models using the Mal-API-2019 dataset. The aim is to advance cybersecurity capabilities by identifying and mitigating threats more effectively. Both ensemble and non-ensemble machine learning methods, such as Random Forest, XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special emphasis is placed on the importance of data pre-processing techniques, particularly <b>TF-IDF</b> representation and Principal Component Analysis, in improving model performance. Results indicate that ensemble methods, particularly Random Forest and XGBoost, exhibit superior accuracy, precision, and recall compared to others, highlighting their effectiveness in malware detection. The paper also discusses limitations and potential future directions, emphasizing the need for continuous adaptation to address the evolving nature of malware. This research contributes to ongoing discussions in cybersecurity and provides practical insights for developing more robust malware detection systems in the digital era.</p></p class="citation"></blockquote><h3 id=68--223309-mts-bringing-multi-tenancy-to-virtual-networking-kashyap-thimmaraju-et-al-2024>(6/8 | 223/309) MTS: Bringing Multi-Tenancy to Virtual Networking (Kashyap Thimmaraju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kashyap Thimmaraju, Saad Hermak, Gábor Rétvári, Stefan Schmid. (2024)<br><strong>MTS: Bringing Multi-Tenancy to Virtual Networking</strong><br><button class=copy-to-clipboard title="MTS: Bringing Multi-Tenancy to Virtual Networking" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-NI, cs.CR<br>Keyword Score: 10<br>Keywords: Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01862v1.pdf filename=2403.01862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-tenant cloud computing provides great benefits in terms of resource sharing, elastic pricing, and scalability, however, it also changes the security landscape and introduces the need for strong isolation between the tenants, also inside the network. This paper is motivated by the observation that while multi-tenancy is widely used in cloud computing, the virtual switch designs currently used for network virtualization lack sufficient support for tenant isolation. Hence, we present, implement, and evaluate a virtual switch architecture, <b>MTS,</b> which brings secure design best-practice to the context of multi-tenant virtual networking: compartmentalization of virtual switches, least-privilege execution, complete mediation of all network communication, and reducing the trusted computing base shared between tenants. We build <b>MTS</b> from commodity components, providing an incrementally deployable and inexpensive upgrade path to cloud operators. Our extensive experiments, extending to both micro-benchmarks and cloud applications, show that, depending on the way it is deployed, <b>MTS</b> may produce 1.5-2x the throughput compared to state-of-the-art, with similar or better latency and modest resource overhead (1 extra CPU). <b>MTS</b> is available as open source software.</p></p class="citation"></blockquote><h3 id=78--224309-deployment-challenges-of-industrial-intrusion-detection-systems-konrad-wolsing-et-al-2024>(7/8 | 224/309) Deployment Challenges of Industrial Intrusion Detection Systems (Konrad Wolsing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Konrad Wolsing, Eric Wagner, Frederik Basels, Patrick Wagner, Klaus Wehrle. (2024)<br><strong>Deployment Challenges of Industrial Intrusion Detection Systems</strong><br><button class=copy-to-clipboard title="Deployment Challenges of Industrial Intrusion Detection Systems" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01809v1.pdf filename=2403.01809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the escalating threats posed by cyberattacks on Industrial Control Systems (ICSs), the development of customized Industrial Intrusion Detection Systems (IIDSs) received significant attention in research. While existing literature proposes effective IIDS solutions evaluated in controlled environments, their deployment in real-world industrial settings poses several challenges. This paper highlights two critical yet often overlooked aspects that significantly impact their practical deployment, i.e., the need for sufficient amounts of data to train the IIDS models and the challenges associated with finding suitable hyperparameters, especially for IIDSs training only on genuine ICS data. Through empirical experiments conducted on multiple state-of-the-art IIDSs and diverse datasets, we establish the criticality of these issues in deploying IIDSs. Our findings show the necessity of extensive malicious training data for <b>supervised</b> IIDSs, which can be impractical considering the complexity of recording and labeling attacks in actual industrial environments. Furthermore, while other IIDSs circumvent the previous issue by requiring only benign training data, these can suffer from the difficulty of setting appropriate hyperparameters, which likewise can diminish their performance. By shedding light on these challenges, we aim to enhance the understanding of the limitations and considerations necessary for deploying effective cybersecurity solutions in ICSs, which might be one reason why IIDSs see few deployments.</p></p class="citation"></blockquote><h3 id=88--225309-i-just-hated-it-and-i-want-my-money-back-data-driven-understanding-of-mobile-vpn-service-switching-preferences-in-the-wild-rohit-raj-et-al-2024>(8/8 | 225/309) &lsquo;I just hated it and I want my money back&rsquo;: Data-driven Understanding of Mobile VPN Service Switching Preferences in The Wild (Rohit Raj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohit Raj, Mridul Newar, Mainack Mondal. (2024)<br><strong>&lsquo;I just hated it and I want my money back&rsquo;: Data-driven Understanding of Mobile VPN Service Switching Preferences in The Wild</strong><br><button class=copy-to-clipboard title="'I just hated it and I want my money back': Data-driven Understanding of Mobile VPN Service Switching Preferences in The Wild" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-HC, cs.CR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01648v1.pdf filename=2403.01648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Virtual Private Networks (VPNs) are a crucial Privacy-Enhancing Technology (PET) leveraged by millions of users and catered by multiple VPN providers worldwide; thus, understanding the user preferences for the choice of VPN apps should be of importance and interest to the security community. To that end, prior studies looked into the usage, awareness and adoption of VPN users and the perceptions of providers. However, no study so far has looked into the user preferences and underlying reasons for switching among VPN providers and identified features that presumably enhance users&rsquo; VPN experience. This work aims to bridge this gap and shed light on the underlying factors that drive existing users when they switch from one VPN to another. In this work, we analyzed over 1.3 million reviews from 20 leading VPN apps, identifying 1,305 explicit mentions and intents to switch. Our NLP-based analysis unveiled distinct clusters of factors motivating users to switch. An examination of 376 blogs from six popular VPN <b>recommendation</b> sites revealed biases in the content, and we found ignorance towards user preferences. We conclude by identifying the key implications of our work for different stakeholders. The data and code for this work is available at <a href=https://github.com/Mainack/switch-vpn-datacode-sec24>https://github.com/Mainack/switch-vpn-datacode-sec24</a>.</p></p class="citation"></blockquote><h2 id=csni-4>cs.NI (4)</h2><h3 id=14--226309-towards-intent-based-network-management-large-language-models-for-intent-extraction-in-5g-core-networks-dimitrios-michael-manias-et-al-2024>(1/4 | 226/309) Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks (Dimitrios Michael Manias et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitrios Michael Manias, Ali Chouman, Abdallah Shami. (2024)<br><strong>Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks</strong><br><button class=copy-to-clipboard title="Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-NI, cs.NI<br>Keyword Score: 40<br>Keywords: Human Intervention, Intent Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02238v1.pdf filename=2403.02238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of Machine Learning and Artificial Intelligence (ML/AI) into fifth-generation (5G) networks has made evident the limitations of network intelligence with ever-increasing, strenuous requirements for current and next-generation devices. This transition to ubiquitous intelligence demands high connectivity, synchronicity, and end-to-end communication between users and network operators, and will pave the way towards full network automation without <b>human</b> <b>intervention.</b> <b>Intent-based</b> <b>networking</b> is a key factor in the reduction of <b>human</b> <b>actions,</b> roles, and responsibilities while shifting towards novel extraction and interpretation of automated network management. This paper presents the development of a custom <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> for 5G and next-generation <b>intent-based</b> <b>networking</b> and provides insights into future <b>LLM</b> developments and integrations to realize end-to-end <b>intent-based</b> <b>networking</b> for fully automated network intelligence.</p></p class="citation"></blockquote><h3 id=24--227309-graph-neural-network-for-in-network-placement-of-real-time-metaverse-tasks-in-next-generation-network-sulaiman-muhammad-rashid-et-al-2024>(2/4 | 227/309) Graph neural network for in-network placement of real-time metaverse tasks in next-generation network (Sulaiman Muhammad Rashid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sulaiman Muhammad Rashid, Ibrahim Aliyu, Il-Kwon Jeong, Tai-Won Um, Jinsul Kim. (2024)<br><strong>Graph neural network for in-network placement of real-time metaverse tasks in next-generation network</strong><br><button class=copy-to-clipboard title="Graph neural network for in-network placement of real-time metaverse tasks in next-generation network" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-DC, cs-NI, cs.NI<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01780v1.pdf filename=2403.01780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study addresses the challenge of real-time metaverse applications by proposing an in-network placement and task-offloading solution for delay-constrained computing tasks in next-generation networks. The metaverse, envisioned as a parallel virtual world, requires seamless real-time experiences across diverse applications. The study introduces a software-defined networking (SDN)-based architecture and employs <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> techniques for intelligent and adaptive task allocation in in-network computing (INC). Considering time constraints and computing capabilities, the proposed model optimally decides whether to offload rendering tasks to INC nodes or edge server. Extensive experiments demonstrate the superior performance of the proposed <b>GNN</b> model, achieving 97% accuracy compared to 72% for multilayer perceptron (MLP) and 70% for decision trees (DTs). The study fills the research gap in in-network placement for real-time metaverse applications, offering insights into efficient rendering task handling.</p></p class="citation"></blockquote><h3 id=34--228309-towards-fair-and-efficient-learning-based-congestion-control-xudong-liao-et-al-2024>(3/4 | 228/309) Towards Fair and Efficient Learning-based Congestion Control (Xudong Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xudong Liao, Han Tian, Chaoliang Zeng, Xinchen Wan, Kai Chen. (2024)<br><strong>Towards Fair and Efficient Learning-based Congestion Control</strong><br><button class=copy-to-clipboard title="Towards Fair and Efficient Learning-based Congestion Control" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-LG, cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Fairness, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01798v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01798v1.pdf filename=2403.01798v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have witnessed a plethora of learning-based solutions for congestion control (CC) that demonstrate better performance over traditional TCP schemes. However, they fail to provide consistently good convergence properties, including {\em fairness}, {\em fast convergence} and {\em stability}, due to the mismatch between their objective functions and these properties. Despite being intuitive, integrating these properties into existing learning-based CC is challenging, because: 1) their training environments are designed for the performance optimization of single flow but incapable of cooperative multi-flow optimization, and 2) there is no directly measurable metric to represent these properties into the training objective function. We present Astraea, a new learning-based congestion control that ensures fast convergence to <b>fairness</b> with stability. At the heart of Astraea is a multi-agent deep <b>reinforcement</b> <b>learning</b> framework that explicitly optimizes these convergence properties during the training process by enabling the learning of interactive policy between multiple competing flows, while maintaining high performance. We further build a faithful multi-flow environment that emulates the competing behaviors of concurrent flows, explicitly expressing convergence properties to enable their optimization during training. We have fully implemented Astraea and our comprehensive experiments show that Astraea can quickly converge to <b>fairness</b> point and exhibit better stability than its counterparts. For example, \sys achieves near-optimal bandwidth sharing (i.e., <b>fairness)</b> when multiple flows compete for the same bottleneck, delivers up to 8.4$\times$ faster convergence speed and 2.8$\times$ smaller throughput deviation, while achieving comparable or even better performance over prior solutions.</p></p class="citation"></blockquote><h3 id=44--229309-probabilistic-fault-tolerant-robust-traffic-grooming-in-otn-over-dwdm-networks-dimitrios-michael-manias-et-al-2024>(4/4 | 229/309) Probabilistic Fault-Tolerant Robust Traffic Grooming in OTN-over-DWDM Networks (Dimitrios Michael Manias et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitrios Michael Manias, Joe Naoum-Sawaya, Abbas Javadtalab, Abdallah Shami. (2024)<br><strong>Probabilistic Fault-Tolerant Robust Traffic Grooming in OTN-over-DWDM Networks</strong><br><button class=copy-to-clipboard title="Probabilistic Fault-Tolerant Robust Traffic Grooming in OTN-over-DWDM Networks" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02254v1.pdf filename=2403.02254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of next-generation networks is revolutionizing network operators&rsquo; management and orchestration practices worldwide. The critical services supported by these networks require increasingly stringent performance requirements, especially when considering the aspect of network reliability. This increase in reliability, coupled with the mass generation and consumption of information <b>stemming</b> from the increasing complexity of the network and the integration of artificial intelligence agents, affects transport networks, which will be required to allow the feasibility of such services to materialize. To this end, traditional recovery schemes are inadequate to ensure the resilience requirements of next-generation critical services given the increasingly dynamic nature of the network. The work presented in this paper proposes a probabilistic and fault-tolerant robust traffic grooming model for OTN-over-DWDM networks. The model&rsquo;s parameterization gives network operators the ability to control the level of protection and reliability required to meet their quality of service and service level agreement guarantees. The results demonstrate that the robust solution can ensure fault tolerance even in the face of demand uncertainty without service disruptions and the need for reactive network maintenance.</p></p class="citation"></blockquote><h2 id=astro-phco-1>astro-ph.CO (1)</h2><h3 id=11--230309-predicting-large-scale-cosmological-structure-evolution-with-gan-based-autoencoders-marion-ullmo-et-al-2024>(1/1 | 230/309) Predicting large scale cosmological structure evolution with GAN-based autoencoders (Marion Ullmo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marion Ullmo, Nabila Aghnim, Aurélien Decelle, Miguel Aragon-Calvo. (2024)<br><strong>Predicting large scale cosmological structure evolution with GAN-based autoencoders</strong><br><button class=copy-to-clipboard title="Predicting large scale cosmological structure evolution with GAN-based autoencoders" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.CO<br>Categories: astro-ph-CO, astro-ph.CO, cs-LG<br>Keyword Score: 40<br>Keywords: Autoencoder, Generative Adversarial Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02171v1.pdf filename=2403.02171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cosmological <b>simulations</b> play a key role in the prediction and understanding of large scale structure formation from initial conditions. We make use of <b>GAN-based</b> <b>Autoencoders</b> (AEs) in an attempt to predict structure evolution within <b>simulations.</b> The AEs are trained on images and cubes issued from respectively 2D and 3D N-body <b>simulations</b> describing the evolution of the dark matter (DM) field. We find that while the AEs can predict structure evolution for 2D <b>simulations</b> of DM fields well, using only the density fields as input, they perform significantly more poorly in similar conditions for 3D <b>simulations.</b> However, additionally providing velocity fields as inputs greatly improves results, with similar predictions regardless of time-difference between input and target.</p></p class="citation"></blockquote><h2 id=cssd-4>cs.SD (4)</h2><h3 id=14--231309-sa-sot-speaker-aware-serialized-output-training-for-multi-talker-asr-zhiyun-fan-et-al-2024>(1/4 | 231/309) SA-SOT: Speaker-Aware Serialized Output Training for Multi-Talker ASR (Zhiyun Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyun Fan, Linhao Dong, Jun Zhang, Lu Lu, Zejun Ma. (2024)<br><strong>SA-SOT: Speaker-Aware Serialized Output Training for Multi-Talker ASR</strong><br><button class=copy-to-clipboard title="SA-SOT: Speaker-Aware Serialized Output Training for Multi-Talker ASR" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 40<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02010v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02010v1.pdf filename=2403.02010v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-talker <b>automatic</b> <b>speech</b> <b>recognition</b> plays a crucial role in scenarios involving multi-party interactions, such as meetings and conversations. Due to its inherent complexity, this task has been receiving increasing attention. Notably, the serialized output training (SOT) stands out among various approaches because of its simplistic architecture and exceptional performance. However, the frequent speaker changes in token-level SOT (t-SOT) present challenges for the autoregressive decoder in effectively utilizing context to predict output sequences. To address this issue, we introduce a masked t-SOT label, which serves as the cornerstone of an auxiliary training loss. Additionally, we utilize a speaker similarity matrix to refine the <b>self-attention</b> mechanism of the decoder. This strategic adjustment enhances contextual relationships within the same speaker&rsquo;s tokens while minimizing interactions between different speakers&rsquo; tokens. We denote our method as speaker-aware SOT (SA-SOT). Experiments on the Librispeech datasets demonstrate that our SA-SOT obtains a relative cpWER reduction ranging from 12.75% to 22.03% on the multi-talker test sets. Furthermore, with more extensive training, our method achieves an impressive cpWER of 3.41%, establishing a new state-of-the-art result on the LibrispeechMix dataset.</p></p class="citation"></blockquote><h3 id=24--232309-robust-wake-word-spotting-with-frame-level-cross-modal-attention-based-audio-visual-conformer-haoxu-wang-et-al-2024>(2/4 | 232/309) Robust Wake Word Spotting With Frame-Level Cross-Modal Attention Based Audio-Visual Conformer (Haoxu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxu Wang, Ming Cheng, Qiang Fu, Ming Li. (2024)<br><strong>Robust Wake Word Spotting With Frame-Level Cross-Modal Attention Based Audio-Visual Conformer</strong><br><button class=copy-to-clipboard title="Robust Wake Word Spotting With Frame-Level Cross-Modal Attention Based Audio-Visual Conformer" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-MM, cs-SD, cs.SD, eess-AS<br>Keyword Score: 13<br>Keywords: Fine-tuning, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01700v1.pdf filename=2403.01700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, neural network-based Wake Word Spotting achieves good performance on clean audio samples but struggles in noisy environments. Audio-Visual Wake Word Spotting (AVWWS) receives lots of attention because visual lip movement information is not affected by complex acoustic scenes. Previous works usually use simple addition or concatenation for <b>multi-modal</b> fusion. The inter-modal correlation remains relatively under-explored. In this paper, we propose a novel module called Frame-Level Cross-Modal Attention (FLCMA) to improve the performance of AVWWS systems. This module can help model <b>multi-modal</b> information at the frame-level through synchronous lip movements and speech signals. We train the end-to-end FLCMA based Audio-Visual Conformer and further improve the performance by <b>fine-tuning</b> pre-trained uni-modal models for the AVWWS task. The proposed system achieves a new state-of-the-art result (4.57% WWS score) on the far-field MISP dataset.</p></p class="citation"></blockquote><h3 id=34--233309-a-robust-audio-deepfake-detection-system-via-multi-view-feature-yujie-yang-et-al-2024>(3/4 | 233/309) A robust audio deepfake detection system via multi-view feature (Yujie Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujie Yang, Haochen Qin, Hang Zhou, Chengcheng Wang, Tianyu Guo, Kai Han, Yunhe Wang. (2024)<br><strong>A robust audio deepfake detection system via multi-view feature</strong><br><button class=copy-to-clipboard title="A robust audio deepfake detection system via multi-view feature" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01960v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01960v1.pdf filename=2403.01960v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advancement of generative modeling techniques, synthetic human speech becomes increasingly indistinguishable from real, and tricky challenges are elicited for the audio deepfake detection (ADD) system. In this paper, we exploit audio features to improve the generalizability of ADD systems. Investigation of the ADD task performance is conducted over a broad range of audio features, including various handcrafted features and learning-based features. Experiments show that learning-based audio features pretrained on a large amount of data generalize better than hand-crafted features on <b>out-of-domain</b> scenarios. Subsequently, we further improve the generalizability of the ADD system using proposed multi-feature approaches to incorporate complimentary information from features of different views. The model trained on ASV2019 data achieves an equal error rate of 24.27% on the In-the-Wild dataset.</p></p class="citation"></blockquote><h3 id=44--234309-consep-a-noise--and-reverberation-robust-speech-separation-framework-by-magnitude-conditioning-kuan-hsun-ho-et-al-2024>(4/4 | 234/309) ConSep: a Noise- and Reverberation-Robust Speech Separation Framework by Magnitude Conditioning (Kuan-Hsun Ho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kuan-Hsun Ho, Jeih-weih Hung, Berlin Chen. (2024)<br><strong>ConSep: a Noise- and Reverberation-Robust Speech Separation Framework by Magnitude Conditioning</strong><br><button class=copy-to-clipboard title="ConSep: a Noise- and Reverberation-Robust Speech Separation Framework by Magnitude Conditioning" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Cohere<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01792v1.pdf filename=2403.01792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speech separation has recently made significant progress thanks to the fine-grained vision used in time-domain methods. However, several studies have shown that adopting Short-Time Fourier Transform (STFT) for feature extraction could be beneficial when encountering harsher conditions, such as noise or reverberation. Therefore, we propose a magnitude-conditioned time-domain framework, ConSep, to inherit the beneficial characteristics. The experiment shows that ConSep promotes performance in anechoic, noisy, and reverberant settings compared to two celebrated methods, SepFormer and Bi-Sep. Furthermore, we visualize the components of ConSep to strengthen the advantages and <b>cohere</b> with the actualities we have found in preliminary studies.</p></p class="citation"></blockquote><h2 id=econgn-1>econ.GN (1)</h2><h3 id=11--235309-the-heterogeneous-productivity-effects-of-generative-ai-david-kreitmeir-et-al-2024>(1/1 | 235/309) The Heterogeneous Productivity Effects of Generative AI (David Kreitmeir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Kreitmeir, Paul A. Raschky. (2024)<br><strong>The Heterogeneous Productivity Effects of Generative AI</strong><br><button class=copy-to-clipboard title="The Heterogeneous Productivity Effects of Generative AI" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.GN<br>Categories: cs-AI, econ-GN, econ.GN, q-fin-EC<br>Keyword Score: 40<br>Keywords: Generative AI, ChatGPT, Transformer, Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01964v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01964v1.pdf filename=2403.01964v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We analyse the individual productivity effects of Italy&rsquo;s ban on <b>ChatGPT,</b> a <b>generative</b> <b>pretrained</b> <b>transformer</b> <b>chatbot.</b> We compile data on the daily coding output quantity and quality of over 36,000 GitHub users in Italy and other European countries and combine these data with the sudden announcement of the ban in a difference-in-differences framework. Among the affected users in Italy, we find a short-term increase in output quantity and quality for less experienced users and a decrease in productivity on more routine tasks for experienced users.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--236309-pi-astrodeconv-a-physics-informed-unsupervised-learning-method-for-astronomical-image-deconvolution-shulei-ni-et-al-2024>(1/1 | 236/309) PI-AstroDeconv: A Physics-Informed Unsupervised Learning Method for Astronomical Image Deconvolution (Shulei Ni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shulei Ni, Yisheng Qiu, Yunchun Chen, Zihao Song, Hao Chen, Xuejian Jiang, Huaxi Chen. (2024)<br><strong>PI-AstroDeconv: A Physics-Informed Unsupervised Learning Method for Astronomical Image Deconvolution</strong><br><button class=copy-to-clipboard title="PI-AstroDeconv: A Physics-Informed Unsupervised Learning Method for Astronomical Image Deconvolution" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-GA, astro-ph-IM, astro-ph.IM, cs-CV, eess-IV<br>Keyword Score: 40<br>Keywords: Autoencoder, Convolution, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01692v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01692v1.pdf filename=2403.01692v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the imaging process of an astronomical telescope, the deconvolution of its beam or Point Spread Function (PSF) is a crucial task. However, deconvolution presents a classical and challenging inverse computation problem. In scenarios where the beam or PSF is complex or inaccurately measured, such as in interferometric arrays and certain radio telescopes, the resultant blurry images are often challenging to interpret visually or analyze using traditional physical detection methods. We argue that traditional methods frequently lack specific prior knowledge, thereby leading to suboptimal performance. To address this issue and achieve image deconvolution and reconstruction, we propose an <b>unsupervised</b> <b>network</b> architecture that incorporates prior physical information. The network adopts an encoder-decoder structure while leveraging the telescope&rsquo;s PSF as prior knowledge. During network training, we introduced accelerated Fast Fourier Transform (FFT) <b>convolution</b> to enable efficient processing of high-resolution input images and PSFs. We explored various classic regression networks, including <b>autoencoder</b> (AE) and U-Net, and conducted a comprehensive performance evaluation through comparative analysis.</p></p class="citation"></blockquote><h2 id=cshc-7>cs.HC (7)</h2><h3 id=17--237309-situated-understanding-of-older-adults-interactions-with-voice-assistants-a-month-long-in-home-study-amama-mahmood-et-al-2024>(1/7 | 237/309) Situated Understanding of Older Adults&rsquo; Interactions with Voice Assistants: A Month-long In-home Study (Amama Mahmood et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amama Mahmood, Junxiang Wang, Chien-Ming Huang. (2024)<br><strong>Situated Understanding of Older Adults&rsquo; Interactions with Voice Assistants: A Month-long In-home Study</strong><br><button class=copy-to-clipboard title="Situated Understanding of Older Adults' Interactions with Voice Assistants: A Month-long In-home Study" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02421v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02421v1.pdf filename=2403.02421v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our work addresses the challenges older adults face with commercial Voice Assistants (VAs), notably in conversation breakdowns and error handling. Traditional methods of collecting user experiences-usage logs and post-hoc interviews-do not fully capture the intricacies of older adults&rsquo; interactions with VAs, particularly regarding their reactions to errors. To bridge this gap, we equipped 15 older adults&rsquo; homes with Amazon smart speakers integrated with custom audio recorders to collect ``in-the-wild&rsquo;&rsquo; audio interaction data for detailed error analysis. Recognizing the conversational limitations of current VAs, our study also explored the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to handle natural and imperfect text for improving VAs. Midway through our study, we deployed <b>ChatGPT-powered</b> Alexa skill to investigate its efficacy for older adults. Our research suggests leveraging vocal and verbal responses combined with <b>LLMs&rsquo;</b> contextual capabilities for enhanced error prevention and management in VAs, while proposing design considerations to align VA capabilities with older adults&rsquo; expectations.</p></p class="citation"></blockquote><h3 id=27--238309-towards-a-diffractive-analysis-of-prompt-based-generative-ai-nina-rajcic-et-al-2024>(2/7 | 238/309) Towards A Diffractive Analysis of Prompt-Based Generative AI (Nina Rajcic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nina Rajcic, Maria Teresa Llano, Jon McCormack. (2024)<br><strong>Towards A Diffractive Analysis of Prompt-Based Generative AI</strong><br><button class=copy-to-clipboard title="Towards A Diffractive Analysis of Prompt-Based Generative AI" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: J-5; H-1-2; H-5, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Fine-tuning, Generative AI, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01783v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01783v1.pdf filename=2403.01783v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent developments in <b>prompt-based</b> <b>generative</b> <b>AI</b> has given rise to discourse surrounding the perceived ethical concerns, economic implications, and consequences for the future of cultural production. As <b>generative</b> <b>imagery</b> becomes pervasive in mainstream society, dominated primarily by emerging industry leaders, we encourage that the role of the CHI community be one of inquiry; to investigate the numerous ways in which <b>generative</b> <b>AI</b> has the potential to, and already is, augmenting human creativity. In this paper, we conducted a diffractive analysis exploring the potential role of <b>prompt-based</b> interfaces in artists&rsquo; creative practice. Over a two week period, seven visual artists were given access to a personalised instance of Stable Diffusion, <b>fine-tuned</b> on a dataset of their work. In the following diffractive analysis, we identified two dominant modes adopted by participants, AI for ideation, and AI for production. We furthermore present a number of ethical design considerations for the future development of <b>generative</b> <b>AI</b> interfaces.</p></p class="citation"></blockquote><h3 id=37--239309-human-ai-collaboration-increases-skill-tagging-speed-but-degrades-accuracy-cheng-ren-et-al-2024>(3/7 | 239/309) Human-AI Collaboration Increases Skill Tagging Speed but Degrades Accuracy (Cheng Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Ren, Zachary Pardos, Zhi Li. (2024)<br><strong>Human-AI Collaboration Increases Skill Tagging Speed but Degrades Accuracy</strong><br><button class=copy-to-clipboard title="Human-AI Collaboration Increases Skill Tagging Speed but Degrades Accuracy" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Recommendation, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02259v1.pdf filename=2403.02259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI approaches are progressing besting humans at game-related tasks (e.g. chess). The next stage is expected to be Human-AI collaboration; however, the research on this subject has been mixed and is in need of additional data points. We add to this nascent literature by studying Human-AI collaboration on a common administrative educational task. Education is a special domain in its relation to AI and has been slow to adopt AI approaches in practice, concerned with the educational enterprise losing its humanistic touch and because standard of quality is demanded because of the impact on a person&rsquo;s career and developmental trajectory. In this study (N = 22), we design an experiment to explore the effect of Human-AI collaboration on the task of tagging educational content with skills from the US common core taxonomy. Our results show that the experiment group (with AI <b>recommendations)</b> saved around 50% time (p &lt; 0.01) in the execution of their tagging task but at the sacrifice of 7.7% recall (p = 0.267) and 35% accuracy (p= 0.1170) compared with the non-AI involved control group, placing the AI+human group in between the AI alone (lowest performance) and the human alone (highest performance). We further analyze log data from this AI collaboration experiment to explore under what circumstances humans still exercised their discernment when receiving <b>recommendations.</b> Finally, we outline how this study can assist in implementing AI tools, like <b>ChatGPT,</b> in education.</p></p class="citation"></blockquote><h3 id=47--240309-memoro-using-large-language-models-to-realize-a-concise-interface-for-real-time-memory-augmentation-wazeer-zulfikar-et-al-2024>(4/7 | 240/309) Memoro: Using Large Language Models to Realize a Concise Interface for Real-Time Memory Augmentation (Wazeer Zulfikar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wazeer Zulfikar, Samantha Chan, Pattie Maes. (2024)<br><strong>Memoro: Using Large Language Models to Realize a Concise Interface for Real-Time Memory Augmentation</strong><br><button class=copy-to-clipboard title="Memoro: Using Large Language Models to Realize a Concise Interface for Real-Time Memory Augmentation" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02135v1.pdf filename=2403.02135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>People have to remember an ever-expanding volume of information. Wearables that use information capture and retrieval for memory augmentation can help but can be disruptive and cumbersome in real-world tasks, such as in social settings. To address this, we developed Memoro, a wearable audio-based memory assistant with a concise user interface. Memoro uses a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to infer the user&rsquo;s memory needs in a conversational context, semantically search memories, and present minimal suggestions. The assistant has two interaction modes: Query Mode for voicing queries and Queryless Mode for on-demand predictive assistance, without explicit query. Our study of (N=20) participants engaged in a real-time conversation demonstrated that using Memoro reduced device interaction time and increased recall confidence while preserving conversational quality. We report quantitative results and discuss the preferences and experiences of users. This work contributes towards utilizing <b>LLMs</b> to design wearable memory augmentation systems that are minimally disruptive.</p></p class="citation"></blockquote><h3 id=57--241309-ivie-lightweight-anchored-explanations-of-just-generated-code-litao-yan-et-al-2024>(5/7 | 241/309) Ivie: Lightweight Anchored Explanations of Just-Generated Code (Litao Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Litao Yan, Alyssa Hwang, Zhiyuan Wu, Andrew Head. (2024)<br><strong>Ivie: Lightweight Anchored Explanations of Just-Generated Code</strong><br><button class=copy-to-clipboard title="Ivie: Lightweight Anchored Explanations of Just-Generated Code" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02491v1.pdf filename=2403.02491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Programming assistants have reshaped the experience of programming into one where programmers spend less time writing and more time critically examining code. In this paper, we explore how programming assistants can be extended to accelerate the inspection of generated code. We introduce an extension to the programming assistant called Ivie, or instantly visible in-situ explanations. When using Ivie, a programmer&rsquo;s generated code is instantly accompanied by explanations positioned just adjacent to the code. Our design was optimized for extremely low-cost invocation and dismissal. Explanations are compact and informative. They describe meaningful expressions, from individual variables to entire blocks of code. We present an implementation of Ivie that forks VS Code, applying a modern <b>LLM</b> for timely segmentation and explanation of generated code. In a lab study, we compared Ivie to a contemporary baseline tool for code understanding. Ivie improved understanding of generated code, and was received by programmers as a highly useful, low distraction, desirable complement to the programming assistant.</p></p class="citation"></blockquote><h3 id=67--242309-beyond-recommender-an-exploratory-study-of-the-effects-of-different-ai-roles-in-ai-assisted-decision-making-shuai-ma-et-al-2024>(6/7 | 242/309) Beyond Recommender: An Exploratory Study of the Effects of Different AI Roles in AI-Assisted Decision Making (Shuai Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Ma, Chenyi Zhang, Xinru Wang, Xiaojuan Ma, Ming Yin. (2024)<br><strong>Beyond Recommender: An Exploratory Study of the Effects of Different AI Roles in AI-Assisted Decision Making</strong><br><button class=copy-to-clipboard title="Beyond Recommender: An Exploratory Study of the Effects of Different AI Roles in AI-Assisted Decision Making" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01791v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01791v1.pdf filename=2403.01791v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial Intelligence (AI) is increasingly employed in various decision-making tasks, typically as a Recommender, providing <b>recommendations</b> that the AI deems correct. However, recent studies suggest this may diminish human analytical thinking and lead to humans&rsquo; inappropriate reliance on AI, impairing the synergy in human-AI teams. In contrast, human advisors in group decision-making perform various roles, such as analyzing alternative options or criticizing decision-makers to encourage their critical thinking. This diversity of roles has not yet been empirically explored in AI assistance. In this paper, we examine three AI roles: Recommender, Analyzer, and Devil&rsquo;s Advocate, and evaluate their effects across two AI performance levels. Our results show each role&rsquo;s distinct strengths and limitations in task performance, reliance appropriateness, and user experience. Notably, the Recommender role is not always the most effective, especially if the AI performance level is low, the Analyzer role may be preferable. These insights offer valuable implications for designing AI assistants with adaptive functional roles according to different situations.</p></p class="citation"></blockquote><h3 id=77--243309-closing-the-knowledge-gap-in-designing-data-annotation-interfaces-for-ai-powered-disaster-management-analytic-systems-zinat-ara-et-al-2024>(7/7 | 243/309) Closing the Knowledge Gap in Designing Data Annotation Interfaces for AI-powered Disaster Management Analytic Systems (Zinat Ara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zinat Ara, Hossein Salemi, Sungsoo Ray Hong, Yasas Senarath, Steve Peterson, Amanda Lee Hughes, Hemant Purohit. (2024)<br><strong>Closing the Knowledge Gap in Designing Data Annotation Interfaces for AI-powered Disaster Management Analytic Systems</strong><br><button class=copy-to-clipboard title="Closing the Knowledge Gap in Designing Data Annotation Interfaces for AI-powered Disaster Management Analytic Systems" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01722v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01722v1.pdf filename=2403.01722v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data annotation interfaces predominantly leverage ground truth labels to guide annotators toward accurate responses. With the growing adoption of Artificial Intelligence (AI) in domain-specific professional tasks, it has become increasingly important to help beginning annotators identify how their early-stage knowledge can lead to inaccurate answers, which in turn, helps to ensure quality annotations at scale. To investigate this issue, we conducted a formative study involving eight individuals from the field of disaster management, each possessing varying levels of expertise. The goal was to understand the prevalent factors contributing to disagreements among annotators when classifying Twitter messages related to disasters and to analyze their respective responses. Our analysis identified two primary causes of disagreement between expert and beginner annotators: 1) a lack of contextual knowledge or uncertainty about the situation, and 2) the absence of visual or supplementary cues. Based on these findings, we designed a Context interface, which generates aids that help beginners identify potential mistakes and provide the hidden context of the presented tweet. The summative study compares Context design with two widely used designs in data annotation UI, Highlight and <b>Reasoning-based</b> interfaces. We found significant differences between these designs in terms of attitudinal and behavioral data. We conclude with implications for designing future interfaces aiming at closing the knowledge gap among annotators.</p></p class="citation"></blockquote><h2 id=eessas-3>eess.AS (3)</h2><h3 id=13--244309-speech-emotion-recognition-from-voice-messages-recorded-in-the-wild-lucía-gómez-zaragozá-et-al-2024>(1/3 | 244/309) Speech emotion recognition from voice messages recorded in the wild (Lucía Gómez-Zaragozá et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucía Gómez-Zaragozá, Óscar Valls, Rocío del Amor, María José Castro-Bleda, Valery Naranjo, Mariano Alcañiz Raya, Javier Marín-Morales. (2024)<br><strong>Speech emotion recognition from voice messages recorded in the wild</strong><br><button class=copy-to-clipboard title="Speech emotion recognition from voice messages recorded in the wild" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: I-5-1; I-5-4, cs-AI, cs-CL, cs-SD, eess-AS, eess.AS<br>Keyword Score: 30<br>Keywords: Fairness, Transformer, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02167v1.pdf filename=2403.02167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Emotion</b> <b>datasets</b> used for Speech <b>Emotion</b> <b>Recognition</b> (SER) often contain acted or elicited speech, limiting their applicability in real-world scenarios. In this work, we used the <b>Emotional</b> <b>Voice</b> Messages (EMOVOME) database, including spontaneous voice messages from conversations of 100 Spanish speakers on a messaging app, labeled in continuous and discrete <b>emotions</b> <b>by</b> expert and non-expert annotators. We created speaker-independent SER models using the eGeMAPS features, <b>transformer-based</b> models and their combination. We compared the results with reference databases and analyzed the influence of annotators and gender <b>fairness.</b> The pre-trained Unispeech-L model and its combination with eGeMAPS achieved the highest results, with 61.64% and 55.57% Unweighted Accuracy (UA) for 3-class valence and arousal prediction respectively, a 10% improvement over baseline models. For the <b>emotion</b> <b>categories,</b> 42.58% UA was obtained. EMOVOME performed lower than the acted RAVDESS database. The elicited IEMOCAP database also outperformed EMOVOME in the prediction of <b>emotion</b> <b>categories,</b> while similar results were obtained in valence and arousal. Additionally, EMOVOME outcomes varied with annotator labels, showing superior results and better <b>fairness</b> when combining expert and non-expert annotations. This study significantly contributes to the evaluation of SER models in real-life situations, advancing in the development of applications for analyzing spontaneous voice messages.</p></p class="citation"></blockquote><h3 id=23--245309-neurovoz-a-castillian-spanish-corpus-of-parkinsonian-speech-janaína-mendes-laureano-et-al-2024>(2/3 | 245/309) NeuroVoz: a Castillian Spanish corpus of parkinsonian speech (Janaína Mendes-Laureano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Janaína Mendes-Laureano, Jorge A. Gómez-García, Alejandro Guerrero-López, Elisa Luque-Buzo, Julián D. Arias-Londoño, Francisco J. Grandas-Pérez, Juan I. Godino-Llorente. (2024)<br><strong>NeuroVoz: a Castillian Spanish corpus of parkinsonian speech</strong><br><button class=copy-to-clipboard title="NeuroVoz: a Castillian Spanish corpus of parkinsonian speech" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-CL, cs-SD, eess-AS, eess.AS<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02371v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02371v2.pdf filename=2403.02371v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of Parkinson&rsquo;s Disease (PD) diagnosis through speech analysis is hindered by a notable lack of publicly available, diverse language datasets, limiting the reproducibility and further exploration of existing research. In response to this gap, we introduce a comprehensive corpus from 108 native Castilian Spanish speakers, comprising 55 healthy controls and 53 individuals diagnosed with PD, all of whom were under pharmacological treatment and recorded in their medication-optimized state. This unique dataset features a wide array of speech tasks, including sustained phonation of the five Spanish vowels, diadochokinetic tests, 16 listen-and-repeat utterances, and free monologues. The dataset emphasizes accuracy and reliability through specialist manual transcriptions of the listen-and-repeat tasks and utilizes Whisper for automated monologue transcriptions, making it the most complete public corpus of Parkinsonian speech, and the first in Castillian Spanish. NeuroVoz is composed by 2,903 audio recordings averaging $26.88 \pm 3.35$ recordings per participant, offering a substantial resource for the scientific exploration of PD&rsquo;s impact on speech. This dataset has already underpinned several studies, achieving a <b>benchmark</b> accuracy of 89% in PD speech pattern identification, indicating marked speech alterations attributable to PD. Despite these advances, the broader challenge of conducting a language-agnostic, cross-corpora analysis of Parkinsonian speech patterns remains an open area for future research. This contribution not only fills a critical void in PD speech analysis resources but also sets a new standard for the global research community in leveraging speech as a diagnostic tool for neurodegenerative diseases.</p></p class="citation"></blockquote><h3 id=33--246309-6dof-seld-sound-event-localization-and-detection-using-microphones-and-motion-tracking-sensors-on-self-motioning-human-masahiro-yasuda-et-al-2024>(3/3 | 246/309) 6DoF SELD: Sound Event Localization and Detection Using Microphones and Motion Tracking Sensors on self-motioning human (Masahiro Yasuda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masahiro Yasuda, Shoichiro Saito, Akira Nakayama, Noboru Harada. (2024)<br><strong>6DoF SELD: Sound Event Localization and Detection Using Microphones and Motion Tracking Sensors on self-motioning human</strong><br><button class=copy-to-clipboard title="6DoF SELD: Sound Event Localization and Detection Using Microphones and Motion Tracking Sensors on self-motioning human" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01670v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01670v1.pdf filename=2403.01670v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We aim to perform sound event localization and detection (SELD) using wearable equipment for a moving human, such as a pedestrian. Conventional SELD tasks have dealt only with microphone arrays located in static positions. However, self-motion with three rotational and three translational degrees of freedom (6DoF) shall be considered for wearable microphone arrays. A system trained only with a dataset using microphone arrays in a fixed position would be unable to adapt to the fast relative motion of sound events associated with self-motion, resulting in the degradation of SELD performance. To address this, we designed 6DoF SELD Dataset for wearable systems, the first SELD dataset considering the self-motion of microphones. Furthermore, we proposed a <b>multi-modal</b> SELD system that jointly utilizes audio and motion tracking sensor signals. These sensor signals are expected to help the system find useful acoustic cues for SELD on the basis of the current self-motion state. Experimental results on our dataset show that the proposed method effectively improves SELD performance with a mechanism to extract acoustic features conditioned by sensor signals.</p></p class="citation"></blockquote><h2 id=statml-5>stat.ML (5)</h2><h3 id=15--247309-differential-privacy-of-noisy-sgd-under-heavy-tailed-perturbations-umut-şimşekli-et-al-2024>(1/5 | 247/309) Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations (Umut Şimşekli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Umut Şimşekli, Mert Gürbüzbalaban, Sinan Yıldırım, Lingjiong Zhu. (2024)<br><strong>Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations</strong><br><button class=copy-to-clipboard title="Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CR, cs-LG, math-ST, stat-ML, stat-TH, stat.ML<br>Keyword Score: 30<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02051v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02051v1.pdf filename=2403.02051v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Injecting heavy-tailed noise to the iterates of <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> has received increasing attention over the past few years. While various theoretical properties of the resulting algorithm have been analyzed mainly from learning theory and optimization perspectives, their privacy preservation properties have not yet been established. Aiming to bridge this gap, we provide <b>differential</b> <b>privacy</b> (DP) guarantees for noisy <b>SGD,</b> when the injected noise follows an $\alpha$-stable distribution, which includes a spectrum of heavy-tailed distributions (with infinite variance) as well as the Gaussian distribution. Considering the $(\epsilon, \delta)$-DP framework, we show that <b>SGD</b> with heavy-tailed perturbations achieves $(0, \tilde{\mathcal{O}}(1/n))$-DP for a broad class of loss functions which can be non-convex, where $n$ is the number of data points. As a remarkable byproduct, contrary to prior work that necessitates bounded sensitivity for the gradients or clipping the iterates, our theory reveals that under mild assumptions, such a projection step is not actually necessary. We illustrate that the heavy-tailed noising mechanism achieves similar DP guarantees compared to the Gaussian case, which suggests that it can be a viable alternative to its light-tailed counterparts.</p></p class="citation"></blockquote><h3 id=25--248309-soft-constrained-schrodinger-bridge-a-stochastic-control-approach-jhanvi-garg-et-al-2024>(2/5 | 248/309) Soft-constrained Schrodinger Bridge: a Stochastic Control Approach (Jhanvi Garg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jhanvi Garg, Xianyang Zhang, Quan Zhou. (2024)<br><strong>Soft-constrained Schrodinger Bridge: a Stochastic Control Approach</strong><br><button class=copy-to-clipboard title="Soft-constrained Schrodinger Bridge: a Stochastic Control Approach" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-OC, stat-CO, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Diffusion Model, MNIST, Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01717v1.pdf filename=2403.01717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Schr"{o}dinger bridge can be viewed as a <b>continuous-time</b> <b>stochastic</b> control problem where the goal is to find an optimally controlled <b>diffusion</b> <b>process</b> with a pre-specified terminal distribution $\mu_T$. We propose to generalize this stochastic control problem by allowing the terminal distribution to differ from $\mu_T$ but penalizing the Kullback-Leibler divergence between the two distributions. We call this new control problem soft-constrained Schr"{o}dinger bridge (SSB). The main contribution of this work is a theoretical derivation of the solution to SSB, which shows that the terminal distribution of the optimally controlled process is a geometric mixture of $\mu_T$ and some other distribution. This result is further extended to a time series setting. One application of SSB is the development of robust generative <b>diffusion</b> <b>models.</b> We propose a score matching-based algorithm for sampling from geometric mixtures and showcase its use via a numerical example for the <b>MNIST</b> data set.</p></p class="citation"></blockquote><h3 id=35--249309-bipartite-graph-variational-auto-encoder-with-fair-latent-representation-to-account-for-sampling-bias-in-ecological-networks-emre-anakok-et-al-2024>(3/5 | 249/309) Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks (Emre Anakok et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emre Anakok, Pierre Barbillon, Colin Fontaine, Elisa Thebault. (2024)<br><strong>Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks</strong><br><button class=copy-to-clipboard title="Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, cs-SI, stat-ML, stat.ML<br>Keyword Score: 23<br>Keywords: Graph, Graph Embedding, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02011v1.pdf filename=2403.02011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a method to represent bipartite networks using <b>graph</b> <b>embeddings</b> tailored to tackle the challenges of studying ecological networks, such as the ones linking plants and pollinators, where many covariates need to be accounted for, in particular to control for sampling bias. We adapt the variational <b>graph</b> <b>auto-encoder</b> approach to the bipartite case, which enables us to generate embeddings in a latent space where the two sets of nodes are positioned based on their probability of connection. We translate the <b>fairness</b> framework commonly considered in sociology in order to address sampling bias in ecology. By incorporating the Hilbert-Schmidt independence criterion (HSIC) as an additional penalty term in the loss we optimize, we ensure that the structure of the latent space is independent of continuous variables, which are related to the sampling process. Finally, we show how our approach can change our understanding of ecological networks when applied to the Spipoll data set, a citizen science monitoring program of plant-pollinator interactions to which many observers contribute, making it prone to sampling bias.</p></p class="citation"></blockquote><h3 id=45--250309-on-the-impact-of-measure-pre-conditionings-on-general-parametric-ml-models-and-transfer-learning-via-domain-adaptation-joaquín-sánchez-garcía-2024>(4/5 | 250/309) On the impact of measure pre-conditionings on general parametric ML models and transfer learning via domain adaptation (Joaquín Sánchez García, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joaquín Sánchez García. (2024)<br><strong>On the impact of measure pre-conditionings on general parametric ML models and transfer learning via domain adaptation</strong><br><button class=copy-to-clipboard title="On the impact of measure pre-conditionings on general parametric ML models and transfer learning via domain adaptation" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-OC, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Transfer Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02432v1.pdf filename=2403.02432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a new technique for understanding convergence of learning agents under small modifications of data. We show that such convergence can be understood via an analogue of Fatou&rsquo;s lemma which yields gamma-convergence. We show it&rsquo;s relevance and applications in general machine learning tasks and <b>domain</b> <b>adaptation</b> <b>transfer</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=55--251309-improving-generalisation-via-anchor-multivariate-analysis-homer-durand-et-al-2024>(5/5 | 251/309) Improving generalisation via anchor multivariate analysis (Homer Durand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Homer Durand, Gherardo Varando, Gustau Camps-Valls, Nathan Mankovich. (2024)<br><strong>Improving generalisation via anchor multivariate analysis</strong><br><button class=copy-to-clipboard title="Improving generalisation via anchor multivariate analysis" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: 62Hxx, cs-LG, stat-AP, stat-ME, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Distribution Shift, Distribution Shift, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01865v1.pdf filename=2403.01865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a causal regularisation extension to anchor regression (AR) for improved <b>out-of-distribution</b> (OOD) generalisation. We present anchor-compatible losses, aligning with the anchor framework to ensure robustness against <b>distribution</b> <b>shifts.</b> Various multivariate analysis (MVA) algorithms, such as (Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We observe that simple regularisation enhances robustness in OOD settings. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with MVA approaches and its role in enhancing replicability while guarding against <b>distribution</b> <b>shifts.</b> The extended AR framework advances causal inference methodologies, addressing the need for reliable OOD generalisation.</p></p class="citation"></blockquote><h2 id=csdc-4>cs.DC (4)</h2><h3 id=14--252309-mpi-errors-detection-using-gnn-embedding-and-vector-embedding-over-llvm-ir-jad-el-karchi-et-al-2024>(1/4 | 252/309) MPI Errors Detection using GNN Embedding and Vector Embedding over LLVM IR (Jad El Karchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jad El Karchi, Hanze Chen, Ali TehraniJamsaz, Ali Jannesari, Mihail Popov, Emmanuelle Saillard. (2024)<br><strong>MPI Errors Detection using GNN Embedding and Vector Embedding over LLVM IR</strong><br><button class=copy-to-clipboard title="MPI Errors Detection using GNN Embedding and Vector Embedding over LLVM IR" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-SE, cs.DC<br>Keyword Score: 26<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02518v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02518v1.pdf filename=2403.02518v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying errors in parallel MPI programs is a challenging task. Despite the growing number of verification tools, debugging parallel programs remains a significant challenge. This paper is the first to utilize embedding and deep learning <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> to tackle the issue of identifying bugs in MPI programs. Specifically, we have designed and developed two models that can determine, from a code&rsquo;s LLVM Intermediate Representation (IR), whether the code is correct or contains a known MPI error. We tested our models using two dedicated MPI <b>benchmark</b> suites for verification: MBI and MPI-CorrBench. By training and validating our models on the same <b>benchmark</b> suite, we achieved a prediction accuracy of 92% in detecting error types. Additionally, we trained and evaluated our models on distinct <b>benchmark</b> suites (e.g., transitioning from MBI to MPI-CorrBench) and achieved a promising accuracy of over 80%. Finally, we investigated the interaction between different MPI errors and quantified our models&rsquo; generalization capabilities over new unseen errors. This involved removing error types during training and assessing whether our models could still predict them. The detection accuracy of removed errors varies significantly between 20% to 80%, indicating connected error patterns.</p></p class="citation"></blockquote><h3 id=24--253309-déjàvu-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-foteini-strati-et-al-2024>(2/4 | 253/309) DéjàVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving (Foteini Strati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Foteini Strati, Sara Mcallister, Amar Phanishayee, Jakub Tarnawski, Ana Klimovic. (2024)<br><strong>DéjàVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving</strong><br><button class=copy-to-clipboard title="DéjàVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01876v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01876v1.pdf filename=2403.01876v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed <b>LLM</b> serving is costly and often underutilizes hardware accelerators due to three key challenges: bubbles in pipeline-parallel deployments caused by the bimodal latency of <b>prompt</b> and token processing, GPU memory overprovisioning, and long recovery times in case of failures. In this paper, we propose D'ej`aVu, a system to address all these challenges using a versatile and efficient KV cache streaming library (D'ej`aVuLib). Using D'ej`aVuLib, we propose and implement efficient <b>prompt-token</b> disaggregation to reduce pipeline bubbles, microbatch swapping for efficient GPU memory management, and state replication for fault-tolerance. We highlight the efficacy of these solutions on a range of large models across cloud deployments.</p></p class="citation"></blockquote><h3 id=34--254309-daedalus-self-adaptive-horizontal-autoscaling-for-resource-efficiency-of-distributed-stream-processing-systems-benjamin-j-j-pfister-et-al-2024>(3/4 | 254/309) Daedalus: Self-Adaptive Horizontal Autoscaling for Resource Efficiency of Distributed Stream Processing Systems (Benjamin J. J. Pfister et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin J. J. Pfister, Dominik Scheinert, Morgan K. Geldenhuys, Odej Kao. (2024)<br><strong>Daedalus: Self-Adaptive Horizontal Autoscaling for Resource Efficiency of Distributed Stream Processing Systems</strong><br><button class=copy-to-clipboard title="Daedalus: Self-Adaptive Horizontal Autoscaling for Resource Efficiency of Distributed Stream Processing Systems" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Self-adaption<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02093v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02093v2.pdf filename=2403.02093v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed Stream Processing (DSP) systems are capable of processing large streams of unbounded data, offering high throughput and low latencies. To maintain a stable Quality of Service (QoS), these systems require a sufficient allocation of resources. At the same time, over-provisioning can result in wasted energy and high operating costs. Therefore, to maximize resource utilization, autoscaling methods have been proposed that aim to efficiently match the resource allocation with the incoming workload. However, determining when and by how much to scale remains a significant challenge. Given the long-running nature of DSP jobs, scaling actions need to be executed at runtime, and to maintain a good QoS, they should be both accurate and infrequent. To address the challenges of autoscaling, the concept of self-adaptive systems is particularly fitting. These systems monitor themselves and their environment, adapting to changes with minimal need for expert involvement. This paper introduces Daedalus, a self-adaptive manager for autoscaling in DSP systems, which draws on the principles of <b>self-adaption</b> to address the challenge of efficient autoscaling. Daedalus monitors a running DSP job and builds performance models, aiming to predict the maximum processing capacity at different scale-outs. When combined with time series forecasting to predict future workloads, Daedalus proactively scales DSP jobs, optimizing for maximum throughput and minimizing both latencies and resource usage. We conducted experiments using Apache Flink and Kafka Streams to evaluate the performance of Daedalus against two state-of-the-art approaches. Daedalus was able to achieve comparable latencies while reducing resource usage by up to 71%.</p></p class="citation"></blockquote><h3 id=44--255309-demeter-resource-efficient-distributed-stream-processing-under-dynamic-loads-with-multi-configuration-optimization-morgan-geldenhuys-et-al-2024>(4/4 | 255/309) Demeter: Resource-Efficient Distributed Stream Processing under Dynamic Loads with Multi-Configuration Optimization (Morgan Geldenhuys et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Morgan Geldenhuys, Dominik Scheinert, Odej Kao, Lauritz Thamsen. (2024)<br><strong>Demeter: Resource-Efficient Distributed Stream Processing under Dynamic Loads with Multi-Configuration Optimization</strong><br><button class=copy-to-clipboard title="Demeter: Resource-Efficient Distributed Stream Processing under Dynamic Loads with Multi-Configuration Optimization" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02129v1.pdf filename=2403.02129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed Stream Processing (DSP) focuses on the near real-time processing of large streams of unbounded data. To increase processing capacities, DSP systems are able to dynamically scale across a cluster of commodity nodes, ensuring a good Quality of Service despite variable workloads. However, selecting scaleout configurations which maximize resource utilization remains a challenge. This is especially true in environments where workloads change over time and node failures are all but inevitable. Furthermore, configuration parameters such as memory allocation and checkpointing intervals impact performance and resource usage as well. Sub-optimal configurations easily lead to high operational costs, poor performance, or unacceptable loss of service. In this paper, we present Demeter, a method for dynamically optimizing key DSP system configuration parameters for resource efficiency. Demeter uses Time Series Forecasting to predict future workloads and Multi-Objective Bayesian Optimization to model runtime behaviors in relation to parameter settings and workload rates. Together, these techniques allow us to determine whether or not enough is known about the predicted workload rate to proactively initiate short-lived parallel profiling runs for data gathering. Once trained, the models guide the adjustment of multiple, potentially dependent system configuration parameters ensuring optimized performance and resource usage in response to changing workload rates. Our experiments on a commodity cluster using Apache Flink demonstrate that Demeter significantly improves the operational efficiency of long-running <b>benchmark</b> jobs.</p></p class="citation"></blockquote><h2 id=cspl-6>cs.PL (6)</h2><h3 id=16--256309-let-a-thousand-flowers-bloom-an-algebraic-representation-for-edge-graphs-jack-liell-cock-et-al-2024>(1/6 | 256/309) Let a Thousand Flowers Bloom: An Algebraic Representation for Edge Graphs (Jack Liell-Cock et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jack Liell-Cock, Tom Schrijvers. (2024)<br><strong>Let a Thousand Flowers Bloom: An Algebraic Representation for Edge Graphs</strong><br><button class=copy-to-clipboard title="Let a Thousand Flowers Bloom: An Algebraic Representation for Edge Graphs" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 23<br>Keywords: Graph, BLOOM, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02273v1.pdf filename=2403.02273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context: Edge <b>graphs</b> are <b>graphs</b> whose edges are labelled with identifiers, and nodes can have multiple edges between them. They are used to model a wide range of systems, including networks with distances or degrees of connection and complex relational data. Inquiry: Unfortunately, the homogeneity of this <b>graph</b> structure prevents an effective representation in (functional) programs. Either their interface is riddled with partial functions, or the representations are computationally inefficient to process. Approach: We present a novel data type for edge <b>graphs,</b> based on total and recursive definitions, that prevents usage errors from partial APIs and promotes structurally recursive computations. We follow an algebraic approach and provide a set of primitive constructors and combinators, along with equational laws that identify semantically equivalent constructions. Knowledge: This algebra translates directly into an implementation using algebraic data types, and its homomorphisms give rise to functions for manipulating and transforming these edge <b>graphs.</b> <b>Grounding:</b> We exploit the fact that many common <b>graph</b> algorithms are such homomorphisms to implement them in our framework. Importance: In giving a theoretical <b>grounding</b> for the edge <b>graph</b> data type, we can formalise properties such as soundness and completeness of the representation while also minimising usage errors and maximising re-usability.</p></p class="citation"></blockquote><h3 id=26--257309-privacy-respecting-type-error-telemetry-at-scale-ben-greenman-et-al-2024>(2/6 | 257/309) Privacy-Respecting Type Error Telemetry at Scale (Ben Greenman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ben Greenman, Alan Jeffrey, Shriram Krishnamurthi, Mitesh Shah. (2024)<br><strong>Privacy-Respecting Type Error Telemetry at Scale</strong><br><button class=copy-to-clipboard title="Privacy-Respecting Type Error Telemetry at Scale" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02409v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02409v1.pdf filename=2403.02409v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context: Roblox Studio lets millions of creators build interactive experiences by programming in a variant of Lua called Luau. The creators form a broad group, ranging from novices writing their first script to professional developers; thus, Luau must support a wide audience. As part of its efforts to support all kinds of programmers, Luau includes an optional, gradual type system and goes to great lengths to minimize false positive errors. Inquiry: Since Luau is currently being used by many creators, we want to collect data to improve the language and, in particular, the type system. The standard way to collect data is to deploy client-side telemetry; however, we cannot scrape personal data or proprietary information, which means we cannot collect source code fragments, error messages, or even filepaths. The research questions are thus about how to conduct telemetry that is not invasive and obtain insights from it about type errors. Approach: We designed and implemented a pseudonymized, randomly-sampling telemetry system for Luau. Telemetry records include a timestamp, a session id, a reason for sending, and a numeric summary of the most recent type analyses. This information lets us study type errors over time without revealing private data. We deployed the system in Roblox Studio during Spring 2023 and collected over 1.5 million telemetry records from over 340,000 sessions. Knowledge: We present several findings about Luau, all of which suggest that telemetry is an effective way to study type error pragmatics. One of the less-surprising findings is that opt-in gradual types are unpopular: there is an 100x gap between the number of untyped Luau sessions and the number of typed ones. One surprise is that the strict mode for type analysis is overly conservative about interactions with data assets. A reassuring finding is that type analysis rarely hits its internal limits on problem size. <b>Grounding:</b> Our findings are supported by a dataset of over 1.5 million telemetry records. The data and scripts for analyzing it are available in an artifact. Importance: Beyond the immediate benefits to Luau, our findings about types and type errors have implications for adoption and ergonomics in other gradual languages such as TypeScript, Elixir, and Typed Racket. Our telemetry design is of broad interest, as it reports on type errors without revealing sensitive information.</p></p class="citation"></blockquote><h3 id=36--258309-reactive-programming-without-functions-bjarno-oeyen-et-al-2024>(3/6 | 258/309) Reactive Programming without Functions (Bjarno Oeyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bjarno Oeyen, Joeri De Koster, Wolfgang De Meuter. (2024)<br><strong>Reactive Programming without Functions</strong><br><button class=copy-to-clipboard title="Reactive Programming without Functions" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02296v1.pdf filename=2403.02296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context: Reactive programming (RP) is a declarative programming paradigm suitable for expressing the handling of events. It enables programmers to create applications that react automatically to changes over time. Whenever a time-varying signal changes &ndash; e.g. in response to values produced by event stream (e.g., sensor data, user input&mldr;) &ndash; the program state is updated automatically in tandem with that change. This makes RP well-suited for building interactive applications and reactive (soft real-time) systems. Inquiry: RP Language implementations are often built on top of an existing (host) language as an Embedded Domain Specific Language (EDSL). This results in application code in which reactive code and non-reactive code is inherently entangled. Using a mechanism known as lifting, one usually has access to the full feature set of the (non-reactive) host language in the RP program. However, lifting is also dangerous. First, host code expressed in a Turing-complete language may diverge, resulting in unresponsive programs: i.e. reactive programs that are not actually reactive. Second, the bi-directional integration of reactive and non-reactive code results in a paradigmatic mismatch that, when unchecked, leads to faulty behaviour in programs. Approach: We propose a new reactive programming language, that has been meticulously designed to be reactive-only. We start with a simple (first-order) model for reactivity, based on reactors (i.e. uninstantiated descriptions of signals and their dependencies) and deployments (i.e. instances of reactors) that consist of signals. The language does not have the notion of functions, and thus unlike other RP languages there is no lifting either. We extend this simple model incrementally with additional features found in other programming languages, RP or otherwise. These features include stateful reactors (that allow for time-based accumulation), signals with dynamic dependencies by means of conditionals and polymorphic deployments, recursively-defined reactors, and (anonymous) reactors with lexical scope. Knowledge: In our description of these language features, we not only describe the syntax and semantics, but also how each features compares to the problems that exist in (EDSL) RP languages. I.e. by starting from a reactive-only model, we identify which reactive features (that, in other RP languages are typically expressed in non-reactive code) affect the reactive guarantees that can be enforced by the language. <b>Grounding:</b> We base our arguments by analysing the effect that each feature has on our language: e.g., by analysing how signals are updated, how they are created and how dependencies between signals can be affected. When applicable, we draw parallels with other languages: i.e. similarities shared with other RP languages will be highlighted and thoroughly analysed, and where relevant the same will also be done with non-reactive languages. Importance: Our language shows how a purely reactive programming is able to express the same kinds of programs as in other RP languages that require the use of (unchecked) functions. By considering reactive programs as a collection of pure (reactive-only) reactors, we aim to increase how reactive programming is comprehended by both language designers and its users.</p></p class="citation"></blockquote><h3 id=46--259309-scheduling-garbage-collection-for-energy-efficiency-on-asymmetric-multicore-processors-marina-shimchenko-et-al-2024>(4/6 | 259/309) Scheduling Garbage Collection for Energy Efficiency on Asymmetric Multicore Processors (Marina Shimchenko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marina Shimchenko, Erik Österlund, Tobias Wrigstad. (2024)<br><strong>Scheduling Garbage Collection for Energy Efficiency on Asymmetric Multicore Processors</strong><br><button class=copy-to-clipboard title="Scheduling Garbage Collection for Energy Efficiency on Asymmetric Multicore Processors" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02200v1.pdf filename=2403.02200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing concern for energy efficiency in the Information and Communication Technology (ICT) sector has <b>prompted</b> the exploration of resource management techniques. While hardware architectures, such as single-ISA asymmetric multicore processors (AMP), offer potential energy savings, there is still untapped potential for software optimizations. This paper aims to bridge this gap by investigating the scheduling of garbage collection (GC) activities on a heterogeneous architecture with both performance cores (&ldquo;p-cores&rdquo;) and energy cores (&ldquo;e-cores&rdquo;) to achieve energy savings. Our study focuses on the concurrent ZGC collector in the context of Java Virtual Machines (JVM), as the energy aspect is not well studied in the context of latency-sensitive Java workloads. By comparing the energy efficiency, performance, latency, and memory utilization of executing GC on p-cores versus e-cores, we present compelling findings. We demonstrate that scheduling GC work on e-cores overall leads to approximately 3% energy savings without performance and mean latency degradation while requiring no additional effort from developers. Overall energy reduction can increase to 5.3$\pm$0.0225% by tuning the number of e-cores (still not changing the program!). Our findings highlight the practicality and benefits of scheduling GC on e-cores, showcasing the potential for energy savings in heterogeneous architectures running Java workloads while meeting critical latency requirements. Our research contributes to the ongoing efforts toward achieving a more sustainable and efficient ICT sector.</p></p class="citation"></blockquote><h3 id=56--260309-liverec-prototyping-probes-by-framing-debug-protocols-jean-baptiste-döderlein-et-al-2024>(5/6 | 260/309) LiveRec: Prototyping Probes by Framing Debug Protocols (Jean-Baptiste Döderlein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jean-Baptiste Döderlein, Riemer van Rozen, Tijs van der Storm. (2024)<br><strong>LiveRec: Prototyping Probes by Framing Debug Protocols</strong><br><button class=copy-to-clipboard title="LiveRec: Prototyping Probes by Framing Debug Protocols" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02161v1.pdf filename=2403.02161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context: In the first part of his 2012 presentation &ldquo;Inventing on Principle&rdquo;, Bret Victor gives a demo of a live code editor for Javascript which shows the dynamic history of values of variables in real time. This form of live programming has become known as &ldquo;probes&rdquo;. Probes provide the programmer with permanent and continuous insight into the dynamic evolution of function or method variables, thus improving feedback and developer experience. Inquiry: Although Victor shows a working prototype of live probes in the context of Javascript, he does not discuss strategies for implementing them. Later work provides an implementation approach, but this requires a programming language to be implemented on top of the GraalVM runtime. In this paper we present <strong>LiveRec</strong>, a generic approach for implementing probes which can be applied in the context of many programming languages, without requiring the modification of compilers or run-time systems. Approach: <strong>LiveRec</strong> is based on reusing existing debug protocols to implement probes. Methods or functions are compiled after every code change and executed inside the debugger. During execution the evolution of all local variables in the current stack frame are recorded and communicated back to the editor or IDE for display to the user. Knowledge: It turns out that mainstream debug protocols are rich enough for implementing live probes. Step-wise execution, code hot swapping, and stack frame inspection provide the right granularity and sufficient information to realize live probes, without modifying compilers or language runtimes. Furthermore, it turns out that the recently proposed Debugger Adapter Protocol (DAP) provides an even more generic approach of implementing live probes, but, in some cases, at the cost of a significant performance penalty. <b>Grounding:</b> We have applied <strong>LiveRec</strong> to implement probes using stack recording natively for Java through the Java Debug Interface (JDI), and through the DAP for Java, Python, C, and Javascript, all requiring just modest amounts of configuration code. We evaluate the run-time performance of all four probes prototypes, decomposed into: compile-after-change, hot swap, single step overhead, and stack recording overhead. Our initial results show that live probes on top of native debug APIs can be performant enough for interactive use. In the case of DAP, however, it highly depends on characteristics of the programming language implementation and its associated debugging infrastructure. Importance: Live programming improves the programmer experience by providing immediate feedback about a program&rsquo;s execution and eliminating disruptive edit-compile-restart sequences. Probes are one way to shorten the programmer feedback loop at the level of functions and methods. Although probes are not new, and have been implemented in (prototype) systems, <strong>LiveRec</strong>&rsquo;s approach of building live probes on top of existing and generic debug protocols promises a path towards probes for a host of mainstream programming languages, with reasonable effort.</p></p class="citation"></blockquote><h3 id=66--261309-arrays-in-practice-an-empirical-study-of-array-access-patterns-on-the-jvm-beatrice-åkerblom-et-al-2024>(6/6 | 261/309) Arrays in Practice: An Empirical Study of Array Access Patterns on the JVM (Beatrice Åkerblom et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beatrice Åkerblom, Elias Castegren. (2024)<br><strong>Arrays in Practice: An Empirical Study of Array Access Patterns on the JVM</strong><br><button class=copy-to-clipboard title="Arrays in Practice: An Empirical Study of Array Access Patterns on the JVM" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02416v1.pdf filename=2403.02416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The array is a data structure used in a wide range of programs. Its compact storage and constant time random access makes it highly efficient, but arbitrary indexing complicates the analysis of code containing array accesses. Such analyses are important for compiler optimisations such as bounds check elimination. The aim of this work is to gain a better understanding of how arrays are used in real-world programs. While previous work has applied static analyses to understand how arrays are accessed and used, we take a dynamic approach. We empirically examine various characteristics of array usage by instrumenting programs to log all array accesses, allowing for analysis of array sizes, element types, from where arrays are accessed and to which extent sequences of array accesses form recognizable patterns. The programs in the study were collected from the Renaissance <b>benchmark</b> suite, all running on the Java Virtual Machine. We account for characteristics displayed by the arrays investigated, finding that most arrays have a small size, are accessed by only one or two classes and by a single thread. On average over the <b>benchmarks,</b> 69.8% of the access patterns consist of uncomplicated traversals. Most of the instrumented classes (over 95%) do not use arrays directly at all. These results come from tracing data covering 3,803,043,390 array accesses made across 168,686 classes. While our analysis has only been applied to the Renaissance <b>benchmark</b> suite, the methodology can be applied to any program running on the Java Virtual Machine. This study, and the methodology in general, can inform future runtime implementations and compiler optimisations.</p></p class="citation"></blockquote><h2 id=mathpr-1>math.PR (1)</h2><h3 id=11--262309-emergence-of-multivariate-extremes-in-multilayer-inhomogeneous-random-graphs-daniel-cirkovic-et-al-2024>(1/1 | 262/309) Emergence of Multivariate Extremes in Multilayer Inhomogeneous Random Graphs (Daniel Cirkovic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Cirkovic, Tiandong Wang, Daren B. H. Cline. (2024)<br><strong>Emergence of Multivariate Extremes in Multilayer Inhomogeneous Random Graphs</strong><br><button class=copy-to-clipboard title="Emergence of Multivariate Extremes in Multilayer Inhomogeneous Random Graphs" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.PR<br>Categories: 05C80, 60G70, 05C82, 60F05, cs-SI, math-PR, math-ST, math.PR, physics-soc-ph, stat-TH<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02220v1.pdf filename=2403.02220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a multilayer inhomogeneous random <b>graph</b> model (MIRG), whose layers may consist of both single-edge and multi-edge <b>graphs.</b> In the single layer case, it has been shown that the regular variation of the weight distribution underlying the inhomogeneous random <b>graph</b> implies the regular variation of the typical degree distribution. We extend this correspondence to the multilayer case by showing that the multivariate regular variation of the weight distribution implies the multivariate regular variation of the asymptotic degree distribution. Furthermore, in certain circumstances, the extremal dependence structure present in the weight distribution will be adopted by the asymptotic degree distribution. By considering the asymptotic degree distribution, a wider class of Chung-Lu and Norros-Reittu <b>graphs</b> may be incorporated into the MIRG layers. Additionally, we prove consistency of the Hill estimator when applied to degrees of the MIRG that have a tail index greater than 1. <b>Simulation</b> results indicate that, in practice, hidden regular variation may be consistently detected from an observed MIRG.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--263309-large-language-models-surpass-human-experts-in-predicting-neuroscience-results-xiaoliang-luo-et-al-2024>(1/1 | 263/309) Large language models surpass human experts in predicting neuroscience results (Xiaoliang Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K. Nejad, Felipe Yáñez, Bati Yilmaz, Kangjoo Lee, Alexandra O. Cohen, Valentina Borghesani, Anton Pashkov, Daniele Marinazzo, Jonathan Nicholas, Alessandro Salatiello, Ilia Sucholutsky, Pasquale Minervini, Sepehr Razavi, Roberta Rocca, Elkhan Yusifov, Tereza Okalova, Nianlong Gu, Martin Ferianc, Mikail Khona, Kaustubh R. Patil, Pui-Shee Lee, Rui Mata, Nicholas E. Myers, Jennifer K Bizley, Sebastian Musslick, Isil Poyraz Bilgin, Guiomar Niso, Justin M. Ales, Michael Gaebler, N Apurva Ratan Murty, Chloe M. Hall, Jessica Dafflon, Sherry Dongqi Bao, Bradley C. Love. (2024)<br><strong>Large language models surpass human experts in predicting neuroscience results</strong><br><button class=copy-to-clipboard title="Large language models surpass human experts in predicting neuroscience results" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-AI, q-bio-NC, q-bio.NC<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03230v1.pdf filename=2403.03230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> offer a solution. <b>LLMs</b> trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking <b>benchmark</b> for predicting neuroscience results. We find that <b>LLMs</b> surpass experts in predicting experimental outcomes. BrainGPT, an <b>LLM</b> we tuned on the neuroscience literature, performed better yet. Like human experts, when <b>LLMs</b> were confident in their predictions, they were more likely to be correct, which presages a future where humans and <b>LLMs</b> team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.</p></p class="citation"></blockquote><h2 id=mathna-5>math.NA (5)</h2><h3 id=15--264309-numerical-simulation-of-phase-transition-with-the-hyperbolic-godunov-peshkov-romenski-model-pascal-mossier-et-al-2024>(1/5 | 264/309) Numerical Simulation of Phase Transition with the Hyperbolic Godunov-Peshkov-Romenski Model (Pascal Mossier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pascal Mossier, Steven Jöns, Simone Chiocchetti, Andrea D. Beck, Claus-Dieter Munz. (2024)<br><strong>Numerical Simulation of Phase Transition with the Hyperbolic Godunov-Peshkov-Romenski Model</strong><br><button class=copy-to-clipboard title="Numerical Simulation of Phase Transition with the Hyperbolic Godunov-Peshkov-Romenski Model" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01847v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01847v1.pdf filename=2403.01847v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, a thermodynamically consistent solution of the interfacial Riemann problem for the first-order hyperbolic continuum model of Godunov, Peshkov and Romenski (GPR model) is presented. In the presence of phase transition, interfacial physics are governed by molecular interaction on a microscopic scale, beyond the scope of the macroscopic continuum model in the bulk phases. The developed two-phase Riemann solvers tackle this multi-scale problem, by incorporating a local thermodynamic model to predict the interfacial entropy production. Using phenomenological relations of non-equilibrium thermodynamics, interfacial mass and heat fluxes are derived from the entropy production and provide closure at the phase boundary. We employ the proposed Riemann solvers in an efficient sharp interface level-set Ghost-Fluid framework to provide coupling conditions at phase interfaces under phase transition. As a single-phase <b>benchmark,</b> a Rayleigh-B'enard convection is studied to compare the hyperbolic thermal relaxation formulation of the GPR model against the hyperbolic-parabolic Euler-Fourier system. The novel interfacial Riemann solvers are validated against molecular dynamics <b>simulations</b> of evaporating shock tubes with the Lennard-Jones shifted and truncated potential. On a macroscopic scale, evaporating shock tubes are computed for the material n-Dodecane and compared against Euler-Fourier results. Finally, the efficiency and robustness of the scheme is demonstrated with shock-droplet interaction <b>simulations</b> that involve both phase transfer and surface tension, while featuring severe interface deformations.</p></p class="citation"></blockquote><h3 id=25--265309-improving-the-accuracy-of-the-newmark-method-through-backward-error-analysis-donát-m-takács-et-al-2024>(2/5 | 265/309) Improving the accuracy of the Newmark method through backward error analysis (Donát M. Takács et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donát M. Takács, Tamás Fülöp. (2024)<br><strong>Improving the accuracy of the Newmark method through backward error analysis</strong><br><button class=copy-to-clipboard title="Improving the accuracy of the Newmark method through backward error analysis" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02029v1.pdf filename=2403.02029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We use backward error analysis for differential equations to obtain modified or distorted equations describing the behaviour of the Newmark scheme applied to the transient structural dynamics equation. Using these results, we show how to construct compensation terms from the original parameters of the system, which improve the performance of Newmark <b>simulations</b> without changing the time step or modifying the scheme itself. Two such compensations are given: one eliminates numerical damping, while the other achieves fourth-order accurate calculations using the traditionally second-order Newmark method.</p></p class="citation"></blockquote><h3 id=35--266309-simulation-based-high-speed-elongational-rheometer-for-carreau-type-materials-lukas-kannengiesser-et-al-2024>(3/5 | 266/309) Simulation-based High-Speed Elongational Rheometer for Carreau-type Materials (Lukas Kannengiesser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas Kannengiesser, Walter Arne, Alexander Bier, Nicole Marheineke, Dirk W. Schubert, Raimund Wegener. (2024)<br><strong>Simulation-based High-Speed Elongational Rheometer for Carreau-type Materials</strong><br><button class=copy-to-clipboard title="Simulation-based High-Speed Elongational Rheometer for Carreau-type Materials" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 76-XX, 34B08, 34H05, 65-XX, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01812v1.pdf filename=2403.01812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For the <b>simulation-based</b> design of fiber melt spinning processes, the accurate modeling of the processed polymer with regard to its material behavior is crucial. In this work, we develop a high-speed elongational rheometer for Carreau-type materials, making use of process <b>simulations</b> and fiber diameter measurements. The procedure is based on a unified formulation of the fiber spinning model for all material types (Newtonian and non-Newtonian), whose material laws are strictly monotone in the strain rate. The parametrically described material law for the elongational viscosity implies a nonlinear optimization problem for the parameter identification, for which we propose an efficient, robust gradient-based method. The work can be understood as a proof of concept, a generalization to other, more complex materials is possible.</p></p class="citation"></blockquote><h3 id=45--267309-bayesian-inference-via-geometric-optics-approximation-zejun-sun-et-al-2024>(4/5 | 267/309) Bayesian inference via geometric optics approximation (Zejun Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zejun Sun, Guang-Hui Zheng. (2024)<br><strong>Bayesian inference via geometric optics approximation</strong><br><button class=copy-to-clipboard title="Bayesian inference via geometric optics approximation" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math-ST, math.NA, stat-TH<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01655v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01655v1.pdf filename=2403.01655v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Markov chain Monte Carlo (MCMC) <b>simulations</b> have been widely used to generate samples from the complex posterior distribution in Bayesian inferences. However, these <b>simulations</b> often require multiple computations of the forward model in the likelihood function for each drawn sample. This computational burden renders MCMC sampling impractical when the forward model is computationally expensive, such as in the case of partial differential equation models. In this paper, we propose a novel sampling approach called the geometric optics approximation method (GOAM) for Bayesian inverse problems, which entirely circumvents the need for MCMC <b>simulations.</b> Our method is rooted in the problem of reflector shape design, which focuses on constructing a reflecting surface that redirects rays from a source, with a predetermined density, towards a target domain while achieving a desired density distribution. The key idea is to consider the unnormalized Bayesian posterior as the density on the target domain within the optical system and define a geometric optics approximation measure with respect to posterior by a reflecting surface. Consequently, once such a reflecting surface is obtained, we can utilize it to draw an arbitrary number of independent and uncorrelated samples from the posterior measure for Bayesian inverse problems. In theory, we have shown that the geometric optics approximation measure is well-posed. The efficiency and robustness of our proposed sampler, employing the geometric optics approximation method, are demonstrated through several numerical examples provided in this paper.</p></p class="citation"></blockquote><h3 id=55--268309-analysis-on-aggregation-and-block-smoothers-in-multigrid-methods-for-block-toeplitz-linear-systems-matthias-bolten-et-al-2024>(5/5 | 268/309) Analysis on aggregation and block smoothers in multigrid methods for block Toeplitz linear systems (Matthias Bolten et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthias Bolten, Marco Donatelli, Paola Ferrari, Isabella Furci. (2024)<br><strong>Analysis on aggregation and block smoothers in multigrid methods for block Toeplitz linear systems</strong><br><button class=copy-to-clipboard title="Analysis on aggregation and block smoothers in multigrid methods for block Toeplitz linear systems" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02139v1.pdf filename=2403.02139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present novel improvements in the context of symbol-based multigrid procedures for solving large block structured linear systems. We study the application of an aggregation-based grid transfer operator that transforms the symbol of a block Toeplitz matrix from matrix-valued to scalar-valued at the coarser level. Our convergence analysis of the Two-Grid Method (TGM) reveals the connection between the features of the scalar-valued symbol at the coarser level and the properties of the original matrix-valued one. This allows us to prove the convergence of a V-cycle multigrid with standard grid transfer operators for scalar Toeplitz systems at the coarser levels. Consequently, we extend the class of suitable smoothers for block Toeplitz matrices, focusing on the efficiency of block strategies, particularly the relaxed block Jacobi method. General conditions on smoothing parameters are derived, with emphasis on practical applications where these parameters can be calculated with negligible computational cost. We test the proposed strategies on linear systems <b>stemming</b> from the discretization of differential problems with $\mathbb{Q}_{d} $ Lagrangian FEM or B-spline with non-maximal regularity. The numerical results show in both cases computational advantages compared to existing methods for block structured linear systems.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--269309-a-novel-shortest-path-query-algorithm-based-on-optimized-adaptive-topology-structure-xiao-fang-et-al-2024>(1/1 | 269/309) A Novel Shortest Path Query Algorithm Based on Optimized Adaptive Topology Structure (Xiao Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Fang, Xuyang Song, Jiyuan Ma, Guanhua Liu, Shurong Pang, Wenbo Zhao, Cong Cao, Ling Fan. (2024)<br><strong>A Novel Shortest Path Query Algorithm Based on Optimized Adaptive Topology Structure</strong><br><button class=copy-to-clipboard title="A Novel Shortest Path Query Algorithm Based on Optimized Adaptive Topology Structure" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01826v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01826v1.pdf filename=2403.01826v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Urban rail transit is a fundamental component of public transportation, however, commonly station-based path search algorithms often overlook the impact of transfer times on search results, leading to decreased accuracy. To solve this problem, this paper proposes a novel shortest path query algorithm based on adaptive topology optimization called the Adaptive Topology Extension Road Network Structure (ATEN). This algorithm categorizes transfer stations into different types and treats travel time and transfer time equivalently as weights for edges in the topological <b>graph.</b> The proposed algorithm introduces virtual stations to differentiate between pedestrian paths and train paths, eliminating the need for additional operations on transfer stations. The algorithm controls the extent of expansion in the urban rail transit topology, overcoming query errors caused by mishandling of transfer stations in the existing algorithm. Finally, a series of <b>simulation</b> experiments were conducted on Beijing&rsquo;s urban rail transit network to validate both correctness and efficiency of the proposed adaptive topology optimization algorithm. The results demonstrate significant advantages compared to existing similar algorithms.</p></p class="citation"></blockquote><h2 id=mathds-1>math.DS (1)</h2><h3 id=11--270309-koopman-operators-with-intrinsic-observables-in-rigged-reproducing-kernel-hilbert-spaces-isao-ishikawa-et-al-2024>(1/1 | 270/309) Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces (Isao Ishikawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Isao Ishikawa, Yuka Hashimoto, Masahiro Ikeda, Yoshinobu Kawahara. (2024)<br><strong>Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces</strong><br><button class=copy-to-clipboard title="Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.DS<br>Categories: 47B33, 37M10, 65P99, 65F99, 47A10, cs-LG, math-DS, math-FA, math-SP, math.DS, stat-ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02524v1.pdf filename=2403.02524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel approach for estimating the Koopman operator defined on a reproducing kernel Hilbert space (RKHS) and its spectra. We propose an estimation method, what we call Jet Dynamic Mode Decomposition (JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion known as jets to enhance the estimation of the Koopman operator. This method refines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy, especially in the numerical estimation of eigenvalues. This paper proves JetDMD&rsquo;s superiority through explicit error bounds and convergence rate for special positive definite kernels, offering a solid theoretical foundation for its performance. We also delve into the spectral analysis of the Koopman operator, proposing the notion of extended Koopman operator within a framework of rigged Hilbert space. This notion leads to a deeper understanding of estimated Koopman eigenfunctions and capturing them outside the original function space. Through the theory of rigged Hilbert space, our study provides a principled methodology to analyze the estimated spectrum and eigenfunctions of Koopman operators, and enables eigendecomposition within a rigged RKHS. We also propose a new effective method for reconstructing the dynamical system from temporally-sampled trajectory data of the dynamical system with solid theoretical guarantee. We conduct several numerical <b>simulations</b> using the van der Pol oscillator, the Duffing oscillator, the H'enon map, and the Lorenz attractor, and illustrate the performance of JetDMD with clear numerical computations of eigenvalues and accurate predictions of the dynamical systems.</p></p class="citation"></blockquote><h2 id=cssy-1>cs.SY (1)</h2><h3 id=11--271309-collision-avoidance-and-geofencing-for-fixed-wing-aircraft-with-control-barrier-functions-tamas-g-molnar-et-al-2024>(1/1 | 271/309) Collision Avoidance and Geofencing for Fixed-wing Aircraft with Control Barrier Functions (Tamas G. Molnar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tamas G. Molnar, Suresh K. Kannan, James Cunningham, Kyle Dunlap, Kerianne L. Hobbs, Aaron D. Ames. (2024)<br><strong>Collision Avoidance and Geofencing for Fixed-wing Aircraft with Control Barrier Functions</strong><br><button class=copy-to-clipboard title="Collision Avoidance and Geofencing for Fixed-wing Aircraft with Control Barrier Functions" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SY<br>Categories: cs-RO, cs-SY, cs.SY, math-DS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02508v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02508v2.pdf filename=2403.02508v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safety-critical failures often have fatal consequences in aerospace control. Control systems on aircraft, therefore, must ensure the strict satisfaction of safety constraints, preferably with formal guarantees of safe behavior. This paper establishes the safety-critical control of fixed-wing aircraft in collision avoidance and geofencing tasks. A control framework is developed wherein a run-time assurance (RTA) system modulates the nominal flight controller of the aircraft whenever necessary to prevent it from colliding with other aircraft or crossing a boundary (geofence) in space. The RTA is formulated as a safety filter using control barrier functions (CBFs) with formal guarantees of safe behavior. CBFs are constructed and compared for a nonlinear kinematic fixed-wing aircraft model. The proposed CBF-based controllers showcase the capability of safely executing simultaneous collision avoidance and geofencing, as demonstrated by <b>simulations</b> on the kinematic model and a high-fidelity dynamical model.</p></p class="citation"></blockquote><h2 id=q-finpm-1>q-fin.PM (1)</h2><h3 id=11--272309-rvrae-a-dynamic-factor-model-based-on-variational-recurrent-autoencoder-for-stock-returns-prediction-yilun-wang-et-al-2024>(1/1 | 272/309) RVRAE: A Dynamic Factor Model Based on Variational Recurrent Autoencoder for Stock Returns Prediction (Yilun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yilun Wang, Shengjie Guo. (2024)<br><strong>RVRAE: A Dynamic Factor Model Based on Variational Recurrent Autoencoder for Stock Returns Prediction</strong><br><button class=copy-to-clipboard title="RVRAE: A Dynamic Factor Model Based on Variational Recurrent Autoencoder for Stock Returns Prediction" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.PM<br>Categories: cs-LG, q-fin-PM, q-fin-PR, q-fin.PM<br>Keyword Score: 20<br>Keywords: Autoencoder, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02500v1.pdf filename=2403.02500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the dynamic factor model has emerged as a dominant tool in economics and finance, particularly for investment strategies. This model offers improved handling of complex, nonlinear, and noisy market conditions compared to traditional static factor models. The advancement of machine learning, especially in dealing with nonlinear data, has further enhanced asset pricing methodologies. This paper introduces a groundbreaking dynamic factor model named RVRAE. This model is a probabilistic approach that addresses the temporal dependencies and noise in market data. RVRAE ingeniously combines the principles of dynamic factor modeling with the variational recurrent <b>autoencoder</b> (VRAE) from deep learning. A key feature of RVRAE is its use of a prior-posterior learning method. This method <b>fine-tunes</b> the model&rsquo;s learning process by seeking an optimal posterior factor model informed by future data. Notably, RVRAE is adept at risk modeling in volatile stock markets, estimating variances from latent space distributions while also predicting returns. Our empirical tests with real stock market data underscore RVRAE&rsquo;s superior performance compared to various established baseline methods.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--273309-the-complexity-of-computing-in-continuous-time-space-complexity-is-precision-manon-blanc-et-al-2024>(1/1 | 273/309) The complexity of computing in continuous time: space complexity is precision (Manon Blanc et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manon Blanc, Olivier Bournez. (2024)<br><strong>The complexity of computing in continuous time: space complexity is precision</strong><br><button class=copy-to-clipboard title="The complexity of computing in continuous time: space complexity is precision" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs.CC<br>Keyword Score: 20<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02499v1.pdf filename=2403.02499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Models of computations over the integers are equivalent from a computability and complexity theory point of view by the Church-Turing thesis. It is not possible to unify <b>discrete-time</b> <b>models</b> over the reals. The situation is unclear but simpler for <b>continuous-time</b> <b>models,</b> as there is a unifying mathematical model provided by ordinary differential equations (ODEs). For example, the GPAC model of Shannon is known to correspond to polynomial ODEs. However, the question of a robust complexity theory for such models and its relations to classical <b>(discrete)</b> <b>computation</b> theory is an old problem. There was some recent significant progress: it has been proved that (classical) time complexity corresponds to the length of the involved curves. The question of whether there is a simple and robust way to measure space complexity remains. We argue that space complexity corresponds to precision and conversely. We propose and prove an algebraic characterisation of FPSPACE, using <b>continuous</b> <b>ODEs.</b> Recent papers proposed algebraic characterisations of polynomial-time and -space complexity classes over the reals, but with a <b>discrete-time:</b> <b>those</b> algebras rely on <b>discrete</b> <b>ODE</b> schemes. Here, we use classical <b>(continuous)</b> <b>ODEs,</b> with the classic definition of derivation and hence with the more natural context of <b>continuous-time</b> <b>associated</b> with ODEs. We characterise both the case of polynomial space functions over the integers and the reals. We prove that Turing machines, with a proper representation of real numbers, can be simulated by <b>continuous</b> <b>ODEs</b> and not just <b>discrete</b> <b>ODEs.</b> A major consequence is that the associated space complexity is provably related to the numerical stability of involved schemas and the associated required precision. We obtain that a problem can be solved in polynomial space if and only if it can be simulated by some numerically stable ODE, using a polynomial precision.</p></p class="citation"></blockquote><h2 id=mathap-1>math.AP (1)</h2><h3 id=11--274309-exploring-well-posedness-and-asymptotic-behavior-in-an-advection-diffusion-reaction-adr-model-mohammed-elghandouri-et-al-2024>(1/1 | 274/309) Exploring Well-Posedness and Asymptotic Behavior in an Advection-Diffusion-Reaction (ADR) Model (Mohammed Elghandouri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammed Elghandouri, Khalil Ezzinbi, Lamiae Saidi. (2024)<br><strong>Exploring Well-Posedness and Asymptotic Behavior in an Advection-Diffusion-Reaction (ADR) Model</strong><br><button class=copy-to-clipboard title="Exploring Well-Posedness and Asymptotic Behavior in an Advection-Diffusion-Reaction (ADR) Model" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.AP<br>Categories: 35K57, 47D60, 35A01, 34D45, 65C20, cs-NA, math-AP, math-DS, math-NA, math.AP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02339v1.pdf filename=2403.02339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, the existence, uniqueness, and positivity of solutions, as well as the asymptotic behavior through a finite fractal dimensional global attractor for a general Advection-Diffusion-Reaction (ADR) equation, are investigated. Our findings are innovative, as we employ semigroups and global attractors theories to achieve these results. Also, an analytical solution of a two-dimensional Advection-Diffusion Equation is presented. And finally, two Explicit Finite Difference schemes are used to simulate solutions in the two- and three-dimensional cases. The numerical <b>simulations</b> are conducted with predefined initial and Dirichlet boundary conditions.</p></p class="citation"></blockquote><h2 id=csne-4>cs.NE (4)</h2><h3 id=14--275309-deep-reinforcement-learning-for-dynamic-algorithm-selection-a-proof-of-principle-study-on-differential-evolution-hongshu-guo-et-al-2024>(1/4 | 275/309) Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution (Hongshu Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongshu Guo, Yining Ma, Zeyuan Ma, Jiacheng Chen, Xinglin Zhang, Zhiguang Cao, Jun Zhang, Yue-Jiao Gong. (2024)<br><strong>Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02131v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02131v3.pdf filename=2403.02131v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evolutionary algorithms, such as Differential Evolution, excel in solving real-parameter optimization challenges. However, the effectiveness of a single algorithm varies across different problem instances, necessitating considerable efforts in algorithm selection or configuration. This paper aims to address the limitation by leveraging the complementary strengths of a group of algorithms and dynamically scheduling them throughout the optimization progress for specific problems. We propose a deep <b>reinforcement</b> <b>learning-based</b> dynamic algorithm selection framework to accomplish this task. Our approach models the dynamic algorithm selection a <b>Markov</b> <b>Decision</b> <b>Process,</b> training an agent in a policy gradient manner to select the most suitable algorithm according to the features observed during the optimization process. To empower the agent with the necessary information, our framework incorporates a thoughtful design of landscape and algorithmic features. Meanwhile, we employ a sophisticated deep neural network model to infer the optimal action, ensuring informed algorithm selections. Additionally, an algorithm context restoration mechanism is embedded to facilitate smooth switching among different algorithms. These mechanisms together enable our framework to seamlessly select and switch algorithms in a dynamic online fashion. Notably, the proposed framework is simple and generic, offering potential improvements across a broad spectrum of evolutionary algorithms. As a proof-of-principle study, we apply this framework to a group of Differential Evolution algorithms. The experimental results showcase the remarkable effectiveness of the proposed framework, not only enhancing the overall optimization performance but also demonstrating favorable generalization ability across different problem classes.</p></p class="citation"></blockquote><h3 id=24--276309-universality-of-reservoir-systems-with-recurrent-neural-networks-hiroki-yasumoto-et-al-2024>(2/4 | 276/309) Universality of reservoir systems with recurrent neural networks (Hiroki Yasumoto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hiroki Yasumoto, Toshiyuki Tanaka. (2024)<br><strong>Universality of reservoir systems with recurrent neural networks</strong><br><button class=copy-to-clipboard title="Universality of reservoir systems with recurrent neural networks" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-LG, cs-NE, cs.NE<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01900v1.pdf filename=2403.01900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Approximation capability of reservoir systems whose reservoir is a <b>recurrent</b> <b>neural</b> <b>network</b> <b>(RNN)</b> is discussed. In our problem setting, a reservoir system approximates a set of functions just by adjusting its linear readout while the reservoir is fixed. We will show what we call uniform strong universality of a family of <b>RNN</b> reservoir systems for a certain class of functions to be approximated. This means that, for any positive number, we can construct a sufficiently large <b>RNN</b> reservoir system whose approximation error for each function in the class of functions to be approximated is bounded from above by the positive number. Such <b>RNN</b> reservoir systems are constructed via parallel concatenation of <b>RNN</b> reservoirs.</p></p class="citation"></blockquote><h3 id=34--277309-analysis-and-fully-memristor-based-reservoir-computing-for-temporal-data-classification-ankur-singh-et-al-2024>(3/4 | 277/309) Analysis and Fully Memristor-based Reservoir Computing for Temporal Data Classification (Ankur Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ankur Singh, Sanghyeon Choi, Gunuk Wang, Maryaradhiya Daimari, Byung-Geun Lee. (2024)<br><strong>Analysis and Fully Memristor-based Reservoir Computing for Temporal Data Classification</strong><br><button class=copy-to-clipboard title="Analysis and Fully Memristor-based Reservoir Computing for Temporal Data Classification" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 13<br>Keywords: Benchmarking, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01827v1.pdf filename=2403.01827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reservoir computing (RC) offers a neuromorphic framework that is particularly effective for processing spatiotemporal signals. Known for its temporal processing prowess, RC significantly lowers training costs compared to conventional <b>recurrent</b> <b>neural</b> <b>networks.</b> A key component in its hardware deployment is the ability to generate dynamic reservoir states. Our research introduces a novel dual-memory RC system, integrating a short-term memory via a WOx-based memristor, capable of achieving 16 distinct states encoded over 4 bits, and a long-term memory component using a TiOx-based memristor within the readout layer. We thoroughly examine both memristor types and leverage the RC system to process temporal data sets. The performance of the proposed RC system is validated through two <b>benchmark</b> tasks: isolated spoken digit recognition with incomplete inputs and Mackey-Glass time series prediction. The system delivered an impressive 98.84% accuracy in digit recognition and sustained a low normalized root mean square error (NRMSE) of 0.036 in the time series prediction task, underscoring its capability. This study illuminates the adeptness of memristor-based RC systems in managing intricate temporal challenges, laying the groundwork for further innovations in neuromorphic computing.</p></p class="citation"></blockquote><h3 id=44--278309-toward-neuromic-computing-neurons-as-autoencoders-larry-bull-2024>(4/4 | 278/309) Toward Neuromic Computing: Neurons as Autoencoders (Larry Bull, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Larry Bull. (2024)<br><strong>Toward Neuromic Computing: Neurons as Autoencoders</strong><br><button class=copy-to-clipboard title="Toward Neuromic Computing: Neurons as Autoencoders" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02331v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02331v2.pdf filename=2403.02331v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The computational capabilities of dendrites have become increasingly clear. This letter presents the idea that neural backpropagation is using dendritic processing to enable individual neurons to perform autoencoding. Using a very simple connection weight search heuristic and artificial neural network model, the effects of interleaving autoencoding for each neuron in a hidden layer of a feedforward network are explored. This is contrasted to the standard layered approach to autoencoding. It is shown that such individualised processing is not detrimental and can improve network learning.</p></p class="citation"></blockquote><h2 id=quant-ph-3>quant-ph (3)</h2><h3 id=13--279309-hybrid-quantum-neural-network-advantage-for-radar-based-drone-detection-and-classification-in-low-signal-to-noise-ratio-aiswariya-sweety-malarvanan-2024>(1/3 | 279/309) Hybrid Quantum Neural Network Advantage for Radar-Based Drone Detection and Classification in Low Signal-to-Noise Ratio (Aiswariya Sweety Malarvanan, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aiswariya Sweety Malarvanan. (2024)<br><strong>Hybrid Quantum Neural Network Advantage for Radar-Based Drone Detection and Classification in Low Signal-to-Noise Ratio</strong><br><button class=copy-to-clipboard title="Hybrid Quantum Neural Network Advantage for Radar-Based Drone Detection and Classification in Low Signal-to-Noise Ratio" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, eess-SP, physics-app-ph, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02080v1.pdf filename=2403.02080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the performance of a Hybrid Quantum Neural Network (HQNN) and a comparable classical <b>Convolution</b> Neural Network <b>(CNN)</b> for detection and classification problem using a radar. Specifically, we take a fairly complex radar time-series model derived from electromagnetic theory, namely the Martin-Mulgrew model, that is used to simulate radar returns of objects with rotating blades, such as drones. We find that when that signal-to-noise ratio (SNR) is high, <b>CNN</b> outperforms the HQNN for detection and classification. However, in the low SNR regime (which is of greatest interest in practice) the performance of HQNN is found to be superior to that of the <b>CNN</b> of a similar architecture.</p></p class="citation"></blockquote><h3 id=23--280309-classification-of-the-fashion-mnist-dataset-on-a-quantum-computer-kevin-shen-et-al-2024>(2/3 | 280/309) Classification of the Fashion-MNIST Dataset on a Quantum Computer (Kevin Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin Shen, Bernhard Jobst, Elvira Shishenina, Frank Pollmann. (2024)<br><strong>Classification of the Fashion-MNIST Dataset on a Quantum Computer</strong><br><button class=copy-to-clipboard title="Classification of the Fashion-MNIST Dataset on a Quantum Computer" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02405v1.pdf filename=2403.02405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The potential impact of quantum machine learning algorithms on industrial applications remains an exciting open question. Conventional methods for encoding classical data into quantum computers are not only too costly for a potential quantum advantage in the algorithms but also severely limit the scale of feasible experiments on current hardware. Therefore, recent works, despite claiming the near-term suitability of their algorithms, do not provide experimental <b>benchmarking</b> on standard machine learning datasets. We attempt to solve the data encoding problem by improving a recently proposed variational algorithm [1] that approximately prepares the encoded data, using asymptotically shallow circuits that fit the native gate set and topology of currently available quantum computers. We apply the improved algorithm to encode the Fashion-MNIST dataset [2], which can be directly used in future empirical studies of quantum machine learning algorithms. We deploy simple quantum variational classifiers trained on the encoded dataset on a current quantum computer ibmq-kolkata [3] and achieve moderate accuracies, providing a proof of concept for the near-term usability of our data encoding method.</p></p class="citation"></blockquote><h3 id=33--281309-hybrid-quantum-programming-with-pennylane-lightning-on-hpc-platforms-ali-asadi-et-al-2024>(3/3 | 281/309) Hybrid quantum programming with PennyLane Lightning on HPC platforms (Ali Asadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Asadi, Amintor Dusko, Chae-Yeun Park, Vincent Michaud-Rioux, Isidor Schoch, Shuli Shu, Trevor Vincent, Lee James O&rsquo;Riordan. (2024)<br><strong>Hybrid quantum programming with PennyLane Lightning on HPC platforms</strong><br><button class=copy-to-clipboard title="Hybrid quantum programming with PennyLane Lightning on HPC platforms" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-DC, cs-ET, physics-comp-ph, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02512v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02512v1.pdf filename=2403.02512v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce PennyLane&rsquo;s Lightning suite, a collection of high-performance state-vector simulators targeting CPU, GPU, and HPC-native architectures and workloads. Quantum applications such as QAOA, VQE, and synthetic workloads are implemented to demonstrate the supported classical computing architectures and showcase the scale of problems that can be simulated using our tooling. We <b>benchmark</b> the performance of Lightning with backends supporting CPUs, as well as NVidia and AMD GPUs, and compare the results to other commonly used high-performance simulator packages, demonstrating where Lightning&rsquo;s implementations give performance leads. We show improved CPU performance by employing explicit SIMD intrinsics and multi-threading, batched task-based execution across multiple GPUs, and distributed forward and gradient-based quantum circuit executions across multiple nodes. Our data shows we can comfortably simulate a variety of circuits, giving examples with up to 30 qubits on a single device or node, and up to 41 qubits using multiple nodes.</p></p class="citation"></blockquote><h2 id=csit-1>cs.IT (1)</h2><h3 id=11--282309-otfs-vs-ofdm-which-is-superior-in-multiuser-leo-satellite-communications-yu-liu-et-al-2024>(1/1 | 282/309) OTFS vs OFDM: Which is Superior in Multiuser LEO Satellite Communications (Yu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Liu, Ming Chen, Cunhua Pan, Tantao Gong, Jinhong Yuan, Jiangzhou Wang. (2024)<br><strong>OTFS vs OFDM: Which is Superior in Multiuser LEO Satellite Communications</strong><br><button class=copy-to-clipboard title="OTFS vs OFDM: Which is Superior in Multiuser LEO Satellite Communications" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02012v1.pdf filename=2403.02012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Orthogonal time frequency space (OTFS) modulation, a delay-Doppler (DD) domain communication scheme exhibiting strong robustness against the Doppler shifts, has the potentials to be employed in LEO satellite communications. However, the performance comparison with the orthogonal frequency division multiplexing (OFDM) modulation and the resource allocation scheme for multiuser OTFS-based LEO satellite communication system have rarely been investigated. In this paper, we conduct a performance comparison under various channel conditions between the OTFS and OFDM modulations, encompassing evaluations of sum-rate and bit error ratio (BER). Additionally, we investigate the joint optimal allocation of power and delay-Doppler resource blocks aiming at maximizing sum-rate for multiuser downlink OTFS-based LEO satellite communication systems. Unlike the conventional modulations relaying on complex input-output relations within the Time-Frequency (TF) domain, the OTFS modulation exploits both time and frequency diversities, i.e., delay and Doppler shifts remain constant during a OTFS frame, which facilitates a DD domain input-output simple relation for our investigation. We transform the resulting non-convex and combinatorial optimization problem into an equivalent difference of convex problem by decoupling the conditional constraints, and solve the transformed problem via penalty convex-concave procedure algorithm. <b>Simulation</b> results demonstrate that the OTFS modulation is robust to carrier frequency offsets (CFO) caused by high-mobility of LEO satellites, and has superior performance to the OFDM modulation. Moreover, numerical results indicate that our proposed resource allocation scheme has higher sum-rate than existed schemes for the OTFS modulation, such as delay divided multiple access and Doppler divided multiple access, especially in the high signal-to-noise ratio (SNR) regime.</p></p class="citation"></blockquote><h2 id=eesssy-4>eess.SY (4)</h2><h3 id=14--283309-progressive-smoothing-for-motion-planning-in-real-time-nmpc-rudolf-reiter-et-al-2024>(1/4 | 283/309) Progressive Smoothing for Motion Planning in Real-Time NMPC (Rudolf Reiter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rudolf Reiter, Katrin Baumgärtner, Rien Quirynen, Moritz Diehl. (2024)<br><strong>Progressive Smoothing for Motion Planning in Real-Time NMPC</strong><br><button class=copy-to-clipboard title="Progressive Smoothing for Motion Planning in Real-Time NMPC" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01830v1.pdf filename=2403.01830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nonlinear model predictive control (NMPC) is a popular strategy for solving motion planning problems, including obstacle avoidance constraints, in autonomous driving applications. Non-smooth obstacle shapes, such as rectangles, introduce additional local minima in the underlying optimization problem. Smooth over-approximations, e.g., ellipsoidal shapes, limit the performance due to their conservativeness. We propose to vary the smoothness and the related over-approximation by a homotopy. Instead of varying the smoothness in consecutive sequential quadratic programming iterations, we use formulations that decrease the smooth over-approximation from the end towards the beginning of the prediction horizon. Thus, the real-time iterations algorithm is applicable to the proposed NMPC formulation. Different formulations are compared in <b>simulation</b> experiments and shown to successfully improve performance indicators without increasing the computation time.</p></p class="citation"></blockquote><h3 id=24--284309-tuning-and-testing-an-online-feedback-optimization-controller-to-provide-curative-distribution-grid-flexibility-lukas-ortmann-et-al-2024>(2/4 | 284/309) Tuning and Testing an Online Feedback Optimization Controller to Provide Curative Distribution Grid Flexibility (Lukas Ortmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas Ortmann, Fabian Böhm, Florian Klein-Helmkamp, Andreas Ulbig, Saverio Bolognani, Florian Dörfler. (2024)<br><strong>Tuning and Testing an Online Feedback Optimization Controller to Provide Curative Distribution Grid Flexibility</strong><br><button class=copy-to-clipboard title="Tuning and Testing an Online Feedback Optimization Controller to Provide Curative Distribution Grid Flexibility" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01782v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01782v1.pdf filename=2403.01782v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to more volatile generation, flexibility will become more important in transmission grids. One potential source of this flexibility can be distribution grids. A flexibility request from the transmission grid to a distribution grid then needs to be split up onto the different flexibility providing units (FPU) in the distribution grid. One potential way to do this is Online Feedback Optimization (OFO). OFO is a new control method that steers power systems to the optimal solution of an optimization problem using minimal model information and computation power. This paper will show how to choose the optimization problem and how to tune the OFO controller. Afterward, we test the resulting controller on a real distribution grid laboratory and show its performance, its interaction with other controllers in the grid, and how it copes with disturbances. Overall, the paper makes a clear <b>recommendation</b> on how to phrase the optimization problem and tune the OFO controller. Furthermore, it experimentally verifies that an OFO controller is a powerful tool to disaggregate flexibility requests onto FPUs while satisfying operational constraints inside the flexibility providing distribution grid.</p></p class="citation"></blockquote><h3 id=34--285309-cooperative-and-interaction-aware-driver-model-for-lane-change-maneuver-jemin-woo-et-al-2024>(3/4 | 285/309) Cooperative and Interaction-aware Driver Model for Lane Change Maneuver (Jemin Woo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jemin Woo, Changsun Ahn. (2024)<br><strong>Cooperative and Interaction-aware Driver Model for Lane Change Maneuver</strong><br><button class=copy-to-clipboard title="Cooperative and Interaction-aware Driver Model for Lane Change Maneuver" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01752v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01752v1.pdf filename=2403.01752v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To achieve complete autonomous vehicles, it is crucial for autonomous vehicles to communicate and interact with their surrounding vehicles. Especially, since the lane change scenarios do not have traffic signals and traffic rules, the interactions between vehicles need to be considered for the autonomous vehicles. To address this issue, we propose a cooperative and interaction-aware decision-making algorithm for autonomous vehicles that stochastically considers the future behavior of surrounding vehicles based on actual driving data. The algorithm is designed for both lane changing and lane keeping vehicles, and effectively considers interaction by using an interaction model based on relative information between vehicles with fewer states. To design the decision-making, the interaction model is defined as <b>Markov</b> <b>decision</b> <b>process,</b> and stochastic dynamic programming is used to solve the <b>Markov</b> <b>decision</b> <b>process.</b> We validate the effectiveness of our proposed algorithm in lane change scenarios that require interaction. Our results demonstrate that the proposed algorithm enables cooperative and interaction-aware decision-making while accommodating various driving styles. Additionally, by comparing it with other methods, such as the intelligent driver model and game theory-based decision-making, we validate the safety and comfortable decision-making of our proposed algorithm. Furthermore, through driving with a human-driven vehicle, it is confirmed that the proposed decision-making enables to cooperatively and effectively drive with humans.</p></p class="citation"></blockquote><h3 id=44--286309-a-frequency-domain-approach-for-enhanced-performance-and-task-flexibility-in-finite-time-ilc-max-van-haren-et-al-2024>(4/4 | 286/309) A Frequency-Domain Approach for Enhanced Performance and Task Flexibility in Finite-Time ILC (Max van Haren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Max van Haren, Kentaro Tsurumoto, Masahiro Mae, Lennart Blanken, Wataru Ohnishi, Tom Oomen. (2024)<br><strong>A Frequency-Domain Approach for Enhanced Performance and Task Flexibility in Finite-Time ILC</strong><br><button class=copy-to-clipboard title="A Frequency-Domain Approach for Enhanced Performance and Task Flexibility in Finite-Time ILC" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02039v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02039v1.pdf filename=2403.02039v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Iterative learning control (ILC) is capable of improving the tracking performance of repetitive control systems by utilizing data from past iterations. The aim of this paper is to achieve both task flexibility, which is often achieved by ILC with basis functions, and the performance of frequency-domain ILC, with an intuitive design procedure. The cost function of norm-optimal ILC is determined that recovers frequency-domain ILC, and consequently, the feedforward signal is parameterized in terms of basis functions and frequency-domain ILC. The resulting method has the performance and design procedure of frequency-domain ILC and the task flexibility of basis functions ILC, and are complimentary to each other. Validation on a <b>benchmark</b> example confirms the capabilities of the framework.</p></p class="citation"></blockquote><h2 id=csfl-1>cs.FL (1)</h2><h3 id=11--287309-active-learning-of-mealy-machines-with-timers-véronique-bruyère-et-al-2024>(1/1 | 287/309) Active Learning of Mealy Machines with Timers (Véronique Bruyère et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Véronique Bruyère, Bharat Garhewal, Guillermo A. Pérez, Gaëtan Staquet, Frits W. Vaandrager. (2024)<br><strong>Active Learning of Mealy Machines with Timers</strong><br><button class=copy-to-clipboard title="Active Learning of Mealy Machines with Timers" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.FL<br>Categories: 68Q45, F-4-3, cs-FL, cs-LG, cs.FL<br>Keyword Score: 18<br>Keywords: Active Learning, Benchmarking, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02019v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02019v1.pdf filename=2403.02019v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the first algorithm for query learning of a general class of Mealy machines with timers (MMTs) in a <b>black-box</b> <b>context.</b> Our algorithm is an extension of the L# algorithm of Vaandrager et al. to a timed setting. Like the algorithm for learning timed automata proposed by Waga, our algorithm is inspired by ideas of Maler & Pnueli. Based on the elementary languages of, both Waga&rsquo;s and our algorithm use symbolic queries, which are then implemented using finitely many concrete queries. However, whereas Waga needs exponentially many concrete queries to implement a single symbolic query, we only need a polynomial number. This is because in order to learn a timed automaton, a learner needs to determine the exact guard and reset for each transition (out of exponentially many possibilities), whereas for learning an MMT a learner only needs to figure out which of the preceding transitions caused a timeout. As shown in our previous work, this can be done efficiently for a subclass of MMTs that are race-avoiding: if a timeout is caused by a preceding input then a slight change in the timing of this input will induce a corresponding change in the timing of the timeout (&ldquo;wiggling&rdquo;). Experiments with a prototype implementation, written in Rust, show that our algorithm is able to efficiently learn realistic <b>benchmarks.</b></p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--288309-magicclay-sculpting-meshes-with-generative-neural-fields-amir-barda-et-al-2024>(1/1 | 288/309) MagicClay: Sculpting Meshes With Generative Neural Fields (Amir Barda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Barda, Vladimir G. Kim, Noam Aigerman, Amit H. Bermano, Thibault Groueix. (2024)<br><strong>MagicClay: Sculpting Meshes With Generative Neural Fields</strong><br><button class=copy-to-clipboard title="MagicClay: Sculpting Meshes With Generative Neural Fields" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 15<br>Keywords: Geometry, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02460v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02460v1.pdf filename=2403.02460v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent developments in neural fields have brought phenomenal capabilities to the field of shape generation, but they lack crucial properties, such as incremental control - a fundamental requirement for artistic work. Triangular meshes, on the other hand, are the representation of choice for most <b>geometry</b> related tasks, offering efficiency and intuitive control, but do not lend themselves to neural optimization. To support downstream tasks, previous art typically proposes a two-step approach, where first a shape is generated using neural fields, and then a mesh is extracted for further processing. Instead, in this paper we introduce a hybrid approach that maintains both a mesh and a Signed Distance Field (SDF) representations consistently. Using this representation, we introduce MagicClay - an artist friendly tool for sculpting regions of a mesh according to textual <b>prompts</b> while keeping other regions untouched. Our framework carefully and efficiently balances consistency between the representations and regularizations in every step of the shape optimization; Relying on the mesh representation, we show how to render the SDF at higher resolutions and faster. In addition, we employ recent work in differentiable mesh reconstruction to adaptively allocate triangles in the mesh where required, as indicated by the SDF. Using an implemented prototype, we demonstrate superior generated <b>geometry</b> compared to the state-of-the-art, and novel consistent control, allowing sequential <b>prompt-based</b> edits to the same mesh for the first time.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--289309-preserving-smart-grid-integrity-a-differential-privacy-framework-for-secure-detection-of-false-data-injection-attacks-in-the-smart-grid-nikhil-ravi-et-al-2024>(1/1 | 289/309) Preserving Smart Grid Integrity: A Differential Privacy Framework for Secure Detection of False Data Injection Attacks in the Smart Grid (Nikhil Ravi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikhil Ravi, Anna Scaglione, Sean Peisert, Parth Pradhan. (2024)<br><strong>Preserving Smart Grid Integrity: A Differential Privacy Framework for Secure Detection of False Data Injection Attacks in the Smart Grid</strong><br><button class=copy-to-clipboard title="Preserving Smart Grid Integrity: A Differential Privacy Framework for Secure Detection of False Data Injection Attacks in the Smart Grid" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-CR, eess-SP, eess.SP<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02324v1.pdf filename=2403.02324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a framework based on <b>differential</b> <b>privacy</b> (DP) for querying electric power measurements to detect system anomalies or bad data caused by false data injections (FDIs). Our DP approach conceals consumption and system matrix data, while simultaneously enabling an untrusted third party to test hypotheses of anomalies, such as an FDI attack, by releasing a randomized sufficient statistic for hypothesis-testing. We consider a measurement model corrupted by Gaussian noise and a sparse noise vector representing the attack, and we observe that the optimal test statistic is a chi-square random variable. To detect possible attacks, we propose a novel DP chi-square noise mechanism that ensures the test does not reveal private information about power injections or the system matrix. The proposed framework provides a robust solution for detecting FDIs while preserving the privacy of sensitive power system data.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--290309-policy-space-response-oracles-a-survey-ariyan-bighashdel-et-al-2024>(1/1 | 290/309) Policy Space Response Oracles: A Survey (Ariyan Bighashdel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ariyan Bighashdel, Yongzhao Wang, Stephen McAleer, Rahul Savani, Frans A. Oliehoek. (2024)<br><strong>Policy Space Response Oracles: A Survey</strong><br><button class=copy-to-clipboard title="Policy Space Response Oracles: A Survey" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-AI, cs-GT, cs-MA, cs.GT<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02227v1.pdf filename=2403.02227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In game theory, a game refers to a model of interaction among rational decision-makers or players, making choices with the goal of achieving their individual objectives. Understanding their behavior in games is often referred to as game <b>reasoning.</b> This survey provides a comprehensive overview of a fast-developing game-reasoning framework for large games, known as Policy Space Response Oracles (PSRO). We first motivate PSRO, provide historical context, and position PSRO within game-reasoning approaches. We then focus on the strategy exploration issue for PSRO, the challenge of assembling an effective strategy portfolio for modeling the underlying game with minimum computational cost. We also survey current research directions for enhancing the efficiency of PSRO, and explore the applications of PSRO across various domains. We conclude by discussing open questions and future research.</p></p class="citation"></blockquote><h2 id=cslo-2>cs.LO (2)</h2><h3 id=12--291309-unknown-biases-and-timing-constraints-in-timed-automata-darion-haase-et-al-2024>(1/2 | 291/309) Unknown Biases and Timing Constraints in Timed Automata (Darion Haase et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Darion Haase, Joost-Pieter Katoen. (2024)<br><strong>Unknown Biases and Timing Constraints in Timed Automata</strong><br><button class=copy-to-clipboard title="Unknown Biases and Timing Constraints in Timed Automata" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-FL, cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02210v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02210v1.pdf filename=2403.02210v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Timed automata are the formal model for real-time systems. Extensions with discrete probabilistic branching have been considered in the literature and successfully applied. Probabilistic timed automata (PTA) do require all branching probabilities and clock constraints to be constants. This report investigates PTA in which this constraint is relaxed: both branching probabilities and clock constraints can be parametric. We formally define this PTA variant and define its semantics by an uncountable parametric <b>Markov</b> <b>Decision</b> <b>Process</b> (pMDP). We show that reachability probabilities in parametric L/U-PTA can be reduced to considering PTA with only parametric branching probabilities. This enables the usage of existing techniques from the literature. Finally, we generalize the symbolic backward and digital clock semantics of PTA to the setting with parametric probabilities and constraints.</p></p class="citation"></blockquote><h3 id=22--292309-deciding-separation-logic-with-pointer-arithmetic-and-inductive-definitions-wanyun-su-et-al-2024>(2/2 | 292/309) Deciding Separation Logic with Pointer Arithmetic and Inductive Definitions (Wanyun Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanyun Su, Zhilin Wu, Mihaela Sighireanu. (2024)<br><strong>Deciding Separation Logic with Pointer Arithmetic and Inductive Definitions</strong><br><button class=copy-to-clipboard title="Deciding Separation Logic with Pointer Arithmetic and Inductive Definitions" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01867v1.pdf filename=2403.01867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pointer arithmetic is widely used in low-level programs, e.g. memory allocators. The specification of such programs usually requires using pointer arithmetic inside inductive definitions to define the common data structures, e.g. heap lists in memory allocators. In this work, we investigate decision problems for SLAH, a separation logic fragment that allows pointer arithmetic inside inductive definitions, thus enabling specification of properties for programs manipulating heap lists. Pointer arithmetic inside inductive definitions is challenging for automated <b>reasoning.</b> We tackle this challenge and achieve decision procedures for both satisfiability and entailment of SLAH formulas. The crux of our decision procedure for satisfiability is to compute summaries of inductive definitions. We show that although the summary is naturally expressed as an existentially quantified non-linear arithmetic formula, it can actually be transformed into an equivalent linear arithmetic formula. The decision procedure for entailment, on the other hand, has to match and split the spatial atoms according to the arithmetic relation between address variables. We report on the implementation of these decision procedures and their good performance in solving problems issued from the verification of building block programs used in memory allocators.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--293309-a-multi-agent-reinforcement-learning-study-of-evolution-of-communication-and-teaching-under-libertarian-and-utilitarian-governing-systems-aslan-s-dizaji-2024>(1/1 | 293/309) A Multi-agent Reinforcement Learning Study of Evolution of Communication and Teaching under Libertarian and Utilitarian Governing Systems (Aslan S. Dizaji, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aslan S. Dizaji. (2024)<br><strong>A Multi-agent Reinforcement Learning Study of Evolution of Communication and Teaching under Libertarian and Utilitarian Governing Systems</strong><br><button class=copy-to-clipboard title="A Multi-agent Reinforcement Learning Study of Evolution of Communication and Teaching under Libertarian and Utilitarian Governing Systems" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02369v1.pdf filename=2403.02369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Laboratory experiments have shown that communication plays an important role in solving social dilemmas. Here, by extending the AI-Economist, a mixed motive multi-agent <b>reinforcement</b> <b>learning</b> environment, I intend to find an answer to the following descriptive question: which governing system does facilitate the emergence and evolution of communication and teaching among agents? To answer this question, the AI-Economist is extended by a voting mechanism to simulate three different governing systems across individualistic-collectivistic axis, from full-libertarian to Full-Utilitarian governing systems. Moreover, the AI-Economist is further extended to include communication with possible misalignment, a variant of signalling game, by letting agents to build houses together if they are able to name mutually complement material resources by the same letter. Moreover, another extension is made to the AI-Economist to include teaching with possible misalignment, again a variant of signalling game, by letting half the agents as teachers who know how to use mutually complement material resources to build houses but are not capable of building actual houses, and the other half as students who do not have this information but are able to actually build those houses if teachers teach them. I found a strong evidence that collectivistic environment such as Full-Utilitarian system is more favourable for the emergence of communication and teaching, or more precisely, evolution of language alignment. Moreover, I found some evidence that evolution of language alignment through communication and teaching under collectivistic governing systems makes individuals more advantageously inequity averse. As a result, there is a positive correlation between evolution of language alignment and equality in the society.</p></p class="citation"></blockquote><h2 id=q-bioto-1>q-bio.TO (1)</h2><h3 id=11--294309-embracing-uncertainty-flexibility-harnessing-a-supervised-tree-kernel-to-empower-ensemble-modelling-for-2d-echocardiography-based-prediction-of-right-ventricular-volume-tuan-a-bohoran-et-al-2024>(1/1 | 294/309) Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of Right Ventricular Volume (Tuan A. Bohoran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuan A. Bohoran, Polydoros N. Kampaktsis, Laura McLaughlin, Jay Leb, Gerry P. McCann, Archontis Giannakidis. (2024)<br><strong>Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of Right Ventricular Volume</strong><br><button class=copy-to-clipboard title="Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of Right Ventricular Volume" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.TO<br>Categories: cs-LG, eess-IV, math-AP, q-bio-TO, q-bio.TO<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03229v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03229v1.pdf filename=2403.03229v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The right ventricular (RV) function deterioration strongly predicts clinical outcomes in numerous circumstances. To boost the clinical deployment of ensemble regression methods that quantify RV volumes using tabular data from the widely available two-dimensional echocardiography (2DE), we propose to complement the volume predictions with uncertainty scores. To this end, we employ an instance-based method which uses the learned tree structure to identify the nearest training samples to a target instance and then uses a number of distribution types to more flexibly model the output. The probabilistic and point-prediction performances of the proposed framework are evaluated on a relatively small-scale dataset, comprising 100 end-diastolic and end-systolic RV volumes. The reference values for point performance were obtained from MRI. The results demonstrate that our flexible approach yields improved probabilistic and point performances over other state-of-the-art methods. The appropriateness of the proposed framework is showcased by providing exemplar cases. The estimated uncertainty embodies both aleatoric and epistemic types. This work aligns with trustworthy artificial intelligence since it can be used to enhance the decision-making process and reduce risks. The feature importance scores of our framework can be exploited to reduce the number of required 2DE views which could enhance the proposed pipeline&rsquo;s clinical application.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--295309-tsallis-entropy-regularization-for-linearly-solvable-mdp-and-linear-quadratic-regulator-yota-hashizume-et-al-2024>(1/1 | 295/309) Tsallis Entropy Regularization for Linearly Solvable MDP and Linear Quadratic Regulator (Yota Hashizume et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yota Hashizume, Koshi Oishi, Kenji Kashima. (2024)<br><strong>Tsallis Entropy Regularization for Linearly Solvable MDP and Linear Quadratic Regulator</strong><br><button class=copy-to-clipboard title="Tsallis Entropy Regularization for Linearly Solvable MDP and Linear Quadratic Regulator" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01805v1.pdf filename=2403.01805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Shannon entropy regularization is widely adopted in optimal control due to its ability to promote exploration and enhance robustness, e.g., maximum entropy <b>reinforcement</b> <b>learning</b> known as Soft Actor-Critic. In this paper, Tsallis entropy, which is a one-parameter extension of Shannon entropy, is used for the regularization of linearly solvable MDP and linear quadratic regulators. We derive the solution for these problems and demonstrate its usefulness in balancing between exploration and sparsity of the obtained control law.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--296309-rcoco-contrastive-collective-link-prediction-across-multiplex-network-in-riemannian-space-li-sun-et-al-2024>(1/1 | 296/309) RCoCo: Contrastive Collective Link Prediction across Multiplex Network in Riemannian Space (Li Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Sun, Mengjie Li, Yong Yang, Xiao Li, Lin Liu, Pengfei Zhang, Haohua Du. (2024)<br><strong>RCoCo: Contrastive Collective Link Prediction across Multiplex Network in Riemannian Space</strong><br><button class=copy-to-clipboard title="RCoCo: Contrastive Collective Link Prediction across Multiplex Network in Riemannian Space" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-LG, cs-SI, cs.SI<br>Keyword Score: 8<br>Keywords: Graph, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01864v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01864v1.pdf filename=2403.01864v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Link prediction typically studies the probability of future interconnection among nodes with the observation in a single social network. More often than not, real scenario is presented as a multiplex network with common (anchor) users active in multiple social networks. In the literature, most existing works study either the intra-link prediction in a single network or inter-link prediction among networks (a.k.a. network alignment), and consider two learning tasks are independent from each other, which is still away from the fact. On the representation space, the vast majority of existing methods are built upon the traditional Euclidean space, unaware of the inherent <b>geometry</b> of social networks. The third issue is on the scarce anchor users. Annotating anchor users is laborious and expensive, and thus it is impractical to work with quantities of anchor users. Herein, in light of the issues above, we propose to study a challenging yet practical problem of <b>Geometry-aware</b> Collective Link Prediction across Multiplex Network. To address this problem, we present a novel contrastive model, RCoCo, which collaborates intra- and inter-network behaviors in Riemannian spaces. In RCoCo, we design a curvature-aware <b>graph</b> attention network ($\kappa-$GAT), conducting attention mechanism in Riemannian manifold whose curvature is estimated by the Ricci curvatures over the network. Thereafter, we formulate intra- and inter-contrastive loss in the manifolds, in which we augment <b>graphs</b> by exploring the high-order structure of community and information transfer on anchor users. Finally, we conduct extensive experiments with 14 strong baselines on 8 real-world datasets, and show the effectiveness of RCoCo.</p></p class="citation"></blockquote><h2 id=astro-phsr-1>astro-ph.SR (1)</h2><h3 id=11--297309-forecasting-sep-events-during-solar-cycles-23-and-24-using-interpretable-machine-learning-spiridon-kasapis-et-al-2024>(1/1 | 297/309) Forecasting SEP Events During Solar Cycles 23 and 24 Using Interpretable Machine Learning (Spiridon Kasapis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Spiridon Kasapis, Irina N. Kitiashvili, Paul Kosovich, Alexander G. Kosovichev, Viacheslav M. Sadykov, Patrick O&rsquo;Keefe, Vincent Wang. (2024)<br><strong>Forecasting SEP Events During Solar Cycles 23 and 24 Using Interpretable Machine Learning</strong><br><button class=copy-to-clipboard title="Forecasting SEP Events During Solar Cycles 23 and 24 Using Interpretable Machine Learning" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.SR<br>Categories: astro-ph-SR, astro-ph.SR, cs-LG, physics-space-ph<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02536v1.pdf filename=2403.02536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prediction of the Solar Energetic Particle (SEP) events garner increasing interest as space missions extend beyond Earth&rsquo;s protective magnetosphere. These events, which are, in most cases, products of magnetic reconnection-driven processes during solar flares or fast coronal-mass-ejection-driven shock waves, pose significant radiation hazards to aviation, space-based electronics, and particularly, space exploration. In this work, we utilize the recently developed dataset that combines the Solar Dynamics Observatory/Helioseismic and Magnetic Imager&rsquo;s (SDO/HMI) Space weather HMI Active Region Patches (SHARP) and the Solar and Heliospheric Observatory/Michelson Doppler Imager&rsquo;s (SoHO/MDI) Space Weather MDI Active Region Patches (SMARP). We employ a suite of machine learning strategies, including Support Vector Machines (SVM) and regression models, to evaluate the predictive potential of this new data product for a forecast of post-solar flare SEP events. Our study indicates that despite the augmented volume of data, the prediction accuracy reaches 0.7 +- 0.1, which aligns with but does not exceed these published <b>benchmarks.</b> A linear SVM model with training and testing configurations that mimic an operational setting (positive-negative imbalance) reveals a slight increase (+ 0.04 +- 0.05) in the accuracy of a 14-hour SEP forecast compared to previous studies. This outcome emphasizes the imperative for more sophisticated, physics-informed models to better understand the underlying processes leading to SEP events.</p></p class="citation"></blockquote><h2 id=econem-1>econ.EM (1)</h2><h3 id=11--298309-applied-causal-inference-powered-by-ml-and-ai-victor-chernozhukov-et-al-2024>(1/1 | 298/309) Applied Causal Inference Powered by ML and AI (Victor Chernozhukov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Chernozhukov, Christian Hansen, Nathan Kallus, Martin Spindler, Vasilis Syrgkanis. (2024)<br><strong>Applied Causal Inference Powered by ML and AI</strong><br><button class=copy-to-clipboard title="Applied Causal Inference Powered by ML and AI" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.EM<br>Categories: cs-LG, econ-EM, econ.EM, stat-ME, stat-ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02467v1.pdf filename=2403.02467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical <b>graphs</b> (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools.</p></p class="citation"></blockquote><h2 id=mathco-3>math.CO (3)</h2><h3 id=13--299309-minimum-acyclic-number-and-maximum-dichromatic-number-of-oriented-triangle-free-graphs-of-a-given-order-pierre-aboulker-et-al-2024>(1/3 | 299/309) Minimum acyclic number and maximum dichromatic number of oriented triangle-free graphs of a given order (Pierre Aboulker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Aboulker, Frédéric Havet, François Pirot, Juliette Schabanel. (2024)<br><strong>Minimum acyclic number and maximum dichromatic number of oriented triangle-free graphs of a given order</strong><br><button class=copy-to-clipboard title="Minimum acyclic number and maximum dichromatic number of oriented triangle-free graphs of a given order" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C20, 05C55, cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02298v1.pdf filename=2403.02298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Let $D$ be a digraph. Its acyclic number $\vec{\alpha}(D)$ is the maximum order of an acyclic induced subdigraph and its dichromatic number $\vec{\chi}(D)$ is the least integer $k$ such that $V(D)$ can be partitioned into $k$ subsets inducing acyclic subdigraphs. We study ${\vec a}(n)$ and $\vec t(n)$ which are the minimum of $\vec\alpha(D)$ and the maximum of $\vec{\chi}(D)$, respectively, over all oriented triangle-free <b>graphs</b> of order $n$. For every $\epsilon>0$ and $n$ large enough, we show $(1/\sqrt{2} - \epsilon) \sqrt{n\log n} \leq \vec{a}(n) \leq \frac{107}{8} \sqrt n \log n$ and $\frac{8}{107} \sqrt n/\log n \leq \vec{t}(n) \leq (\sqrt 2 + \epsilon) \sqrt{n/\log n}$. We also construct an oriented triangle-free <b>graph</b> on 25 vertices with dichromatic number~3, and show that every oriented triangle-free <b>graph</b> of order at most 17 has dichromatic number at most 2.</p></p class="citation"></blockquote><h3 id=23--300309-characterization-of-chordal-circular-arc-graphs-i-split-graphs-yixin-cao-et-al-2024>(2/3 | 300/309) Characterization of Chordal Circular-arc Graphs: I. Split Graphs (Yixin Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixin Cao, Jan Derbisz, Tomasz Krawczyk. (2024)<br><strong>Characterization of Chordal Circular-arc Graphs: I. Split Graphs</strong><br><button class=copy-to-clipboard title="Characterization of Chordal Circular-arc Graphs: I. Split Graphs" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01947v1.pdf filename=2403.01947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The most elusive problem around the class of circular-arc <b>graphs</b> is identifying all minimal <b>graphs</b> that are not in this class. The main obstacle is the lack of a systematic way of enumerating these minimal <b>graphs.</b> McConnell [FOCS 2001] presented a transformation from circular-arc <b>graphs</b> to interval <b>graphs</b> with certain patterns of representations. We fully characterize these interval patterns for circular-arc <b>graphs</b> that are split <b>graphs,</b> thereby building a connection between minimal split <b>graphs</b> that are not circular-arc <b>graphs</b> and minimal non-interval <b>graphs.</b> This connection enables us to identify all minimal split <b>graphs</b> that are not circular-arc <b>graphs.</b> As a byproduct, we develop a linear-time certifying recognition algorithm for circular-arc <b>graphs</b> when the input is a split <b>graph.</b></p></p class="citation"></blockquote><h3 id=33--301309-weakly-modular-graphs-with-diamond-condition-the-interval-function-and-axiomatic-characterizations-lekshmi-kamal-kamalolbhavan-sheela-et-al-2024>(3/3 | 301/309) Weakly modular graphs with diamond condition, the interval function and axiomatic characterizations (Lekshmi Kamal Kamalolbhavan-Sheela et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lekshmi Kamal Kamalolbhavan-Sheela, Jeny Jacob, Manoj Changat. (2024)<br><strong>Weakly modular graphs with diamond condition, the interval function and axiomatic characterizations</strong><br><button class=copy-to-clipboard title="Weakly modular graphs with diamond condition, the interval function and axiomatic characterizations" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C12, cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01771v1.pdf filename=2403.01771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Weakly modular <b>graphs</b> are defined as the class of <b>graphs</b> that satisfy the \emph{triangle condition ($TC$)} and the \emph{quadrangle condition ($QC$)}. We study an interesting subclass of weakly modular <b>graphs</b> that satisfies a stronger version of the triangle condition, known as the \emph{triangle diamond condition ($TDC$)}. and term this subclass of weakly modular <b>graphs</b> as the \emph{diamond-weakly modular graphs}. It is observed that this class contains the class of bridged <b>graphs</b> and the class of weakly bridged <b>graphs.</b> The interval function $I_G$ of a connected <b>graph</b> $G$ with vertex set $V$ is an important concept in metric <b>graph</b> theory and is one of the prime example of a transit function; a set function defined on the Cartesian product $V\times V$ to the power set of $V$ satisfying the expansive, symmetric and idempotent axioms. In this paper, we derive an interesting axiom denoted as $(J0&rsquo;)$, obtained from a well-known axiom introduced by Marlow Sholander in 1952, denoted as $(J0)$. It is proved that the axiom $(J0&rsquo;)$ is a characterizing axiom of the diamond-weakly modular <b>graphs.</b> We propose certain types of independent first-order betweenness axioms on an arbitrary transit function $R$ and prove that an arbitrary transit function becomes the interval function of a diamond-weakly modular <b>graph</b> if and only if $R$ satisfies these betweenness axioms. Similar characterizations are obtained for the interval function of bridged <b>graphs</b> and weakly bridged <b>graphs.</b></p></p class="citation"></blockquote><h2 id=csdm-2>cs.DM (2)</h2><h3 id=12--302309-payment-scheduling-in-the-interval-debt-model-tom-friedetzky-et-al-2024>(1/2 | 302/309) Payment Scheduling in the Interval Debt Model (Tom Friedetzky et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tom Friedetzky, David C. Kutner, George B. Mertzios, Iain A. Stewart, Amitabh Trehan. (2024)<br><strong>Payment Scheduling in the Interval Debt Model</strong><br><button class=copy-to-clipboard title="Payment Scheduling in the Interval Debt Model" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-CC, cs-CE, cs-DM, cs.DM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02198v1.pdf filename=2403.02198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The network-based study of financial systems has received considerable attention in recent years but has seldom explicitly incorporated the dynamic aspects of such systems. We consider this problem setting from the temporal point of view and introduce the Interval Debt Model (IDM) and some scheduling problems based on it, namely: Bankruptcy Minimization/Maximization, in which the aim is to produce a payment schedule with at most/at least a given number of bankruptcies; Perfect Scheduling, the special case of the minimization variant where the aim is to produce a schedule with no bankruptcies (that is, a perfect schedule); and Bailout Minimization, in which a financial authority must allocate a smallest possible bailout package to enable a perfect schedule. We show that each of these problems is NP-complete, in many cases even on very restricted input instances. On the positive side, we provide for Perfect Scheduling a polynomial-time algorithm on (rooted) out-trees although in contrast we prove NP-completeness on directed acyclic <b>graphs,</b> as well as on instances with a constant number of nodes (and hence also constant treewidth). When we allow non-integer payments, we show by a linear programming argument that the problem Bailout Minimization can be solved in polynomial time.</p></p class="citation"></blockquote><h3 id=22--303309-graph-drawing-applications-in-combinatorial-theory-of-maturity-models-špela-kajzer-et-al-2024>(2/2 | 303/309) Graph drawing applications in combinatorial theory of maturity models (Špela Kajzer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Špela Kajzer, Alexander Dobler, Janja Jerebic, Martin Nöllenburg, Joachim Orthaber, Drago Bokal. (2024)<br><strong>Graph drawing applications in combinatorial theory of maturity models</strong><br><button class=copy-to-clipboard title="Graph drawing applications in combinatorial theory of maturity models" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs.DM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02026v1.pdf filename=2403.02026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce tiled <b>graphs</b> as models of learning and maturing processes. We show how tiled <b>graphs</b> can combine <b>graphs</b> of learning spaces or antimatroids (partial hypercubes) and maturity models (total orders) to yield models of learning processes. For the visualization of these processes it is a natural approach to aim for certain optimal drawings. We show for most of the more detailed models that the drawing problems resulting from them are NP-complete. The terse model of a maturing process that ignores the details of learning, however, results in a polynomially solvable <b>graph</b> drawing problem. In addition, this model provides insight into the process by ordering the subjects at each test of their maturity. We investigate extremal and random instances of this problem, and provide exact results and bounds on their optimal crossing number. <b>Graph-theoretic</b> models offer two approaches to the design of optimal maturity models given observed data: (1) minimizing intra-subject inconsistencies, which manifest as regressions of subjects, is modeled as the well-known feedback arc set problem. We study the alternative of (2) finding a maturity model by minimizing the inter-subject inconsistencies, which manifest as crossings in the respective drawing. We show this to be NP-complete.</p></p class="citation"></blockquote><h2 id=csds-5>cs.DS (5)</h2><h3 id=15--304309-matching-algorithms-in-the-sparse-stochastic-block-model-anna-brandenberger-et-al-2024>(1/5 | 304/309) Matching Algorithms in the Sparse Stochastic Block Model (Anna Brandenberger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Brandenberger, Byron Chin, Nathan S. Sheffield, Divya Shyamal. (2024)<br><strong>Matching Algorithms in the Sparse Stochastic Block Model</strong><br><button class=copy-to-clipboard title="Matching Algorithms in the Sparse Stochastic Block Model" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02140v1.pdf filename=2403.02140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The stochastic block model (SBM) is a generalization of the Erd\H{o}s&ndash;R'enyi model of random <b>graphs</b> that describes the interaction of a finite number of distinct communities. In sparse Erd\H{o}s&ndash;R'enyi <b>graphs,</b> it is known that a linear-time algorithm of Karp and Sipser achieves near-optimal matching sizes asymptotically almost surely, giving a law-of-large numbers for the matching sizes of such <b>graphs</b> in terms of solutions to an ODE. We provide an extension of this analysis, identifying broad ranges of stochastic block model parameters for which the Karp&ndash;Sipser algorithm achieves near-optimal matching sizes, but demonstrating that it cannot perform optimally on general SBM instances. We also consider the problem of constructing a matching online, in which the vertices of one half of a bipartite stochastic block model arrive one-at-a-time, and must be matched as they arrive. We show that the competitive ratio lower bound of 0.837 found by Mastin and Jaillet for the Erd\H{o}s&ndash;R'enyi case is tight whenever the expected degrees in all communities are equal. We propose several linear-time algorithms for online matching in the general stochastic block model, but prove that despite very good experimental performance, none of these achieve online asymptotic optimality.</p></p class="citation"></blockquote><h3 id=25--305309-random-generation-of-git-graphs-julien-courtiel-et-al-2024>(2/5 | 305/309) Random Generation of Git Graphs (Julien Courtiel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julien Courtiel, Martin Pépin. (2024)<br><strong>Random Generation of Git Graphs</strong><br><button class=copy-to-clipboard title="Random Generation of Git Graphs" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01902v1.pdf filename=2403.01902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Version Control Systems, such as Git and Mercurial, manage the history of a project as a Directed Acyclic <b>Graph</b> encoding the various divergences and synchronizations happening in its life cycle. A popular workflow in the industry, called the feature branch workflow, constrains these <b>graphs</b> to be of a particular shape: a unique main branch, and non-interfering feature branches. Here we focus on the uniform random generation of those <b>graphs</b> with n vertices, including k on the main branch, for which we provide three algorithms, for three different use-cases. The first, based on rejection, is efficient when aiming for small values of k (more precisely whenever k = O($\sqrt$ n)). The second takes as input any number k of commits in the main branch, but requires costly precalculation. The last one is a Boltzmann generator and enables us to generate very large <b>graphs</b> while targeting a constant k/n ratio. All these algorithms are linear in the size of their outputs.</p></p class="citation"></blockquote><h3 id=35--306309-the-canadian-traveller-problem-on-outerplanar-graphs-laurent-beaudou-et-al-2024>(3/5 | 306/309) The Canadian Traveller Problem on outerplanar graphs (Laurent Beaudou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laurent Beaudou, Pierre Bergé, Vsevolod Chernyshev, Antoine Dailly, Yan Gerard, Aurélie Lagoutte, Vincent Limouzy, Lucas Pastor. (2024)<br><strong>The Canadian Traveller Problem on outerplanar graphs</strong><br><button class=copy-to-clipboard title="The Canadian Traveller Problem on outerplanar graphs" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01872v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01872v2.pdf filename=2403.01872v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the PSPACE-complete $k$-Canadian Traveller Problem, where a weighted <b>graph</b> $G=(V,E,\omega)$ with a source $s\in V$ and a target $t\in V$ are given. This problem also has a hidden input $E_* \subsetneq E$ of cardinality at most $k$ representing blocked edges. The objective is to travel from $s$ to $t$ with the minimum distance. At the beginning of the walk, the blockages $E_*$ are unknown: the traveller discovers that an edge is blocked when visiting one of its endpoints. Online algorithms, also called strategies, have been proposed for this problem and assessed with the competitive ratio, i.e. the ratio between the distance actually traversed by the traveller divided by the distance we would have traversed knowing the blockages in advance. Even though the optimal competitive ratio is $2k+1$ even on unit-weighted planar <b>graphs</b> of treewidth 2, we design a polynomial-time strategy achieving competitive ratio $9$ on unit-weighted outerplanar <b>graphs.</b> This value $9$ also stands as a lower bound for this family of <b>graphs</b> as we prove that, for any $\varepsilon > 0$, no strategy can achieve a competitive ratio $9-\varepsilon$. Finally, we show that it is not possible to achieve a constant competitive ratio (independent of $G$ and $k$) on weighted outerplanar <b>graphs.</b></p></p class="citation"></blockquote><h3 id=45--307309-fully-polynomial-time-algorithms-parameterized-by-vertex-integrity-using-fast-matrix-multiplication-matthias-bentert-et-al-2024>(4/5 | 307/309) Fully Polynomial-time Algorithms Parameterized by Vertex Integrity Using Fast Matrix Multiplication (Matthias Bentert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthias Bentert, Klaus Heeger, Tomohiro Koana. (2024)<br><strong>Fully Polynomial-time Algorithms Parameterized by Vertex Integrity Using Fast Matrix Multiplication</strong><br><button class=copy-to-clipboard title="Fully Polynomial-time Algorithms Parameterized by Vertex Integrity Using Fast Matrix Multiplication" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01839v1.pdf filename=2403.01839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the computational complexity of several polynomial-time-solvable <b>graph</b> problems parameterized by vertex integrity, a measure of a <b>graph&rsquo;s</b> vulnerability to vertex removal in terms of connectivity. Vertex integrity is the smallest number $\iota$ such that there is a set $S$ of $\iota&rsquo; \le \iota$ vertices such that every connected component of $G-S$ contains at most $\iota-\iota&rsquo;$ vertices. It is known that the vertex integrity lies between the well-studied parameters vertex cover number and tree-depth. Alon and Yuster [ESA 2007] designed algorithms for <b>graphs</b> with small vertex cover number using fast matrix multiplications. We demonstrate that fast matrix multiplication can also be effectively used when parameterizing by vertex integrity $\iota$ by developing efficient algorithms for problems including an $O(\iota^{\omega-1}n)$-time algorithm for computing the girth of a <b>graph,</b> randomized $O(\iota^{\omega - 1}n)$-time algorithms for Maximum Matching and for finding any induced four-vertex subgraph except for a clique or an independent set, and an $O(\iota^{(\omega-1)/2}n^2) \subseteq O(\iota^{0.687} n^2)$-time algorithm for All-Pairs Shortest Paths. These algorithms can be faster than previous algorithms parameterized by tree-depth, for which fast matrix multiplication is not known to be effective.</p></p class="citation"></blockquote><h3 id=55--308309-unleashing-graph-partitioning-for-large-scale-nearest-neighbor-search-lars-gottesbüren-et-al-2024>(5/5 | 308/309) Unleashing Graph Partitioning for Large-Scale Nearest Neighbor Search (Lars Gottesbüren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lars Gottesbüren, Laxman Dhulipala, Rajesh Jayaram, Jakub Lacki. (2024)<br><strong>Unleashing Graph Partitioning for Large-Scale Nearest Neighbor Search</strong><br><button class=copy-to-clipboard title="Unleashing Graph Partitioning for Large-Scale Nearest Neighbor Search" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs-IR, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01797v1.pdf filename=2403.01797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the fundamental problem of decomposing a large-scale approximate nearest neighbor search (ANNS) problem into smaller sub-problems. The goal is to partition the input points into neighborhood-preserving shards, so that the nearest neighbors of any point are contained in only a few shards. When a query arrives, a routing algorithm is used to identify the shards which should be searched for its nearest neighbors. This approach forms the backbone of distributed ANNS, where the dataset is so large that it must be split across multiple machines. In this paper, we design simple and highly efficient routing methods, and prove strong theoretical guarantees on their performance. A crucial characteristic of our routing algorithms is that they are inherently modular, and can be used with any partitioning method. This addresses a key drawback of prior approaches, where the routing algorithms are inextricably linked to their associated partitioning method. In particular, our new routing methods enable the use of balanced <b>graph</b> partitioning, which is a high-quality partitioning method without a naturally associated routing algorithm. Thus, we provide the first methods for routing using balanced <b>graph</b> partitioning that are extremely fast to train, admit low latency, and achieve high recall. We provide a comprehensive evaluation of our full partitioning and routing pipeline on billion-scale datasets, where it outperforms existing scalable partitioning methods by significant margins, achieving up to 2.14x higher QPS at 90% recall$@10$ than the best competitor.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--309309-schema-based-query-optimisation-for-graph-databases-chandan-sharma-et-al-2024>(1/1 | 309/309) Schema-Based Query Optimisation for Graph Databases (Chandan Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chandan Sharma, Pierre Genevès, Nils Gesbert, Nabil Layaïda. (2024)<br><strong>Schema-Based Query Optimisation for Graph Databases</strong><br><button class=copy-to-clipboard title="Schema-Based Query Optimisation for Graph Databases" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01863v1.pdf filename=2403.01863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recursive <b>graph</b> queries are increasingly popular for extracting information from interconnected data found in various domains such as social networks, life sciences, and business analytics. <b>Graph</b> data often come with schema information that describe how nodes and edges are organized. We propose a type inference mechanism that enriches recursive <b>graph</b> queries with relevant structural information contained in a <b>graph</b> schema. We show that this schema information can be useful in order to improve the performance when evaluating acylic recursive <b>graph</b> queries. Furthermore, we prove that the proposed method is sound and complete, ensuring that the semantics of the query is preserved during the schema-enrichment process.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.05</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.07</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cslg-43>cs.LG (43)</a><ul><li><a href=#143--1309-tpllm-a-traffic-prediction-framework-based-on-pretrained-large-language-models-yilong-ren-et-al-2024>(1/43 | 1/309) TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models (Yilong Ren et al., 2024)</a></li><li><a href=#243--2309-enhancing-llm-safety-via-constrained-direct-preference-optimization-zixuan-liu-et-al-2024>(2/43 | 2/309) Enhancing LLM Safety via Constrained Direct Preference Optimization (Zixuan Liu et al., 2024)</a></li><li><a href=#343--3309-towards-efficient-deep-autoencoders-for-multivariate-time-series-anomaly-detection-marcin-pietroń-et-al-2024>(3/43 | 3/309) Towards efficient deep autoencoders for multivariate time series anomaly detection (Marcin Pietroń et al., 2024)</a></li><li><a href=#443--4309-reward-model-learning-vs-direct-policy-optimization-a-comparative-analysis-of-learning-from-human-preferences-andi-nika-et-al-2024>(4/43 | 4/309) Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences (Andi Nika et al., 2024)</a></li><li><a href=#543--5309-wukong-towards-a-scaling-law-for-large-scale-recommendation-buyun-zhang-et-al-2024>(5/43 | 5/309) Wukong: Towards a Scaling Law for Large-Scale Recommendation (Buyun Zhang et al., 2024)</a></li><li><a href=#643--6309-distilled-chatgpt-topic--sentiment-modeling-with-applications-in-finance-olivier-gandouet-et-al-2024>(6/43 | 6/309) Distilled ChatGPT Topic & Sentiment Modeling with Applications in Finance (Olivier Gandouet et al., 2024)</a></li><li><a href=#743--7309-unsupervised-distance-metric-learning-for-anomaly-detection-over-multivariate-time-series-hanyang-yuan-et-al-2024>(7/43 | 7/309) Unsupervised Distance Metric Learning for Anomaly Detection Over Multivariate Time Series (Hanyang Yuan et al., 2024)</a></li><li><a href=#843--8309-better-schedules-for-low-precision-training-of-deep-neural-networks-cameron-r-wolfe-et-al-2024>(8/43 | 8/309) Better Schedules for Low Precision Training of Deep Neural Networks (Cameron R. Wolfe et al., 2024)</a></li><li><a href=#943--9309-a-safe-screening-rule-with-bi-level-optimization-of-ν-support-vector-machine-zhiji-yang-et-al-2024>(9/43 | 9/309) A Safe Screening Rule with Bi-level Optimization of $ν$ Support Vector Machine (Zhiji Yang et al., 2024)</a></li><li><a href=#1043--10309-towards-foundation-time-series-model-to-synthesize-or-not-to-synthesize-kseniia-kuvshinova-et-al-2024>(10/43 | 10/309) Towards Foundation Time Series Model: To Synthesize Or Not To Synthesize? (Kseniia Kuvshinova et al., 2024)</a></li><li><a href=#1143--11309-taming-throughput-latency-tradeoff-in-llm-inference-with-sarathi-serve-amey-agrawal-et-al-2024>(11/43 | 11/309) Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve (Amey Agrawal et al., 2024)</a></li><li><a href=#1243--12309-cola-cross-city-mobility-transformer-for-human-trajectory-simulation-yu-wang-et-al-2024>(12/43 | 12/309) COLA: Cross-city Mobility Transformer for Human Trajectory Simulation (Yu Wang et al., 2024)</a></li><li><a href=#1343--13309-coms2t-a-complementary-spatiotemporal-learning-system-for-data-adaptive-model-evolution-zhengyang-zhou-et-al-2024>(13/43 | 13/309) ComS2T: A complementary spatiotemporal learning system for data-adaptive model evolution (Zhengyang Zhou et al., 2024)</a></li><li><a href=#1443--14309-commit-certifying-robustness-of-multi-sensor-fusion-systems-against-semantic-attacks-zijian-huang-et-al-2024>(14/43 | 14/309) COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against Semantic Attacks (Zijian Huang et al., 2024)</a></li><li><a href=#1543--15309-hear----health-acoustic-representations-sebastien-baur-et-al-2024>(15/43 | 15/309) HeAR &ndash; Health Acoustic Representations (Sebastien Baur et al., 2024)</a></li><li><a href=#1643--16309-sok-challenges-and-opportunities-in-federated-unlearning-hyejun-jeong-et-al-2024>(16/43 | 16/309) SoK: Challenges and Opportunities in Federated Unlearning (Hyejun Jeong et al., 2024)</a></li><li><a href=#1743--17309-are-more-llm-calls-all-you-need-towards-scaling-laws-of-compound-inference-systems-lingjiao-chen-et-al-2024>(17/43 | 17/309) Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems (Lingjiao Chen et al., 2024)</a></li><li><a href=#1843--18309-transformers-provably-learn-feature-position-correlations-in-masked-image-modeling-yu-huang-et-al-2024>(18/43 | 18/309) Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling (Yu Huang et al., 2024)</a></li><li><a href=#1943--19309-dyce-dynamic-configurable-exiting-for-deep-learning-compression-and-scaling-qingyuan-wang-et-al-2024>(19/43 | 19/309) DyCE: Dynamic Configurable Exiting for Deep Learning Compression and Scaling (Qingyuan Wang et al., 2024)</a></li><li><a href=#2043--20309-quantifying-and-predicting-residential-building-flexibility-using-machine-learning-methods-patrick-salter-et-al-2024>(20/43 | 20/309) Quantifying and Predicting Residential Building Flexibility Using Machine Learning Methods (Patrick Salter et al., 2024)</a></li><li><a href=#2143--21309-mitigating-label-noise-on-graph-via-topological-sample-selection-yuhao-wu-et-al-2024>(21/43 | 21/309) Mitigating Label Noise on Graph via Topological Sample Selection (Yuhao Wu et al., 2024)</a></li><li><a href=#2243--22309-geometry-and-stability-of-supervised-learning-problems-facundo-mémoli-et-al-2024>(22/43 | 22/309) Geometry and Stability of Supervised Learning Problems (Facundo Mémoli et al., 2024)</a></li><li><a href=#2343--23309-encodings-for-prediction-based-neural-architecture-search-yash-akhauri-et-al-2024>(23/43 | 23/309) Encodings for Prediction-based Neural Architecture Search (Yash Akhauri et al., 2024)</a></li><li><a href=#2443--24309-addressing-long-tail-noisy-label-learning-problems-a-two-stage-solution-with-label-refurbishment-considering-label-rarity-ying-hsuan-wu-et-al-2024>(24/43 | 24/309) Addressing Long-Tail Noisy Label Learning Problems: a Two-Stage Solution with Label Refurbishment Considering Label Rarity (Ying-Hsuan Wu et al., 2024)</a></li><li><a href=#2543--25309-improving-out-of-distribution-generalization-in-graphs-via-hierarchical-semantic-environments-yinhua-piao-et-al-2024>(25/43 | 25/309) Improving out-of-distribution generalization in graphs via hierarchical semantic environments (Yinhua Piao et al., 2024)</a></li><li><a href=#2643--26309-density-based-isometric-mapping-bardia-yousefi-et-al-2024>(26/43 | 26/309) Density-based Isometric Mapping (Bardia Yousefi et al., 2024)</a></li><li><a href=#2743--27309-on-latency-predictors-for-neural-architecture-search-yash-akhauri-et-al-2024>(27/43 | 27/309) On Latency Predictors for Neural Architecture Search (Yash Akhauri et al., 2024)</a></li><li><a href=#2843--28309-joint-parameter-and-parameterization-inference-with-uncertainty-quantification-through-differentiable-programming-yongquan-qu-et-al-2024>(28/43 | 28/309) Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming (Yongquan Qu et al., 2024)</a></li><li><a href=#2943--29309-flowprecision-advancing-fpga-based-real-time-fluid-flow-estimation-with-linear-quantization-tianheng-ling-et-al-2024>(29/43 | 29/309) FlowPrecision: Advancing FPGA-Based Real-Time Fluid Flow Estimation with Linear Quantization (Tianheng Ling et al., 2024)</a></li><li><a href=#3043--30309-a-survey-on-evaluation-of-out-of-distribution-generalization-han-yu-et-al-2024>(30/43 | 30/309) A Survey on Evaluation of Out-of-Distribution Generalization (Han Yu et al., 2024)</a></li><li><a href=#3143--31309-nash-neural-architecture-search-for-hardware-optimized-machine-learning-models-mengfei-ji-et-al-2024>(31/43 | 31/309) NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models (Mengfei Ji et al., 2024)</a></li><li><a href=#3243--32309-diffusion-ts-interpretable-diffusion-for-general-time-series-generation-xinyu-yuan-et-al-2024>(32/43 | 32/309) Diffusion-TS: Interpretable Diffusion for General Time Series Generation (Xinyu Yuan et al., 2024)</a></li><li><a href=#3343--33309-day-ahead-regional-solar-power-forecasting-with-hierarchical-temporal-convolutional-neural-networks-using-historical-power-generation-and-weather-data-maneesha-perera-et-al-2024>(33/43 | 33/309) Day-ahead regional solar power forecasting with hierarchical temporal convolutional neural networks using historical power generation and weather data (Maneesha Perera et al., 2024)</a></li><li><a href=#3443--34309-inf2guard-an-information-theoretic-framework-for-learning-privacy-preserving-representations-against-inference-attacks-sayedeh-leila-noorbakhsh-et-al-2024>(34/43 | 34/309) Inf2Guard: An Information-Theoretic Framework for Learning Privacy-Preserving Representations against Inference Attacks (Sayedeh Leila Noorbakhsh et al., 2024)</a></li><li><a href=#3543--35309-towards-optimal-customized-architecture-for-heterogeneous-federated-learning-with-contrastive-cloud-edge-model-decoupling-xingyan-chen-et-al-2024>(35/43 | 35/309) Towards Optimal Customized Architecture for Heterogeneous Federated Learning with Contrastive Cloud-Edge Model Decoupling (Xingyan Chen et al., 2024)</a></li><li><a href=#3643--36309-root-causing-prediction-anomalies-using-explainable-ai-ramanathan-vishnampet-et-al-2024>(36/43 | 36/309) Root Causing Prediction Anomalies Using Explainable AI (Ramanathan Vishnampet et al., 2024)</a></li><li><a href=#3743--37309-gradient-correlation-subspace-learning-against-catastrophic-forgetting-tammuz-dubnov-et-al-2024>(37/43 | 37/309) Gradient Correlation Subspace Learning against Catastrophic Forgetting (Tammuz Dubnov et al., 2024)</a></li><li><a href=#3843--38309-neural-redshift-random-networks-are-not-random-functions-damien-teney-et-al-2024>(38/43 | 38/309) Neural Redshift: Random Networks are not Random Functions (Damien Teney et al., 2024)</a></li><li><a href=#3943--39309-mutual-information-estimation-via-normalizing-flows-ivan-butakov-et-al-2024>(39/43 | 39/309) Mutual Information Estimation via Normalizing Flows (Ivan Butakov et al., 2024)</a></li><li><a href=#4043--40309-iterated-q-network-beyond-the-one-step-bellman-operator-théo-vincent-et-al-2024>(40/43 | 40/309) Iterated $Q$-Network: Beyond the One-Step Bellman Operator (Théo Vincent et al., 2024)</a></li><li><a href=#4143--41309-matrix-completion-with-convex-optimization-and-column-subset-selection-antonina-krajewska-et-al-2024>(41/43 | 41/309) Matrix Completion with Convex Optimization and Column Subset Selection (Antonina Krajewska et al., 2024)</a></li><li><a href=#4243--42309-robustness-bounds-on-the-successful-adversarial-examples-theory-and-practice-hiroaki-maeshima-et-al-2024>(42/43 | 42/309) Robustness Bounds on the Successful Adversarial Examples: Theory and Practice (Hiroaki Maeshima et al., 2024)</a></li><li><a href=#4343--43309-permutation-invariant-functions-statistical-tests-dimension-reduction-in-metric-entropy-and-estimation-wee-chaimanowong-et-al-2024>(43/43 | 43/309) Permutation invariant functions: statistical tests, dimension reduction in metric entropy and estimation (Wee Chaimanowong et al., 2024)</a></li></ul></li><li><a href=#cscl-64>cs.CL (64)</a><ul><li><a href=#164--44309-leveraging-weakly-annotated-data-for-hate-speech-detection-in-code-mixed-hinglish-a-feasibility-driven-transfer-learning-approach-with-large-language-models-sargam-yadav-et-al-2024>(1/64 | 44/309) Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language Models (Sargam Yadav et al., 2024)</a></li><li><a href=#264--45309-analyzing-and-adapting-large-language-models-for-few-shot-multilingual-nlu-are-we-there-yet-evgeniia-razumovskaia-et-al-2024>(2/64 | 45/309) Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet? (Evgeniia Razumovskaia et al., 2024)</a></li><li><a href=#364--46309-balancing-enhancement-harmlessness-and-general-capabilities-enhancing-conversational-llms-with-direct-rlhf-chen-zheng-et-al-2024>(3/64 | 46/309) Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF (Chen Zheng et al., 2024)</a></li><li><a href=#464--47309-differentially-private-synthetic-data-via-foundation-model-apis-2-text-chulin-xie-et-al-2024>(4/64 | 47/309) Differentially Private Synthetic Data via Foundation Model APIs 2: Text (Chulin Xie et al., 2024)</a></li><li><a href=#564--48309-sciassess-benchmarking-llm-proficiency-in-scientific-literature-analysis-hengxing-cai-et-al-2024>(5/64 | 48/309) SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis (Hengxing Cai et al., 2024)</a></li><li><a href=#664--49309-phantom-personality-has-an-effect-on-theory-of-mind-reasoning-in-large-language-models-fiona-anting-tan-et-al-2024>(6/64 | 49/309) PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models (Fiona Anting Tan et al., 2024)</a></li><li><a href=#764--50309-human-evaluation-of-english--irish-transformer-based-nmt-séamus-lankford-et-al-2024>(7/64 | 50/309) Human Evaluation of English&ndash;Irish Transformer-Based NMT (Séamus Lankford et al., 2024)</a></li><li><a href=#864--51309-key-point-driven-data-synthesis-with-its-enhancement-on-mathematical-reasoning-yiming-huang-et-al-2024>(8/64 | 51/309) Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning (Yiming Huang et al., 2024)</a></li><li><a href=#964--52309-using-llms-for-the-extraction-and-normalization-of-product-attribute-values-nick-baumann-et-al-2024>(9/64 | 52/309) Using LLMs for the Extraction and Normalization of Product Attribute Values (Nick Baumann et al., 2024)</a></li><li><a href=#1064--53309-adaptmllm-fine-tuning-multilingual-language-models-on-low-resource-languages-with-integrated-llm-playgrounds-séamus-lankford-et-al-2024>(10/64 | 53/309) adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds (Séamus Lankford et al., 2024)</a></li><li><a href=#1164--54309-llm-oriented-retrieval-tuner-si-sun-et-al-2024>(11/64 | 54/309) LLM-Oriented Retrieval Tuner (Si Sun et al., 2024)</a></li><li><a href=#1264--55309-daco-towards-application-driven-and-comprehensive-data-analysis-via-code-generation-xueqing-wu-et-al-2024>(12/64 | 55/309) DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation (Xueqing Wu et al., 2024)</a></li><li><a href=#1364--56309-riff-learning-to-rephrase-inputs-for-few-shot-fine-tuning-of-language-models-saeed-najafi-et-al-2024>(13/64 | 56/309) RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models (Saeed Najafi et al., 2024)</a></li><li><a href=#1464--57309-protrix-building-models-for-planning-and-reasoning-over-tables-with-sentence-context-zirui-wu-et-al-2024>(14/64 | 57/309) ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context (Zirui Wu et al., 2024)</a></li><li><a href=#1564--58309-to-generate-or-to-retrieve-on-the-effectiveness-of-artificial-contexts-for-medical-open-domain-question-answering-giacomo-frisoni-et-al-2024>(15/64 | 58/309) To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering (Giacomo Frisoni et al., 2024)</a></li><li><a href=#1664--59309-fenice-factuality-evaluation-of-summarization-based-on-natural-language-inference-and-claim-extraction-alessandro-scirè-et-al-2024>(16/64 | 59/309) FENICE: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction (Alessandro Scirè et al., 2024)</a></li><li><a href=#1764--60309-multi-perspective-improvement-of-knowledge-graph-completion-with-large-language-models-derong-xu-et-al-2024>(17/64 | 60/309) Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models (Derong Xu et al., 2024)</a></li><li><a href=#1864--61309-how-does-architecture-influence-the-base-capabilities-of-pre-trained-language-models-a-case-study-based-on-ffn-wider-transformer-models-xin-lu-et-al-2024>(18/64 | 61/309) How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models (Xin Lu et al., 2024)</a></li><li><a href=#1964--62309-masked-thought-simply-masking-partial-reasoning-steps-can-improve-mathematical-reasoning-learning-of-language-models-changyu-chen-et-al-2024>(19/64 | 62/309) Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models (Changyu Chen et al., 2024)</a></li><li><a href=#2064--63309-as-es-learning-towards-efficient-cot-learning-in-small-models-nuwa-xi-et-al-2024>(20/64 | 63/309) AS-ES Learning: Towards Efficient CoT Learning in Small Models (Nuwa Xi et al., 2024)</a></li><li><a href=#2164--64309-fakenewsgpt4-advancing-multimodal-fake-news-detection-through-knowledge-augmented-lvlms-xuannan-liu-et-al-2024>(21/64 | 64/309) FakeNewsGPT4: Advancing Multimodal Fake News Detection through Knowledge-Augmented LVLMs (Xuannan Liu et al., 2024)</a></li><li><a href=#2264--65309-nphardeval4v-a-dynamic-reasoning-benchmark-of-multimodal-large-language-models-lizhou-fan-et-al-2024>(22/64 | 65/309) NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models (Lizhou Fan et al., 2024)</a></li><li><a href=#2364--66309-an-improved-traditional-chinese-evaluation-suite-for-foundation-model-zhi-rui-tam-et-al-2024>(23/64 | 66/309) An Improved Traditional Chinese Evaluation Suite for Foundation Model (Zhi-Rui Tam et al., 2024)</a></li><li><a href=#2464--67309-birbal-an-efficient-7b-instruct-model-fine-tuned-with-curated-datasets-ashvini-kumar-jindal-et-al-2024>(24/64 | 67/309) Birbal: An efficient 7B instruct-model fine-tuned with curated datasets (Ashvini Kumar Jindal et al., 2024)</a></li><li><a href=#2564--68309-transformers-for-low-resource-languagesis-féidir-linn-séamus-lankford-et-al-2024>(25/64 | 68/309) Transformers for Low-Resource Languages:Is Féidir Linn! (Séamus Lankford et al., 2024)</a></li><li><a href=#2664--69309-eee-qa-exploring-effective-and-efficient-question-answer-representations-zhanghao-hu-et-al-2024>(26/64 | 69/309) EEE-QA: Exploring Effective and Efficient Question-Answer Representations (Zhanghao Hu et al., 2024)</a></li><li><a href=#2764--70309-adaptnmt-an-open-source-language-agnostic-development-environment-for-neural-machine-translation-séamus-lankford-et-al-2024>(27/64 | 70/309) adaptNMT: an open-source, language-agnostic development environment for Neural Machine Translation (Séamus Lankford et al., 2024)</a></li><li><a href=#2864--71309-offlandat-a-community-based-implicit-offensive-language-dataset-generated-by-large-language-model-through-prompt-engineering-amit-das-et-al-2024>(28/64 | 71/309) OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering (Amit Das et al., 2024)</a></li><li><a href=#2964--72309-not-all-layers-of-llms-are-necessary-during-inference-siqi-fan-et-al-2024>(29/64 | 72/309) Not all Layers of LLMs are Necessary during Inference (Siqi Fan et al., 2024)</a></li><li><a href=#3064--73309-large-language-models-in-fire-engineering-an-examination-of-technical-questions-against-domain-knowledge-haley-hostetter-et-al-2024>(30/64 | 73/309) Large Language Models in Fire Engineering: An Examination of Technical Questions Against Domain Knowledge (Haley Hostetter et al., 2024)</a></li><li><a href=#3164--74309-automated-generation-of-multiple-choice-cloze-questions-for-assessing-english-vocabulary-using-gpt-turbo-35-qiao-wang-et-al-2024>(31/64 | 74/309) Automated Generation of Multiple-Choice Cloze Questions for Assessing English Vocabulary Using GPT-turbo 3.5 (Qiao Wang et al., 2024)</a></li><li><a href=#3264--75309-decider-a-rule-controllable-decoding-strategy-for-language-generation-by-imitating-dual-system-cognitive-theory-chen-xu-et-al-2024>(32/64 | 75/309) DECIDER: A Rule-Controllable Decoding Strategy for Language Generation by Imitating Dual-System Cognitive Theory (Chen Xu et al., 2024)</a></li><li><a href=#3364--76309-llm-vs-lawyers-identifying-a-subset-of-summary-judgments-in-a-large-uk-case-law-dataset-ahmed-izzidien-et-al-2024>(33/64 | 76/309) LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset (Ahmed Izzidien et al., 2024)</a></li><li><a href=#3464--77309-rethinking-llm-language-adaptation-a-case-study-on-chinese-mixtral-yiming-cui-et-al-2024>(34/64 | 77/309) Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral (Yiming Cui et al., 2024)</a></li><li><a href=#3564--78309-enhancing-multi-domain-automatic-short-answer-grading-through-an-explainable-neuro-symbolic-pipeline-felix-künnecke-et-al-2024>(35/64 | 78/309) Enhancing Multi-Domain Automatic Short Answer Grading through an Explainable Neuro-Symbolic Pipeline (Felix Künnecke et al., 2024)</a></li><li><a href=#3664--79309-kenetknowledge-enhanced-doc-label-attention-network-for-multi-label-text-classification-bo-li-et-al-2024>(36/64 | 79/309) KeNet:Knowledge-enhanced Doc-Label Attention Network for Multi-label text classification (Bo Li et al., 2024)</a></li><li><a href=#3764--80309-topicdiff-a-topic-enriched-diffusion-approach-for-multimodal-conversational-emotion-detection-jiamin-luo-et-al-2024>(37/64 | 80/309) TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection (Jiamin Luo et al., 2024)</a></li><li><a href=#3864--81309-breaking-the-language-barrier-can-direct-inference-outperform-pre-translation-in-multilingual-llm-applications-yotam-intrator-et-al-2024>(38/64 | 81/309) Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications? (Yotam Intrator et al., 2024)</a></li><li><a href=#3964--82309-varierr-nli-separating-annotation-error-from-human-label-variation-leon-weber-genzel-et-al-2024>(39/64 | 82/309) VariErr NLI: Separating Annotation Error from Human Label Variation (Leon Weber-Genzel et al., 2024)</a></li><li><a href=#4064--83309-fostering-the-ecosystem-of-open-neural-encoders-for-portuguese-with-albertina-pt-family-rodrigo-santos-et-al-2024>(40/64 | 83/309) Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina PT* Family (Rodrigo Santos et al., 2024)</a></li><li><a href=#4164--84309-nusabert-teaching-indobert-to-be-multilingual-and-multicultural-wilson-wongso-et-al-2024>(41/64 | 84/309) NusaBERT: Teaching IndoBERT to be Multilingual and Multicultural (Wilson Wongso et al., 2024)</a></li><li><a href=#4264--85309-spuq-perturbation-based-uncertainty-quantification-for-large-language-models-xiang-gao-et-al-2024>(42/64 | 85/309) SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models (Xiang Gao et al., 2024)</a></li><li><a href=#4364--86309-trial-and-error-exploration-based-trajectory-optimization-for-llm-agents-yifan-song-et-al-2024>(43/64 | 86/309) Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents (Yifan Song et al., 2024)</a></li><li><a href=#4464--87309-what-has-lebenchmark-learnt-about-french-syntax-zdravko-dugonjić-et-al-2024>(44/64 | 87/309) What has LeBenchmark Learnt about French Syntax? (Zdravko Dugonjić et al., 2024)</a></li><li><a href=#4564--88309-topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic-vasudevan-nedumpozhimana-et-al-2024>(45/64 | 88/309) Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic? (Vasudevan Nedumpozhimana et al., 2024)</a></li><li><a href=#4664--89309-vanilla-transformers-are-transfer-capability-teachers-xin-lu-et-al-2024>(46/64 | 89/309) Vanilla Transformers are Transfer Capability Teachers (Xin Lu et al., 2024)</a></li><li><a href=#4764--90309-language-and-speech-technology-for-central-kurdish-varieties-sina-ahmadi-et-al-2024>(47/64 | 90/309) Language and Speech Technology for Central Kurdish Varieties (Sina Ahmadi et al., 2024)</a></li><li><a href=#4864--91309-webcites-attributed-query-focused-summarization-on-chinese-web-search-results-with-citations-haolin-deng-et-al-2024>(48/64 | 91/309) WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations (Haolin Deng et al., 2024)</a></li><li><a href=#4964--92309-derivative-free-optimization-for-low-rank-adaptation-in-large-language-models-feihu-jin-et-al-2024>(49/64 | 92/309) Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models (Feihu Jin et al., 2024)</a></li><li><a href=#5064--93309-decode-neural-signal-as-speech-yiqian-yang-et-al-2024>(50/64 | 93/309) Decode Neural signal as Speech (Yiqian Yang et al., 2024)</a></li><li><a href=#5164--94309-hypertext-entity-extraction-in-webpage-yifei-yang-et-al-2024>(51/64 | 94/309) Hypertext Entity Extraction in Webpage (Yifei Yang et al., 2024)</a></li><li><a href=#5264--95309-fcds-fusing-constituency-and-dependency-syntax-into-document-level-relation-extraction-xudong-zhu-et-al-2024>(52/64 | 95/309) FCDS: Fusing Constituency and Dependency Syntax into Document-Level Relation Extraction (Xudong Zhu et al., 2024)</a></li><li><a href=#5364--96309-brilla-ai-ai-contestant-for-the-national-science-and-maths-quiz-george-boateng-et-al-2024>(53/64 | 96/309) Brilla AI: AI Contestant for the National Science and Maths Quiz (George Boateng et al., 2024)</a></li><li><a href=#5464--97309-a-tutorial-on-the-pretrain-finetune-paradigm-for-natural-language-processing-yu-wang-2024>(54/64 | 97/309) A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing (Yu Wang, 2024)</a></li><li><a href=#5564--98309-subjective-textitisms-on-the-danger-of-conflating-hate-and-offence-in-abusive-language-detection-amanda-cercas-curry-et-al-2024>(55/64 | 98/309) Subjective $\textit{Isms}$? On the Danger of Conflating Hate and Offence in Abusive Language Detection (Amanda Cercas Curry et al., 2024)</a></li><li><a href=#5664--99309-indicvoices-towards-building-an-inclusive-multilingual-speech-dataset-for-indian-languages-tahir-javed-et-al-2024>(56/64 | 99/309) IndicVoices: Towards building an Inclusive Multilingual Speech Dataset for Indian Languages (Tahir Javed et al., 2024)</a></li><li><a href=#5764--100309-arabic-text-sentiment-analysis-reinforcing-human-performed-surveys-with-wider-topic-analysis-latifah-almurqren-et-al-2024>(57/64 | 100/309) Arabic Text Sentiment Analysis: Reinforcing Human-Performed Surveys with Wider Topic Analysis (Latifah Almurqren et al., 2024)</a></li><li><a href=#5864--101309-online-training-of-large-language-models-learn-while-chatting-juhao-liang-et-al-2024>(58/64 | 101/309) Online Training of Large Language Models: Learn while chatting (Juhao Liang et al., 2024)</a></li><li><a href=#5964--102309-making-pre-trained-language-models-great-on-tabular-prediction-jiahuan-yan-et-al-2024>(59/64 | 102/309) Making Pre-trained Language Models Great on Tabular Prediction (Jiahuan Yan et al., 2024)</a></li><li><a href=#6064--103309-topic-modeling-analysis-of-aviation-accident-reports-a-comparative-study-between-lda-and-nmf-models-aziida-nanyonga-et-al-2024>(60/64 | 103/309) Topic Modeling Analysis of Aviation Accident Reports: A Comparative Study between LDA and NMF Models (Aziida Nanyonga et al., 2024)</a></li><li><a href=#6164--104309-cet2-modelling-topic-transitions-for-coherent-and-engaging-knowledge-grounded-conversations-lin-xu-et-al-2024>(61/64 | 104/309) CET2: Modelling Topic Transitions for Coherent and Engaging Knowledge-Grounded Conversations (Lin Xu et al., 2024)</a></li><li><a href=#6264--105309-choose-your-own-adventure-interactive-e-books-to-improve-word-knowledge-and-comprehension-skills-stephanie-day-et-al-2024>(62/64 | 105/309) Choose Your Own Adventure: Interactive E-Books to Improve Word Knowledge and Comprehension Skills (Stephanie Day et al., 2024)</a></li><li><a href=#6364--106309-detection-of-non-recorded-word-senses-in-english-and-swedish-jonathan-lautenschlager-et-al-2024>(63/64 | 106/309) Detection of Non-recorded Word Senses in English and Swedish (Jonathan Lautenschlager et al., 2024)</a></li><li><a href=#6464--107309-views-are-my-own-but-also-yours-benchmarking-theory-of-mind-using-common-ground-adil-soubki-et-al-2024>(64/64 | 107/309) Views Are My Own, But Also Yours: Benchmarking Theory of Mind using Common Ground (Adil Soubki et al., 2024)</a></li></ul></li><li><a href=#csse-2>cs.SE (2)</a><ul><li><a href=#12--108309-can-llms-generate-architectural-design-decisions--an-exploratory-empirical-study-rudra-dhar-et-al-2024>(1/2 | 108/309) Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study (Rudra Dhar et al., 2024)</a></li><li><a href=#22--109309-contrastrepair-enhancing-conversation-based-automated-program-repair-via-contrastive-test-case-pairs-jiaolong-kong-et-al-2024>(2/2 | 109/309) ContrastRepair: Enhancing Conversation-Based Automated Program Repair via Contrastive Test Case Pairs (Jiaolong Kong et al., 2024)</a></li></ul></li><li><a href=#csir-5>cs.IR (5)</a><ul><li><a href=#15--110309-notellm-a-retrievable-large-language-model-for-note-recommendation-chao-zhang-et-al-2024>(1/5 | 110/309) NoteLLM: A Retrievable Large Language Model for Note Recommendation (Chao Zhang et al., 2024)</a></li><li><a href=#25--111309-evaluating-the-explainability-of-neural-rankers-saran-pandian-et-al-2024>(2/5 | 111/309) Evaluating the Explainability of Neural Rankers (Saran Pandian et al., 2024)</a></li><li><a href=#35--112309-code-accord-a-corpus-of-building-regulatory-data-for-rule-generation-towards-automatic-compliance-checking-hansi-hettiarachchi-et-al-2024>(3/5 | 112/309) CODE-ACCORD: A Corpus of Building Regulatory Data for Rule Generation towards Automatic Compliance Checking (Hansi Hettiarachchi et al., 2024)</a></li><li><a href=#45--113309-magnetic-localization-for-in-body-nano-communication-medical-systems-krzysztof-skos-et-al-2024>(4/5 | 113/309) Magnetic Localization for In-body Nano-communication Medical Systems (Krzysztof Skos et al., 2024)</a></li><li><a href=#55--114309-recommending-missed-citations-identified-by-reviewers-a-new-task-dataset-and-baselines-kehan-long-et-al-2024>(5/5 | 114/309) Recommending Missed Citations Identified by Reviewers: A New Task, Dataset and Baselines (Kehan Long et al., 2024)</a></li></ul></li><li><a href=#cscy-3>cs.CY (3)</a><ul><li><a href=#13--115309-ai-language-models-could-both-help-and-harm-equity-in-marine-policymaking-the-case-study-of-the-bbnj-question-answering-bot-matt-ziegler-et-al-2024>(1/3 | 115/309) AI Language Models Could Both Help and Harm Equity in Marine Policymaking: The Case Study of the BBNJ Question-Answering Bot (Matt Ziegler et al., 2024)</a></li><li><a href=#23--116309-towards-implicit-prompt-for-text-to-image-models-yue-yang-et-al-2024>(2/3 | 116/309) Towards Implicit Prompt For Text-To-Image Models (Yue Yang et al., 2024)</a></li><li><a href=#33--117309-recommendations-for-government-development-and-use-of-advanced-automated-systems-to-make-decisions-about-individuals-susan-landau-et-al-2024>(3/3 | 117/309) Recommendations for Government Development and Use of Advanced Automated Systems to Make Decisions about Individuals (Susan Landau et al., 2024)</a></li></ul></li><li><a href=#eessiv-7>eess.IV (7)</a><ul><li><a href=#17--118309-afbt-gan-enhanced-explainability-and-diagnostic-performance-for-cognitive-decline-by-counterfactual-generative-adversarial-network-xiongri-shen-et-al-2024>(1/7 | 118/309) AFBT GAN: enhanced explainability and diagnostic performance for cognitive decline by counterfactual generative adversarial network (Xiongri Shen et al., 2024)</a></li><li><a href=#27--119309-harnessing-intra-group-variations-via-a-population-level-context-for-pathology-detection-p-bilha-githinji-et-al-2024>(2/7 | 119/309) Harnessing Intra-group Variations Via a Population-Level Context for Pathology Detection (P. Bilha Githinji et al., 2024)</a></li><li><a href=#37--120309-bayesian-uncertainty-estimation-by-hamiltonian-monte-carlo-applications-to-cardiac-mri-segmentation-yidong-zhao-et-al-2024>(3/7 | 120/309) Bayesian Uncertainty Estimation by Hamiltonian Monte Carlo: Applications to Cardiac MRI Segmentation (Yidong Zhao et al., 2024)</a></li><li><a href=#47--121309-domain-adaptation-explainability--fairness-in-ai-for-medical-image-analysis-diagnosis-of-covid-19-based-on-3-d-chest-ct-scans-dimitrios-kollias-et-al-2024>(4/7 | 121/309) Domain adaptation, Explainability & Fairness in AI for Medical Image Analysis: Diagnosis of COVID-19 based on 3-D Chest CT-scans (Dimitrios Kollias et al., 2024)</a></li><li><a href=#57--122309-a-spatio-temporal-aligned-sunet-model-for-low-light-video-enhancement-ruirui-lin-et-al-2024>(5/7 | 122/309) A Spatio-temporal Aligned SUNet Model for Low-light Video Enhancement (Ruirui Lin et al., 2024)</a></li><li><a href=#67--123309-real-colon-a-dataset-for-developing-real-world-ai-applications-in-colonoscopy-carlo-biffi-et-al-2024>(6/7 | 123/309) REAL-Colon: A dataset for developing real-world AI applications in colonoscopy (Carlo Biffi et al., 2024)</a></li><li><a href=#77--124309-iterative-occlusion-aware-light-field-depth-estimation-using-4d-geometrical-cues-rui-lourenço-et-al-2024>(7/7 | 124/309) Iterative Occlusion-Aware Light Field Depth Estimation using 4D Geometrical Cues (Rui Lourenço et al., 2024)</a></li></ul></li><li><a href=#cscv-62>cs.CV (62)</a><ul><li><a href=#162--125309-regiongpt-towards-region-understanding-vision-language-model-qiushan-guo-et-al-2024>(1/62 | 125/309) RegionGPT: Towards Region Understanding Vision Language Model (Qiushan Guo et al., 2024)</a></li><li><a href=#262--126309-perceptive-self-supervised-learning-network-for-noisy-image-watermark-removal-chunwei-tian-et-al-2024>(2/62 | 126/309) Perceptive self-supervised learning network for noisy image watermark removal (Chunwei Tian et al., 2024)</a></li><li><a href=#362--127309-locr-location-guided-transformer-for-optical-character-recognition-yu-sun-et-al-2024>(3/62 | 127/309) LOCR: Location-Guided Transformer for Optical Character Recognition (Yu Sun et al., 2024)</a></li><li><a href=#462--128309-vtg-gpt-tuning-free-zero-shot-video-temporal-grounding-with-gpt-yifang-xu-et-al-2024>(4/62 | 128/309) VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT (Yifang Xu et al., 2024)</a></li><li><a href=#562--129309-lightweight-object-detection-a-study-based-on-yolov7-integrated-with-shufflenetv2-and-vision-transformer-wenkai-gong-2024>(5/62 | 129/309) Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer (Wenkai Gong, 2024)</a></li><li><a href=#662--130309-beyond-specialization-assessing-the-capabilities-of-mllms-in-age-and-gender-estimation-maksim-kuprashevich-et-al-2024>(6/62 | 130/309) Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation (Maksim Kuprashevich et al., 2024)</a></li><li><a href=#762--131309-contrastive-region-guidance-improving-grounding-in-vision-language-models-without-training-david-wan-et-al-2024>(7/62 | 131/309) Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training (David Wan et al., 2024)</a></li><li><a href=#862--132309-ninformer-a-network-in-network-transformer-with-token-mixing-generated-gating-function-abdullah-nazhat-abdullah-et-al-2024>(8/62 | 132/309) NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function (Abdullah Nazhat Abdullah et al., 2024)</a></li><li><a href=#962--133309-self-supervised-facial-representation-learning-with-facial-region-awareness-zheng-gao-et-al-2024>(9/62 | 133/309) Self-Supervised Facial Representation Learning with Facial Region Awareness (Zheng Gao et al., 2024)</a></li><li><a href=#1062--134309-vision-language-models-for-medical-report-generation-and-visual-question-answering-a-review-iryna-hartsock-et-al-2024>(10/62 | 134/309) Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review (Iryna Hartsock et al., 2024)</a></li><li><a href=#1162--135309-superpixel-graph-contrastive-clustering-with-semantic-invariant-augmentations-for-hyperspectral-images-jianhan-qi-et-al-2024>(11/62 | 135/309) Superpixel Graph Contrastive Clustering with Semantic-Invariant Augmentations for Hyperspectral Images (Jianhan Qi et al., 2024)</a></li><li><a href=#1262--136309-explicit-motion-handling-and-interactive-prompting-for-video-camouflaged-object-detection-xin-zhang-et-al-2024>(12/62 | 136/309) Explicit Motion Handling and Interactive Prompting for Video Camouflaged Object Detection (Xin Zhang et al., 2024)</a></li><li><a href=#1362--137309-xt-nested-tokenization-for-larger-context-in-large-images-ritwik-gupta-et-al-2024>(13/62 | 137/309) xT: Nested Tokenization for Larger Context in Large Images (Ritwik Gupta et al., 2024)</a></li><li><a href=#1462--138309-one-prompt-word-is-enough-to-boost-adversarial-robustness-for-pre-trained-vision-language-models-lin-li-et-al-2024>(14/62 | 138/309) One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models (Lin Li et al., 2024)</a></li><li><a href=#1562--139309-tnf-tri-branch-neural-fusion-for-multimodal-medical-data-classification-tong-zheng-et-al-2024>(15/62 | 139/309) TNF: Tri-branch Neural Fusion for Multimodal Medical Data Classification (Tong Zheng et al., 2024)</a></li><li><a href=#1662--140309-when-do-convolutional-neural-networks-stop-learning-sahan-ahmad-et-al-2024>(16/62 | 140/309) When do Convolutional Neural Networks Stop Learning? (Sahan Ahmad et al., 2024)</a></li><li><a href=#1762--141309-enhancing-information-maximization-with-distance-aware-contrastive-learning-for-source-free-cross-domain-few-shot-learning-huali-xu-et-al-2024>(17/62 | 141/309) Enhancing Information Maximization with Distance-Aware Contrastive Learning for Source-Free Cross-Domain Few-Shot Learning (Huali Xu et al., 2024)</a></li><li><a href=#1862--142309-freea-human-object-interaction-detection-using-free-annotation-labels-yuxiao-wang-et-al-2024>(18/62 | 142/309) FreeA: Human-object Interaction Detection using Free Annotation Labels (Yuxiao Wang et al., 2024)</a></li><li><a href=#1962--143309-unictrl-improving-the-spatiotemporal-consistency-of-text-to-video-diffusion-models-via-training-free-unified-attention-control-xuweiyi-chen-et-al-2024>(19/62 | 143/309) UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control (Xuweiyi Chen et al., 2024)</a></li><li><a href=#2062--144309-vision-rwkv-efficient-and-scalable-visual-perception-with-rwkv-like-architectures-yuchen-duan-et-al-2024>(20/62 | 144/309) Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures (Yuchen Duan et al., 2024)</a></li><li><a href=#2162--145309-3dtopia-large-text-to-3d-generation-model-with-hybrid-diffusion-priors-fangzhou-hong-et-al-2024>(21/62 | 145/309) 3DTopia: Large Text-to-3D Generation Model with Hybrid Diffusion Priors (Fangzhou Hong et al., 2024)</a></li><li><a href=#2262--146309-mim-istd-mamba-in-mamba-for-efficient-infrared-small-target-detection-tianxiang-chen-et-al-2024>(22/62 | 146/309) MiM-ISTD: Mamba-in-Mamba for Efficient Infrared Small Target Detection (Tianxiang Chen et al., 2024)</a></li><li><a href=#2362--147309-ub-finenet-urban-building-fine-grained-classification-network-for-open-access-satellite-images-zhiyi-he-et-al-2024>(23/62 | 147/309) UB-FineNet: Urban Building Fine-grained Classification Network for Open-access Satellite Images (Zhiyi He et al., 2024)</a></li><li><a href=#2462--148309-resadapter-domain-consistent-resolution-adapter-for-diffusion-models-jiaxiang-cheng-et-al-2024>(24/62 | 148/309) ResAdapter: Domain Consistent Resolution Adapter for Diffusion Models (Jiaxiang Cheng et al., 2024)</a></li><li><a href=#2562--149309-cse-surface-anomaly-detection-with-contrastively-selected-embedding-simon-thomine-et-al-2024>(25/62 | 149/309) CSE: Surface Anomaly Detection with Contrastively Selected Embedding (Simon Thomine et al., 2024)</a></li><li><a href=#2662--150309-viewdiff-3d-consistent-image-generation-with-text-to-image-models-lukas-höllein-et-al-2024>(26/62 | 150/309) ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models (Lukas Höllein et al., 2024)</a></li><li><a href=#2762--151309-handiffuser-text-to-image-generation-with-realistic-hand-appearances-supreeth-narasimhaswamy-et-al-2024>(27/62 | 151/309) HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances (Supreeth Narasimhaswamy et al., 2024)</a></li><li><a href=#2862--152309-zero-shot-generalizable-incremental-learning-for-vision-language-object-detection-jieren-deng-et-al-2024>(28/62 | 152/309) Zero-shot Generalizable Incremental Learning for Vision-Language Object Detection (Jieren Deng et al., 2024)</a></li><li><a href=#2962--153309-modeling-multimodal-social-interactions-new-challenges-and-baselines-with-densely-aligned-representations-sangmin-lee-et-al-2024>(29/62 | 153/309) Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations (Sangmin Lee et al., 2024)</a></li><li><a href=#3062--154309-scalable-vision-based-3d-object-detection-and-monocular-depth-estimation-for-autonomous-driving-yuxuan-liu-2024>(30/62 | 154/309) Scalable Vision-Based 3D Object Detection and Monocular Depth Estimation for Autonomous Driving (Yuxuan Liu, 2024)</a></li><li><a href=#3162--155309-differentially-private-representation-learning-via-image-captioning-tom-sander-et-al-2024>(31/62 | 155/309) Differentially Private Representation Learning via Image Captioning (Tom Sander et al., 2024)</a></li><li><a href=#3262--156309-dragtex-generative-point-based-texture-editing-on-3d-mesh-yudi-zhang-et-al-2024>(32/62 | 156/309) DragTex: Generative Point-Based Texture Editing on 3D Mesh (Yudi Zhang et al., 2024)</a></li><li><a href=#3362--157309-triposr-fast-3d-object-reconstruction-from-a-single-image-dmitry-tochilkin-et-al-2024>(33/62 | 157/309) TripoSR: Fast 3D Object Reconstruction from a Single Image (Dmitry Tochilkin et al., 2024)</a></li><li><a href=#3462--158309-multi-spectral-remote-sensing-image-retrieval-using-geospatial-foundation-models-benedikt-blumenstiel-et-al-2024>(34/62 | 158/309) Multi-Spectral Remote Sensing Image Retrieval Using Geospatial Foundation Models (Benedikt Blumenstiel et al., 2024)</a></li><li><a href=#3562--159309-physics-informed-learning-for-time-resolved-angiographic-contrast-agent-concentration-reconstruction-noah-maul-et-al-2024>(35/62 | 159/309) Physics-Informed Learning for Time-Resolved Angiographic Contrast Agent Concentration Reconstruction (Noah Maul et al., 2024)</a></li><li><a href=#3662--160309-place-adaptive-layout-semantic-fusion-for-semantic-image-synthesis-zhengyao-lv-et-al-2024>(36/62 | 160/309) PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis (Zhengyao Lv et al., 2024)</a></li><li><a href=#3762--161309-pointcore-efficient-unsupervised-point-cloud-anomaly-detector-using-local-global-features-baozhu-zhao-et-al-2024>(37/62 | 161/309) PointCore: Efficient Unsupervised Point Cloud Anomaly Detector Using Local-Global Features (Baozhu Zhao et al., 2024)</a></li><li><a href=#3862--162309-ootdiffusion-outfitting-fusion-based-latent-diffusion-for-controllable-virtual-try-on-yuhao-xu-et-al-2024>(38/62 | 162/309) OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on (Yuhao Xu et al., 2024)</a></li><li><a href=#3962--163309-mca-moment-channel-attention-networks-yangbo-jiang-et-al-2024>(39/62 | 163/309) MCA: Moment Channel Attention Networks (Yangbo Jiang et al., 2024)</a></li><li><a href=#4062--164309-a-new-perspective-on-smiling-and-laughter-detection-intensity-levels-matter-hugo-bohy-et-al-2024>(40/62 | 164/309) A New Perspective on Smiling and Laughter Detection: Intensity Levels Matter (Hugo Bohy et al., 2024)</a></li><li><a href=#4162--165309-facechain-imagineid-freely-crafting-high-fidelity-diverse-talking-faces-from-disentangled-audio-chao-xu-et-al-2024>(41/62 | 165/309) FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio (Chao Xu et al., 2024)</a></li><li><a href=#4262--166309-modality-aware-and-shift-mixer-for-multi-modal-brain-tumor-segmentation-zhongzhen-huang-et-al-2024>(42/62 | 166/309) Modality-Aware and Shift Mixer for Multi-modal Brain Tumor Segmentation (Zhongzhen Huang et al., 2024)</a></li><li><a href=#4362--167309-allspark-reborn-labeled-features-from-unlabeled-in-transformer-for-semi-supervised-semantic-segmentation-haonan-wang-et-al-2024>(43/62 | 167/309) AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation (Haonan Wang et al., 2024)</a></li><li><a href=#4462--168309-exposing-the-deception-uncovering-more-forgery-clues-for-deepfake-detection-zhongjie-ba-et-al-2024>(44/62 | 168/309) Exposing the Deception: Uncovering More Forgery Clues for Deepfake Detection (Zhongjie Ba et al., 2024)</a></li><li><a href=#4562--169309-optimizing-illuminant-estimation-in-dual-exposure-hdr-imaging-mahmoud-afifi-et-al-2024>(45/62 | 169/309) Optimizing Illuminant Estimation in Dual-Exposure HDR Imaging (Mahmoud Afifi et al., 2024)</a></li><li><a href=#4662--170309-brand-visibility-in-packaging-a-deep-learning-approach-for-logo-detection-saliency-map-prediction-and-logo-placement-analysis-alireza-hosseini-et-al-2024>(46/62 | 170/309) Brand Visibility in Packaging: A Deep Learning Approach for Logo Detection, Saliency-Map Prediction, and Logo Placement Analysis (Alireza Hosseini et al., 2024)</a></li><li><a href=#4762--171309-non-autoregressive-sequence-to-sequence-vision-language-models-kunyu-shi-et-al-2024>(47/62 | 171/309) Non-autoregressive Sequence-to-Sequence Vision-Language Models (Kunyu Shi et al., 2024)</a></li><li><a href=#4862--172309-diffmot-a-real-time-diffusion-based-multiple-object-tracker-with-non-linear-prediction-weiyi-lv-et-al-2024>(48/62 | 172/309) DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction (Weiyi Lv et al., 2024)</a></li><li><a href=#4962--173309-depth-guided-robust-and-fast-point-cloud-fusion-nerf-for-sparse-input-views-shuai-guo-et-al-2024>(49/62 | 173/309) Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input Views (Shuai Guo et al., 2024)</a></li><li><a href=#5062--174309-leveraging-anchor-based-lidar-3d-object-detection-via-point-assisted-sample-selection-shitao-chen-et-al-2024>(50/62 | 174/309) Leveraging Anchor-based LiDAR 3D Object Detection via Point Assisted Sample Selection (Shitao Chen et al., 2024)</a></li><li><a href=#5162--175309-fourier-basis-functions-to-bridge-augmentation-gap-rethinking-frequency-augmentation-in-image-classification-puru-vaish-et-al-2024>(51/62 | 175/309) Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification (Puru Vaish et al., 2024)</a></li><li><a href=#5262--176309-semi-supervised-semantic-segmentation-based-on-pseudo-labels-a-survey-lingyan-ran-et-al-2024>(52/62 | 176/309) Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey (Lingyan Ran et al., 2024)</a></li><li><a href=#5362--177309-atomovideo-high-fidelity-image-to-video-generation-litong-gong-et-al-2024>(53/62 | 177/309) AtomoVideo: High Fidelity Image-to-Video Generation (Litong Gong et al., 2024)</a></li><li><a href=#5462--178309-integrating-efficient-optimal-transport-and-functional-maps-for-unsupervised-shape-correspondence-learning-tung-le-et-al-2024>(54/62 | 178/309) Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning (Tung Le et al., 2024)</a></li><li><a href=#5562--179309-attention-guidance-mechanism-for-handwritten-mathematical-expression-recognition-yutian-liu-et-al-2024>(55/62 | 179/309) Attention Guidance Mechanism for Handwritten Mathematical Expression Recognition (Yutian Liu et al., 2024)</a></li><li><a href=#5662--180309-training-free-pretrained-model-merging-zhengqi-xu-et-al-2024>(56/62 | 180/309) Training-Free Pretrained Model Merging (Zhengqi Xu et al., 2024)</a></li><li><a href=#5762--181309-pillargen-enhancing-radar-point-cloud-density-and-quality-via-pillar-based-point-generation-network-jisong-kim-et-al-2024>(57/62 | 181/309) PillarGen: Enhancing Radar Point Cloud Density and Quality via Pillar-based Point Generation Network (Jisong Kim et al., 2024)</a></li><li><a href=#5862--182309-neural-network-assisted-lifting-steps-for-improved-fully-scalable-lossy-image-compression-in-jpeg-2000-xinyue-li-et-al-2024>(58/62 | 182/309) Neural Network Assisted Lifting Steps For Improved Fully Scalable Lossy Image Compression in JPEG 2000 (Xinyue Li et al., 2024)</a></li><li><a href=#5962--183309-3d-hand-reconstruction-via-aggregating-intra-and-inter-graphs-guided-by-prior-knowledge-for-hand-object-interaction-scenario-feng-shuang-et-al-2024>(59/62 | 183/309) 3D Hand Reconstruction via Aggregating Intra and Inter Graphs Guided by Prior Knowledge for Hand-Object Interaction Scenario (Feng Shuang et al., 2024)</a></li><li><a href=#6062--184309-a-generative-approach-for-wikipedia-scale-visual-entity-recognition-mathilde-caron-et-al-2024>(60/62 | 184/309) A Generative Approach for Wikipedia-Scale Visual Entity Recognition (Mathilde Caron et al., 2024)</a></li><li><a href=#6162--185309-towards-calibrated-deep-clustering-network-yuheng-jia-et-al-2024>(61/62 | 185/309) Towards Calibrated Deep Clustering Network (Yuheng Jia et al., 2024)</a></li><li><a href=#6262--186309-tree-counting-by-bridging-3d-point-clouds-with-imagery-lei-li-et-al-2024>(62/62 | 186/309) Tree Counting by Bridging 3D Point Clouds with Imagery (Lei Li et al., 2024)</a></li></ul></li><li><a href=#csro-20>cs.RO (20)</a><ul><li><a href=#120--187309-zsl-rppo-zero-shot-learning-for-quadrupedal-locomotion-in-challenging-terrains-using-recurrent-proximal-policy-optimization-yao-zhao-et-al-2024>(1/20 | 187/309) ZSL-RPPO: Zero-Shot Learning for Quadrupedal Locomotion in Challenging Terrains using Recurrent Proximal Policy Optimization (Yao Zhao et al., 2024)</a></li><li><a href=#220--188309-offline-goal-conditioned-reinforcement-learning-for-safety-critical-tasks-with-recovery-policy-chenyang-cao-et-al-2024>(2/20 | 188/309) Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy (Chenyang Cao et al., 2024)</a></li><li><a href=#320--189309-aspire-an-informative-trajectory-planner-with-mutual-information-approximation-for-target-search-and-tracking-kangjie-zhou-et-al-2024>(3/20 | 189/309) ASPIRe: An Informative Trajectory Planner with Mutual Information Approximation for Target Search and Tracking (Kangjie Zhou et al., 2024)</a></li><li><a href=#420--190309-pseudo-labeling-and-contextual-curriculum-learning-for-online-grasp-learning-in-robotic-bin-picking-huy-le-et-al-2024>(4/20 | 190/309) Pseudo-Labeling and Contextual Curriculum Learning for Online Grasp Learning in Robotic Bin Picking (Huy Le et al., 2024)</a></li><li><a href=#520--191309-twisting-lids-off-with-two-hands-toru-lin-et-al-2024>(5/20 | 191/309) Twisting Lids Off with Two Hands (Toru Lin et al., 2024)</a></li><li><a href=#620--192309-an-efficient-model-based-approach-on-learning-agile-motor-skills-without-reinforcement-haojie-shi-et-al-2024>(6/20 | 192/309) An Efficient Model-Based Approach on Learning Agile Motor Skills without Reinforcement (Haojie Shi et al., 2024)</a></li><li><a href=#720--193309-sensor-based-multi-robot-search-and-coverage-with-spatial-separation-in-unstructured-environments-xinyi-wang-et-al-2024>(7/20 | 193/309) Sensor-based Multi-Robot Search and Coverage with Spatial Separation in Unstructured Environments (Xinyi Wang et al., 2024)</a></li><li><a href=#820--194309-tac-man-tactile-informed-prior-free-manipulation-of-articulated-objects-zihang-zhao-et-al-2024>(8/20 | 194/309) Tac-Man: Tactile-Informed Prior-Free Manipulation of Articulated Objects (Zihang Zhao et al., 2024)</a></li><li><a href=#920--195309-purpose-for-open-ended-learning-robots-a-computational-taxonomy-definition-and-operationalisation-gianluca-baldassarre-et-al-2024>(9/20 | 195/309) Purpose for Open-Ended Learning Robots: A Computational Taxonomy, Definition, and Operationalisation (Gianluca Baldassarre et al., 2024)</a></li><li><a href=#1020--196309-cross-domain-policy-transfer-with-effect-cycle-consistency-ruiqi-zhu-et-al-2024>(10/20 | 196/309) Cross Domain Policy Transfer with Effect Cycle-Consistency (Ruiqi Zhu et al., 2024)</a></li><li><a href=#1120--197309-rt-h-action-hierarchies-using-language-suneel-belkhale-et-al-2024>(11/20 | 197/309) RT-H: Action Hierarchies Using Language (Suneel Belkhale et al., 2024)</a></li><li><a href=#1220--198309-natsgd-a-dataset-with-speech-gestures-and-demonstrations-for-robot-learning-in-natural-human-robot-interaction-snehesh-shrestha-et-al-2024>(12/20 | 198/309) NatSGD: A Dataset with Speech, Gestures, and Demonstrations for Robot Learning in Natural Human-Robot Interaction (Snehesh Shrestha et al., 2024)</a></li><li><a href=#1320--199309-saqiel-ultra-light-and-safe-manipulator-with-passive-3d-wire-alignment-mechanism-temma-suzuki-et-al-2024>(13/20 | 199/309) SAQIEL: Ultra-Light and Safe Manipulator with Passive 3D Wire Alignment Mechanism (Temma Suzuki et al., 2024)</a></li><li><a href=#1420--200309-exposure-conscious-path-planning-for-equal-exposure-corridors-eugene-t-hamzezadeh-et-al-2024>(14/20 | 200/309) Exposure-Conscious Path Planning for Equal-Exposure Corridors (Eugene T. Hamzezadeh et al., 2024)</a></li><li><a href=#1520--201309-uncertainty-aware-prediction-and-application-in-planning-for-autonomous-driving-definitions-methods-and-comparison-wenbo-shao-et-al-2024>(15/20 | 201/309) Uncertainty-Aware Prediction and Application in Planning for Autonomous Driving: Definitions, Methods, and Comparison (Wenbo Shao et al., 2024)</a></li><li><a href=#1620--202309-tightly-coupled-lidar-visual-inertial-slam-and-large-scale-volumetric-occupancy-mapping-simon-boche-et-al-2024>(16/20 | 202/309) Tightly-Coupled LiDAR-Visual-Inertial SLAM and Large-Scale Volumetric Occupancy Mapping (Simon Boche et al., 2024)</a></li><li><a href=#1720--203309-lista-geometric-object-based-change-detection-in-cluttered-environments-joseph-rowell-et-al-2024>(17/20 | 203/309) LiSTA: Geometric Object-Based Change Detection in Cluttered Environments (Joseph Rowell et al., 2024)</a></li><li><a href=#1820--204309-skater-a-novel-bi-modal-bi-copter-robot-for-adaptive-locomotion-in-air-and-diverse-terrain-junxiao-lin-et-al-2024>(18/20 | 204/309) Skater: A Novel Bi-modal Bi-copter Robot for Adaptive Locomotion in Air and Diverse Terrain (Junxiao Lin et al., 2024)</a></li><li><a href=#1920--205309-tta-nav-test-time-adaptive-reconstruction-for-point-goal-navigation-under-visual-corruptions-maytus-piriyajitakonkij-et-al-2024>(19/20 | 205/309) TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation under Visual Corruptions (Maytus Piriyajitakonkij et al., 2024)</a></li><li><a href=#2020--206309-aerial-tensile-perching-and-disentangling-mechanism-for-long-term-environmental-monitoring-tian-lan-et-al-2024>(20/20 | 206/309) Aerial Tensile Perching and Disentangling Mechanism for Long-Term Environmental Monitoring (Tian Lan et al., 2024)</a></li></ul></li><li><a href=#csai-11>cs.AI (11)</a><ul><li><a href=#111--207309-large-language-model-based-evolutionary-optimizer-reasoning-with-elitism-shuvayan-brahmachary-et-al-2024>(1/11 | 207/309) Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism (Shuvayan Brahmachary et al., 2024)</a></li><li><a href=#211--208309-the-ink-splotch-effect-a-case-study-on-chatgpt-as-a-co-creative-game-designer-asad-anjum-et-al-2024>(2/11 | 208/309) The Ink Splotch Effect: A Case Study on ChatGPT as a Co-Creative Game Designer (Asad Anjum et al., 2024)</a></li><li><a href=#311--209309-catcode-a-comprehensive-evaluation-framework-for-llms-on-the-mixture-of-code-and-text-zhenru-lin-et-al-2024>(3/11 | 209/309) CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text (Zhenru Lin et al., 2024)</a></li><li><a href=#411--210309-how-multimodal-integration-boost-the-performance-of-llm-for-optimization-case-study-on-capacitated-vehicle-routing-problems-yuxiao-huang-et-al-2024>(4/11 | 210/309) How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems (Yuxiao Huang et al., 2024)</a></li><li><a href=#511--211309-koopman-assisted-reinforcement-learning-preston-rozwood-et-al-2024>(5/11 | 211/309) Koopman-Assisted Reinforcement Learning (Preston Rozwood et al., 2024)</a></li><li><a href=#611--212309-cognition-is-all-you-need----the-next-layer-of-ai-above-large-language-models-nova-spivack-et-al-2024>(6/11 | 212/309) Cognition is All You Need &ndash; The Next Layer of AI Above Large Language Models (Nova Spivack et al., 2024)</a></li><li><a href=#711--213309-a-scoping-review-of-energy-efficient-driving-behaviors-and-applied-state-of-the-art-ai-methods-zhipeng-ma-et-al-2024>(7/11 | 213/309) A Scoping Review of Energy-Efficient Driving Behaviors and Applied State-of-the-Art AI Methods (Zhipeng Ma et al., 2024)</a></li><li><a href=#811--214309-transformer-for-times-series-an-application-to-the-sp500-pierre-brugiere-et-al-2024>(8/11 | 214/309) Transformer for Times Series: an Application to the S&amp;P500 (Pierre Brugiere et al., 2024)</a></li><li><a href=#911--215309-smaug-a-sliding-multidimensional-task-window-based-marl-framework-for-adaptive-real-time-subtask-recognition-wenjing-zhang-et-al-2024>(9/11 | 215/309) SMAUG: A Sliding Multidimensional Task Window-Based MARL Framework for Adaptive Real-Time Subtask Recognition (Wenjing Zhang et al., 2024)</a></li><li><a href=#1011--216309-fast-benchmarking-of-asynchronous-multi-fidelity-optimization-on-zero-cost-benchmarks-shuhei-watanabe-et-al-2024>(10/11 | 216/309) Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks (Shuhei Watanabe et al., 2024)</a></li><li><a href=#1111--217309-morbdd-multiobjective-restricted-binary-decision-diagrams-by-learning-to-sparsify-rahul-patel-et-al-2024>(11/11 | 217/309) MORBDD: Multiobjective Restricted Binary Decision Diagrams by Learning to Sparsify (Rahul Patel et al., 2024)</a></li></ul></li><li><a href=#cscr-8>cs.CR (8)</a><ul><li><a href=#18--218309-unveiling-hidden-links-between-unseen-security-entities-daniel-alfasi-et-al-2024>(1/8 | 218/309) Unveiling Hidden Links Between Unseen Security Entities (Daniel Alfasi et al., 2024)</a></li><li><a href=#28--219309-knowphish-large-language-models-meet-multimodal-knowledge-graphs-for-enhancing-reference-based-phishing-detection-yuexin-li-et-al-2024>(2/8 | 219/309) KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection (Yuexin Li et al., 2024)</a></li><li><a href=#38--220309-malignnoma-gnn-based-malicious-circuit-classifier-for-secure-cloud-fpgas-lilas-alrahis-et-al-2024>(3/8 | 220/309) MaliGNNoma: GNN-Based Malicious Circuit Classifier for Secure Cloud FPGAs (Lilas Alrahis et al., 2024)</a></li><li><a href=#48--221309-k-stars-ldp-a-novel-framework-for-p-q-clique-enumeration-under-local-differential-privacy-henan-sun-et-al-2024>(4/8 | 221/309) K-stars LDP: A Novel Framework for (p, q)-clique Enumeration under Local Differential Privacy (Henan Sun et al., 2024)</a></li><li><a href=#58--222309-comprehensive-evaluation-of-mal-api-2019-dataset-by-machine-learning-in-malware-detection-zhenglin-li-et-al-2024>(5/8 | 222/309) Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection (Zhenglin Li et al., 2024)</a></li><li><a href=#68--223309-mts-bringing-multi-tenancy-to-virtual-networking-kashyap-thimmaraju-et-al-2024>(6/8 | 223/309) MTS: Bringing Multi-Tenancy to Virtual Networking (Kashyap Thimmaraju et al., 2024)</a></li><li><a href=#78--224309-deployment-challenges-of-industrial-intrusion-detection-systems-konrad-wolsing-et-al-2024>(7/8 | 224/309) Deployment Challenges of Industrial Intrusion Detection Systems (Konrad Wolsing et al., 2024)</a></li><li><a href=#88--225309-i-just-hated-it-and-i-want-my-money-back-data-driven-understanding-of-mobile-vpn-service-switching-preferences-in-the-wild-rohit-raj-et-al-2024>(8/8 | 225/309) &lsquo;I just hated it and I want my money back&rsquo;: Data-driven Understanding of Mobile VPN Service Switching Preferences in The Wild (Rohit Raj et al., 2024)</a></li></ul></li><li><a href=#csni-4>cs.NI (4)</a><ul><li><a href=#14--226309-towards-intent-based-network-management-large-language-models-for-intent-extraction-in-5g-core-networks-dimitrios-michael-manias-et-al-2024>(1/4 | 226/309) Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks (Dimitrios Michael Manias et al., 2024)</a></li><li><a href=#24--227309-graph-neural-network-for-in-network-placement-of-real-time-metaverse-tasks-in-next-generation-network-sulaiman-muhammad-rashid-et-al-2024>(2/4 | 227/309) Graph neural network for in-network placement of real-time metaverse tasks in next-generation network (Sulaiman Muhammad Rashid et al., 2024)</a></li><li><a href=#34--228309-towards-fair-and-efficient-learning-based-congestion-control-xudong-liao-et-al-2024>(3/4 | 228/309) Towards Fair and Efficient Learning-based Congestion Control (Xudong Liao et al., 2024)</a></li><li><a href=#44--229309-probabilistic-fault-tolerant-robust-traffic-grooming-in-otn-over-dwdm-networks-dimitrios-michael-manias-et-al-2024>(4/4 | 229/309) Probabilistic Fault-Tolerant Robust Traffic Grooming in OTN-over-DWDM Networks (Dimitrios Michael Manias et al., 2024)</a></li></ul></li><li><a href=#astro-phco-1>astro-ph.CO (1)</a><ul><li><a href=#11--230309-predicting-large-scale-cosmological-structure-evolution-with-gan-based-autoencoders-marion-ullmo-et-al-2024>(1/1 | 230/309) Predicting large scale cosmological structure evolution with GAN-based autoencoders (Marion Ullmo et al., 2024)</a></li></ul></li><li><a href=#cssd-4>cs.SD (4)</a><ul><li><a href=#14--231309-sa-sot-speaker-aware-serialized-output-training-for-multi-talker-asr-zhiyun-fan-et-al-2024>(1/4 | 231/309) SA-SOT: Speaker-Aware Serialized Output Training for Multi-Talker ASR (Zhiyun Fan et al., 2024)</a></li><li><a href=#24--232309-robust-wake-word-spotting-with-frame-level-cross-modal-attention-based-audio-visual-conformer-haoxu-wang-et-al-2024>(2/4 | 232/309) Robust Wake Word Spotting With Frame-Level Cross-Modal Attention Based Audio-Visual Conformer (Haoxu Wang et al., 2024)</a></li><li><a href=#34--233309-a-robust-audio-deepfake-detection-system-via-multi-view-feature-yujie-yang-et-al-2024>(3/4 | 233/309) A robust audio deepfake detection system via multi-view feature (Yujie Yang et al., 2024)</a></li><li><a href=#44--234309-consep-a-noise--and-reverberation-robust-speech-separation-framework-by-magnitude-conditioning-kuan-hsun-ho-et-al-2024>(4/4 | 234/309) ConSep: a Noise- and Reverberation-Robust Speech Separation Framework by Magnitude Conditioning (Kuan-Hsun Ho et al., 2024)</a></li></ul></li><li><a href=#econgn-1>econ.GN (1)</a><ul><li><a href=#11--235309-the-heterogeneous-productivity-effects-of-generative-ai-david-kreitmeir-et-al-2024>(1/1 | 235/309) The Heterogeneous Productivity Effects of Generative AI (David Kreitmeir et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--236309-pi-astrodeconv-a-physics-informed-unsupervised-learning-method-for-astronomical-image-deconvolution-shulei-ni-et-al-2024>(1/1 | 236/309) PI-AstroDeconv: A Physics-Informed Unsupervised Learning Method for Astronomical Image Deconvolution (Shulei Ni et al., 2024)</a></li></ul></li><li><a href=#cshc-7>cs.HC (7)</a><ul><li><a href=#17--237309-situated-understanding-of-older-adults-interactions-with-voice-assistants-a-month-long-in-home-study-amama-mahmood-et-al-2024>(1/7 | 237/309) Situated Understanding of Older Adults&rsquo; Interactions with Voice Assistants: A Month-long In-home Study (Amama Mahmood et al., 2024)</a></li><li><a href=#27--238309-towards-a-diffractive-analysis-of-prompt-based-generative-ai-nina-rajcic-et-al-2024>(2/7 | 238/309) Towards A Diffractive Analysis of Prompt-Based Generative AI (Nina Rajcic et al., 2024)</a></li><li><a href=#37--239309-human-ai-collaboration-increases-skill-tagging-speed-but-degrades-accuracy-cheng-ren-et-al-2024>(3/7 | 239/309) Human-AI Collaboration Increases Skill Tagging Speed but Degrades Accuracy (Cheng Ren et al., 2024)</a></li><li><a href=#47--240309-memoro-using-large-language-models-to-realize-a-concise-interface-for-real-time-memory-augmentation-wazeer-zulfikar-et-al-2024>(4/7 | 240/309) Memoro: Using Large Language Models to Realize a Concise Interface for Real-Time Memory Augmentation (Wazeer Zulfikar et al., 2024)</a></li><li><a href=#57--241309-ivie-lightweight-anchored-explanations-of-just-generated-code-litao-yan-et-al-2024>(5/7 | 241/309) Ivie: Lightweight Anchored Explanations of Just-Generated Code (Litao Yan et al., 2024)</a></li><li><a href=#67--242309-beyond-recommender-an-exploratory-study-of-the-effects-of-different-ai-roles-in-ai-assisted-decision-making-shuai-ma-et-al-2024>(6/7 | 242/309) Beyond Recommender: An Exploratory Study of the Effects of Different AI Roles in AI-Assisted Decision Making (Shuai Ma et al., 2024)</a></li><li><a href=#77--243309-closing-the-knowledge-gap-in-designing-data-annotation-interfaces-for-ai-powered-disaster-management-analytic-systems-zinat-ara-et-al-2024>(7/7 | 243/309) Closing the Knowledge Gap in Designing Data Annotation Interfaces for AI-powered Disaster Management Analytic Systems (Zinat Ara et al., 2024)</a></li></ul></li><li><a href=#eessas-3>eess.AS (3)</a><ul><li><a href=#13--244309-speech-emotion-recognition-from-voice-messages-recorded-in-the-wild-lucía-gómez-zaragozá-et-al-2024>(1/3 | 244/309) Speech emotion recognition from voice messages recorded in the wild (Lucía Gómez-Zaragozá et al., 2024)</a></li><li><a href=#23--245309-neurovoz-a-castillian-spanish-corpus-of-parkinsonian-speech-janaína-mendes-laureano-et-al-2024>(2/3 | 245/309) NeuroVoz: a Castillian Spanish corpus of parkinsonian speech (Janaína Mendes-Laureano et al., 2024)</a></li><li><a href=#33--246309-6dof-seld-sound-event-localization-and-detection-using-microphones-and-motion-tracking-sensors-on-self-motioning-human-masahiro-yasuda-et-al-2024>(3/3 | 246/309) 6DoF SELD: Sound Event Localization and Detection Using Microphones and Motion Tracking Sensors on self-motioning human (Masahiro Yasuda et al., 2024)</a></li></ul></li><li><a href=#statml-5>stat.ML (5)</a><ul><li><a href=#15--247309-differential-privacy-of-noisy-sgd-under-heavy-tailed-perturbations-umut-şimşekli-et-al-2024>(1/5 | 247/309) Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations (Umut Şimşekli et al., 2024)</a></li><li><a href=#25--248309-soft-constrained-schrodinger-bridge-a-stochastic-control-approach-jhanvi-garg-et-al-2024>(2/5 | 248/309) Soft-constrained Schrodinger Bridge: a Stochastic Control Approach (Jhanvi Garg et al., 2024)</a></li><li><a href=#35--249309-bipartite-graph-variational-auto-encoder-with-fair-latent-representation-to-account-for-sampling-bias-in-ecological-networks-emre-anakok-et-al-2024>(3/5 | 249/309) Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks (Emre Anakok et al., 2024)</a></li><li><a href=#45--250309-on-the-impact-of-measure-pre-conditionings-on-general-parametric-ml-models-and-transfer-learning-via-domain-adaptation-joaquín-sánchez-garcía-2024>(4/5 | 250/309) On the impact of measure pre-conditionings on general parametric ML models and transfer learning via domain adaptation (Joaquín Sánchez García, 2024)</a></li><li><a href=#55--251309-improving-generalisation-via-anchor-multivariate-analysis-homer-durand-et-al-2024>(5/5 | 251/309) Improving generalisation via anchor multivariate analysis (Homer Durand et al., 2024)</a></li></ul></li><li><a href=#csdc-4>cs.DC (4)</a><ul><li><a href=#14--252309-mpi-errors-detection-using-gnn-embedding-and-vector-embedding-over-llvm-ir-jad-el-karchi-et-al-2024>(1/4 | 252/309) MPI Errors Detection using GNN Embedding and Vector Embedding over LLVM IR (Jad El Karchi et al., 2024)</a></li><li><a href=#24--253309-déjàvu-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-foteini-strati-et-al-2024>(2/4 | 253/309) DéjàVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving (Foteini Strati et al., 2024)</a></li><li><a href=#34--254309-daedalus-self-adaptive-horizontal-autoscaling-for-resource-efficiency-of-distributed-stream-processing-systems-benjamin-j-j-pfister-et-al-2024>(3/4 | 254/309) Daedalus: Self-Adaptive Horizontal Autoscaling for Resource Efficiency of Distributed Stream Processing Systems (Benjamin J. J. Pfister et al., 2024)</a></li><li><a href=#44--255309-demeter-resource-efficient-distributed-stream-processing-under-dynamic-loads-with-multi-configuration-optimization-morgan-geldenhuys-et-al-2024>(4/4 | 255/309) Demeter: Resource-Efficient Distributed Stream Processing under Dynamic Loads with Multi-Configuration Optimization (Morgan Geldenhuys et al., 2024)</a></li></ul></li><li><a href=#cspl-6>cs.PL (6)</a><ul><li><a href=#16--256309-let-a-thousand-flowers-bloom-an-algebraic-representation-for-edge-graphs-jack-liell-cock-et-al-2024>(1/6 | 256/309) Let a Thousand Flowers Bloom: An Algebraic Representation for Edge Graphs (Jack Liell-Cock et al., 2024)</a></li><li><a href=#26--257309-privacy-respecting-type-error-telemetry-at-scale-ben-greenman-et-al-2024>(2/6 | 257/309) Privacy-Respecting Type Error Telemetry at Scale (Ben Greenman et al., 2024)</a></li><li><a href=#36--258309-reactive-programming-without-functions-bjarno-oeyen-et-al-2024>(3/6 | 258/309) Reactive Programming without Functions (Bjarno Oeyen et al., 2024)</a></li><li><a href=#46--259309-scheduling-garbage-collection-for-energy-efficiency-on-asymmetric-multicore-processors-marina-shimchenko-et-al-2024>(4/6 | 259/309) Scheduling Garbage Collection for Energy Efficiency on Asymmetric Multicore Processors (Marina Shimchenko et al., 2024)</a></li><li><a href=#56--260309-liverec-prototyping-probes-by-framing-debug-protocols-jean-baptiste-döderlein-et-al-2024>(5/6 | 260/309) LiveRec: Prototyping Probes by Framing Debug Protocols (Jean-Baptiste Döderlein et al., 2024)</a></li><li><a href=#66--261309-arrays-in-practice-an-empirical-study-of-array-access-patterns-on-the-jvm-beatrice-åkerblom-et-al-2024>(6/6 | 261/309) Arrays in Practice: An Empirical Study of Array Access Patterns on the JVM (Beatrice Åkerblom et al., 2024)</a></li></ul></li><li><a href=#mathpr-1>math.PR (1)</a><ul><li><a href=#11--262309-emergence-of-multivariate-extremes-in-multilayer-inhomogeneous-random-graphs-daniel-cirkovic-et-al-2024>(1/1 | 262/309) Emergence of Multivariate Extremes in Multilayer Inhomogeneous Random Graphs (Daniel Cirkovic et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--263309-large-language-models-surpass-human-experts-in-predicting-neuroscience-results-xiaoliang-luo-et-al-2024>(1/1 | 263/309) Large language models surpass human experts in predicting neuroscience results (Xiaoliang Luo et al., 2024)</a></li></ul></li><li><a href=#mathna-5>math.NA (5)</a><ul><li><a href=#15--264309-numerical-simulation-of-phase-transition-with-the-hyperbolic-godunov-peshkov-romenski-model-pascal-mossier-et-al-2024>(1/5 | 264/309) Numerical Simulation of Phase Transition with the Hyperbolic Godunov-Peshkov-Romenski Model (Pascal Mossier et al., 2024)</a></li><li><a href=#25--265309-improving-the-accuracy-of-the-newmark-method-through-backward-error-analysis-donát-m-takács-et-al-2024>(2/5 | 265/309) Improving the accuracy of the Newmark method through backward error analysis (Donát M. Takács et al., 2024)</a></li><li><a href=#35--266309-simulation-based-high-speed-elongational-rheometer-for-carreau-type-materials-lukas-kannengiesser-et-al-2024>(3/5 | 266/309) Simulation-based High-Speed Elongational Rheometer for Carreau-type Materials (Lukas Kannengiesser et al., 2024)</a></li><li><a href=#45--267309-bayesian-inference-via-geometric-optics-approximation-zejun-sun-et-al-2024>(4/5 | 267/309) Bayesian inference via geometric optics approximation (Zejun Sun et al., 2024)</a></li><li><a href=#55--268309-analysis-on-aggregation-and-block-smoothers-in-multigrid-methods-for-block-toeplitz-linear-systems-matthias-bolten-et-al-2024>(5/5 | 268/309) Analysis on aggregation and block smoothers in multigrid methods for block Toeplitz linear systems (Matthias Bolten et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--269309-a-novel-shortest-path-query-algorithm-based-on-optimized-adaptive-topology-structure-xiao-fang-et-al-2024>(1/1 | 269/309) A Novel Shortest Path Query Algorithm Based on Optimized Adaptive Topology Structure (Xiao Fang et al., 2024)</a></li></ul></li><li><a href=#mathds-1>math.DS (1)</a><ul><li><a href=#11--270309-koopman-operators-with-intrinsic-observables-in-rigged-reproducing-kernel-hilbert-spaces-isao-ishikawa-et-al-2024>(1/1 | 270/309) Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces (Isao Ishikawa et al., 2024)</a></li></ul></li><li><a href=#cssy-1>cs.SY (1)</a><ul><li><a href=#11--271309-collision-avoidance-and-geofencing-for-fixed-wing-aircraft-with-control-barrier-functions-tamas-g-molnar-et-al-2024>(1/1 | 271/309) Collision Avoidance and Geofencing for Fixed-wing Aircraft with Control Barrier Functions (Tamas G. Molnar et al., 2024)</a></li></ul></li><li><a href=#q-finpm-1>q-fin.PM (1)</a><ul><li><a href=#11--272309-rvrae-a-dynamic-factor-model-based-on-variational-recurrent-autoencoder-for-stock-returns-prediction-yilun-wang-et-al-2024>(1/1 | 272/309) RVRAE: A Dynamic Factor Model Based on Variational Recurrent Autoencoder for Stock Returns Prediction (Yilun Wang et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--273309-the-complexity-of-computing-in-continuous-time-space-complexity-is-precision-manon-blanc-et-al-2024>(1/1 | 273/309) The complexity of computing in continuous time: space complexity is precision (Manon Blanc et al., 2024)</a></li></ul></li><li><a href=#mathap-1>math.AP (1)</a><ul><li><a href=#11--274309-exploring-well-posedness-and-asymptotic-behavior-in-an-advection-diffusion-reaction-adr-model-mohammed-elghandouri-et-al-2024>(1/1 | 274/309) Exploring Well-Posedness and Asymptotic Behavior in an Advection-Diffusion-Reaction (ADR) Model (Mohammed Elghandouri et al., 2024)</a></li></ul></li><li><a href=#csne-4>cs.NE (4)</a><ul><li><a href=#14--275309-deep-reinforcement-learning-for-dynamic-algorithm-selection-a-proof-of-principle-study-on-differential-evolution-hongshu-guo-et-al-2024>(1/4 | 275/309) Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution (Hongshu Guo et al., 2024)</a></li><li><a href=#24--276309-universality-of-reservoir-systems-with-recurrent-neural-networks-hiroki-yasumoto-et-al-2024>(2/4 | 276/309) Universality of reservoir systems with recurrent neural networks (Hiroki Yasumoto et al., 2024)</a></li><li><a href=#34--277309-analysis-and-fully-memristor-based-reservoir-computing-for-temporal-data-classification-ankur-singh-et-al-2024>(3/4 | 277/309) Analysis and Fully Memristor-based Reservoir Computing for Temporal Data Classification (Ankur Singh et al., 2024)</a></li><li><a href=#44--278309-toward-neuromic-computing-neurons-as-autoencoders-larry-bull-2024>(4/4 | 278/309) Toward Neuromic Computing: Neurons as Autoencoders (Larry Bull, 2024)</a></li></ul></li><li><a href=#quant-ph-3>quant-ph (3)</a><ul><li><a href=#13--279309-hybrid-quantum-neural-network-advantage-for-radar-based-drone-detection-and-classification-in-low-signal-to-noise-ratio-aiswariya-sweety-malarvanan-2024>(1/3 | 279/309) Hybrid Quantum Neural Network Advantage for Radar-Based Drone Detection and Classification in Low Signal-to-Noise Ratio (Aiswariya Sweety Malarvanan, 2024)</a></li><li><a href=#23--280309-classification-of-the-fashion-mnist-dataset-on-a-quantum-computer-kevin-shen-et-al-2024>(2/3 | 280/309) Classification of the Fashion-MNIST Dataset on a Quantum Computer (Kevin Shen et al., 2024)</a></li><li><a href=#33--281309-hybrid-quantum-programming-with-pennylane-lightning-on-hpc-platforms-ali-asadi-et-al-2024>(3/3 | 281/309) Hybrid quantum programming with PennyLane Lightning on HPC platforms (Ali Asadi et al., 2024)</a></li></ul></li><li><a href=#csit-1>cs.IT (1)</a><ul><li><a href=#11--282309-otfs-vs-ofdm-which-is-superior-in-multiuser-leo-satellite-communications-yu-liu-et-al-2024>(1/1 | 282/309) OTFS vs OFDM: Which is Superior in Multiuser LEO Satellite Communications (Yu Liu et al., 2024)</a></li></ul></li><li><a href=#eesssy-4>eess.SY (4)</a><ul><li><a href=#14--283309-progressive-smoothing-for-motion-planning-in-real-time-nmpc-rudolf-reiter-et-al-2024>(1/4 | 283/309) Progressive Smoothing for Motion Planning in Real-Time NMPC (Rudolf Reiter et al., 2024)</a></li><li><a href=#24--284309-tuning-and-testing-an-online-feedback-optimization-controller-to-provide-curative-distribution-grid-flexibility-lukas-ortmann-et-al-2024>(2/4 | 284/309) Tuning and Testing an Online Feedback Optimization Controller to Provide Curative Distribution Grid Flexibility (Lukas Ortmann et al., 2024)</a></li><li><a href=#34--285309-cooperative-and-interaction-aware-driver-model-for-lane-change-maneuver-jemin-woo-et-al-2024>(3/4 | 285/309) Cooperative and Interaction-aware Driver Model for Lane Change Maneuver (Jemin Woo et al., 2024)</a></li><li><a href=#44--286309-a-frequency-domain-approach-for-enhanced-performance-and-task-flexibility-in-finite-time-ilc-max-van-haren-et-al-2024>(4/4 | 286/309) A Frequency-Domain Approach for Enhanced Performance and Task Flexibility in Finite-Time ILC (Max van Haren et al., 2024)</a></li></ul></li><li><a href=#csfl-1>cs.FL (1)</a><ul><li><a href=#11--287309-active-learning-of-mealy-machines-with-timers-véronique-bruyère-et-al-2024>(1/1 | 287/309) Active Learning of Mealy Machines with Timers (Véronique Bruyère et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--288309-magicclay-sculpting-meshes-with-generative-neural-fields-amir-barda-et-al-2024>(1/1 | 288/309) MagicClay: Sculpting Meshes With Generative Neural Fields (Amir Barda et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--289309-preserving-smart-grid-integrity-a-differential-privacy-framework-for-secure-detection-of-false-data-injection-attacks-in-the-smart-grid-nikhil-ravi-et-al-2024>(1/1 | 289/309) Preserving Smart Grid Integrity: A Differential Privacy Framework for Secure Detection of False Data Injection Attacks in the Smart Grid (Nikhil Ravi et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--290309-policy-space-response-oracles-a-survey-ariyan-bighashdel-et-al-2024>(1/1 | 290/309) Policy Space Response Oracles: A Survey (Ariyan Bighashdel et al., 2024)</a></li></ul></li><li><a href=#cslo-2>cs.LO (2)</a><ul><li><a href=#12--291309-unknown-biases-and-timing-constraints-in-timed-automata-darion-haase-et-al-2024>(1/2 | 291/309) Unknown Biases and Timing Constraints in Timed Automata (Darion Haase et al., 2024)</a></li><li><a href=#22--292309-deciding-separation-logic-with-pointer-arithmetic-and-inductive-definitions-wanyun-su-et-al-2024>(2/2 | 292/309) Deciding Separation Logic with Pointer Arithmetic and Inductive Definitions (Wanyun Su et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--293309-a-multi-agent-reinforcement-learning-study-of-evolution-of-communication-and-teaching-under-libertarian-and-utilitarian-governing-systems-aslan-s-dizaji-2024>(1/1 | 293/309) A Multi-agent Reinforcement Learning Study of Evolution of Communication and Teaching under Libertarian and Utilitarian Governing Systems (Aslan S. Dizaji, 2024)</a></li></ul></li><li><a href=#q-bioto-1>q-bio.TO (1)</a><ul><li><a href=#11--294309-embracing-uncertainty-flexibility-harnessing-a-supervised-tree-kernel-to-empower-ensemble-modelling-for-2d-echocardiography-based-prediction-of-right-ventricular-volume-tuan-a-bohoran-et-al-2024>(1/1 | 294/309) Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of Right Ventricular Volume (Tuan A. Bohoran et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--295309-tsallis-entropy-regularization-for-linearly-solvable-mdp-and-linear-quadratic-regulator-yota-hashizume-et-al-2024>(1/1 | 295/309) Tsallis Entropy Regularization for Linearly Solvable MDP and Linear Quadratic Regulator (Yota Hashizume et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--296309-rcoco-contrastive-collective-link-prediction-across-multiplex-network-in-riemannian-space-li-sun-et-al-2024>(1/1 | 296/309) RCoCo: Contrastive Collective Link Prediction across Multiplex Network in Riemannian Space (Li Sun et al., 2024)</a></li></ul></li><li><a href=#astro-phsr-1>astro-ph.SR (1)</a><ul><li><a href=#11--297309-forecasting-sep-events-during-solar-cycles-23-and-24-using-interpretable-machine-learning-spiridon-kasapis-et-al-2024>(1/1 | 297/309) Forecasting SEP Events During Solar Cycles 23 and 24 Using Interpretable Machine Learning (Spiridon Kasapis et al., 2024)</a></li></ul></li><li><a href=#econem-1>econ.EM (1)</a><ul><li><a href=#11--298309-applied-causal-inference-powered-by-ml-and-ai-victor-chernozhukov-et-al-2024>(1/1 | 298/309) Applied Causal Inference Powered by ML and AI (Victor Chernozhukov et al., 2024)</a></li></ul></li><li><a href=#mathco-3>math.CO (3)</a><ul><li><a href=#13--299309-minimum-acyclic-number-and-maximum-dichromatic-number-of-oriented-triangle-free-graphs-of-a-given-order-pierre-aboulker-et-al-2024>(1/3 | 299/309) Minimum acyclic number and maximum dichromatic number of oriented triangle-free graphs of a given order (Pierre Aboulker et al., 2024)</a></li><li><a href=#23--300309-characterization-of-chordal-circular-arc-graphs-i-split-graphs-yixin-cao-et-al-2024>(2/3 | 300/309) Characterization of Chordal Circular-arc Graphs: I. Split Graphs (Yixin Cao et al., 2024)</a></li><li><a href=#33--301309-weakly-modular-graphs-with-diamond-condition-the-interval-function-and-axiomatic-characterizations-lekshmi-kamal-kamalolbhavan-sheela-et-al-2024>(3/3 | 301/309) Weakly modular graphs with diamond condition, the interval function and axiomatic characterizations (Lekshmi Kamal Kamalolbhavan-Sheela et al., 2024)</a></li></ul></li><li><a href=#csdm-2>cs.DM (2)</a><ul><li><a href=#12--302309-payment-scheduling-in-the-interval-debt-model-tom-friedetzky-et-al-2024>(1/2 | 302/309) Payment Scheduling in the Interval Debt Model (Tom Friedetzky et al., 2024)</a></li><li><a href=#22--303309-graph-drawing-applications-in-combinatorial-theory-of-maturity-models-špela-kajzer-et-al-2024>(2/2 | 303/309) Graph drawing applications in combinatorial theory of maturity models (Špela Kajzer et al., 2024)</a></li></ul></li><li><a href=#csds-5>cs.DS (5)</a><ul><li><a href=#15--304309-matching-algorithms-in-the-sparse-stochastic-block-model-anna-brandenberger-et-al-2024>(1/5 | 304/309) Matching Algorithms in the Sparse Stochastic Block Model (Anna Brandenberger et al., 2024)</a></li><li><a href=#25--305309-random-generation-of-git-graphs-julien-courtiel-et-al-2024>(2/5 | 305/309) Random Generation of Git Graphs (Julien Courtiel et al., 2024)</a></li><li><a href=#35--306309-the-canadian-traveller-problem-on-outerplanar-graphs-laurent-beaudou-et-al-2024>(3/5 | 306/309) The Canadian Traveller Problem on outerplanar graphs (Laurent Beaudou et al., 2024)</a></li><li><a href=#45--307309-fully-polynomial-time-algorithms-parameterized-by-vertex-integrity-using-fast-matrix-multiplication-matthias-bentert-et-al-2024>(4/5 | 307/309) Fully Polynomial-time Algorithms Parameterized by Vertex Integrity Using Fast Matrix Multiplication (Matthias Bentert et al., 2024)</a></li><li><a href=#55--308309-unleashing-graph-partitioning-for-large-scale-nearest-neighbor-search-lars-gottesbüren-et-al-2024>(5/5 | 308/309) Unleashing Graph Partitioning for Large-Scale Nearest Neighbor Search (Lars Gottesbüren et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--309309-schema-based-query-optimisation-for-graph-databases-chandan-sharma-et-al-2024>(1/1 | 309/309) Schema-Based Query Optimisation for Graph Databases (Chandan Sharma et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>