<!doctype html><html><head><title>arXiv @ 2024.03.17</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.17"><meta property="og:description" content="Primary Categories astro-ph.IM (1) cond-mat.mtrl-sci (1) cs.AI (8) cs.CE (1) cs.CG (2) cs.CL (28) cs.CR (11) cs.CV (104) cs.CY (3) cs.DB (1) cs.DC (4) cs.DS (2) cs.GT (3) cs.HC (2) cs.IR (5) cs.IT (6) cs.LG (36) cs.LO (1) cs.MA (1) cs.NE (2) cs.NI (5) cs.RO (25) cs.SD (3) cs.SE (6) eess.IV (12) eess.SP (3) eess.SY (5) math.FA (1) math.NA (5) math.OC (1) physics.chem-ph (1) physics.comp-ph (1) physics.geo-ph (1) q-bio.QM (1) q-fin."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240317000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-17T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-17T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.17"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240317000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Sunday, Mar 17, 2024</p></div><div class=title><h1>arXiv @ 2024.03.17</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#csai-8>cs.AI (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#cscg-2>cs.CG (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#cscl-28>cs.CL (28)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#cscr-11>cs.CR (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#cscv-104>cs.CV (104)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#cscy-3>cs.CY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#csdc-4>cs.DC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#csgt-3>cs.GT (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#cshc-2>cs.HC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#csir-5>cs.IR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#csit-6>cs.IT (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#cslg-36>cs.LG (36)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#csni-5>cs.NI (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#csro-25>cs.RO (25)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#cssd-3>cs.SD (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#csse-6>cs.SE (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#eessiv-12>eess.IV (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#eesssp-3>eess.SP (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#eesssy-5>eess.SY (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#mathfa-1>math.FA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#mathna-5>math.NA (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#physicschem-ph-1>physics.chem-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#physicscomp-ph-1>physics.comp-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#physicsgeo-ph-1>physics.geo-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#q-fincp-1>q-fin.CP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#quant-ph-2>quant-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CR</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th><th>eess.IV</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td>2</td><td>3</td><td>3</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>1</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Aspect-based Sentiment Analysis</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td>1</td><td>6</td><td></td><td></td><td></td></tr><tr><td>BLOOM</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Bard</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>4</td><td>4</td><td>25</td><td>8</td><td>6</td><td>2</td></tr><tr><td>Black Box</td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Chain-of-thought Prompt</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td>1</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td></td><td>5</td><td></td><td></td><td></td></tr><tr><td>ControlNet</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Convolution</td><td>1</td><td></td><td>5</td><td>4</td><td>1</td><td>2</td></tr><tr><td>Convolutional Neural Network</td><td>1</td><td></td><td>13</td><td>6</td><td>2</td><td>3</td></tr><tr><td>Counter-factual</td><td></td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Curriculum Learning</td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Data Augmentation</td><td></td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>14</td><td></td><td></td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>4</td><td>10</td><td></td><td>2</td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Event Detection</td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Fact Verification</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Few-shot</td><td>1</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>7</td><td></td><td>12</td><td>2</td><td></td><td>2</td></tr><tr><td>Foundation Model</td><td></td><td></td><td>4</td><td>1</td><td>1</td><td></td></tr><tr><td>GPT</td><td>1</td><td>1</td><td>2</td><td></td><td></td><td></td></tr><tr><td>GPT-3</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Gemini</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>3</td><td></td><td></td><td>2</td></tr><tr><td>Geometry</td><td></td><td></td><td>8</td><td></td><td></td><td>1</td></tr><tr><td>Graph</td><td>2</td><td></td><td>4</td><td>8</td><td>1</td><td></td></tr><tr><td>Graph Anomaly Detection</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Graph Classification</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Convolutional Network</td><td>2</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td>1</td><td></td><td></td><td>5</td><td>2</td><td></td></tr><tr><td>Grounding</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>High-Resource</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Image2text</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>In-context Learning</td><td>2</td><td></td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Information Compression</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>2</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>1</td><td></td><td>5</td><td>2</td><td>1</td><td>3</td></tr><tr><td>Knowledge Graph</td><td></td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>LLaMA</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td>1</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>30</td><td>1</td><td>15</td><td>4</td><td>5</td><td></td></tr><tr><td>Logistic Regression</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Masked Language Model</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Model Compression</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Model Extraction</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>4</td><td></td><td>19</td><td>3</td><td>3</td><td>1</td></tr><tr><td>Mutual Information</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>6</td><td></td><td>1</td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>Out-of-domain</td><td></td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>PaLM</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Perplexity</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>5</td><td>1</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>4</td><td></td><td>8</td><td></td><td>3</td><td>2</td></tr><tr><td>Pruning</td><td></td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Question Answering</td><td>2</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>2</td><td></td><td>5</td><td>1</td><td>1</td><td></td></tr><tr><td>Recommendation</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Reinforcement Learning</td><td>2</td><td></td><td></td><td>6</td><td>6</td><td></td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Rerank</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>7</td><td>2</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>5</td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td>2</td><td></td><td></td><td>1</td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td>1</td><td>1</td><td>8</td><td>1</td></tr><tr><td>Simulator</td><td></td><td></td><td>1</td><td>1</td><td>8</td><td>1</td></tr><tr><td>Structured Bandit</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Style Transfer</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td>3</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td></td><td></td><td>9</td><td>3</td><td></td><td>1</td></tr><tr><td>Text Classification</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Mining</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Summarization</td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Textual Reinforcement Learning</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Topic Model</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Transformer</td><td>3</td><td></td><td>17</td><td>2</td><td>1</td><td>3</td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td>6</td><td>1</td><td></td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>10</td><td></td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td></td><td>8</td><td></td><td>3</td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>5</td><td></td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Word Embedding</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Yolo</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td></td><td></td><td>7</td><td>1</td><td>1</td><td>1</td></tr><tr><td>human-in-the-loop</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscv-104>cs.CV (104)</h2><h3 id=1104--1298-few-shot-image-classification-and-segmentation-as-visual-question-answering-using-vision-language-models-tian-meng-et-al-2024>(1/104 | 1/298) Few-Shot Image Classification and Segmentation as Visual Question Answering Using Vision-Language Models (Tian Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Meng, Yang Tao, Ruilin Lyu, Wuliang Yin. (2024)<br><strong>Few-Shot Image Classification and Segmentation as Visual Question Answering Using Vision-Language Models</strong><br><button class=copy-to-clipboard title="Few-Shot Image Classification and Segmentation as Visual Question Answering Using Vision-Language Models" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 100<br>Keywords: Yolo, Few-shot, Question Answering, Visual Question Answering, Visual Question Answering, Chain-of-thought Prompt, In-context Learning, In-context Learning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10287v1.pdf filename=2403.10287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of <b>few-shot</b> image classification and segmentation (FS-CS) involves classifying and segmenting target objects in a query image, given only a few examples of the target classes. We introduce the Vision-Instructed Segmentation and Evaluation (VISE) method that transforms the FS-CS problem into the <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> problem, utilising <b>Vision-Language</b> Models (VLMs), and addresses it in a training-free manner. By enabling a VLM to interact with off-the-shelf vision models as tools, the proposed method is capable of classifying and segmenting target objects using only image-level labels. Specifically, <b>chain-of-thought</b> <b>prompting</b> and <b>in-context</b> <b>learning</b> guide the VLM to answer multiple-choice <b>questions</b> <b>like</b> a human; vision models such as <b>YOLO</b> and Segment Anything Model (SAM) assist the VLM in completing the task. The modular framework of the proposed method makes it easily extendable. Our approach achieves state-of-the-art performance on the Pascal-5i and COCO-20i datasets.</p></p class="citation"></blockquote><h3 id=2104--2298-knowledge-condensation-and-reasoning-for-knowledge-based-vqa-dongze-hao-et-al-2024>(2/104 | 2/298) Knowledge Condensation and Reasoning for Knowledge-based VQA (Dongze Hao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongze Hao, Jian Jia, Longteng Guo, Qunbo Wang, Te Yang, Yan Li, Yanhua Cheng, Bo Wang, Quan Chen, Han Li, Jing Liu. (2024)<br><strong>Knowledge Condensation and Reasoning for Knowledge-based VQA</strong><br><button class=copy-to-clipboard title="Knowledge Condensation and Reasoning for Knowledge-based VQA" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 96<br>Keywords: Knowledge Distillation, Multi-modal, Multi-modal, GPT, GPT-3, Question Answering, Reasoning, Visual Question Answering, Visual Question Answering, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10037v1.pdf filename=2403.10037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge-based <b>visual</b> <b>question</b> <b>answering</b> (KB-VQA) is a challenging task, which requires the model to leverage external knowledge for comprehending and answering <b>questions</b> <b>grounded</b> in <b>visual</b> <b>content.</b> <b>Recent</b> studies retrieve the knowledge passages from external knowledge bases and then use them to answer <b>questions.</b> <b>However,</b> these retrieved knowledge passages often contain irrelevant or noisy information, which limits the performance of the model. To address the challenge, we propose two synergistic models: Knowledge Condensation model and Knowledge <b>Reasoning</b> model. We condense the retrieved knowledge passages from two perspectives. First, we leverage the <b>multimodal</b> perception and <b>reasoning</b> ability of the <b>visual-language</b> <b>models</b> <b>to</b> <b>distill</b> concise knowledge concepts from retrieved lengthy passages, ensuring relevance to both the <b>visual</b> <b>content</b> <b>and</b> the <b>question.</b> <b>Second,</b> we leverage the text comprehension ability of the <b>large</b> <b>language</b> <b>models</b> to <b>summarize</b> and condense the passages into the knowledge essence which helps answer the <b>question.</b> <b>These</b> two types of condensed knowledge are then seamlessly integrated into our Knowledge <b>Reasoning</b> model, which judiciously navigates through the amalgamated information to arrive at the conclusive answer. Extensive experiments validate the superiority of the proposed method. Compared to previous methods, our method achieves state-of-the-art performance on knowledge-based <b>VQA</b> datasets (65.1% on OK-VQA and 60.1% on A-OKVQA) without resorting to the knowledge produced by <b>GPT-3</b> (175B).</p></p class="citation"></blockquote><h3 id=3104--3298-a-survey-of-synthetic-data-augmentation-methods-in-computer-vision-alhassan-mumuni-et-al-2024>(3/104 | 3/298) A survey of synthetic data augmentation methods in computer vision (Alhassan Mumuni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alhassan Mumuni, Fuseini Mumuni, Nana Kobina Gerrar. (2024)<br><strong>A survey of synthetic data augmentation methods in computer vision</strong><br><button class=copy-to-clipboard title="A survey of synthetic data augmentation methods in computer vision" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 90<br>Keywords: Autoencoder, Convolution, Convolutional Neural Network, Convolutional Neural Network, Data Augmentation, Generative Adversarial Network, Generative Adversarial Network, Variational Autoencoder, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10075v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10075v2.pdf filename=2403.10075v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The standard approach to tackling computer vision problems is to train deep <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> models using large-scale image datasets which are representative of the target task. However, in many scenarios, it is often challenging to obtain sufficient image <b>data</b> <b>for</b> the target task. <b>Data</b> <b>augmentation</b> is a way to mitigate this challenge. A common practice is to explicitly transform existing images in desired ways so as to create the required volume and variability of training <b>data</b> <b>necessary</b> to achieve good generalization performance. In situations where <b>data</b> <b>for</b> the target domain is not accessible, a viable workaround is to synthesize training <b>data</b> <b>from</b> scratch&ndash;i.e., synthetic <b>data</b> <b>augmentation.</b> This paper presents an extensive review of synthetic <b>data</b> <b>augmentation</b> techniques. It covers <b>data</b> <b>synthesis</b> approaches based on realistic 3D graphics modeling, neural <b>style</b> <b>transfer</b> (NST), differential neural rendering, and <b>generative</b> <b>artificial</b> <b>intelligence</b> (AI) techniques such as <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs)</b> and <b>variational</b> <b>autoencoders</b> (VAEs). For each of these classes of methods, we focus on the important <b>data</b> <b>generation</b> and augmentation techniques, general scope of application and specific use-cases, as well as existing limitations and possible workarounds. Additionally, we provide a summary of common synthetic datasets for training computer vision models, highlighting the main features, application domains and supported tasks. Finally, we discuss the effectiveness of synthetic <b>data</b> <b>augmentation</b> methods. Since this is the first paper to explore synthetic <b>data</b> <b>augmentation</b> methods in great detail, we are hoping to equip readers with the necessary background information and in-depth knowledge of existing methods and their attendant issues.</p></p class="citation"></blockquote><h3 id=4104--4298-medslip-medical-dual-stream-language-image-pre-training-for-fine-grained-alignment-wenrui-fan-et-al-2024>(4/104 | 4/298) MeDSLIP: Medical Dual-Stream Language-Image Pre-training for Fine-grained Alignment (Wenrui Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenrui Fan, Mohammod Naimul Islam Suvon, Shuo Zhou, Xianyuan Liu, Samer Alabed, Venet Osmani, Andrew Swift, Chen Chen, Haiping Lu. (2024)<br><strong>MeDSLIP: Medical Dual-Stream Language-Image Pre-training for Fine-grained Alignment</strong><br><button class=copy-to-clipboard title="MeDSLIP: Medical Dual-Stream Language-Image Pre-training for Fine-grained Alignment" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 85<br>Keywords: Contrastive Learning, Convolutional Neural Network, Fine-tuning, Representation Learning, Supervised Learning, Zero-shot, Grounding, In-context Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10635v1.pdf filename=2403.10635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> pre-training (VLP) models have shown significant advancements in the medical domain. Yet, most VLP models align raw reports to images at a very coarse level, without modeling fine-grained relationships between anatomical and pathological concepts outlined in reports and the corresponding semantic counterparts in images. To address this problem, we propose a Medical Dual-Stream Language-Image Pre-training (MeDSLIP) framework. Specifically, MeDSLIP establishes <b>vision-language</b> fine-grained alignments via disentangling visual and textual <b>representations</b> <b>into</b> anatomy-relevant and pathology-relevant streams. Moreover, a novel <b>vision-language</b> Prototypical Contr-astive Learning (ProtoCL) method is adopted in MeDSLIP to enhance the alignment within the anatomical and pathological streams. MeDSLIP further employs cross-stream Intra-image <b>Contrastive</b> <b>Learning</b> <b>(ICL)</b> to ensure the consistent coexistence of paired anatomical and pathological concepts within the same image. Such a cross-stream regularization encourages the model to exploit the synchrony between two streams for a more comprehensive <b>representation</b> <b>learning.</b> MeDSLIP is evaluated under <b>zero-shot</b> and <b>supervised</b> <b>fine-tuning</b> settings on three public datasets: NIH CXR14, RSNA Pneumonia, and SIIM-ACR Pneumothorax. Under these settings, MeDSLIP outperforms six leading <b>CNN-based</b> models on classification, <b>grounding,</b> and segmentation tasks.</p></p class="citation"></blockquote><h3 id=5104--5298-improving-medical-multi-modal-contrastive-learning-with-expert-annotations-yogesh-kumar-et-al-2024>(5/104 | 5/298) Improving Medical Multi-modal Contrastive Learning with Expert Annotations (Yogesh Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yogesh Kumar, Pekka Marttinen. (2024)<br><strong>Improving Medical Multi-modal Contrastive Learning with Expert Annotations</strong><br><button class=copy-to-clipboard title="Improving Medical Multi-modal Contrastive Learning with Expert Annotations" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 73<br>Keywords: Contrastive Learning, Multi-modal, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Zero-shot, Large Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10153v1.pdf filename=2403.10153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in <b>contrastive</b> <b>multi-modal</b> medical imaging analysis, notably data scarcity and the &ldquo;modality gap&rdquo; &ndash; a significant disparity between image and <b>text</b> <b>embeddings</b> that diminishes the quality of representations and hampers cross-modal interoperability. eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model&rsquo;s learning effectiveness. eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture. Through detailed evaluations across several tasks, including <b>zero-shot</b> inference, linear probing, cross-modal <b>retrieval,</b> <b>and</b> <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)</b> of radiology reports using a frozen <b>Large</b> <b>Language</b> <b>Model,</b> eCLIP showcases consistent improvements in embedding quality. The outcomes reveal enhanced alignment and uniformity, affirming eCLIP&rsquo;s capability to harness high-quality annotations for enriched <b>multi-modal</b> analysis in the medical imaging domain.</p></p class="citation"></blockquote><h3 id=6104--6298-vitcn-vision-transformer-contrastive-network-for-reasoning-bo-song-et-al-2024>(6/104 | 6/298) ViTCN: Vision Transformer Contrastive Network For Reasoning (Bo Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Song, Yuanhao Xu, Yichao Wu. (2024)<br><strong>ViTCN: Vision Transformer Contrastive Network For Reasoning</strong><br><button class=copy-to-clipboard title="ViTCN: Vision Transformer Contrastive Network For Reasoning" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 73<br>Keywords: Vision Transformer, Benchmarking, GPT, Transformer, Reasoning, Large Language Model, Large Language Model, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09962v1.pdf filename=2403.09962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning models have achieved significant milestones in various domains, for example, computer <b>vision</b> <b>models</b> have an exceptional result in object recognition, and in natural language processing, where <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> like <b>GPT</b> can start a conversation with human-like proficiency. However, abstract <b>reasoning</b> remains a challenge for these models, Can AI really thinking like a human? still be a question yet to be answered. Raven Progressive Matrices (RPM) is a metric designed to assess human <b>reasoning</b> capabilities. It presents a series of eight images as a problem set, where the participant should try to discover the underlying rules among these images and select the most appropriate image from eight possible options that best completes the sequence. This task always be used to test human <b>reasoning</b> abilities and IQ. Zhang et al proposed a dataset called RAVEN which can be used to test Machine Learning model abstract <b>reasoning</b> ability. In this paper, we purposed <b>Vision</b> <b>Transformer</b> Contrastive Network which build on previous work with the Contrastive Perceptual Inference network (CoPiNet), which set a new <b>benchmark</b> for permutationinvariant models Raven Progressive Matrices by incorporating contrast effects from psychology, cognition, and education, and extends this foundation by leveraging the cutting-edge <b>Vision</b> <b>Transformer</b> architecture. This integration aims to further refine the machine ability to process and reason about spatial-temporal information from pixel-level inputs and global wise features on RAVEN dataset.</p></p class="citation"></blockquote><h3 id=7104--7298-on-the-low-shot-transferability-of-v-mamba-diganta-misra-et-al-2024>(7/104 | 7/298) On the low-shot transferability of [V]-Mamba (Diganta Misra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Diganta Misra, Jay Gala, Antonio Orvieto. (2024)<br><strong>On the low-shot transferability of [V]-Mamba</strong><br><button class=copy-to-clipboard title="On the low-shot transferability of [V]-Mamba" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 70<br>Keywords: Vision Transformer, Few-shot, Few-shot Learning, Transfer Learning, Transformer, Prompt, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10696v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10696v1.pdf filename=2403.10696v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The strength of modern large-scale neural networks lies in their ability to efficiently adapt to new tasks with few examples. Although extensive research has investigated the transferability of <b>Vision</b> <b>Transformers</b> (ViTs) to various downstream tasks under diverse constraints, this study shifts focus to explore the <b>transfer</b> <b>learning</b> potential of [V]-Mamba. We compare its performance with ViTs across different <b>few-shot</b> <b>data</b> budgets and efficient <b>transfer</b> <b>methods.</b> Our analysis yields three key insights into [V]-Mamba&rsquo;s <b>few-shot</b> <b>transfer</b> <b>performance:</b> (a) [V]-Mamba demonstrates superior or equivalent <b>few-shot</b> <b>learning</b> capabilities compared to ViTs when utilizing linear probing (LP) for <b>transfer,</b> <b>(b)</b> Conversely, [V]-Mamba exhibits weaker or similar <b>few-shot</b> <b>learning</b> performance compared to ViTs when employing visual <b>prompting</b> (VP) as the <b>transfer</b> <b>method,</b> and (c) We observe a weak positive correlation between the performance gap in <b>transfer</b> <b>via</b> LP and VP and the scale of the [V]-Mamba model. This preliminary analysis lays the foundation for more comprehensive studies aimed at furthering our understanding of the capabilities of [V]-Mamba variants and their distinctions from ViTs.</p></p class="citation"></blockquote><h3 id=8104--8298-mitigating-dialogue-hallucination-for-large-multi-modal-models-via-adversarial-instruction-tuning-dongmin-park-et-al-2024>(8/104 | 8/298) Mitigating Dialogue Hallucination for Large Multi-modal Models via Adversarial Instruction Tuning (Dongmin Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongmin Park, Zhaofang Qian, Guangxing Han, Ser-Nam Lim. (2024)<br><strong>Mitigating Dialogue Hallucination for Large Multi-modal Models via Adversarial Instruction Tuning</strong><br><button class=copy-to-clipboard title="Mitigating Dialogue Hallucination for Large Multi-modal Models via Adversarial Instruction Tuning" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, Zero-shot, Instruction Following, Visual Question Answering, Instruction Tuning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10492v1.pdf filename=2403.10492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mitigating hallucinations of Large <b>Multi-modal</b> Models(LMMs) is crucial to enhance their reliability for general-purpose assistants. This paper shows that such hallucinations of LMMs can be significantly exacerbated by preceding user-system dialogues. To precisely measure this, we first present an evaluation <b>benchmark</b> by extending popular <b>multi-modal</b> <b>benchmark</b> datasets with prepended hallucinatory dialogues generated by our novel <b>Adversarial</b> <b>Question</b> Generator, which can automatically generate image-related yet <b>adversarial</b> <b>dialogues</b> by adopting <b>adversarial</b> <b>attacks</b> on LMMs. On our <b>benchmark,</b> the <b>zero-shot</b> performance of state-of-the-art LMMs dropped significantly for both the <b>VQA</b> and Captioning tasks. Next, we further reveal this hallucination is mainly due to the prediction bias toward preceding dialogues rather than visual content. To reduce this bias, we propose <b>Adversarial</b> <b>Instruction</b> <b>Tuning</b> that robustly <b>fine-tunes</b> LMMs on augmented <b>multi-modal</b> <b>instruction-following</b> <b>datasets</b> with hallucinatory dialogues. Extensive experiments show that our proposed approach successfully reduces dialogue hallucination while maintaining or even improving performance.</p></p class="citation"></blockquote><h3 id=9104--9298-textblockv2-towards-precise-detection-free-scene-text-spotting-with-pre-trained-language-model-jiahao-lyu-et-al-2024>(9/104 | 9/298) TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model (Jiahao Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Lyu, Jin Wei, Gangyan Zeng, Zeng Li, Enze Xie, Wei Wang, Yu Zhou. (2024)<br><strong>TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model</strong><br><button class=copy-to-clipboard title="TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Optical Character Recognition, Benchmarking, Fine-tuning, Large Language Model, Large Language Model, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10047v1.pdf filename=2403.10047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing scene text spotters are designed to locate and transcribe texts from images. However, it is challenging for a spotter to achieve precise detection and recognition of scene texts simultaneously. Inspired by the glimpse-focus spotting pipeline of human beings and impressive performances of <b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> on visual tasks, we ask: 1) &ldquo;Can machines spot texts without precise detection just like human beings?&rdquo;, and if yes, 2) &ldquo;Is text block another alternative for scene text spotting other than word or character?&rdquo; To this end, our proposed scene text spotter leverages advanced <b>PLMs</b> to enhance performance without fine-grained detection. Specifically, we first use a simple detector for block-level text detection to obtain rough positional information. Then, we <b>finetune</b> a <b>PLM</b> using a <b>large-scale</b> <b>OCR</b> <b>dataset</b> to achieve accurate recognition. Benefiting from the comprehensive language knowledge gained during the pre-training phase, the <b>PLM-based</b> recognition module effectively handles complex scenarios, including multi-line, reversed, occluded, and incomplete-detection texts. Taking advantage of the <b>fine-tuned</b> language model on scene recognition <b>benchmarks</b> and the paradigm of text block detection, extensive experiments demonstrate the superior performance of our scene text spotter across multiple public <b>benchmarks.</b> Additionally, we attempt to spot texts directly from an entire scene image to demonstrate the potential of <b>PLMs,</b> even <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b></p></p class="citation"></blockquote><h3 id=10104--10298-videoagent-long-form-video-understanding-with-large-language-model-as-agent-xiaohan-wang-et-al-2024>(10/104 | 10/298) VideoAgent: Long-form Video Understanding with Large Language Model as Agent (Xiaohan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohan Wang, Yuhui Zhang, Orr Zohar, Serena Yeung-Levy. (2024)<br><strong>VideoAgent: Long-form Video Understanding with Large Language Model as Agent</strong><br><button class=copy-to-clipboard title="VideoAgent: Long-form Video Understanding with Large Language Model as Agent" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-IR, cs.CV<br>Keyword Score: 56<br>Keywords: Benchmarking, Foundation Model, Multi-modal, Zero-shot, Reasoning, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10517v1.pdf filename=2403.10517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of <b>reasoning</b> over long <b>multi-modal</b> sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive <b>reasoning</b> and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a <b>large</b> <b>language</b> <b>model</b> as a central agent to iteratively identify and compile crucial information to answer a question, with <b>vision-language</b> <b>foundation</b> <b>models</b> serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA <b>benchmarks,</b> VideoAgent achieves 54.1% and 71.3% <b>zero-shot</b> accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-based approaches in advancing long-form video understanding.</p></p class="citation"></blockquote><h3 id=11104--11298-real-world-computational-aberration-correction-via-quantized-domain-mixing-representation-qi-jiang-et-al-2024>(11/104 | 11/298) Real-World Computational Aberration Correction via Quantized Domain-Mixing Representation (Qi Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Jiang, Zhonghua Yi, Shaohua Gao, Yao Gao, Xiaolong Qian, Hao Shi, Lei Sun, Zhijie Xu, Kailun Yang, Kaiwei Wang. (2024)<br><strong>Real-World Computational Aberration Correction via Quantized Domain-Mixing Representation</strong><br><button class=copy-to-clipboard title="Real-World Computational Aberration Correction via Quantized Domain-Mixing Representation" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV, eess-IV, physics-optics<br>Keyword Score: 53<br>Keywords: Benchmarking, Quantization, Simulation, Simulator, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10012v1.pdf filename=2403.10012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Relying on paired synthetic data, existing learning-based Computational Aberration Correction (CAC) methods are confronted with the intricate and multifaceted synthetic-to-real <b>domain</b> <b>gap,</b> which leads to suboptimal performance in real-world applications. In this paper, in contrast to improving the <b>simulation</b> pipeline, we deliver a novel insight into real-world CAC from the perspective of <b>Unsupervised</b> <b>Domain</b> <b>Adaptation</b> (UDA). By incorporating readily accessible unpaired real-world data into training, we formalize the <b>Domain</b> <b>Adaptive</b> CAC (DACAC) task, and then introduce a comprehensive Real-world aberrated images (Realab) dataset to <b>benchmark</b> it. The setup task presents a formidable challenge due to the intricacy of understanding the target aberration <b>domain.</b> <b>To</b> this intent, we propose a novel Quntized <b>Domain-Mixing</b> <b>Representation</b> (QDMR) framework as a potent solution to the issue. QDMR adapts the CAC model to the target <b>domain</b> <b>from</b> three key aspects: (1) reconstructing aberrated images of both <b>domains</b> <b>by</b> a VQGAN to learn a <b>Domain-Mixing</b> <b>Codebook</b> (DMC) which characterizes the degradation-aware priors; (2) modulating the deep features in CAC model with DMC to transfer the target <b>domain</b> <b>knowledge;</b> and (3) leveraging the trained VQGAN to generate pseudo target aberrated images from the source ones for convincing target <b>domain</b> <b>supervision.</b> Extensive experiments on both synthetic and real-world <b>benchmarks</b> reveal that the models with QDMR consistently surpass the competitive methods in mitigating the synthetic-to-real gap, which produces visually pleasant real-world CAC results with fewer artifacts. Codes and datasets will be made publicly available.</p></p class="citation"></blockquote><h3 id=12104--12298-visual-foundation-models-boost-cross-modal-unsupervised-domain-adaptation-for-3d-semantic-segmentation-jingyi-xu-et-al-2024>(12/104 | 12/298) Visual Foundation Models Boost Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation (Jingyi Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyi Xu, Weidong Yang, Lingdong Kong, Youquan Liu, Rui Zhang, Qingyuan Zhou, Ben Fei. (2024)<br><strong>Visual Foundation Models Boost Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Visual Foundation Models Boost Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Foundation Model, Multi-modal, Supervised Learning, Unsupervised Learning, Image2text, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10001v1.pdf filename=2403.10001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> <b>domain</b> <b>adaptation</b> (UDA) is vital for alleviating the workload of labeling 3D point cloud data and mitigating the absence of labels when facing a newly defined <b>domain.</b> <b>Various</b> methods of utilizing images to enhance the performance of cross-domain 3D segmentation have recently emerged. However, the pseudo labels, which are generated from models trained on the source <b>domain</b> <b>and</b> provide additional <b>supervised</b> signals for the unseen <b>domain,</b> <b>are</b> inadequate when utilized for 3D segmentation due to their inherent noisiness and consequently restrict the accuracy of neural networks. With the advent of 2D visual <b>foundation</b> <b>models</b> (VFMs) and their abundant knowledge prior, we propose a novel pipeline VFMSeg to further enhance the cross-modal <b>unsupervised</b> <b>domain</b> <b>adaptation</b> framework by leveraging these models. In this work, we study how to harness the knowledge priors learned by VFMs to produce more accurate labels for unlabeled target <b>domains</b> <b>and</b> improve overall performance. We first utilize a <b>multi-modal</b> VFM, which is pre-trained on large scale <b>image-text</b> pairs, to provide <b>supervised</b> labels (VFM-PL) for images and point clouds from the target <b>domain.</b> <b>Then,</b> another VFM trained on fine-grained 2D masks is adopted to guide the generation of semantically augmented images and point clouds to enhance the performance of neural networks, which mix the data from source and target <b>domains</b> <b>like</b> view frustums (FrustumMixing). Finally, we merge class-wise prediction across modalities to produce more accurate annotations for unlabeled target <b>domains.</b> <b>Our</b> method is evaluated on various autonomous driving datasets and the results demonstrate a significant improvement for 3D segmentation task.</p></p class="citation"></blockquote><h3 id=13104--13298-multi-criteria-token-fusion-with-one-step-ahead-attention-for-efficient-vision-transformers-sanghyeok-lee-et-al-2024>(13/104 | 13/298) Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers (Sanghyeok Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanghyeok Lee, Joonmyung Choi, Hyunwoo J. Kim. (2024)<br><strong>Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers</strong><br><button class=copy-to-clipboard title="Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Pruning, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10030v1.pdf filename=2403.10030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision</b> <b>Transformer</b> (ViT) has emerged as a prominent backbone for computer <b>vision.</b> <b>For</b> more efficient ViTs, recent works lessen the quadratic cost of the <b>self-attention</b> layer by <b>pruning</b> or fusing the redundant tokens. However, these works faced the speed-accuracy trade-off caused by the loss of information. Here, we argue that token fusion needs to consider diverse relations between tokens to minimize information loss. In this paper, we propose a Multi-criteria Token Fusion (MCTF), that gradually fuses the tokens based on multi-criteria (e.g., similarity, informativeness, and size of fused tokens). Further, we utilize the one-step-ahead attention, which is the improved approach to capture the informativeness of the tokens. By training the model equipped with MCTF using a token reduction consistency, we achieve the best speed-accuracy trade-off in the image classification (ImageNet1K). Experimental results prove that MCTF consistently surpasses the previous reduction methods with and without training. Specifically, DeiT-T and DeiT-S with MCTF reduce FLOPs by about 44% while improving the performance (+0.5%, and +0.3%) over the base model, respectively. We also demonstrate the applicability of MCTF in various <b>Vision</b> <b>Transformers</b> (e.g., T2T-ViT, LV-ViT), achieving at least 31% speedup without performance degradation. Code is available at <a href=https://github.com/mlvlab/MCTF>https://github.com/mlvlab/MCTF</a>.</p></p class="citation"></blockquote><h3 id=14104--14298-st-ldm-a-universal-framework-for-text-grounded-object-generation-in-real-images-xiangtian-xue-et-al-2024>(14/104 | 14/298) ST-LDM: A Universal Framework for Text-Grounded Object Generation in Real Images (Xiangtian Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangtian Xue, Jiasong Wu, Youyong Kong, Lotfi Senhadji, Huazhong Shu. (2024)<br><strong>ST-LDM: A Universal Framework for Text-Grounded Object Generation in Real Images</strong><br><button class=copy-to-clipboard title="ST-LDM: A Universal Framework for Text-Grounded Object Generation in Real Images" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Diffusion Model, Autoencoder, Multi-modal, Multi-modal, Transformer, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10004v1.pdf filename=2403.10004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel image editing scenario termed Text-grounded Object Generation (TOG), defined as generating a new object in the real image spatially conditioned by textual descriptions. Existing <b>diffusion</b> <b>models</b> exhibit limitations of spatial perception in complex real-world scenes, relying on additional modalities to enforce constraints, and TOG imposes heightened challenges on scene comprehension under the <b>weak</b> <b>supervision</b> of linguistic information. We propose a universal framework ST-LDM based on Swin-Transformer, which can be integrated into any latent <b>diffusion</b> <b>model</b> with training-free backward guidance. ST-LDM encompasses a global-perceptual <b>autoencoder</b> with adaptable compression scales and hierarchical visual features, parallel with deformable <b>multimodal</b> <b>transformer</b> to generate region-wise guidance for the subsequent denoising process. We transcend the limitation of traditional attention mechanisms that only focus on existing visual features by introducing deformable feature alignment to hierarchically refine spatial positioning fused with multi-scale visual and linguistic information. Extensive Experiments demonstrate that our model enhances the localization of attention mechanisms while preserving the generative capabilities inherent to <b>diffusion</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=15104--15298-isotropic3d-image-to-3d-generation-based-on-a-single-clip-embedding-pengkun-liu-et-al-2024>(15/104 | 15/298) Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding (Pengkun Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengkun Liu, Yikai Wang, Fuchun Sun, Jiafang Li, Hang Xiao, Hongxiang Xue, Xinzhou Wang. (2024)<br><strong>Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding</strong><br><button class=copy-to-clipboard title="Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 45<br>Keywords: Diffusion Model, Fine-tuning, Fine-tuning, Geometry, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10395v1.pdf filename=2403.10395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Encouraged by the growing availability of pre-trained 2D <b>diffusion</b> <b>models,</b> image-to-3D generation by leveraging Score <b>Distillation</b> Sampling (SDS) is making remarkable progress. Most existing methods combine novel-view lifting from 2D <b>diffusion</b> <b>models</b> which usually take the reference image as a condition while applying hard L2 image supervision at the reference view. Yet heavily adhering to the image is prone to corrupting the inductive knowledge of the 2D <b>diffusion</b> <b>model</b> leading to flat or distorted 3D generation frequently. In this work, we reexamine image-to-3D in a novel perspective and present Isotropic3D, an image-to-3D generation pipeline that takes only an image CLIP embedding as input. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuth angle by solely resting on the SDS loss. The core of our framework lies in a two-stage <b>diffusion</b> <b>model</b> <b>fine-tuning.</b> Firstly, we <b>fine-tune</b> a text-to-3D <b>diffusion</b> <b>model</b> by substituting its text encoder with an image encoder, by which the model preliminarily acquires image-to-image capabilities. Secondly, we perform <b>fine-tuning</b> using our Explicit Multi-view Attention (EMA) which combines noisy multi-view images with the noise-free reference image as an explicit condition. CLIP embedding is sent to the <b>diffusion</b> <b>model</b> throughout the whole process while reference images are discarded once after <b>fine-tuning.</b> As a result, with a single image CLIP embedding, Isotropic3D is capable of generating multi-view mutually consistent images and also a 3D model with more symmetrical and neat content, well-proportioned <b>geometry,</b> rich colored texture, and less distortion compared with existing image-to-3D methods while still preserving the similarity to the reference image to a large extent. The project page is available at <a href=https://isotropic3d.github.io/>https://isotropic3d.github.io/</a>. The code and models are available at <a href=https://github.com/pkunliu/Isotropic3D>https://github.com/pkunliu/Isotropic3D</a>.</p></p class="citation"></blockquote><h3 id=16104--16298-pame-self-supervised-masked-autoencoder-for-no-reference-point-cloud-quality-assessment-ziyu-shan-et-al-2024>(16/104 | 16/298) PAME: Self-Supervised Masked Autoencoder for No-Reference Point Cloud Quality Assessment (Ziyu Shan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyu Shan, Yujie Zhang, Qi Yang, Haichen Yang, Yiling Xu, Shan Liu. (2024)<br><strong>PAME: Self-Supervised Masked Autoencoder for No-Reference Point Cloud Quality Assessment</strong><br><button class=copy-to-clipboard title="PAME: Self-Supervised Masked Autoencoder for No-Reference Point Cloud Quality Assessment" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 43<br>Keywords: Autoencoder, Benchmarking, Fine-tuning, Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10061v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10061v1.pdf filename=2403.10061v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>No-reference point cloud quality assessment (NR-PCQA) aims to automatically predict the perceptual quality of point clouds without reference, which has achieved remarkable performance due to the utilization of deep learning-based models. However, these data-driven models suffer from the scarcity of labeled data and perform unsatisfactorily in cross-dataset evaluations. To address this problem, we propose a <b>self-supervised</b> <b>pre-training</b> framework using masked <b>autoencoders</b> (PAME) to help the model learn useful representations without labels. Specifically, after projecting point clouds into images, our PAME employs dual-branch <b>autoencoders,</b> reconstructing masked patches from distorted images into the original patches within reference and distorted images. In this manner, the two branches can separately learn content-aware features and distortion-aware features from the projected images. Furthermore, in the model <b>fine-tuning</b> stage, the learned content-aware features serve as a guide to fuse the point cloud quality features extracted from different perspectives. Extensive experiments show that our method outperforms the state-of-the-art NR-PCQA methods on popular <b>benchmarks</b> in terms of prediction accuracy and generalizability.</p></p class="citation"></blockquote><h3 id=17104--17298-leveraging-synthetic-data-for-generalizable-and-fair-facial-action-unit-detection-liupei-lu-et-al-2024>(17/104 | 17/298) Leveraging Synthetic Data for Generalizable and Fair Facial Action Unit Detection (Liupei Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liupei Lu, Yufeng Yin, Yuming Gu, Yizhen Wu, Pratusha Prasad, Yajie Zhao, Mohammad Soleymani. (2024)<br><strong>Leveraging Synthetic Data for Generalizable and Fair Facial Action Unit Detection</strong><br><button class=copy-to-clipboard title="Leveraging Synthetic Data for Generalizable and Fair Facial Action Unit Detection" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fairness, Supervised Learning, Supervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10737v1.pdf filename=2403.10737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial action unit (AU) detection is a fundamental block for objective facial expression analysis. <b>Supervised</b> <b>learning</b> approaches require a large amount of manual labeling which is costly. The limited labeled data are also not diverse in terms of gender which can affect model <b>fairness.</b> In this paper, we propose to use synthetically generated data and multi-source <b>domain</b> <b>adaptation</b> (MSDA) to address the problems of the scarcity of labeled data and the diversity of subjects. Specifically, we propose to generate a diverse dataset through synthetic facial expression re-targeting by transferring the expressions from real faces to synthetic avatars. Then, we use MSDA to transfer the AU detection knowledge from a real dataset and the synthetic dataset to a target dataset. Instead of aligning the overall distributions of different <b>domains,</b> <b>we</b> propose Paired Moment Matching (PM2) to align the features of the paired real and synthetic data with the same facial expression. To further improve gender <b>fairness,</b> PM2 matches the features of the real data with a female and a male synthetic image. Our results indicate that synthetic data and the proposed model improve both AU detection performance and <b>fairness</b> across genders, demonstrating its potential to solve AU detection in-the-wild.</p></p class="citation"></blockquote><h3 id=18104--18298-approximate-nullspace-augmented-finetuning-for-robust-vision-transformers-haoyang-liu-et-al-2024>(18/104 | 18/298) Approximate Nullspace Augmented Finetuning for Robust Vision Transformers (Haoyang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyang Liu, Aditya Singh, Yijiang Li, Haohan Wang. (2024)<br><strong>Approximate Nullspace Augmented Finetuning for Robust Vision Transformers</strong><br><button class=copy-to-clipboard title="Approximate Nullspace Augmented Finetuning for Robust Vision Transformers" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Vision Transformer, Fine-tuning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10476v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10476v1.pdf filename=2403.10476v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enhancing the robustness of deep learning models, particularly in the realm of <b>vision</b> <b>transformers</b> (ViTs), is crucial for their real-world deployment. In this work, we provide a <b>finetuning</b> approach to enhance the robustness of <b>vision</b> <b>transformers</b> inspired by the concept of nullspace from linear algebra. Our investigation centers on whether a <b>vision</b> <b>transformer</b> can exhibit resilience to input variations akin to the nullspace property in linear mappings, implying that perturbations sampled from this nullspace do not influence the model&rsquo;s output when added to the input. Firstly, we show that for many pretrained ViTs, a non-trivial nullspace exists due to the presence of the patch embedding layer. Secondly, as nullspace is a concept associated with linear algebra, we demonstrate that it is possible to synthesize approximate nullspace elements for the non-linear blocks of ViTs employing an optimisation strategy. Finally, we propose a <b>fine-tuning</b> strategy for ViTs wherein we augment the training data with synthesized approximate nullspace noise. After <b>finetuning,</b> we find that the model demonstrates robustness to adversarial and natural image perbutations alike.</p></p class="citation"></blockquote><h3 id=19104--19298-deep-learning-for-multi-level-detection-and-localization-of-myocardial-scars-based-on-regional-strain-validated-on-virtual-patients-müjde-akdeniz-et-al-2024>(19/104 | 19/298) Deep Learning for Multi-Level Detection and Localization of Myocardial Scars Based on Regional Strain Validated on Virtual Patients (Müjde Akdeniz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Müjde Akdeniz, Claudia Alessandra Manetti, Tijmen Koopsen, Hani Nozari Mirar, Sten Roar Snare, Svein Arne Aase, Joost Lumens, Jurica Šprem, Kristin Sarah McLeod. (2024)<br><strong>Deep Learning for Multi-Level Detection and Localization of Myocardial Scars Based on Regional Strain Validated on Virtual Patients</strong><br><button class=copy-to-clipboard title="Deep Learning for Multi-Level Detection and Localization of Myocardial Scars Based on Regional Strain Validated on Virtual Patients" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10291v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10291v1.pdf filename=2403.10291v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How well the heart is functioning can be quantified through measurements of myocardial deformation via echocardiography. Clinical assessment of cardiac function is generally focused on global indices of relative shortening, however, territorial, and segmental strain indices have shown to be abnormal in regions of myocardial disease, such as scar. In this work, we propose a single framework to predict myocardial disease substrates at global, territorial, and segmental levels using regional myocardial strain traces as input to a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)-based</b> classification algorithm. An anatomically meaningful representation of the input data from the clinically standard bullseye representation to a multi-channel 2D image is proposed, to formulate the task as an image classification problem, thus enabling the use of state-of-the-art neural network configurations. A Fully <b>Convolutional</b> <b>Network</b> <b>(FCN)</b> is trained to detect and localize myocardial scar from regional left ventricular (LV) strain patterns. Simulated regional strain data from a controlled dataset of virtual patients with varying degrees and locations of myocardial scar is used for training and validation. The proposed method successfully detects and localizes the scars on 98% of the 5490 left ventricle (LV) segments of the 305 patients in the test set using strain traces only. Due to the sparse existence of scar, only 10% of the LV segments in the virtual patient cohort have scar. Taking the imbalance into account, the class balanced accuracy is calculated as 95%. The performance is reported on global, territorial, and segmental levels. The proposed method proves successful on the strain traces of the virtual cohort and offers the potential to solve the regional myocardial scar detection problem on the strain traces of the real patient cohorts.</p></p class="citation"></blockquote><h3 id=20104--20298-coleclip-open-domain-continual-learning-via-joint-task-prompt-and-vocabulary-learning-yukun-li-et-al-2024>(20/104 | 20/298) CoLeCLIP: Open-Domain Continual Learning via Joint Task Prompt and Vocabulary Learning (Yukun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yukun Li, Guansong Pang, Wei Suo, Chenchen Jing, Yuling Xi, Lingqiao Liu, Hao Chen, Guoqiang Liang, Peng Wang. (2024)<br><strong>CoLeCLIP: Open-Domain Continual Learning via Joint Task Prompt and Vocabulary Learning</strong><br><button class=copy-to-clipboard title="CoLeCLIP: Open-Domain Continual Learning via Joint Task Prompt and Vocabulary Learning" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Continual Learning, Zero-shot, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10245v1.pdf filename=2403.10245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the problem of <b>continual</b> <b>learning</b> (CL) of <b>vision-language</b> models (VLMs) in open domains, where the models need to perform <b>continual</b> <b>updating</b> and inference on a streaming of datasets from diverse seen and unseen domains with novel classes. Such a capability is crucial for various applications in open environments, e.g., AI assistants, autonomous driving systems, and robotics. Current CL studies mostly focus on closed-set scenarios in a single domain with known classes. Large pre-trained VLMs like CLIP have demonstrated superior <b>zero-shot</b> recognition ability, and a number of recent studies leverage this ability to mitigate catastrophic forgetting in CL, but they focus on closed-set CL in a single domain dataset. Open-domain CL of large VLMs is significantly more challenging due to 1) large class correlations and domain gaps across the datasets and 2) the forgetting of <b>zero-shot</b> knowledge in the pre-trained VLMs in addition to the knowledge learned from the newly adapted datasets. In this work we introduce a novel approach, termed CoLeCLIP, that learns an open-domain CL model based on CLIP. It addresses these challenges by a joint learning of a set of task <b>prompts</b> and a cross-domain class vocabulary. Extensive experiments on 11 domain datasets show that CoLeCLIP outperforms state-of-the-art methods for open-domain CL under both task- and class-incremental learning settings.</p></p class="citation"></blockquote><h3 id=21104--21298-enhancing-human-centered-dynamic-scene-understanding-via-multiple-llms-collaborated-reasoning-hang-zhang-et-al-2024>(21/104 | 21/298) Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning (Hang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Zhang, Wenxiao Zhang, Haoxuan Qu, Jun Liu. (2024)<br><strong>Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning</strong><br><button class=copy-to-clipboard title="Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs.CV<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10107v1.pdf filename=2403.10107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human-centered dynamic scene understanding plays a pivotal role in enhancing the capability of robotic and autonomous systems, in which Video-based Human-Object Interaction (V-HOI) detection is a crucial task in semantic scene understanding, aimed at comprehensively understanding HOI relationships within a video to benefit the behavioral decisions of mobile robots and autonomous driving systems. Although previous V-HOI detection models have made significant strides in accurate detection on specific datasets, they still lack the general <b>reasoning</b> ability like human beings to effectively induce HOI relationships. In this study, we propose V-HOI Multi-LLMs Collaborated <b>Reasoning</b> (V-HOI MLCR), a novel framework consisting of a series of plug-and-play modules that could facilitate the performance of current V-HOI detection models by leveraging the strong <b>reasoning</b> ability of different off-the-shelf pre-trained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We design a two-stage collaboration system of different <b>LLMs</b> for the V-HOI task. Specifically, in the first stage, we design a Cross-Agents <b>Reasoning</b> scheme to leverage the <b>LLM</b> conduct <b>reasoning</b> from different aspects. In the second stage, we perform Multi-LLMs Debate to get the final <b>reasoning</b> answer based on the different knowledge in different <b>LLMs.</b> Additionally, we devise an auxiliary training strategy that utilizes CLIP, a <b>large</b> <b>vision-language</b> <b>model</b> to enhance the base V-HOI models&rsquo; discriminative ability to better cooperate with <b>LLMs.</b> We validate the superiority of our design by demonstrating its effectiveness in improving the prediction accuracy of the base V-HOI model via <b>reasoning</b> from multiple perspectives.</p></p class="citation"></blockquote><h3 id=22104--22298-efficientvmamba-atrous-selective-scan-for-light-weight-visual-mamba-xiaohuan-pei-et-al-2024>(22/104 | 22/298) EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba (Xiaohuan Pei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohuan Pei, Tao Huang, Chang Xu. (2024)<br><strong>EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba</strong><br><button class=copy-to-clipboard title="EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Transformer, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09977v1.pdf filename=2403.09977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior efforts in light-weight model development mainly centered on <b>CNN</b> and <b>Transformer-based</b> designs yet faced persistent challenges. <b>CNNs</b> adept at local feature extraction compromise resolution while <b>Transformers</b> offer global reach but escalate computational demands $\mathcal{O}(N^2)$. This ongoing trade-off between accuracy and efficiency remains a significant hurdle. Recently, state space models (SSMs), such as Mamba, have shown outstanding performance and competitiveness in various tasks such as language modeling and computer vision, while reducing the time complexity of global <b>information</b> <b>extraction</b> to $\mathcal{O}(N)$. Inspired by this, this work proposes to explore the potential of visual state space models in light-weight model design and introduce a novel efficient model variant dubbed EfficientVMamba. Concretely, our EfficientVMamba integrates a atrous-based selective scan approach by efficient skip sampling, constituting building blocks designed to harness both global and local representational features. Additionally, we investigate the integration between SSM blocks and <b>convolutions,</b> and introduce an efficient visual state space block combined with an additional <b>convolution</b> branch, which further elevate the model performance. Experimental results show that, EfficientVMamba scales down the computational complexity while yields competitive results across a variety of vision tasks. For example, our EfficientVMamba-S with $1.3$G FLOPs improves Vim-Ti with $1.5$G FLOPs by a large margin of $5.6%$ accuracy on ImageNet. Code is available at: \url{https://github.com/TerryPei/EfficientVMamba}.</p></p class="citation"></blockquote><h3 id=23104--23298-leveraging-clip-for-inferring-sensitive-information-and-improving-model-fairness-miao-zhang-et-al-2024>(23/104 | 23/298) Leveraging CLIP for Inferring Sensitive Information and Improving Model Fairness (Miao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miao Zhang, Rumi Chunara. (2024)<br><strong>Leveraging CLIP for Inferring Sensitive Information and Improving Model Fairness</strong><br><button class=copy-to-clipboard title="Leveraging CLIP for Inferring Sensitive Information and Improving Model Fairness" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Benchmarking, Clustering, Fairness, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10624v1.pdf filename=2403.10624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Performance disparities across sub-populations are known to exist in deep learning-based vision recognition models, but previous work has largely addressed such <b>fairness</b> concerns assuming knowledge of sensitive attribute labels. To overcome this reliance, previous strategies have involved separate learning structures to expose and adjust for disparities. In this work, we explore a new paradigm that does not require sensitive attribute labels, and evades the need for extra training by leveraging the <b>vision-language</b> model, CLIP, as a rich knowledge source to infer sensitive information. We present sample <b>clustering</b> based on similarity derived from image and attribute-specified language embeddings and assess their correspondence to true attribute distribution. We train a target model by re-sampling and augmenting under-performed clusters. Extensive experiments on multiple <b>benchmark</b> bias datasets show clear <b>fairness</b> gains of the model over existing baselines, which indicate that CLIP can extract discriminative sensitive information <b>prompted</b> by language, and used to promote model <b>fairness.</b></p></p class="citation"></blockquote><h3 id=24104--24298-magic-tokens-select-diverse-tokens-for-multi-modal-object-re-identification-pingping-zhang-et-al-2024>(24/104 | 24/298) Magic Tokens: Select Diverse Tokens for Multi-modal Object Re-Identification (Pingping Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pingping Zhang, Yuhao Wang, Yang Liu, Zhengzheng Tu, Huchuan Lu. (2024)<br><strong>Magic Tokens: Select Diverse Tokens for Multi-modal Object Re-Identification</strong><br><button class=copy-to-clipboard title="Magic Tokens: Select Diverse Tokens for Multi-modal Object Re-Identification" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-IR, cs-MM, cs.CV<br>Keyword Score: 36<br>Keywords: Vision Transformer, Benchmarking, Multi-modal, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10254v1.pdf filename=2403.10254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single-modal object re-identification (ReID) faces great challenges in maintaining robustness within complex visual scenarios. In contrast, <b>multi-modal</b> object ReID utilizes complementary information from diverse modalities, showing great potentials for practical applications. However, previous methods may be easily affected by irrelevant backgrounds and usually ignore the modality gaps. To address above issues, we propose a novel learning framework named \textbf{EDITOR} to select diverse tokens from <b>vision</b> <b>Transformers</b> for <b>multi-modal</b> object ReID. We begin with a shared <b>vision</b> <b>Transformer</b> to extract tokenized features from different input modalities. Then, we introduce a Spatial-Frequency Token Selection (SFTS) module to adaptively select object-centric tokens with both spatial and frequency information. Afterwards, we employ a Hierarchical Masked Aggregation (HMA) module to facilitate feature interactions within and across modalities. Finally, to further reduce the effect of backgrounds, we propose a Background Consistency Constraint (BCC) and an Object-Centric Feature Refinement (OCFR). They are formulated as two new loss functions, which improve the feature discrimination with background suppression. As a result, our framework can generate more discriminative features for <b>multi-modal</b> object ReID. Extensive experiments on three <b>multi-modal</b> ReID <b>benchmarks</b> verify the effectiveness of our methods. The code is available at <a href=https://github.com/924973292/EDITOR>https://github.com/924973292/EDITOR</a>.</p></p class="citation"></blockquote><h3 id=25104--25298-controllable-text-to-3d-generation-via-surface-aligned-gaussian-splatting-zhiqi-li-et-al-2024>(25/104 | 25/298) Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting (Zhiqi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu. (2024)<br><strong>Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting</strong><br><button class=copy-to-clipboard title="Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: ControlNet, Diffusion Model, Geometry, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09981v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09981v2.pdf filename=2403.09981v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While text-to-3D and image-to-3D generation tasks have received considerable attention, one important but under-explored field between them is controllable text-to-3D generation, which we mainly focus on in this work. To address this task, 1) we introduce Multi-view <b>ControlNet</b> (MVControl), a novel neural network architecture designed to enhance existing pre-trained multi-view <b>diffusion</b> <b>models</b> by integrating additional input conditions, such as edge, depth, normal, and scribble maps. Our innovation lies in the introduction of a conditioning module that controls the base <b>diffusion</b> <b>model</b> using both local and global embeddings, which are computed from the input condition images and camera poses. Once trained, MVControl is able to offer 3D <b>diffusion</b> <b>guidance</b> for optimization-based 3D generation. And, 2) we propose an efficient multi-stage 3D generation pipeline that leverages the benefits of recent large reconstruction models and score <b>distillation</b> algorithm. Building upon our MVControl architecture, we employ a unique hybrid <b>diffusion</b> <b>guidance</b> method to direct the optimization process. In pursuit of efficiency, we adopt 3D Gaussians as our representation instead of the commonly used implicit representations. We also pioneer the use of SuGaR, a hybrid representation that binds Gaussians to mesh triangle faces. This approach alleviates the issue of poor <b>geometry</b> in 3D Gaussians and enables the direct sculpting of fine-grained <b>geometry</b> on the mesh. Extensive experiments demonstrate that our method achieves robust generalization and enables the controllable generation of high-quality 3D content.</p></p class="citation"></blockquote><h3 id=26104--26298-hawkeye-training-video-text-llms-for-grounding-text-in-videos-yueqian-wang-et-al-2024>(26/104 | 26/298) HawkEye: Training Video-Text LLMs for Grounding Text in Videos (Yueqian Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, Dongyan Zhao. (2024)<br><strong>HawkEye: Training Video-Text LLMs for Grounding Text in Videos</strong><br><button class=copy-to-clipboard title="HawkEye: Training Video-Text LLMs for Grounding Text in Videos" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Multi-modal, Grounding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10228v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10228v1.pdf filename=2403.10228v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video-text <b>Large</b> <b>Language</b> <b>Models</b> (video-text <b>LLMs)</b> have shown remarkable performance in answering questions and holding conversations on simple videos. However, they perform almost the same as random on <b>grounding</b> text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images. In this paper, we propose HawkEye, one of the first video-text <b>LLMs</b> that can perform temporal video <b>grounding</b> in a fully text-to-text manner. To collect training data that is applicable for temporal video <b>grounding,</b> we construct InternVid-G, a <b>large-scale</b> <b>video-text</b> <b>corpus</b> with segment-level captions and negative spans, with which we introduce two new time-aware training objectives to video-text <b>LLMs.</b> We also propose a coarse-grained method of representing segments in videos, which is more robust and easier for <b>LLMs</b> to learn and follow than other alternatives. Extensive experiments show that HawkEye is better at temporal video <b>grounding</b> and comparable on other video-text tasks with existing video-text <b>LLMs,</b> which verifies its superior video-text <b>multi-modal</b> understanding abilities.</p></p class="citation"></blockquote><h3 id=27104--27298-crossglg-llm-guides-one-shot-skeleton-based-3d-action-recognition-in-a-cross-level-manner-tingbing-yan-et-al-2024>(27/104 | 27/298) CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner (Tingbing Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tingbing Yan, Wenzheng Zeng, Yang Xiao, Xingyu Tong, Bo Tan, Zhiwen Fang, Zhiguo Cao, Joey Tianyi Zhou. (2024)<br><strong>CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner</strong><br><button class=copy-to-clipboard title="CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10082v1.pdf filename=2403.10082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most existing one-shot skeleton-based action recognition focuses on raw low-level information (e.g., joint location), and may suffer from local information loss and low generalization ability. To alleviate these, we propose to leverage text description generated from <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> that contain high-level human knowledge, to guide feature learning, in a global-local-global way. Particularly, during training, we design $2$ <b>prompts</b> to gain global and local text descriptions of each action from an <b>LLM.</b> We first utilize the global text description to guide the skeleton encoder focus on informative joints (i.e.,global-to-local). Then we build non-local interaction between local text and joint features, to form the final global representation (i.e., local-to-global). To mitigate the asymmetry issue between the training and inference phases, we further design a dual-branch architecture that allows the model to perform novel class inference without any text input, also making the additional inference cost neglectable compared with the base skeleton encoder. Extensive experiments on three different <b>benchmarks</b> show that CrossGLG consistently outperforms the existing SOTA methods with <b>large</b> <b>margins,</b> <b>and</b> the inference cost (model size) is only $2.8$% than the previous SOTA. CrossGLG can also serve as a plug-and-play module that can substantially enhance the performance of different SOTA skeleton encoders with a neglectable cost during inference. The source code will be released soon.</p></p class="citation"></blockquote><h3 id=28104--28298-real-time-image-segmentation-via-hybrid-convolutional-transformer-architecture-search-hongyuan-yu-et-al-2024>(28/104 | 28/298) Real-Time Image Segmentation via Hybrid Convolutional-Transformer Architecture Search (Hongyuan Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyuan Yu, Cheng Wan, Mengchen Liu, Dongdong Chen, Bin Xiao, Xiyang Dai. (2024)<br><strong>Real-Time Image Segmentation via Hybrid Convolutional-Transformer Architecture Search</strong><br><button class=copy-to-clipboard title="Real-Time Image Segmentation via Hybrid Convolutional-Transformer Architecture Search" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10413v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10413v1.pdf filename=2403.10413v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image segmentation is one of the most fundamental problems in computer vision and has drawn a lot of attentions due to its vast applications in image understanding and autonomous driving. However, designing effective and efficient segmentation neural architectures is a labor-intensive process that may require lots of trials by human experts. In this paper, we address the challenge of integrating multi-head <b>self-attention</b> into high resolution representation <b>CNNs</b> efficiently, by leveraging architecture search. Manually replacing <b>convolution</b> layers with multi-head <b>self-attention</b> is non-trivial due to the costly overhead in memory to maintain high resolution. By contrast, we develop a multi-target multi-branch supernet method, which not only fully utilizes the advantages of high-resolution features, but also finds the proper location for placing multi-head <b>self-attention</b> module. Our search algorithm is optimized towards multiple objective s (e.g., latency and mIoU) and capable of finding architectures on Pareto frontier with arbitrary number of branches in a single search. We further present a series of model via Hybrid <b>Convolutional-Transformer</b> Architecture Search (HyCTAS) method that searched for the best hybrid combination of light-weight <b>convolution</b> layers and memory-efficient <b>self-attention</b> layers between branches from different resolutions and fuse to high resolution for both efficiency and effectiveness. Extensive experiments demonstrate that HyCTAS outperforms previous methods on semantic segmentation task. Code and models are available at \url{https://github.com/MarvinYu1995/HyCTAS}.</p></p class="citation"></blockquote><h3 id=29104--29298-denoising-task-difficulty-based-curriculum-for-training-diffusion-models-jin-young-kim-et-al-2024>(29/104 | 29/298) Denoising Task Difficulty-based Curriculum for Training Diffusion Models (Jin-Young Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jin-Young Kim, Hyojun Go, Soonwoo Kwon, Hyun-Gyoon Kim. (2024)<br><strong>Denoising Task Difficulty-based Curriculum for Training Diffusion Models</strong><br><button class=copy-to-clipboard title="Denoising Task Difficulty-based Curriculum for Training Diffusion Models" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Curriculum Learning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10348v1.pdf filename=2403.10348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion-based</b> <b>generative</b> models have emerged as powerful tools in the realm of generative modeling. Despite extensive research on denoising across various timesteps and noise levels, a conflict persists regarding the relative difficulties of the denoising tasks. While various studies argue that lower timesteps present more challenging tasks, others contend that higher timesteps are more difficult. To address this conflict, our study undertakes a comprehensive examination of task difficulties, focusing on convergence behavior and changes in relative entropy between consecutive probability distributions across timesteps. Our observational study reveals that denoising at earlier timesteps poses challenges characterized by slower convergence and higher relative entropy, indicating increased task difficulty at these lower timesteps. Building on these observations, we introduce an easy-to-hard learning scheme, drawing from <b>curriculum</b> <b>learning,</b> to enhance the training process of <b>diffusion</b> <b>models.</b> By organizing timesteps or noise levels into clusters and training models with descending orders of difficulty, we facilitate an order-aware training regime, progressing from easier to harder denoising tasks, thereby deviating from the conventional approach of training <b>diffusion</b> <b>models</b> simultaneously across all timesteps. Our approach leads to improved performance and faster convergence by leveraging the benefits of <b>curriculum</b> <b>learning,</b> while maintaining orthogonality with existing improvements in <b>diffusion</b> <b>training</b> techniques. We validate these advantages through comprehensive experiments in image generation tasks, including unconditional, class-conditional, and <b>text-to-image</b> generation.</p></p class="citation"></blockquote><h3 id=30104--30298-how-powerful-potential-of-attention-on-image-restoration-cong-wang-et-al-2024>(30/104 | 30/298) How Powerful Potential of Attention on Image Restoration? (Cong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cong Wang, Jinshan Pan, Yeying Jin, Liyan Wang, Wei Wang, Gang Fu, Wenqi Ren, Xiaochun Cao. (2024)<br><strong>How Powerful Potential of Attention on Image Restoration?</strong><br><button class=copy-to-clipboard title="How Powerful Potential of Attention on Image Restoration?" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10336v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10336v1.pdf filename=2403.10336v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have demonstrated their effectiveness in image restoration tasks. Existing <b>Transformer</b> architectures typically comprise two essential components: multi-head <b>self-attention</b> and feed-forward network (FFN). The former captures long-range pixel dependencies, while the latter enables the model to learn complex patterns and relationships in the data. Previous studies have demonstrated that FFNs are key-value memories \cite{geva2020transformer}, which are vital in modern <b>Transformer</b> architectures. In this paper, we conduct an empirical study to explore the potential of attention mechanisms without using FFN and provide novel structures to demonstrate that removing FFN is flexible for image restoration. Specifically, we propose Continuous Scaling Attention (\textbf{CSAttn}), a method that computes attention continuously in three stages without using FFN. To achieve competitive performance, we propose a series of key components within the attention. Our designs provide a closer look at the attention mechanism and reveal that some simple operations can significantly affect the model performance. We apply our \textbf{CSAttn} to several image restoration tasks and show that our model can outperform <b>CNN-based</b> and <b>Transformer-based</b> image restoration approaches.</p></p class="citation"></blockquote><h3 id=31104--31298-generative-region-language-pretraining-for-open-ended-object-detection-chuang-lin-et-al-2024>(31/104 | 31/298) Generative Region-Language Pretraining for Open-Ended Object Detection (Chuang Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuang Lin, Yi Jiang, Lizhen Qu, Zehuan Yuan, Jianfei Cai. (2024)<br><strong>Generative Region-Language Pretraining for Open-Ended Object Detection</strong><br><button class=copy-to-clipboard title="Generative Region-Language Pretraining for Open-Ended Object Detection" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10191v1.pdf filename=2403.10191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent research, significant attention has been devoted to the open-vocabulary <b>object</b> <b>detection</b> task, aiming to generalize beyond the limited number of classes labeled during training and detect <b>objects</b> <b>described</b> by arbitrary category names at inference. Compared with conventional <b>object</b> <b>detection,</b> open vocabulary <b>object</b> <b>detection</b> largely extends the <b>object</b> <b>detection</b> categories. However, it relies on calculating the similarity between image regions and a set of arbitrary category names with a pretrained <b>vision-and-language</b> model. This implies that, despite its open-set nature, the task still needs the predefined <b>object</b> <b>categories</b> during the inference stage. This raises the question: What if we do not have exact knowledge of <b>object</b> <b>categories</b> during inference? In this paper, we call such a new setting as generative open-ended <b>object</b> <b>detection,</b> which is a more general and practical problem. To address it, we formulate <b>object</b> <b>detection</b> as a generative problem and propose a simple framework named GenerateU, which can detect dense <b>objects</b> <b>and</b> generate their names in a free-form way. Particularly, we employ Deformable DETR as a region proposal generator with a language model translating visual regions to <b>object</b> <b>names.</b> To assess the free-form <b>object</b> <b>detection</b> task, we introduce an evaluation method designed to quantitatively measure the performance of generative outcomes. Extensive experiments demonstrate strong <b>zero-shot</b> detection performance of our GenerateU. For example, on the LVIS dataset, our GenerateU achieves comparable results to the open-vocabulary <b>object</b> <b>detection</b> method GLIP, even though the category names are not seen by GenerateU during inference. Code is available at: https:// github.com/FoundationVision/GenerateU .</p></p class="citation"></blockquote><h3 id=32104--32298-diffmac-diffusion-manifold-hallucination-correction-for-high-generalization-blind-face-restoration-nan-gao-et-al-2024>(32/104 | 32/298) DiffMAC: Diffusion Manifold Hallucination Correction for High Generalization Blind Face Restoration (Nan Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nan Gao, Jia Li, Huaibo Huang, Zhi Zeng, Ke Shang, Shuwu Zhang, Ran He. (2024)<br><strong>DiffMAC: Diffusion Manifold Hallucination Correction for High Generalization Blind Face Restoration</strong><br><button class=copy-to-clipboard title="DiffMAC: Diffusion Manifold Hallucination Correction for High Generalization Blind Face Restoration" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Information Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10098v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10098v1.pdf filename=2403.10098v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Blind face restoration (BFR) is a highly challenging problem due to the uncertainty of degradation patterns. Current methods have low generalization across photorealistic and heterogeneous domains. In this paper, we propose a <b>Diffusion-Information-Diffusion</b> <b>(DID)</b> framework to tackle <b>diffusion</b> <b>manifold</b> hallucination correction (DiffMAC), which achieves high-generalization face restoration in diverse degraded scenes and heterogeneous domains. Specifically, the first <b>diffusion</b> <b>stage</b> aligns the restored face with spatial feature embedding of the low-quality face based on AdaIN, which synthesizes degradation-removal results but with uncontrollable artifacts for some hard cases. Based on Stage I, Stage II considers <b>information</b> <b>compression</b> using manifold <b>information</b> <b>bottleneck</b> (MIB) and <b>finetunes</b> the first <b>diffusion</b> <b>model</b> to improve facial fidelity. DiffMAC effectively fights against blind degradation patterns and synthesizes high-quality faces with attribute and identity consistencies. Experimental results demonstrate the superiority of DiffMAC over state-of-the-art methods, with a high degree of generalization in real-world and heterogeneous settings. The source code and models will be public.</p></p class="citation"></blockquote><h3 id=33104--33298-rangeldm-fast-realistic-lidar-point-cloud-generation-qianjiang-hu-et-al-2024>(33/104 | 33/298) RangeLDM: Fast Realistic LiDAR Point Cloud Generation (Qianjiang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianjiang Hu, Zhimin Zhang, Wei Hu. (2024)<br><strong>RangeLDM: Fast Realistic LiDAR Point Cloud Generation</strong><br><button class=copy-to-clipboard title="RangeLDM: Fast Realistic LiDAR Point Cloud Generation" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10094v1.pdf filename=2403.10094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous driving demands high-quality LiDAR data, yet the cost of physical LiDAR sensors presents a significant scaling-up challenge. While recent efforts have explored deep generative models to address this issue, they often consume substantial computational resources with slow generation speeds while suffering from a lack of realism. To address these limitations, we introduce RangeLDM, a novel approach for rapidly generating high-quality range-view LiDAR point clouds via latent <b>diffusion</b> <b>models.</b> We achieve this by correcting range-view data distribution for accurate projection from point clouds to range images via Hough voting, which has a critical impact on generative learning. We then compress the range images into a latent space with a <b>variational</b> <b>autoencoder,</b> and leverage a <b>diffusion</b> <b>model</b> to enhance expressivity. Additionally, we instruct the model to preserve 3D structural fidelity by devising a range-guided discriminator. Experimental results on KITTI-360 and nuScenes datasets demonstrate both the robust expressiveness and fast speed of our LiDAR point cloud generation.</p></p class="citation"></blockquote><h3 id=34104--34298-revisiting-adversarial-training-under-long-tailed-distributions-xinli-yue-et-al-2024>(34/104 | 34/298) Revisiting Adversarial Training under Long-Tailed Distributions (Xinli Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinli Yue, Ningping Mou, Qian Wang, Lingchen Zhao. (2024)<br><strong>Revisiting Adversarial Training under Long-Tailed Distributions</strong><br><button class=copy-to-clipboard title="Revisiting Adversarial Training under Long-Tailed Distributions" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Adversarial Learning, Data Augmentation, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10073v1.pdf filename=2403.10073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks are vulnerable to <b>adversarial</b> <b>attacks,</b> often leading to erroneous outputs. <b>Adversarial</b> <b>training</b> has been recognized as one of the most effective methods to counter such attacks. However, existing <b>adversarial</b> <b>training</b> techniques have predominantly been tested on balanced datasets, whereas real-world <b>data</b> <b>often</b> exhibit a long-tailed distribution, casting doubt on the efficacy of these methods in practical scenarios. In this paper, we delve into <b>adversarial</b> <b>training</b> under long-tailed distributions. Through an analysis of the previous work &ldquo;RoBal&rdquo;, we discover that utilizing Balanced Softmax Loss alone can achieve performance comparable to the complete RoBal approach while significantly reducing training overheads. Additionally, we reveal that, similar to uniform distributions, <b>adversarial</b> <b>training</b> under long-tailed distributions also suffers from robust overfitting. To address this, we explore <b>data</b> <b>augmentation</b> as a solution and unexpectedly discover that, unlike results obtained with balanced <b>data,</b> <b>data</b> <b>augmentation</b> not only effectively alleviates robust overfitting but also significantly improves robustness. We further investigate the reasons behind the improvement of robustness through <b>data</b> <b>augmentation</b> and identify that it is attributable to the increased diversity of examples. Extensive experiments further corroborate that <b>data</b> <b>augmentation</b> alone can significantly improve robustness. Finally, building on these findings, we demonstrate that compared to RoBal, the combination of BSL and <b>data</b> <b>augmentation</b> leads to a +6.66% improvement in model robustness under AutoAttack on CIFAR-10-LT. Our code is available at <a href=https://github.com/NISPLab/AT-BSL>https://github.com/NISPLab/AT-BSL</a> .</p></p class="citation"></blockquote><h3 id=35104--35298-rethinking-low-quality-optical-flow-in-unsupervised-surgical-instrument-segmentation-peiran-wu-et-al-2024>(35/104 | 35/298) Rethinking Low-quality Optical Flow in Unsupervised Surgical Instrument Segmentation (Peiran Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peiran Wu, Yang Liu, Jiayu Huo, Gongyu Zhang, Christos Bergeles, Rachel Sparks, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin. (2024)<br><strong>Rethinking Low-quality Optical Flow in Unsupervised Surgical Instrument Segmentation</strong><br><button class=copy-to-clipboard title="Rethinking Low-quality Optical Flow in Unsupervised Surgical Instrument Segmentation" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10039v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10039v1.pdf filename=2403.10039v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video-based surgical instrument segmentation plays an important role in robot-assisted surgeries. Unlike <b>supervised</b> settings, <b>unsupervised</b> segmentation relies heavily on motion cues, which are challenging to discern due to the typically lower quality of optical flow in surgical footage compared to natural scenes. This presents a considerable burden for the advancement of <b>unsupervised</b> segmentation techniques. In our work, we address the challenge of enhancing model performance despite the inherent limitations of low-quality optical flow. Our methodology employs a three-pronged approach: extracting boundaries directly from the optical flow, selectively discarding frames with inferior flow quality, and employing a <b>fine-tuning</b> process with variable frame rates. We thoroughly evaluate our strategy on the EndoVis2017 VOS dataset and Endovis2017 Challenge dataset, where our model demonstrates promising results, achieving a mean Intersection-over-Union (mIoU) of 0.75 and 0.72, respectively. Our findings suggest that our approach can greatly decrease the need for manual annotations in clinical environments and may facilitate the annotation process for new datasets. The code is available at <a href=https://github.com/wpr1018001/Rethinking-Low-quality-Optical-Flow.git>https://github.com/wpr1018001/Rethinking-Low-quality-Optical-Flow.git</a></p></p class="citation"></blockquote><h3 id=36104--36298-fbpt-a-fully-binary-point-transformer-zhixing-hou-et-al-2024>(36/104 | 36/298) FBPT: A Fully Binary Point Transformer (Zhixing Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhixing Hou, Yuzhang Shang, Yan Yan. (2024)<br><strong>FBPT: A Fully Binary Point Transformer</strong><br><button class=copy-to-clipboard title="FBPT: A Fully Binary Point Transformer" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09998v1.pdf filename=2403.09998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel Fully Binary Point Cloud <b>Transformer</b> (FBPT) model which has the potential to be widely applied and expanded in the fields of robotics and mobile devices. By compressing the weights and activations of a 32-bit full-precision network to 1-bit binary values, the proposed binary point cloud <b>Transformer</b> network significantly reduces the storage footprint and computational resource requirements of neural network models for point cloud processing tasks, compared to full-precision point cloud networks. However, achieving a fully binary point cloud <b>Transformer</b> network, where all parts except the modules specific to the task are binary, poses challenges and bottlenecks in quantizing the activations of Q, K, V and <b>self-attention</b> in the attention module, as they do not adhere to simple probability distributions and can vary with input data. Furthermore, in our network, the binary attention module undergoes a degradation of the <b>self-attention</b> module due to the uniform distribution that occurs after the softmax operation. The primary focus of this paper is on addressing the performance degradation issue caused by the use of binary point cloud <b>Transformer</b> modules. We propose a novel binarization mechanism called dynamic-static hybridization. Specifically, our approach combines static binarization of the overall network model with fine granularity dynamic binarization of data-sensitive components. Furthermore, we make use of a novel hierarchical training scheme to obtain the optimal model and binarization parameters. These above improvements allow the proposed binarization method to outperform binarization methods applied to <b>convolution</b> neural networks when used in point cloud <b>Transformer</b> structures. To demonstrate the superiority of our algorithm, we conducted experiments on two different tasks: point cloud classification and place recognition.</p></p class="citation"></blockquote><h3 id=37104--37298-interlude-interactions-between-labeled-and-unlabeled-data-to-enhance-semi-supervised-learning-zhe-huang-et-al-2024>(37/104 | 37/298) InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning (Zhe Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhe Huang, Xiaowei Yu, Dajiang Zhu, Michael C. Hughes. (2024)<br><strong>InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning</strong><br><button class=copy-to-clipboard title="InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 28<br>Keywords: Benchmarking, Representation Learning, Semi-Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10658v1.pdf filename=2403.10658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Semi-supervised</b> <b>learning</b> (SSL) seeks to enhance task performance by training on both labeled and unlabeled data. Mainstream SSL image classification methods mostly optimize a loss that additively combines a <b>supervised</b> classification objective with a regularization term derived solely from unlabeled data. This formulation neglects the potential for interaction between labeled and unlabeled images. In this paper, we introduce InterLUDE, a new approach to enhance SSL made of two parts that each benefit from labeled-unlabeled interaction. The first part, embedding fusion, interpolates between labeled and unlabeled embeddings to improve <b>representation</b> <b>learning.</b> The second part is a new loss, grounded in the principle of consistency regularization, that aims to minimize discrepancies in the model&rsquo;s predictions between labeled versus unlabeled inputs. Experiments on standard closed-set SSL <b>benchmarks</b> and a medical SSL task with an uncurated unlabeled set show clear benefits to our approach. On the STL-10 dataset with only 40 labels, InterLUDE achieves 3.2% error rate, while the best previous method reports 14.9%.</p></p class="citation"></blockquote><h3 id=38104--38298-joint-multimodal-transformer-for-dimensional-emotional-recognition-in-the-wild-paul-waligora-et-al-2024>(38/104 | 38/298) Joint Multimodal Transformer for Dimensional Emotional Recognition in the Wild (Paul Waligora et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Waligora, Osama Zeeshan, Haseeb Aslam, Soufiane Belharbi, Alessandro Lameiras Koerich, Marco Pedersoli, Simon Bacon, Eric Granger. (2024)<br><strong>Joint Multimodal Transformer for Dimensional Emotional Recognition in the Wild</strong><br><button class=copy-to-clipboard title="Joint Multimodal Transformer for Dimensional Emotional Recognition in the Wild" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-SD, cs.CV, eess-AS<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Transformer, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10488v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10488v1.pdf filename=2403.10488v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Audiovisual <b>emotion</b> <b>recognition</b> (ER) in videos has immense potential over unimodal performance. It effectively leverages the inter- and intra-modal dependencies between visual and auditory modalities. This work proposes a novel audio-visual <b>emotion</b> <b>recognition</b> system utilizing a joint <b>multimodal</b> <b>transformer</b> architecture with key-based cross-attention. This framework aims to exploit the complementary nature of audio and visual cues (facial expressions and vocal patterns) in videos, leading to superior performance compared to solely relying on a single modality. The proposed model leverages separate backbones for capturing intra-modal temporal dependencies within each modality (audio and visual). Subsequently, a joint <b>multimodal</b> <b>transformer</b> architecture integrates the individual modality embeddings, enabling the model to effectively capture inter-modal (between audio and visual) and intra-modal (within each modality) relationships. Extensive evaluations on the challenging Affwild2 dataset demonstrate that the proposed model significantly outperforms baseline and state-of-the-art methods in ER tasks.</p></p class="citation"></blockquote><h3 id=39104--39298-get-unlocking-the-multi-modal-potential-of-clip-for-generalized-category-discovery-enguang-wang-et-al-2024>(39/104 | 39/298) GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery (Enguang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enguang Wang, Zhimao Peng, Zhengyuan Xie, Xialei Liu, Ming-Ming Cheng. (2024)<br><strong>GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery</strong><br><button class=copy-to-clipboard title="GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Text Embedding, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09974v1.pdf filename=2403.09974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given unlabelled datasets containing both old and new categories, generalized category discovery (GCD) aims to accurately discover new classes while correctly classifying old classes, leveraging the class concepts learned from labeled samples. Current GCD methods only use a single visual modality of information, resulting in poor classification of visually similar classes. Though certain classes are visually confused, their <b>text</b> <b>information</b> might be distinct, motivating us to introduce <b>text</b> <b>information</b> into the GCD task. However, the lack of class names for unlabelled data makes it impractical to utilize <b>text</b> <b>information.</b> To tackle this challenging problem, in this paper, we propose a <b>Text</b> <b>Embedding</b> Synthesizer (TES) to generate pseudo <b>text</b> <b>embeddings</b> for unlabelled samples. Specifically, our TES leverages the property that CLIP can generate aligned <b>vision-language</b> features, converting visual embeddings into tokens of the CLIP&rsquo;s <b>text</b> <b>encoder</b> to generate pseudo <b>text</b> <b>embeddings.</b> Besides, we employ a dual-branch framework, through the joint learning and instance consistency of different modality branches, visual and semantic information mutually enhance each other, promoting the interaction and fusion of visual and <b>text</b> <b>embedding</b> space. Our method unlocks the <b>multi-modal</b> potentials of CLIP and outperforms the baseline methods by a large margin on all GCD <b>benchmarks,</b> achieving new state-of-the-art. The code will be released at \url{https://github.com/enguangW/GET}.</p></p class="citation"></blockquote><h3 id=40104--40298-t4p-test-time-training-of-trajectory-prediction-via-masked-autoencoder-and-actor-specific-token-memory-daehee-park-et-al-2024>(40/104 | 40/298) T4P: Test-Time Training of Trajectory Prediction via Masked Autoencoder and Actor-specific Token Memory (Daehee Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daehee Park, Jaeseok Jeong, Sung-Hoon Yoon, Jaewoo Jeong, Kuk-Jin Yoon. (2024)<br><strong>T4P: Test-Time Training of Trajectory Prediction via Masked Autoencoder and Actor-specific Token Memory</strong><br><button class=copy-to-clipboard title="T4P: Test-Time Training of Trajectory Prediction via Masked Autoencoder and Actor-specific Token Memory" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Autoencoder, Distribution Shift, Distribution Shift, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10052v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10052v1.pdf filename=2403.10052v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trajectory prediction is a challenging problem that requires considering interactions among multiple actors and the surrounding environment. While data-driven approaches have been used to address this complex problem, they suffer from unreliable predictions under <b>distribution</b> <b>shifts</b> during test time. Accordingly, several online learning methods have been proposed using regression loss from the ground truth of observed data leveraging the auto-labeling nature of trajectory prediction task. We mainly tackle the following two issues. First, previous works underfit and overfit as they only optimize the last layer of the motion decoder. To this end, we employ the masked <b>autoencoder</b> (MAE) for <b>representation</b> <b>learning</b> to encourage complex interaction modeling in shifted test <b>distribution</b> <b>for</b> updating deeper layers. Second, utilizing the sequential nature of driving data, we propose an actor-specific token memory that enables the test-time learning of actor-wise motion characteristics. Our proposed method has been validated across various challenging cross-dataset <b>distribution</b> <b>shift</b> scenarios including nuScenes, Lyft, Waymo, and Interaction. Our method surpasses the performance of existing state-of-the-art online learning methods in terms of both prediction accuracy and computational efficiency. The code is available at <a href=https://github.com/daeheepark/T4P>https://github.com/daeheepark/T4P</a>.</p></p class="citation"></blockquote><h3 id=41104--41298-local-positional-graphs-and-attentive-local-features-for-a-data-and-runtime-efficient-hierarchical-place-recognition-pipeline-fangming-yuan-et-al-2024>(41/104 | 41/298) Local positional graphs and attentive local features for a data and runtime-efficient hierarchical place recognition pipeline (Fangming Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangming Yuan, Stefan Schubert, Peter Protzel, Peer Neubert. (2024)<br><strong>Local positional graphs and attentive local features for a data and runtime-efficient hierarchical place recognition pipeline</strong><br><button class=copy-to-clipboard title="Local positional graphs and attentive local features for a data and runtime-efficient hierarchical place recognition pipeline" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Graph, Distribution Shift, Distribution Shift, Rerank<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10283v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10283v1.pdf filename=2403.10283v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale applications of Visual Place Recognition (VPR) require computationally efficient approaches. Further, a well-balanced combination of data-based and training-free approaches can decrease the required amount of training data and effort and can reduce the influence of <b>distribution</b> <b>shifts</b> between the training and application phases. This paper proposes a runtime and data-efficient hierarchical VPR pipeline that extends existing approaches and presents novel ideas. There are three main contributions: First, we propose Local Positional <b>Graphs</b> (LPG), a training-free and runtime-efficient approach to encode spatial context information of local image features. LPG can be combined with existing local feature detectors and descriptors and considerably improves the image-matching quality compared to existing techniques in our experiments. Second, we present Attentive Local SPED (ATLAS), an extension of our previous local features approach with an attention module that improves the feature quality while maintaining high data efficiency. The influence of the proposed modifications is evaluated in an extensive ablation study. Third, we present a hierarchical pipeline that exploits hyperdimensional computing to use the same local features as holistic HDC-descriptors for fast candidate selection and for candidate <b>reranking.</b> We combine all contributions in a runtime and data-efficient VPR pipeline that shows benefits over the state-of-the-art method Patch-NetVLAD on a large collection of standard place recognition datasets with 15$%$ better performance in VPR accuracy, 54$\times$ faster feature comparison speed, and 55$\times$ less descriptor storage occupancy, making our method promising for real-world high-performance large-scale VPR in changing environments. Code will be made available with publication of this paper.</p></p class="citation"></blockquote><h3 id=42104--42298-region-aware-distribution-contrast-a-novel-approach-to-multi-task-partially-supervised-learning-meixuan-li-et-al-2024>(42/104 | 42/298) Region-aware Distribution Contrast: A Novel Approach to Multi-Task Partially Supervised Learning (Meixuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meixuan Li, Tianyu Li, Guoqing Wang, Peng Wang, Yang Yang, Heng Tao Shen. (2024)<br><strong>Region-aware Distribution Contrast: A Novel Approach to Multi-Task Partially Supervised Learning</strong><br><button class=copy-to-clipboard title="Region-aware Distribution Contrast: A Novel Approach to Multi-Task Partially Supervised Learning" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10252v1.pdf filename=2403.10252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we address the intricate challenge of multi-task dense prediction, encompassing tasks such as semantic segmentation, depth estimation, and surface normal estimation, particularly when dealing with partially annotated data (MTPSL). The complexity arises from the absence of complete task labels for each training image. Given the inter-related nature of these pixel-wise dense tasks, our focus is on mining and capturing cross-task relationships. Existing solutions typically rely on learning global image representations for global cross-task image matching, imposing constraints that, unfortunately, sacrifice the finer structures within the images. Attempting local matching as a remedy faces hurdles due to the lack of precise region supervision, making local alignment a challenging endeavor. The introduction of Segment Anything Model (SAM) sheds light on addressing local alignment challenges by providing free and high-quality solutions for region detection. Leveraging SAM-detected regions, the subsequent challenge lies in aligning the representations within these regions. Diverging from conventional methods that directly learn a monolithic image representation, our proposal involves modeling region-wise representations using Gaussian Distributions. Aligning these distributions between corresponding regions from different tasks imparts higher flexibility and capacity to capture intra-region structures, accommodating a broader range of tasks. This innovative approach significantly enhances our ability to effectively capture cross-task relationships, resulting in improved overall performance in partially <b>supervised</b> <b>multi-task</b> dense prediction scenarios. Extensive experiments conducted on two widely used <b>benchmarks</b> underscore the superior effectiveness of our proposed method, showcasing state-of-the-art performance even when compared to fully <b>supervised</b> <b>methods.</b></p></p class="citation"></blockquote><h3 id=43104--43298-sparsefusion-efficient-sparse-multi-modal-fusion-framework-for-long-range-3d-perception-yiheng-li-et-al-2024>(43/104 | 43/298) SparseFusion: Efficient Sparse Multi-Modal Fusion Framework for Long-Range 3D Perception (Yiheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiheng Li, Hongyang Li, Zehao Huang, Hong Chang, Naiyan Wang. (2024)<br><strong>SparseFusion: Efficient Sparse Multi-Modal Fusion Framework for Long-Range 3D Perception</strong><br><button class=copy-to-clipboard title="SparseFusion: Efficient Sparse Multi-Modal Fusion Framework for Long-Range 3D Perception" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10036v1.pdf filename=2403.10036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> 3D <b>object</b> <b>detection</b> has exhibited significant progress in recent years. However, most existing methods can hardly scale to long-range scenarios due to their reliance on dense 3D features, which substantially escalate computational demands and memory usage. In this paper, we introduce SparseFusion, a novel <b>multi-modal</b> fusion framework fully built upon sparse 3D features to facilitate efficient long-range perception. The core of our method is the Sparse View <b>Transformer</b> module, which selectively lifts regions of interest in 2D image space into the unified 3D space. The proposed module introduces sparsity from both semantic and geometric aspects which only fill grids that foreground <b>objects</b> <b>potentially</b> reside in. Comprehensive experiments have verified the efficiency and effectiveness of our framework in long-range 3D perception. Remarkably, on the long-range Argoverse2 dataset, SparseFusion reduces memory footprint and accelerates the inference by about two times compared to dense detectors. It also achieves state-of-the-art performance with mAP of 41.2% and CDS of 32.1%. The versatility of SparseFusion is also validated in the temporal <b>object</b> <b>detection</b> task and 3D lane detection task. Codes will be released upon acceptance.</p></p class="citation"></blockquote><h3 id=44104--44298-giving-a-hand-to-diffusion-models-a-two-stage-approach-to-improving-conditional-human-image-generation-anton-pelykh-et-al-2024>(44/104 | 44/298) Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving Conditional Human Image Generation (Anton Pelykh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anton Pelykh, Ozge Mercanoglu Sincan, Richard Bowden. (2024)<br><strong>Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving Conditional Human Image Generation</strong><br><button class=copy-to-clipboard title="Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving Conditional Human Image Generation" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: ControlNet, Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10731v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10731v1.pdf filename=2403.10731v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have seen significant progress in human image generation, particularly with the advancements in <b>diffusion</b> <b>models.</b> However, existing <b>diffusion</b> <b>methods</b> encounter challenges when producing consistent hand anatomy and the generated images often lack precise control over the hand pose. To address this limitation, we introduce a novel approach to pose-conditioned human image generation, dividing the process into two stages: hand generation and subsequent body out-painting around the hands. We propose training the hand generator in a multi-task setting to produce both hand images and their corresponding segmentation masks, and employ the trained model in the first stage of generation. An adapted <b>ControlNet</b> model is then used in the second stage to outpaint the body around the generated hands, producing the final result. A novel blending technique is introduced to preserve the hand details during the second stage that combines the results of both stages in a coherent way. This involves sequential expansion of the out-painted region while fusing the latent representations, to ensure a seamless and cohesive synthesis of the final image. Experimental evaluations demonstrate the superiority of our proposed method over state-of-the-art techniques, in both pose accuracy and image quality, as validated on the HaGRID dataset. Our approach not only enhances the quality of the generated hands but also offers improved control over hand pose, advancing the capabilities of pose-conditioned human image generation. The source code of the proposed approach is available at <a href=https://github.com/apelykh/hand-to-diffusion>https://github.com/apelykh/hand-to-diffusion</a>.</p></p class="citation"></blockquote><h3 id=45104--45298-swinmtl-a-shared-architecture-for-simultaneous-depth-estimation-and-semantic-segmentation-from-monocular-camera-images-pardis-taghavi-et-al-2024>(45/104 | 45/298) SwinMTL: A Shared Architecture for Simultaneous Depth Estimation and Semantic Segmentation from Monocular Camera Images (Pardis Taghavi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pardis Taghavi, Reza Langari, Gaurav Pandey. (2024)<br><strong>SwinMTL: A Shared Architecture for Simultaneous Depth Estimation and Semantic Segmentation from Monocular Camera Images</strong><br><button class=copy-to-clipboard title="SwinMTL: A Shared Architecture for Simultaneous Depth Estimation and Semantic Segmentation from Monocular Camera Images" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10662v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10662v1.pdf filename=2403.10662v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research paper presents an innovative multi-task learning framework that allows concurrent depth estimation and semantic segmentation using a single camera. The proposed approach is based on a shared encoder-decoder architecture, which integrates various techniques to improve the accuracy of the depth estimation and semantic segmentation task without compromising computational efficiency. Additionally, the paper incorporates an <b>adversarial</b> <b>training</b> component, employing a Wasserstein <b>GAN</b> framework with a critic network, to refine model&rsquo;s predictions. The framework is thoroughly evaluated on two datasets - the outdoor Cityscapes dataset and the indoor NYU Depth V2 dataset - and it outperforms existing state-of-the-art methods in both segmentation and depth estimation tasks. We also conducted ablation studies to analyze the contributions of different components, including pre-training strategies, the inclusion of critics, the use of logarithmic depth scaling, and advanced image augmentations, to provide a better understanding of the proposed framework. The accompanying source code is accessible at \url{https://github.com/PardisTaghavi/SwinMTL}.</p></p class="citation"></blockquote><h3 id=46104--46298-frozen-feature-augmentation-for-few-shot-image-classification-andreas-bär-et-al-2024>(46/104 | 46/298) Frozen Feature Augmentation for Few-Shot Image Classification (Andreas Bär et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas Bär, Neil Houlsby, Mostafa Dehghani, Manoj Kumar. (2024)<br><strong>Frozen Feature Augmentation for Few-Shot Image Classification</strong><br><button class=copy-to-clipboard title="Frozen Feature Augmentation for Few-Shot Image Classification" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Data Augmentation, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10519v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10519v1.pdf filename=2403.10519v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training a linear classifier or lightweight model on top of pretrained vision model outputs, so-called &lsquo;frozen features&rsquo;, leads to impressive performance on a number of downstream <b>few-shot</b> tasks. Currently, frozen features are not modified during training. On the other hand, when networks are trained directly on images, <b>data</b> <b>augmentation</b> is a standard recipe that improves performance with no substantial overhead. In this paper, we conduct an extensive pilot study on <b>few-shot</b> image classification that explores applying <b>data</b> <b>augmentations</b> in the frozen feature space, dubbed &lsquo;frozen feature augmentation (FroFA)&rsquo;, covering twenty augmentations in total. Our study demonstrates that adopting a deceptively simple pointwise FroFA, such as brightness, can improve <b>few-shot</b> performance consistently across three network architectures, three large pretraining datasets, and eight transfer datasets.</p></p class="citation"></blockquote><h3 id=47104--47298-featup-a-model-agnostic-framework-for-features-at-any-resolution-stephanie-fu-et-al-2024>(47/104 | 47/298) FeatUp: A Model-Agnostic Framework for Features at Any Resolution (Stephanie Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephanie Fu, Mark Hamilton, Laura Brandt, Axel Feldman, Zhoutong Zhang, William T. Freeman. (2024)<br><strong>FeatUp: A Model-Agnostic Framework for Features at Any Resolution</strong><br><button class=copy-to-clipboard title="FeatUp: A Model-Agnostic Framework for Features at Any Resolution" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-IR, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Few-shot, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10516v1.pdf filename=2403.10516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or <b>few-shot</b> regime. However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas. In this work, we introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features. We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution. Both approaches use a multi-view consistency loss with deep analogies to NeRFs. Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-training. We show that FeatUp significantly outperforms other feature upsampling and image super-resolution approaches in class activation map generation, <b>transfer</b> <b>learning</b> for segmentation and depth prediction, and end-to-end training for semantic segmentation.</p></p class="citation"></blockquote><h3 id=48104--48298-using-an-llm-to-turn-sign-spottings-into-spoken-language-sentences-ozge-mercanoglu-sincan-et-al-2024>(48/104 | 48/298) Using an LLM to Turn Sign Spottings into Spoken Language Sentences (Ozge Mercanoglu Sincan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ozge Mercanoglu Sincan, Necati Cihan Camgoz, Richard Bowden. (2024)<br><strong>Using an LLM to Turn Sign Spottings into Spoken Language Sentences</strong><br><button class=copy-to-clipboard title="Using an LLM to Turn Sign Spottings into Spoken Language Sentences" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10434v1.pdf filename=2403.10434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sign Language Translation (SLT) is a challenging task that aims to generate spoken language sentences from sign language videos. In this paper, we introduce a hybrid SLT approach, Spotter+GPT, that utilizes a sign spotter and a pretrained <b>large</b> <b>language</b> <b>model</b> to improve SLT performance. Our method builds upon the strengths of both components. The videos are first processed by the spotter, which is trained on a linguistic sign language dataset, to identify individual signs. These spotted signs are then passed to the powerful language model, which transforms them into coherent and contextually appropriate spoken language sentences.</p></p class="citation"></blockquote><h3 id=49104--49298-pasta-towards-flexible-and-efficient-hdr-imaging-via-progressively-aggregated-spatio-temporal-aligment-xiaoning-liu-et-al-2024>(49/104 | 49/298) PASTA: Towards Flexible and Efficient HDR Imaging Via Progressively Aggregated Spatio-Temporal Aligment (Xiaoning Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoning Liu, Ao Li, Zongwei Wu, Yapeng Du, Le Zhang, Yulun Zhang, Radu Timofte, Ce Zhu. (2024)<br><strong>PASTA: Towards Flexible and Efficient HDR Imaging Via Progressively Aggregated Spatio-Temporal Aligment</strong><br><button class=copy-to-clipboard title="PASTA: Towards Flexible and Efficient HDR Imaging Via Progressively Aggregated Spatio-Temporal Aligment" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10376v1.pdf filename=2403.10376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Leveraging <b>Transformer</b> attention has led to great advancements in HDR deghosting. However, the intricate nature of <b>self-attention</b> introduces practical challenges, as existing state-of-the-art methods often demand high-end GPUs or exhibit slow inference speeds, especially for high-resolution images like 2K. Striking an optimal balance between performance and latency remains a critical concern. In response, this work presents PASTA, a novel Progressively Aggregated Spatio-Temporal Alignment framework for HDR deghosting. Our approach achieves effectiveness and efficiency by harnessing hierarchical representation during feature distanglement. Through the utilization of diverse granularities within the hierarchical structure, our method substantially boosts computational speed and optimizes the HDR imaging workflow. In addition, we explore within-scale feature modeling with local and global attention, gradually merging and refining them in a coarse-to-fine fashion. Experimental results showcase PASTA&rsquo;s superiority over current SOTA methods in both visual quality and performance metrics, accompanied by a substantial 3-fold (x3) increase in inference speed.</p></p class="citation"></blockquote><h3 id=50104--50298-context-semantic-quality-awareness-network-for-fine-grained-visual-categorization-qin-xu-et-al-2024>(50/104 | 50/298) Context-Semantic Quality Awareness Network for Fine-Grained Visual Categorization (Qin Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qin Xu, Sitong Li, Jiahui Wang, Bo Jiang, Jinhui Tang. (2024)<br><strong>Context-Semantic Quality Awareness Network for Fine-Grained Visual Categorization</strong><br><button class=copy-to-clipboard title="Context-Semantic Quality Awareness Network for Fine-Grained Visual Categorization" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10298v1.pdf filename=2403.10298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploring and mining subtle yet distinctive features between sub-categories with similar appearances is crucial for fine-grained visual categorization (FGVC). However, less effort has been devoted to assessing the quality of extracted visual representations. Intuitively, the network may struggle to capture discriminative features from low-quality samples, which leads to a significant decline in FGVC performance. To tackle this challenge, we propose a weakly <b>supervised</b> Context-Semantic Quality Awareness Network (CSQA-Net) for FGVC. In this network, to model the spatial contextual relationship between rich part descriptors and global semantics for capturing more discriminative details within the object, we design a novel multi-part and multi-scale cross-attention (MPMSCA) module. Before feeding to the MPMSCA module, the part navigator is developed to address the scale confusion problems and accurately identify the local distinctive regions. Furthermore, we propose a generic multi-level semantic quality evaluation module (MLSQE) to progressively supervise and enhance hierarchical semantics from different levels of the backbone network. Finally, context-aware features from MPMSCA and semantically enhanced features from MLSQE are fed into the corresponding quality probing classifiers to evaluate their quality in real-time, thus boosting the discriminability of feature representations. Comprehensive experiments on four popular and highly competitive FGVC datasets demonstrate the superiority of the proposed CSQA-Net in comparison with the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=51104--51298-blinddiff-empowering-degradation-modelling-in-diffusion-models-for-blind-image-super-resolution-feng-li-et-al-2024>(51/104 | 51/298) BlindDiff: Empowering Degradation Modelling in Diffusion Models for Blind Image Super-Resolution (Feng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Li, Yixuan Wu, Zichao Liang, Runmin Cong, Huihui Bai, Yao Zhao, Meng Wang. (2024)<br><strong>BlindDiff: Empowering Degradation Modelling in Diffusion Models for Blind Image Super-Resolution</strong><br><button class=copy-to-clipboard title="BlindDiff: Empowering Degradation Modelling in Diffusion Models for Blind Image Super-Resolution" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10211v1.pdf filename=2403.10211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> (DM) have achieved remarkable promise in image super-resolution (SR). However, most of them are tailored to solving non-blind inverse problems with fixed known degradation settings, limiting their adaptability to real-world applications that involve complex unknown degradations. In this work, we propose BlindDiff, a DM-based blind SR method to tackle the blind degradation settings in SISR. BlindDiff seamlessly integrates the MAP-based optimization into DMs, which constructs a joint distribution of the low-resolution (LR) observation, high-resolution (HR) data, and degradation kernels for the data and kernel priors, and solves the blind SR problem by unfolding MAP approach along with the reverse process. Unlike most DMs, BlindDiff firstly presents a modulated conditional <b>transformer</b> (MCFormer) that is pre-trained with noise and kernel constraints, further serving as a posterior sampler to provide both priors simultaneously. Then, we plug a simple yet effective kernel-aware gradient term between adjacent sampling iterations that guides the <b>diffusion</b> <b>model</b> to learn degradation consistency knowledge. This also enables to joint refine the degradation model as well as HR images by observing the previous denoised sample. With the MAP-based reverse <b>diffusion</b> <b>process,</b> we show that BlindDiff advocates alternate optimization for blur kernel estimation and HR image restoration in a mutual reinforcing manner. Experiments on both synthetic and real-world datasets show that BlindDiff achieves the state-of-the-art performance with significant model complexity reduction compared to recent DM-based methods. Code will be available at \url{https://github.com/lifengcs/BlindDiff}</p></p class="citation"></blockquote><h3 id=52104--52298-e4c-enhance-editability-for-text-based-image-editing-by-harnessing-efficient-clip-guidance-tianrui-huang-et-al-2024>(52/104 | 52/298) E4C: Enhance Editability for Text-Based Image Editing by Harnessing Efficient CLIP Guidance (Tianrui Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianrui Huang, Pu Cao, Lu Yang, Chun Liu, Mengjie Hu, Zhiwei Liu, Qing Song. (2024)<br><strong>E4C: Enhance Editability for Text-Based Image Editing by Harnessing Efficient CLIP Guidance</strong><br><button class=copy-to-clipboard title="E4C: Enhance Editability for Text-Based Image Editing by Harnessing Efficient CLIP Guidance" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10133v1.pdf filename=2403.10133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion-based image editing is a composite process of preserving the source image content and generating new content or applying modifications. While current editing approaches have made improvements under text guidance, most of them have only focused on preserving the information of the input image, disregarding the importance of editability and alignment to the target <b>prompt.</b> In this paper, we prioritize the editability by proposing a <b>zero-shot</b> image editing method, named \textbf{E}nhance \textbf{E}ditability for text-based image \textbf{E}diting via \textbf{E}fficient \textbf{C}LIP guidance (\textbf{E4C}), which only requires inference-stage optimization to explicitly enhance the edibility and text alignment. Specifically, we develop a unified dual-branch feature-sharing pipeline that enables the preservation of the structure or texture of the source image while allowing the other to be adapted based on the editing task. We further integrate CLIP guidance into our pipeline by utilizing our novel random-gateway optimization mechanism to efficiently enhance the semantic alignment with the target <b>prompt.</b> Comprehensive quantitative and qualitative experiments demonstrate that our method effectively resolves the text alignment issues prevalent in existing methods while maintaining the fidelity to the source image, and performs well across a wide range of editing tasks.</p></p class="citation"></blockquote><h3 id=53104--53298-translandseg-a-transfer-learning-approach-for-landslide-semantic-segmentation-based-on-vision-foundation-model-changhong-hou-et-al-2024>(53/104 | 53/298) TransLandSeg: A Transfer Learning Approach for Landslide Semantic Segmentation Based on Vision Foundation Model (Changhong Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changhong Hou, Junchuan Yu, Daqing Ge, Liu Yang, Laidian Xi, Yunxuan Pang, Yi Wen. (2024)<br><strong>TransLandSeg: A Transfer Learning Approach for Landslide Semantic Segmentation Based on Vision Foundation Model</strong><br><button class=copy-to-clipboard title="TransLandSeg: A Transfer Learning Approach for Landslide Semantic Segmentation Based on Vision Foundation Model" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Foundation Model, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10127v1.pdf filename=2403.10127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Landslides are one of the most destructive natural disasters in the world, posing a serious threat to human life and safety. The development of <b>foundation</b> <b>models</b> has provided a new research paradigm for large-scale landslide detection. The Segment Anything Model (SAM) has garnered widespread attention in the field of image segmentation. However, our experiment found that SAM performed poorly in the task of landslide segmentation. We propose TransLandSeg, which is a <b>transfer</b> <b>learning</b> approach for landslide semantic segmentation based on a vision <b>foundation</b> <b>model</b> (VFM). TransLandSeg outperforms traditional semantic segmentation models on both the Landslide4Sense dataset and the Bijie landslide dataset. Our proposed adaptive <b>transfer</b> <b>learning</b> (ATL) architecture enables the powerful segmentation capability of SAM to be transferred to landslide detection by training only 1.3% of the number of the parameters of SAM, which greatly improves the training efficiency of the model. Finally we also conducted ablation experiments on models with different ATL structures, concluded that the deployment location and residual connection of ATL play an important role in TransLandSeg accuracy improvement.</p></p class="citation"></blockquote><h3 id=54104--54298-codebook-transfer-with-part-of-speech-for-vector-quantized-image-modeling-baoquan-zhang-et-al-2024>(54/104 | 54/298) Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling (Baoquan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baoquan Zhang, Huaibin Wang, Luo Chuyao, Xutao Li, Liang Guotao, Yunming Ye, Xiaochen Qi, Yao He. (2024)<br><strong>Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling</strong><br><button class=copy-to-clipboard title="Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Quantization, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10071v1.pdf filename=2403.10071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vector-Quantized Image Modeling (VQIM) is a fundamental research problem in image synthesis, which aims to represent an image with a discrete token sequence. Existing studies effectively address this problem by learning a discrete codebook from scratch and in a code-independent manner to <b>quantize</b> continuous representations into discrete tokens. However, learning a codebook from scratch and in a code-independent manner is highly challenging, which may be a key reason causing codebook collapse, i.e., some code vectors can rarely be optimized without regard to the relationship between codes and good codebook priors such that die off finally. In this paper, inspired by <b>pretrained</b> <b>language</b> <b>models,</b> we find that these language models have actually <b>pretrained</b> <b>a</b> <b>superior</b> codebook via a large number of text corpus, but such information is rarely exploited in VQIM. To this end, we propose a novel codebook transfer framework with part-of-speech, called VQCT, which aims to transfer a well-trained codebook from <b>pretrained</b> <b>language</b> <b>models</b> to VQIM for robust codebook learning. Specifically, we first introduce a <b>pretrained</b> <b>codebook</b> <b>from</b> language models and part-of-speech knowledge as priors. Then, we construct a vision-related codebook with these priors for achieving codebook transfer. Finally, a novel codebook transfer network is designed to exploit abundant semantic relationships between codes contained in <b>pretrained</b> <b>codebooks</b> <b>for</b> robust VQIM codebook learning. Experimental results on four datasets show that our VQCT method achieves superior VQIM performance over previous state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=55104--55298-boundary-matters-a-bi-level-active-finetuning-framework-han-lu-et-al-2024>(55/104 | 55/298) Boundary Matters: A Bi-Level Active Finetuning Framework (Han Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Lu, Yichen Xie, Xiaokang Yang, Junchi Yan. (2024)<br><strong>Boundary Matters: A Bi-Level Active Finetuning Framework</strong><br><button class=copy-to-clipboard title="Boundary Matters: A Bi-Level Active Finetuning Framework" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Active Learning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10069v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10069v1.pdf filename=2403.10069v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pretraining-finetuning paradigm has gained widespread adoption in vision tasks and other fields, yet it faces the significant challenge of high sample annotation costs. To mitigate this, the concept of <b>active</b> <b>finetuning</b> has emerged, aiming to select the most appropriate samples for model <b>finetuning</b> within a limited budget. Traditional <b>active</b> <b>learning</b> methods often struggle in this setting due to their inherent bias in batch selection. Furthermore, the recent <b>active</b> <b>finetuning</b> approach has primarily concentrated on aligning the distribution of selected subsets with the overall data pool, focusing solely on diversity. In this paper, we propose a Bi-Level <b>Active</b> <b>Finetuning</b> framework to select the samples for annotation in one shot, which includes two stages: core sample selection for diversity, and boundary sample selection for uncertainty. The process begins with the identification of pseudo-class centers, followed by an innovative denoising method and an iterative strategy for boundary sample selection in the high-dimensional feature space, all without relying on ground-truth labels. Our comprehensive experiments provide both qualitative and quantitative evidence of our method&rsquo;s efficacy, outperforming all the existing baselines.</p></p class="citation"></blockquote><h3 id=56104--56298-what-makes-good-collaborative-views-contrastive-mutual-information-maximization-for-multi-agent-perception-wanfang-su-et-al-2024>(56/104 | 56/298) What Makes Good Collaborative Views? Contrastive Mutual Information Maximization for Multi-Agent Perception (Wanfang Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanfang Su, Lixing Chen, Yang Bai, Xi Lin, Gaolei Li, Zhe Qu, Pan Zhou. (2024)<br><strong>What Makes Good Collaborative Views? Contrastive Mutual Information Maximization for Multi-Agent Perception</strong><br><button class=copy-to-clipboard title="What Makes Good Collaborative Views? Contrastive Mutual Information Maximization for Multi-Agent Perception" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MA, cs.CV<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10068v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10068v1.pdf filename=2403.10068v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-agent perception (MAP) allows autonomous systems to understand complex environments by interpreting data from multiple sources. This paper investigates intermediate collaboration for MAP with a specific focus on exploring &ldquo;good&rdquo; properties of collaborative view (i.e., post-collaboration feature) and its underlying relationship to individual views (i.e., pre-collaboration features), which were treated as an opaque procedure by most existing works. We propose a novel framework named CMiMC <b>(Contrastive</b> <b>Mutual</b> <b>Information</b> Maximization for Collaborative Perception) for intermediate collaboration. The core philosophy of CMiMC is to preserve discriminative information of individual views in the collaborative view by maximizing <b>mutual</b> <b>information</b> between pre- and post-collaboration features while enhancing the efficacy of collaborative views by minimizing the loss function of downstream tasks. In particular, we define multi-view <b>mutual</b> <b>information</b> (MVMI) for intermediate collaboration that evaluates correlations between collaborative views and individual views on both global and local scales. We establish CMiMNet based on multi-view <b>contrastive</b> <b>learning</b> to realize estimation and maximization of MVMI, which assists the training of a collaboration encoder for voxel-level feature fusion. We evaluate CMiMC on V2X-Sim 1.0, and it improves the SOTA average precision by 3.08% and 4.44% at 0.5 and 0.7 IoU (Intersection-over-Union) thresholds, respectively. In addition, CMiMC can reduce communication volume to 1/32 while achieving performance comparable to SOTA. Code and Appendix are released at <a href=https://github.com/77SWF/CMiMC>https://github.com/77SWF/CMiMC</a>.</p></p class="citation"></blockquote><h3 id=57104--57298-group-mix-sam-lightweight-solution-for-industrial-assembly-line-applications-wu-liang-et-al-2024>(57/104 | 57/298) Group-Mix SAM: Lightweight Solution for Industrial Assembly Line Applications (Wu Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wu Liang, X. -G. Ma. (2024)<br><strong>Group-Mix SAM: Lightweight Solution for Industrial Assembly Line Applications</strong><br><button class=copy-to-clipboard title="Group-Mix SAM: Lightweight Solution for Industrial Assembly Line Applications" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10053v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10053v1.pdf filename=2403.10053v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since the advent of the Segment Anything Model(SAM) approximately one year ago, it has engendered significant academic interest and has spawned a large number of investigations and publications from various perspectives. However, the deployment of SAM in practical assembly line scenarios has yet to materialize due to its large image encoder, which weighs in at an imposing 632M. In this study, we have replaced the heavyweight image encoder with a lightweight one, thereby enabling the deployment of SAM in practical assembly line scenarios. Specifically, we have employed decoupled <b>distillation</b> to train the encoder of MobileSAM in a resource-limited setting. The entire <b>knowledge</b> <b>distillation</b> experiment can be completed in a single day on a single RTX 4090. The resulting lightweight SAM, referred to as Group-Mix SAM, had 37.63% (2.16M) fewer parameters and 42.5% (15614.7M) fewer floating-point operations compared to MobileSAM. However, on our constructed industrial dataset, MALSD, its mIoU was only marginally lower than that of MobileSAM, at 0.615. Finally, we conducted a comprehensive comparative experiment to demonstrate the superiority of Group-Mix SAM in the industrial domain. With its exceptional performance, our Group-Mix SAM is more suitable for practical assembly line applications.</p></p class="citation"></blockquote><h3 id=58104--58298-trg-net-an-interpretable-and-controllable-rain-generator-zhiqiang-pang-et-al-2024>(58/104 | 58/298) TRG-Net: An Interpretable and Controllable Rain Generator (Zhiqiang Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiqiang Pang, Hong Wang, Qi Xie, Deyu Meng, Zongben Xu. (2024)<br><strong>TRG-Net: An Interpretable and Controllable Rain Generator</strong><br><button class=copy-to-clipboard title="TRG-Net: An Interpretable and Controllable Rain Generator" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Data Augmentation, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09993v1.pdf filename=2403.09993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploring and modeling rain generation mechanism is critical for augmenting paired <b>data</b> <b>to</b> ease training of rainy image processing models. Against this task, this study proposes a novel deep learning based rain generator, which fully takes the physical generation mechanism underlying rains into consideration and well encodes the learning of the fundamental rain factors (i.e., shape, orientation, length, width and sparsity) explicitly into the deep network. Its significance lies in that the generator not only elaborately design essential elements of the rain to simulate expected rains, like conventional artificial strategies, but also finely adapt to complicated and diverse practical rainy images, like deep learning methods. By rationally adopting filter parameterization technique, we first time achieve a deep network that is finely controllable with respect to rain factors and able to learn the distribution of these factors purely from <b>data.</b> <b>Our</b> unpaired generation experiments demonstrate that the rain generated by the proposed rain generator is not only of higher quality, but also more effective for deraining and downstream tasks compared to current state-of-the-art rain generation methods. Besides, the paired <b>data</b> <b>augmentation</b> experiments, including both in-distribution and <b>out-of-distribution</b> (OOD), further validate the diversity of samples generated by our model for in-distribution deraining and OOD generalization tasks.</p></p class="citation"></blockquote><h3 id=59104--59298-radclip-enhancing-radiologic-image-analysis-through-contrastive-language-image-pre-training-zhixiu-lu-et-al-2024>(59/104 | 59/298) RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training (Zhixiu Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhixiu Lu, Hailong Li, Lili He. (2024)<br><strong>RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training</strong><br><button class=copy-to-clipboard title="RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Foundation Model, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09948v1.pdf filename=2403.09948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of artificial intelligence (AI) with radiology has marked a transformative era in medical diagnostics. Vision <b>foundation</b> <b>models</b> have been adopted to enhance radiologic imaging analysis. However, the distinct complexities of radiological imaging, including the interpretation of 2D and 3D radiological data, pose unique challenges that existing models, trained on general non-medical images, fail to address adequately. To bridge this gap and capitalize on the diagnostic precision required in medical imaging, we introduce RadCLIP: a pioneering cross-modal <b>foundational</b> <b>model</b> that harnesses Contrastive Language-Image Pre-training (CLIP) to refine radiologic image analysis. RadCLIP incorporates a novel 3D slice pooling mechanism tailored for volumetric image analysis and is trained using a comprehensive and diverse dataset of radiologic <b>image-text</b> pairs. Our evaluations demonstrate that RadCLIP effectively aligns radiological images with their corresponding textual annotations, and in the meantime, offers a robust vision backbone for radiologic imagery with significant promise.</p></p class="citation"></blockquote><h3 id=60104--60298-quantization-effects-on-neural-networks-perception-how-would-quantization-change-the-perceptual-field-of-vision-models-mohamed-amine-kerkouri-et-al-2024>(60/104 | 60/298) Quantization Effects on Neural Networks Perception: How would quantization change the perceptual field of vision models? (Mohamed Amine Kerkouri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Amine Kerkouri, Marouane Tliba, Aladine Chetouani, Alessandro Bruno. (2024)<br><strong>Quantization Effects on Neural Networks Perception: How would quantization change the perceptual field of vision models?</strong><br><button class=copy-to-clipboard title="Quantization Effects on Neural Networks Perception: How would quantization change the perceptual field of vision models?" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09939v1.pdf filename=2403.09939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural network <b>quantization</b> is an essential technique for deploying models on resource-constrained devices. However, its impact on model perceptual fields, particularly regarding class activation maps (CAMs), remains a significant area of investigation. In this study, we explore how <b>quantization</b> alters the spatial recognition ability of the perceptual field of vision models, shedding light on the alignment between CAMs and visual saliency maps across various architectures. Leveraging a dataset of 10,000 images from ImageNet, we rigorously evaluate six diverse foundational <b>CNNs:</b> VGG16, ResNet50, EfficientNet, MobileNet, SqueezeNet, and DenseNet. We uncover nuanced changes in CAMs and their alignment with human visual saliency maps through systematic <b>quantization</b> techniques applied to these models. Our findings reveal the varying sensitivities of different architectures to <b>quantization</b> and underscore its implications for real-world applications in terms of model performance and interpretability. The primary contribution of this work revolves around deepening our understanding of neural network <b>quantization,</b> providing insights crucial for deploying efficient and interpretable models in practical settings.</p></p class="citation"></blockquote><h3 id=61104--61298-csdnet-detect-salient-object-in-depth-thermal-via-a-lightweight-cross-shallow-and-deep-perception-network-xiaotong-yu-et-al-2024>(61/104 | 61/298) CSDNet: Detect Salient Object in Depth-Thermal via A Lightweight Cross Shallow and Deep Perception Network (Xiaotong Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaotong Yu, Ruihan Xie, Zhihe Zhao, Chang-Wen Chen. (2024)<br><strong>CSDNet: Detect Salient Object in Depth-Thermal via A Lightweight Cross Shallow and Deep Perception Network</strong><br><button class=copy-to-clipboard title="CSDNet: Detect Salient Object in Depth-Thermal via A Lightweight Cross Shallow and Deep Perception Network" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 19<br>Keywords: Object Detection, Benchmarking, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10104v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10104v1.pdf filename=2403.10104v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While we enjoy the richness and informativeness of <b>multimodal</b> data, it also introduces interference and redundancy of information. To achieve optimal domain interpretation with limited resources, we propose CSDNet, a lightweight \textbf{C}ross \textbf{S}hallow and \textbf{D}eep Perception \textbf{Net}work designed to integrate two modalities with less coherence, thereby discarding redundant information or even modality. We implement our CSDNet for Salient <b>Object</b> <b>Detection</b> (SOD) task in robotic perception. The proposed method capitalises on spatial information prescreening and implicit coherence navigation across shallow and deep layers of the depth-thermal (D-T) modality, prioritising integration over fusion to maximise the scene interpretation. To further refine the descriptive capabilities of the encoder for the less-known D-T modalities, we also propose SAMAEP to guide an effective feature mapping to the generalised feature space. Our approach is tested on the VDT-2048 dataset, leveraging the D-T modality outperforms those of SOTA methods using RGB-T or RGB-D modalities for the first time, achieves comparable performance with the RGB-D-T triple-modality <b>benchmark</b> method with 5.97 times faster at runtime and demanding 0.0036 times fewer FLOPs. Demonstrates the proposed CSDNet effectively integrates the information from the D-T modality. The code will be released upon acceptance.</p></p class="citation"></blockquote><h3 id=62104--62298-leveraging-neural-radiance-field-in-descriptor-synthesis-for-keypoints-scene-coordinate-regression-huy-hoang-bui-et-al-2024>(62/104 | 62/298) Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints Scene Coordinate Regression (Huy-Hoang Bui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huy-Hoang Bui, Bach-Thuan Bui, Dinh-Tuan Tran, Joo-Ho Lee. (2024)<br><strong>Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints Scene Coordinate Regression</strong><br><button class=copy-to-clipboard title="Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints Scene Coordinate Regression" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Graph Attention Networks, Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10297v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10297v2.pdf filename=2403.10297v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Classical structural-based visual localization methods offer high accuracy but face trade-offs in terms of storage, speed, and privacy. A recent innovation, keypoint scene coordinate regression (KSCR) named D2S addresses these issues by leveraging <b>graph</b> <b>attention</b> <b>networks</b> to enhance keypoint relationships and predict their 3D coordinates using a simple multilayer perceptron (MLP). Camera pose is then determined via PnP+RANSAC, using established 2D-3D correspondences. While KSCR achieves competitive results, rivaling state-of-the-art image-retrieval methods like HLoc across multiple <b>benchmarks,</b> its performance is hindered when data samples are limited due to the deep learning model&rsquo;s reliance on extensive data. This paper proposes a solution to this challenge by introducing a pipeline for keypoint descriptor synthesis using Neural Radiance Field (NeRF). By generating novel poses and feeding them into a trained NeRF model to create new views, our approach enhances the KSCR&rsquo;s generalization capabilities in data-scarce environments. The proposed system could significantly improve localization accuracy by up to 50% and cost only a fraction of time for data synthesis. Furthermore, its modular design allows for the integration of multiple NeRFs, offering a versatile and efficient solution for visual localization. The implementation is publicly available at: <a href=https://github.com/ais-lab/DescriptorSynthesis4Feat2Map>https://github.com/ais-lab/DescriptorSynthesis4Feat2Map</a>.</p></p class="citation"></blockquote><h3 id=63104--63298-animate-your-motion-turning-still-images-into-dynamic-videos-mingxiao-li-et-al-2024>(63/104 | 63/298) Animate Your Motion: Turning Still Images into Dynamic Videos (Mingxiao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingxiao Li, Bo Wan, Marie-Francine Moens, Tinne Tuytelaars. (2024)<br><strong>Animate Your Motion: Turning Still Images into Dynamic Videos</strong><br><button class=copy-to-clipboard title="Animate Your Motion: Turning Still Images into Dynamic Videos" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Diffusion Model, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10179v1.pdf filename=2403.10179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>diffusion</b> <b>models</b> have made remarkable strides in text-to-video generation, sparking a quest for enhanced control over video outputs to more accurately reflect user intentions. Traditional efforts predominantly focus on employing either semantic cues, like images or depth maps, or motion-based conditions, like moving sketches or object bounding boxes. Semantic inputs offer a rich scene context but lack detailed motion specificity; conversely, motion inputs provide precise trajectory information but miss the broader semantic narrative. For the first time, we integrate both semantic and motion cues within a <b>diffusion</b> <b>model</b> for video generation, as demonstrated in Fig 1. To this end, we introduce the Scene and Motion Conditional <b>Diffusion</b> <b>(SMCD),</b> a novel methodology for managing <b>multimodal</b> inputs. It incorporates a recognized motion conditioning module and investigates various approaches to integrate scene conditions, promoting synergy between different modalities. For model training, we separate the conditions for the two modalities, introducing a two-stage training pipeline. Experimental results demonstrate that our design significantly enhances video quality, motion precision, and semantic coherence.</p></p class="citation"></blockquote><h3 id=64104--64298-benchmarking-adversarial-robustness-of-image-shadow-removal-with-shadow-adaptive-attacks-chong-wang-et-al-2024>(64/104 | 64/298) Benchmarking Adversarial Robustness of Image Shadow Removal with Shadow-adaptive Attacks (Chong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chong Wang, Yi Yu, Lanqing Guo, Bihan Wen. (2024)<br><strong>Benchmarking Adversarial Robustness of Image Shadow Removal with Shadow-adaptive Attacks</strong><br><button class=copy-to-clipboard title="Benchmarking Adversarial Robustness of Image Shadow Removal with Shadow-adaptive Attacks" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10076v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10076v1.pdf filename=2403.10076v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Shadow removal is a task aimed at erasing regional shadows present in images and reinstating visually pleasing natural scenes with consistent illumination. While recent deep learning techniques have demonstrated impressive performance in image shadow removal, their robustness against <b>adversarial</b> <b>attacks</b> remains largely unexplored. Furthermore, many existing attack frameworks typically allocate a uniform budget for perturbations across the entire input image, which may not be suitable for attacking shadow images. This is primarily due to the unique characteristic of spatially varying illumination within shadow images. In this paper, we propose a novel approach, called shadow-adaptive <b>adversarial</b> <b>attack.</b> Different from standard <b>adversarial</b> <b>attacks,</b> our attack budget is adjusted based on the pixel intensity in different regions of shadow images. Consequently, the optimized <b>adversarial</b> <b>noise</b> in the shadowed regions becomes visually less perceptible while permitting a greater tolerance for perturbations in non-shadow regions. The proposed shadow-adaptive attacks naturally align with the varying illumination distribution in shadow images, resulting in perturbations that are less conspicuous. Building on this, we conduct a comprehensive empirical evaluation of existing shadow removal methods, subjecting them to various levels of attack on publicly available datasets.</p></p class="citation"></blockquote><h3 id=65104--65298-parapoint-learning-global-free-boundary-surface-parameterization-of-3d-point-clouds-qijian-zhang-et-al-2024>(65/104 | 65/298) ParaPoint: Learning Global Free-Boundary Surface Parameterization of 3D Point Clouds (Qijian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qijian Zhang, Junhui Hou, Ying He. (2024)<br><strong>ParaPoint: Learning Global Free-Boundary Surface Parameterization of 3D Point Clouds</strong><br><button class=copy-to-clipboard title="ParaPoint: Learning Global Free-Boundary Surface Parameterization of 3D Point Clouds" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10349v1.pdf filename=2403.10349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Surface parameterization is a fundamental <b>geometry</b> processing problem with rich downstream applications. Traditional approaches are designed to operate on well-behaved mesh models with high-quality triangulations that are laboriously produced by specialized 3D modelers, and thus unable to meet the processing demand for the current explosion of ordinary 3D data. In this paper, we seek to perform UV unwrapping on unstructured 3D point clouds. Technically, we propose ParaPoint, an <b>unsupervised</b> neural learning pipeline for achieving global free-boundary surface parameterization by building point-wise mappings between given 3D points and 2D UV coordinates with adaptively deformed boundaries. We ingeniously construct several geometrically meaningful sub-networks with specific functionalities, and assemble them into a bi-directional cycle mapping framework. We also design effective loss functions and auxiliary differential geometric constraints for the optimization of the neural mapping process. To the best of our knowledge, this work makes the first attempt to investigate neural point cloud parameterization that pursues both global mappings and free boundaries. Experiments demonstrate the effectiveness and inspiring potential of our proposed learning paradigm. The code will be publicly available.</p></p class="citation"></blockquote><h3 id=66104--66298-spherediffusion-spherical-geometry-aware-distortion-resilient-diffusion-model-tao-wu-et-al-2024>(66/104 | 66/298) SphereDiffusion: Spherical Geometry-Aware Distortion Resilient Diffusion Model (Tao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Wu, Xuewei Li, Zhongang Qi, Di Hu, Xintao Wang, Ying Shan, Xi Li. (2024)<br><strong>SphereDiffusion: Spherical Geometry-Aware Distortion Resilient Diffusion Model</strong><br><button class=copy-to-clipboard title="SphereDiffusion: Spherical Geometry-Aware Distortion Resilient Diffusion Model" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Diffusion Model, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10044v1.pdf filename=2403.10044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Controllable spherical panoramic image generation holds substantial applicative potential across a variety of domains.However, it remains a challenging task due to the inherent spherical distortion and <b>geometry</b> characteristics, resulting in low-quality content generation.In this paper, we introduce a novel framework of SphereDiffusion to address these unique challenges, for better generating high-quality and precisely controllable spherical panoramic images.For the spherical distortion characteristic, we embed the semantics of the distorted object with text encoding, then explicitly construct the relationship with text-object correspondence to better use the pre-trained knowledge of the planar images.Meanwhile, we employ a deformable technique to mitigate the semantic deviation in latent space caused by spherical distortion.For the spherical <b>geometry</b> characteristic, in virtue of spherical rotation invariance, we improve the data diversity and optimization objectives in the training process, enabling the model to better learn the spherical <b>geometry</b> characteristic.Furthermore, we enhance the denoising process of the <b>diffusion</b> <b>model,</b> enabling it to effectively use the learned geometric characteristic to ensure the boundary continuity of the generated images.With these specific techniques, experiments on Structured3D dataset show that SphereDiffusion significantly improves the quality of controllable spherical image generation and relatively reduces around 35% FID on average.</p></p class="citation"></blockquote><h3 id=67104--67298-palm-pushing-adaptive-learning-rate-mechanisms-for-continual-test-time-adaptation-sarthak-kumar-maharana-et-al-2024>(67/104 | 67/298) PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation (Sarthak Kumar Maharana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarthak Kumar Maharana, Baoming Zhang, Yunhui Guo. (2024)<br><strong>PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation</strong><br><button class=copy-to-clipboard title="PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, PaLM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10650v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10650v1.pdf filename=2403.10650v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world vision models in dynamic environments face rapid shifts in domain distributions, leading to decreased recognition performance. Continual test-time adaptation (CTTA) directly adjusts a pre-trained source discriminative model to these changing domains using test data. A highly effective CTTA method involves applying layer-wise adaptive learning rates, and selectively adapting pre-trained layers. However, it suffers from the poor estimation of domain shift and the inaccuracies arising from the pseudo-labels. In this work, we aim to overcome these limitations by identifying layers through the quantification of model prediction uncertainty without relying on pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by backpropagating the KL divergence between the softmax output and a uniform distribution, to select layers for further adaptation. Subsequently, for the parameters exclusively belonging to these selected layers, with the remaining ones frozen, we evaluate their sensitivity in order to approximate the domain shift, followed by adjusting their learning rates accordingly. Overall, this approach leads to a more robust and stable optimization than prior approaches. We conduct extensive image classification experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C and demonstrate the efficacy of our method against standard <b>benchmarks</b> and prior methods.</p></p class="citation"></blockquote><h3 id=68104--68298-p-mapnet-far-seeing-map-generator-enhanced-by-both-sdmap-and-hdmap-priors-zhou-jiang-et-al-2024>(68/104 | 68/298) P-MapNet: Far-seeing Map Generator Enhanced by both SDMap and HDMap Priors (Zhou Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhou Jiang, Zhenxin Zhu, Pengfei Li, Huan-ang Gao, Tianyuan Yuan, Yongliang Shi, Hang Zhao, Hao Zhao. (2024)<br><strong>P-MapNet: Far-seeing Map Generator Enhanced by both SDMap and HDMap Priors</strong><br><button class=copy-to-clipboard title="P-MapNet: Far-seeing Map Generator Enhanced by both SDMap and HDMap Priors" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Autoencoder, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10521v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10521v2.pdf filename=2403.10521v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous vehicles are gradually entering city roads today, with the help of high-definition maps (HDMaps). However, the reliance on HDMaps prevents autonomous vehicles from stepping into regions without this expensive digital infrastructure. This fact drives many researchers to study online HDMap generation algorithms, but the performance of these algorithms at far regions is still unsatisfying. We present P-MapNet, in which the letter P highlights the fact that we focus on incorporating map priors to improve model performance. Specifically, we exploit priors in both SDMap and HDMap. On one hand, we extract weakly aligned SDMap from OpenStreetMap, and encode it as an additional conditioning branch. Despite the misalignment challenge, our attention-based architecture adaptively attends to relevant SDMap skeletons and significantly improves performance. On the other hand, we exploit a masked <b>autoencoder</b> to capture the prior distribution of HDMap, which can serve as a refinement module to mitigate occlusions and artifacts. We <b>benchmark</b> on the nuScenes and Argoverse2 datasets. Through comprehensive experiments, we show that: (1) our SDMap prior can improve online map generation performance, using both rasterized (by up to $+18.73$ $\rm mIoU$) and vectorized (by up to $+8.50$ $\rm mAP$) output representations. (2) our HDMap prior can improve map perceptual metrics by up to $6.34%$. (3) P-MapNet can be switched into different inference modes that covers different regions of the accuracy-efficiency trade-off landscape. (4) P-MapNet is a far-seeing solution that brings larger improvements on longer ranges. Codes and models are publicly available at <a href=https://jike5.github.io/P-MapNet>https://jike5.github.io/P-MapNet</a>.</p></p class="citation"></blockquote><h3 id=69104--69298-energy-correction-model-in-the-feature-space-for-out-of-distribution-detection-marc-lafon-et-al-2024>(69/104 | 69/298) Energy Correction Model in the Feature Space for Out-of-Distribution Detection (Marc Lafon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marc Lafon, Clément Rambour, Nicolas Thome. (2024)<br><strong>Energy Correction Model in the Feature Space for Out-of-Distribution Detection</strong><br><button class=copy-to-clipboard title="Energy Correction Model in the Feature Space for Out-of-Distribution Detection" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10403v1.pdf filename=2403.10403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we study the <b>out-of-distribution</b> (OOD) detection problem through the use of the feature space of a pre-trained deep classifier. We show that learning the density of in-distribution (ID) features with an energy-based models (EBM) leads to competitive detection results. However, we found that the non-mixing of MCMC sampling during the EBM&rsquo;s training undermines its detection performance. To overcome this an energy-based correction of a mixture of class-conditional Gaussian distributions. We obtains favorable results when compared to a strong baseline like the KNN detector on the CIFAR-10/CIFAR-100 OOD detection <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=70104--70298-anim-accurate-neural-implicit-model-for-human-reconstruction-from-a-single-rgb-d-image-marco-pesavento-et-al-2024>(70/104 | 70/298) ANIM: Accurate Neural Implicit Model for Human Reconstruction from a single RGB-D image (Marco Pesavento et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Pesavento, Yuanlu Xu, Nikolaos Sarafianos, Robert Maier, Ziyan Wang, Chun-Han Yao, Marco Volino, Edmond Boyer, Adrian Hilton, Tony Tung. (2024)<br><strong>ANIM: Accurate Neural Implicit Model for Human Reconstruction from a single RGB-D image</strong><br><button class=copy-to-clipboard title="ANIM: Accurate Neural Implicit Model for Human Reconstruction from a single RGB-D image" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 13<br>Keywords: Fine-tuning, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10357v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10357v2.pdf filename=2403.10357v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent progress in human shape learning, shows that neural implicit models are effective in generating 3D human surfaces from limited number of views, and even from a single RGB image. However, existing monocular approaches still struggle to recover fine geometric details such as face, hands or cloth wrinkles. They are also easily prone to depth ambiguities that result in distorted geometries along the camera optical axis. In this paper, we explore the benefits of incorporating depth observations in the reconstruction process by introducing ANIM, a novel method that reconstructs arbitrary 3D human shapes from single-view RGB-D images with an unprecedented level of accuracy. Our model learns geometric details from both multi-resolution pixel-aligned and voxel-aligned features to leverage depth information and enable spatial relationships, mitigating depth ambiguities. We further enhance the quality of the reconstructed shape by introducing a depth-supervision strategy, which improves the accuracy of the signed distance field estimation of points that lie on the reconstructed surface. Experiments demonstrate that ANIM outperforms state-of-the-art works that use RGB, surface normals, point cloud or RGB-D data as input. In addition, we introduce ANIM-Real, a new <b>multi-modal</b> dataset comprising high-quality scans paired with consumer-grade RGB-D camera, and our protocol to <b>fine-tune</b> ANIM, enabling high-quality reconstruction from real-world human capture.</p></p class="citation"></blockquote><h3 id=71104--71298-learning-spatiotemporal-inconsistency-via-thumbnail-layout-for-face-deepfake-detection-yuting-xu-et-al-2024>(71/104 | 71/298) Learning Spatiotemporal Inconsistency via Thumbnail Layout for Face Deepfake Detection (Yuting Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuting Xu, Jian Liang, Lijun Sheng, Xiao-Yu Zhang. (2024)<br><strong>Learning Spatiotemporal Inconsistency via Thumbnail Layout for Face Deepfake Detection</strong><br><button class=copy-to-clipboard title="Learning Spatiotemporal Inconsistency via Thumbnail Layout for Face Deepfake Detection" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10261v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10261v2.pdf filename=2403.10261v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The deepfake threats to society and cybersecurity have provoked significant public apprehension, driving intensified efforts within the realm of deepfake video detection. Current video-level methods are mostly based on {3D CNNs} resulting in high computational demands, although have achieved good performance. This paper introduces an elegantly simple yet effective strategy named Thumbnail Layout (TALL), which transforms a video clip into a pre-defined layout to realize the preservation of spatial and temporal dependencies. This transformation process involves sequentially masking frames at the same positions within each frame. These frames are then resized into sub-frames and reorganized into the predetermined layout, forming thumbnails. TALL is model-agnostic and has remarkable simplicity, necessitating only minimal code modifications. Furthermore, we introduce a <b>graph</b> <b>reasoning</b> block (GRB) and semantic consistency (SC) loss to strengthen TALL, culminating in TALL++. GRB enhances interactions between different semantic regions to capture semantic-level inconsistency clues. The semantic consistency loss imposes consistency constraints on semantic features to improve model generalization ability. Extensive experiments on intra-dataset, cross-dataset, diffusion-generated image detection, and deepfake generation method recognition show that TALL++ achieves results surpassing or comparable to the state-of-the-art methods, demonstrating the effectiveness of our approaches for various deepfake detection problems. The code is available at <a href=https://github.com/rainy-xu/TALL4Deepfake>https://github.com/rainy-xu/TALL4Deepfake</a>.</p></p class="citation"></blockquote><h3 id=72104--72298-arbitrary-scale-image-generation-and-upsampling-using-latent-diffusion-model-and-implicit-neural-decoder-jinseok-kim-et-al-2024>(72/104 | 72/298) Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder (Jinseok Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinseok Kim, Tae-Kyun Kim. (2024)<br><strong>Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder</strong><br><button class=copy-to-clipboard title="Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10255v1.pdf filename=2403.10255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Super-resolution (SR) and image generation are important tasks in computer vision and are widely adopted in real-world applications. Most existing methods, however, generate images only at fixed-scale magnification and suffer from over-smoothing and artifacts. Additionally, they do not offer enough diversity of output images nor image consistency at different scales. Most relevant work applied Implicit Neural Representation (INR) to the denoising <b>diffusion</b> <b>model</b> to obtain continuous-resolution yet diverse and high-quality SR results. Since this model operates in the image space, the larger the resolution of image is produced, the more memory and inference time is required, and it also does not maintain scale-specific consistency. We propose a novel pipeline that can super-resolve an input image or generate from a random noise a novel image at arbitrary scales. The method consists of a pretrained auto-encoder, a latent <b>diffusion</b> <b>model,</b> and an implicit neural decoder, and their learning strategies. The proposed method adopts <b>diffusion</b> <b>processes</b> in a latent space, thus efficient, yet aligned with output image space decoded by MLPs at arbitrary scales. More specifically, our arbitrary-scale decoder is designed by the symmetric decoder w/o up-scaling from the pretrained auto-encoder, and Local Implicit Image Function (LIIF) in series. The latent <b>diffusion</b> <b>process</b> is learnt by the denoising and the alignment losses jointly. Errors in output images are backpropagated via the fixed decoder, improving the quality of output images. In the extensive experiments using multiple public <b>benchmarks</b> on the two tasks i.e. image super-resolution and novel image generation at arbitrary scales, the proposed method outperforms relevant methods in metrics of image quality, diversity and scale consistency. It is significantly better than the relevant prior-art in the inference speed and memory usage.</p></p class="citation"></blockquote><h3 id=73104--73298-rcooper-a-real-world-large-scale-dataset-for-roadside-cooperative-perception-ruiyang-hao-et-al-2024>(73/104 | 73/298) RCooper: A Real-world Large-scale Dataset for Roadside Cooperative Perception (Ruiyang Hao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiyang Hao, Siqi Fan, Yingru Dai, Zhenlin Zhang, Chenxi Li, Yuntian Wang, Haibao Yu, Wenxian Yang, Jirui Yuan, Zaiqing Nie. (2024)<br><strong>RCooper: A Real-world Large-scale Dataset for Roadside Cooperative Perception</strong><br><button class=copy-to-clipboard title="RCooper: A Real-world Large-scale Dataset for Roadside Cooperative Perception" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-8; I-5-4, cs-CV, cs-RO, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, BLOOM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10145v1.pdf filename=2403.10145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The value of roadside perception, which could extend the boundaries of autonomous driving and traffic management, has gradually become more prominent and acknowledged in recent years. However, existing roadside perception approaches only focus on the single-infrastructure sensor system, which cannot realize a comprehensive understanding of a traffic area because of the limited sensing range and blind spots. Orienting high-quality roadside perception, we need Roadside Cooperative Perception (RCooper) to achieve practical area-coverage roadside perception for restricted traffic areas. Rcooper has its own domain-specific challenges, but further exploration is hindered due to the lack of datasets. We hence release the first real-world, large-scale RCooper dataset to <b>bloom</b> the research on practical roadside cooperative perception, including detection and tracking. The manually annotated dataset comprises 50k images and 30k point clouds, including two representative traffic scenes (i.e., intersection and corridor). The constructed <b>benchmarks</b> prove the effectiveness of roadside cooperation perception and demonstrate the direction of further research. Codes and dataset can be accessed at: <a href=https://github.com/AIR-THU/DAIR-RCooper>https://github.com/AIR-THU/DAIR-RCooper</a>.</p></p class="citation"></blockquote><h3 id=74104--74298-contrastive-pre-training-with-multi-view-fusion-for-no-reference-point-cloud-quality-assessment-ziyu-shan-et-al-2024>(74/104 | 74/298) Contrastive Pre-Training with Multi-View Fusion for No-Reference Point Cloud Quality Assessment (Ziyu Shan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyu Shan, Yujie Zhang, Qi Yang, Haichen Yang, Yiling Xu, Jenq-Neng Hwang, Xiaozhong Xu, Shan Liu. (2024)<br><strong>Contrastive Pre-Training with Multi-View Fusion for No-Reference Point Cloud Quality Assessment</strong><br><button class=copy-to-clipboard title="Contrastive Pre-Training with Multi-View Fusion for No-Reference Point Cloud Quality Assessment" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10066v1.pdf filename=2403.10066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>No-reference point cloud quality assessment (NR-PCQA) aims to automatically evaluate the perceptual quality of distorted point clouds without available reference, which have achieved tremendous improvements due to the utilization of deep neural networks. However, learning-based NR-PCQA methods suffer from the scarcity of labeled data and usually perform suboptimally in terms of generalization. To solve the problem, we propose a novel contrastive pre-training framework tailored for PCQA (CoPA), which enables the pre-trained model to learn quality-aware representations from unlabeled data. To obtain anchors in the representation space, we project point clouds with different distortions into images and randomly mix their local patches to form mixed images with multiple distortions. Utilizing the generated anchors, we constrain the pre-training process via a quality-aware contrastive loss following the philosophy that perceptual quality is closely related to both content and distortion. Furthermore, in the model <b>fine-tuning</b> stage, we propose a semantic-guided multi-view fusion module to effectively integrate the features of projected images from multiple perspectives. Extensive experiments show that our method outperforms the state-of-the-art PCQA methods on popular <b>benchmarks.</b> Further investigations demonstrate that CoPA can also benefit existing learning-based PCQA models.</p></p class="citation"></blockquote><h3 id=75104--75298-autoregressive-queries-for-adaptive-tracking-with-spatio-temporaltransformers-jinxia-xie-et-al-2024>(75/104 | 75/298) Autoregressive Queries for Adaptive Tracking with Spatio-TemporalTransformers (Jinxia Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinxia Xie, Bineng Zhong, Zhiyi Mo, Shengping Zhang, Liangtao Shi, Shuxiang Song, Rongrong Ji. (2024)<br><strong>Autoregressive Queries for Adaptive Tracking with Spatio-TemporalTransformers</strong><br><button class=copy-to-clipboard title="Autoregressive Queries for Adaptive Tracking with Spatio-TemporalTransformers" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10574v1.pdf filename=2403.10574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rich spatio-temporal information is crucial to capture the complicated target appearance variations in visual tracking. However, most top-performing tracking algorithms rely on many hand-crafted components for spatio-temporal information aggregation. Consequently, the spatio-temporal information is far away from being fully explored. To alleviate this issue, we propose an adaptive tracker with spatio-temporal <b>transformers</b> (named AQATrack), which adopts simple autoregressive queries to effectively learn spatio-temporal information without many hand-designed components. Firstly, we introduce a set of learnable and autoregressive queries to capture the instantaneous target appearance changes in a sliding window fashion. Then, we design a novel attention mechanism for the interaction of existing queries to generate a new query in current frame. Finally, based on the initial target template and learnt autoregressive queries, a spatio-temporal information fusion module (STM) is designed for spatiotemporal formation aggregation to locate a target object. Benefiting from the STM, we can effectively combine the static appearance and instantaneous changes to guide robust tracking. Extensive experiments show that our method significantly improves the tracker&rsquo;s performance on six popular tracking <b>benchmarks:</b> LaSOT, LaSOText, TrackingNet, GOT-10k, TNL2K, and UAV123.</p></p class="citation"></blockquote><h3 id=76104--76298-shifting-focus-from-global-semantics-to-local-prominent-features-in-swin-transformer-for-knee-osteoarthritis-severity-assessment-aymen-sekhri-et-al-2024>(76/104 | 76/298) Shifting Focus: From Global Semantics to Local Prominent Features in Swin-Transformer for Knee Osteoarthritis Severity Assessment (Aymen Sekhri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aymen Sekhri, Marouane Tliba, Mohamed Amine Kerkouri, Yassine Nasser, Aladine Chetouani, Alessandro Bruno, Rachid Jennane. (2024)<br><strong>Shifting Focus: From Global Semantics to Local Prominent Features in Swin-Transformer for Knee Osteoarthritis Severity Assessment</strong><br><button class=copy-to-clipboard title="Shifting Focus: From Global Semantics to Local Prominent Features in Swin-Transformer for Knee Osteoarthritis Severity Assessment" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09947v1.pdf filename=2403.09947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional imaging diagnostics frequently encounter bottlenecks due to manual inspection, which can lead to delays and inconsistencies. Although deep learning offers a pathway to automation and enhanced accuracy, foundational models in computer vision often emphasize global context at the expense of local details, which are vital for medical imaging diagnostics. To address this, we harness the Swin <b>Transformer&rsquo;s</b> capacity to discern extended spatial dependencies within images through the hierarchical framework. Our novel contribution lies in refining local feature representations, orienting them specifically toward the final distribution of the classifier. This method ensures that local features are not only preserved but are also enriched with task-specific information, enhancing their relevance and detail at every hierarchical level. By implementing this strategy, our model demonstrates significant robustness and precision, as evidenced by extensive validation of two established <b>benchmarks</b> for Knee OsteoArthritis (KOA) grade classification. These results highlight our approach&rsquo;s effectiveness and its promising implications for the future of medical imaging diagnostics. Our implementation is available on <a href=https://github.com/mtliba/KOA_NLCS2024>https://github.com/mtliba/KOA_NLCS2024</a></p></p class="citation"></blockquote><h3 id=77104--77298-cannabis-seed-variant-detection-using-faster-r-cnn-toqi-tahamid-sarker-et-al-2024>(77/104 | 77/298) Cannabis Seed Variant Detection using Faster R-CNN (Toqi Tahamid Sarker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Toqi Tahamid Sarker, Taminul Islam, Khaled R Ahmed. (2024)<br><strong>Cannabis Seed Variant Detection using Faster R-CNN</strong><br><button class=copy-to-clipboard title="Cannabis Seed Variant Detection using Faster R-CNN" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10722v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10722v1.pdf filename=2403.10722v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Analyzing and detecting cannabis seed variants is crucial for the agriculture industry. It enables precision breeding, allowing cultivators to selectively enhance desirable traits. Accurate identification of seed variants also ensures regulatory compliance, facilitating the cultivation of specific cannabis strains with defined characteristics, ultimately improving agricultural productivity and meeting diverse market demands. This paper presents a study on cannabis seed variant detection by employing a state-of-the-art <b>object</b> <b>detection</b> model Faster R-CNN. This study implemented the model on a locally sourced cannabis seed dataset in Thailand, comprising 17 distinct classes. We evaluate six Faster R-CNN models by comparing performance on various metrics and achieving a mAP score of 94.08% and an F1 score of 95.66%. This paper presents the first known application of deep neural network <b>object</b> <b>detection</b> models to the novel task of visually identifying cannabis seed types.</p></p class="citation"></blockquote><h3 id=78104--78298-robust-influence-based-training-methods-for-noisy-brain-mri-minh-hao-van-et-al-2024>(78/104 | 78/298) Robust Influence-based Training Methods for Noisy Brain MRI (Minh-Hao Van et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minh-Hao Van, Alycia N. Carey, Xintao Wu. (2024)<br><strong>Robust Influence-based Training Methods for Noisy Brain MRI</strong><br><button class=copy-to-clipboard title="Robust Influence-based Training Methods for Noisy Brain MRI" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10698v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10698v1.pdf filename=2403.10698v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Correctly classifying brain tumors is imperative to the <b>prompt</b> and accurate treatment of a patient. While several classification algorithms based on classical image processing or deep learning methods have been proposed to rapidly classify tumors in MR images, most assume the unrealistic setting of noise-free training data. In this work, we study a difficult but realistic setting of training a deep learning model on noisy MR images to classify brain tumors. We propose two training methods that are robust to noisy MRI training data, Influence-based Sample Reweighing (ISR) and Influence-based Sample Perturbation (ISP), which are based on influence functions from robust statistics. Using the influence functions, in ISR, we adaptively reweigh training examples according to how helpful/harmful they are to the training process, while in ISP, we craft and inject helpful perturbation proportional to the influence score. Both ISR and ISP harden the classification model against noisy training data without significantly affecting the generalization ability of the model on test data. We conduct empirical evaluations over a common brain tumor dataset and compare ISR and ISP to three baselines. Our empirical results show that ISR and ISP can efficiently train deep learning models robust against noisy training data.</p></p class="citation"></blockquote><h3 id=79104--79298-lightit-illumination-modeling-and-control-for-diffusion-models-peter-kocsis-et-al-2024>(79/104 | 79/298) LightIt: Illumination Modeling and Control for Diffusion Models (Peter Kocsis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, Yannick Hold-Geoffroy. (2024)<br><strong>LightIt: Illumination Modeling and Control for Diffusion Models</strong><br><button class=copy-to-clipboard title="LightIt: Illumination Modeling and Control for Diffusion Models" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-8; I-2-10, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10615v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10615v1.pdf filename=2403.10615v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce LightIt, a method for explicit illumination control for image generation. Recent generative methods lack lighting control, which is crucial to numerous artistic aspects of image generation such as setting the overall mood or cinematic appearance. To overcome these limitations, we propose to condition the generation on shading and normal maps. We model the lighting with single bounce shading, which includes cast shadows. We first train a shading estimation module to generate a dataset of real-world images and shading pairs. Then, we train a control network using the estimated shading and normals as input. Our method demonstrates high-quality image generation and lighting control in numerous scenes. Additionally, we use our generated dataset to train an identity-preserving relighting model, conditioned on an image and a target shading. Our method is the first that enables the generation of images with controllable, consistent lighting and performs on par with specialized relighting state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=80104--80298-lodge-a-coarse-to-fine-diffusion-network-for-long-dance-generation-guided-by-the-characteristic-dance-primitives-ronghui-li-et-al-2024>(80/104 | 80/298) Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation Guided by the Characteristic Dance Primitives (Ronghui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ronghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, Xiu Li. (2024)<br><strong>Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation Guided by the Characteristic Dance Primitives</strong><br><button class=copy-to-clipboard title="Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation Guided by the Characteristic Dance Primitives" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs-SD, cs.CV, eess-AS<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10518v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10518v1.pdf filename=2403.10518v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose Lodge, a network capable of generating extremely long dance sequences conditioned on given music. We design Lodge as a two-stage coarse to fine <b>diffusion</b> <b>architecture,</b> and propose the characteristic dance primitives that possess significant expressiveness as intermediate representations between two <b>diffusion</b> <b>models.</b> The first stage is global <b>diffusion,</b> <b>which</b> focuses on comprehending the coarse-level music-dance correlation and production characteristic dance primitives. In contrast, the second-stage is the local <b>diffusion,</b> <b>which</b> parallelly generates detailed motion sequences under the guidance of the dance primitives and choreographic rules. In addition, we propose a Foot Refine Block to optimize the contact between the feet and the ground, enhancing the physical realism of the motion. Our approach can parallelly generate dance sequences of extremely long length, striking a balance between global choreographic patterns and local motion quality and expressiveness. Extensive experiments validate the efficacy of our method.</p></p class="citation"></blockquote><h3 id=81104--81298-a-novel-framework-for-multi-person-temporal-gaze-following-and-social-gaze-prediction-anshul-gupta-et-al-2024>(81/104 | 81/298) A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction (Anshul Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anshul Gupta, Samy Tafasca, Arya Farkhondeh, Pierre Vuillecard, Jean-Marc Odobez. (2024)<br><strong>A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction</strong><br><button class=copy-to-clipboard title="A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10511v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10511v1.pdf filename=2403.10511v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gaze following and social gaze prediction are fundamental tasks providing insights into human communication behaviors, intent, and social interactions. Most previous approaches addressed these tasks separately, either by designing highly specialized social gaze models that do not generalize to other social gaze tasks or by considering social gaze inference as an ad-hoc post-processing of the gaze following task. Furthermore, the vast majority of gaze following approaches have proposed static models that can handle only one person at a time, therefore failing to take advantage of social interactions and temporal dynamics. In this paper, we address these limitations and introduce a novel framework to jointly predict the gaze target and social gaze label for all people in the scene. The framework comprises of: (i) a temporal, <b>transformer-based</b> architecture that, in addition to image tokens, handles person-specific tokens capturing the gaze information related to each individual; (ii) a new dataset, VSGaze, that unifies annotation types across multiple gaze following and social gaze datasets. We show that our model trained on VSGaze can address all tasks jointly, and achieves state-of-the-art results for multi-person gaze following and social gaze prediction.</p></p class="citation"></blockquote><h3 id=82104--82298-robust-shape-fitting-for-3d-scene-abstraction-florian-kluger-et-al-2024>(82/104 | 82/298) Robust Shape Fitting for 3D Scene Abstraction (Florian Kluger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Kluger, Eric Brachmann, Michael Ying Yang, Bodo Rosenhahn. (2024)<br><strong>Robust Shape Fitting for 3D Scene Abstraction</strong><br><button class=copy-to-clipboard title="Robust Shape Fitting for 3D Scene Abstraction" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10452v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10452v1.pdf filename=2403.10452v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans perceive and construct the world as an arrangement of simple parametric models. In particular, we can often describe man-made environments using volumetric primitives such as cuboids or cylinders. Inferring these primitives is important for attaining high-level, abstract scene descriptions. Previous approaches for primitive-based abstraction estimate shape parameters directly and are only able to reproduce simple objects. In contrast, we propose a robust estimator for primitive fitting, which meaningfully abstracts complex real-world environments using cuboids. A RANSAC estimator guided by a neural network fits these primitives to a depth map. We condition the network on previously detected parts of the scene, parsing it one-by-one. To obtain cuboids from single RGB images, we additionally optimise a depth estimation <b>CNN</b> end-to-end. Naively minimising point-to-primitive distances leads to large or spurious cuboids occluding parts of the scene. We thus propose an improved occlusion-aware distance metric correctly handling opaque scenes. Furthermore, we present a neural network based cuboid solver which provides more parsimonious scene abstractions while also reducing inference time. The proposed algorithm does not require labour-intensive labels, such as cuboid annotations, for training. Results on the NYU Depth v2 dataset demonstrate that the proposed algorithm successfully abstracts cluttered real-world 3D scene layouts.</p></p class="citation"></blockquote><h3 id=83104--83298-swag-splatting-in-the-wild-images-with-appearance-conditioned-gaussians-hiba-dahmani-et-al-2024>(83/104 | 83/298) SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians (Hiba Dahmani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou. (2024)<br><strong>SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians</strong><br><button class=copy-to-clipboard title="SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10427v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10427v1.pdf filename=2403.10427v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implicit neural representation methods have shown impressive advancements in learning 3D scenes from unstructured in-the-wild photo collections but are still limited by the large computational cost of volumetric rendering. More recently, 3D Gaussian Splatting emerged as a much faster alternative with superior rendering quality and training efficiency, especially for small-scale and object-centric scenarios. Nevertheless, this technique suffers from poor performance on unstructured in-the-wild data. To tackle this, we extend over 3D Gaussian Splatting to handle unstructured image collections. We achieve this by modeling appearance to seize photometric variations in the rendered images. Additionally, we introduce a new mechanism to train transient Gaussians to handle the presence of scene occluders in an <b>unsupervised</b> manner. Experiments on diverse photo collection scenes and multi-pass acquisition of outdoor landmarks show the effectiveness of our method over prior works achieving state-of-the-art results with improved efficiency.</p></p class="citation"></blockquote><h3 id=84104--84298-neuflow-real-time-high-accuracy-optical-flow-estimation-on-robots-using-edge-devices-zhiyong-zhang-et-al-2024>(84/104 | 84/298) NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots Using Edge Devices (Zhiyong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyong Zhang, Huaizu Jiang, Hanumant Singh. (2024)<br><strong>NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots Using Edge Devices</strong><br><button class=copy-to-clipboard title="NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots Using Edge Devices" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10425v1.pdf filename=2403.10425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-time high-accuracy optical flow estimation is a crucial component in various applications, including localization and mapping in robotics, object tracking, and activity recognition in computer vision. While recent learning-based optical flow methods have achieved high accuracy, they often come with heavy computation costs. In this paper, we propose a highly efficient optical flow architecture, called NeuFlow, that addresses both high accuracy and computational cost concerns. The architecture follows a global-to-local scheme. Given the features of the input images extracted at different spatial resolutions, global matching is employed to estimate an initial optical flow on the 1/16 resolution, capturing large displacement, which is then refined on the 1/8 resolution with lightweight <b>CNN</b> layers for better accuracy. We evaluate our approach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency improvements across different computing platforms. We achieve a notable 10x-80x speedup compared to several state-of-the-art methods, while maintaining comparable accuracy. Our approach achieves around 30 FPS on edge computing platforms, which represents a significant breakthrough in deploying complex computer vision tasks such as SLAM on small robots like drones. The full training and evaluation code is available at <a href=https://github.com/neufieldrobotics/NeuFlow>https://github.com/neufieldrobotics/NeuFlow</a>.</p></p class="citation"></blockquote><h3 id=85104--85298-cdmad-class-distribution-mismatch-aware-debiasing-for-class-imbalanced-semi-supervised-learning-hyuck-lee-et-al-2024>(85/104 | 85/298) CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning (Hyuck Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyuck Lee, Heeyoung Kim. (2024)<br><strong>CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning</strong><br><button class=copy-to-clipboard title="CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10391v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10391v1.pdf filename=2403.10391v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pseudo-label-based <b>semi-supervised</b> <b>learning</b> (SSL) algorithms trained on a class-imbalanced set face two cascading challenges: 1) Classifiers tend to be biased towards majority classes, and 2) Biased pseudo-labels are used for training. It is difficult to appropriately re-balance the classifiers in SSL because the class distribution of an unlabeled set is often unknown and could be mismatched with that of a labeled set. We propose a novel class-imbalanced SSL algorithm called class-distribution-mismatch-aware debiasing (CDMAD). For each iteration of training, CDMAD first assesses the classifier&rsquo;s biased degree towards each class by calculating the logits on an image without any patterns (e.g., solid color image), which can be considered irrelevant to the training set. CDMAD then refines biased pseudo-labels of the base SSL algorithm by ensuring the classifier&rsquo;s neutrality. CDMAD uses these refined pseudo-labels during the training of the base SSL algorithm to improve the quality of the representations. In the test phase, CDMAD similarly refines biased class predictions on test samples. CDMAD can be seen as an extension of post-hoc logit adjustment to address a challenge of incorporating the unknown class distribution of the unlabeled set for re-balancing the biased classifier under class distribution mismatch. CDMAD ensures Fisher consistency for the balanced error. Extensive experiments verify the effectiveness of CDMAD.</p></p class="citation"></blockquote><h3 id=86104--86298-simpb-a-single-model-for-2d-and-3d-object-detection-from-multiple-cameras-yingqi-tang-et-al-2024>(86/104 | 86/298) SimPB: A Single Model for 2D and 3D Object Detection from Multiple Cameras (Yingqi Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingqi Tang, Zhaotie Meng, Guoliang Chen, Erkang Cheng. (2024)<br><strong>SimPB: A Single Model for 2D and 3D Object Detection from Multiple Cameras</strong><br><button class=copy-to-clipboard title="SimPB: A Single Model for 2D and 3D Object Detection from Multiple Cameras" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10353v1.pdf filename=2403.10353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of autonomous driving has attracted considerable interest in approaches that directly infer 3D <b>objects</b> <b>in</b> the Bird&rsquo;s Eye View (BEV) from multiple cameras. Some attempts have also explored utilizing 2D detectors from single images to enhance the performance of 3D detection. However, these approaches rely on a two-stage process with separate detectors, where the 2D detection results are utilized only once for token selection or query initialization. In this paper, we present a single model termed SimPB, which simultaneously detects 2D <b>objects</b> <b>in</b> the perspective view and 3D <b>objects</b> <b>in</b> the BEV space from multiple cameras. To achieve this, we introduce a hybrid decoder consisting of several multi-view 2D decoder layers and several 3D decoder layers, specifically designed for their respective detection tasks. A Dynamic Query Allocation module and an Adaptive Query Aggregation module are proposed to continuously update and refine the interaction between 2D and 3D results, in a cyclic 3D-2D-3D manner. Additionally, Query-group Attention is utilized to strengthen the interaction among 2D queries within each camera group. In the experiments, we evaluate our method on the nuScenes dataset and demonstrate promising results for both 2D and 3D detection tasks. Our code is available at: <a href=https://github.com/nullmax-vision/SimPB>https://github.com/nullmax-vision/SimPB</a>.</p></p class="citation"></blockquote><h3 id=87104--87298-scilla-surface-implicit-learning-for-large-urban-area-a-volumetric-hybrid-solution-hala-djeghim-et-al-2024>(87/104 | 87/298) SCILLA: SurfaCe Implicit Learning for Large Urban Area, a volumetric hybrid solution (Hala Djeghim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hala Djeghim, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou, Désiré Sidibé. (2024)<br><strong>SCILLA: SurfaCe Implicit Learning for Large Urban Area, a volumetric hybrid solution</strong><br><button class=copy-to-clipboard title="SCILLA: SurfaCe Implicit Learning for Large Urban Area, a volumetric hybrid solution" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10344v1.pdf filename=2403.10344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural implicit surface representation methods have recently shown impressive 3D reconstruction results. However, existing solutions struggle to reconstruct urban outdoor scenes due to their large, unbounded, and highly detailed nature. Hence, to achieve accurate reconstructions, additional supervision data such as LiDAR, strong geometric priors, and long training times are required. To tackle such issues, we present SCILLA, a new hybrid implicit surface learning method to reconstruct large driving scenes from 2D images. SCILLA&rsquo;s hybrid architecture models two separate implicit fields: one for the volumetric density and another for the signed distance to the surface. To accurately represent urban outdoor scenarios, we introduce a novel volume-rendering strategy that relies on <b>self-supervised</b> probabilistic density estimation to sample points near the surface and transition progressively from volumetric to surface representation. Our solution permits a proper and fast initialization of the signed distance field without relying on any geometric prior on the scene, compared to concurrent methods. By conducting extensive experiments on four outdoor driving datasets, we show that SCILLA can learn an accurate and detailed 3D surface scene representation in various urban scenarios while being two times faster to train compared to previous state-of-the-art solutions.</p></p class="citation"></blockquote><h3 id=88104--88298-fdgaussian-fast-gaussian-splatting-from-single-image-via-geometric-aware-diffusion-model-qijun-feng-et-al-2024>(88/104 | 88/298) FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model (Qijun Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang. (2024)<br><strong>FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model</strong><br><button class=copy-to-clipboard title="FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10242v1.pdf filename=2403.10242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstructing detailed 3D objects from single-view images remains a challenging task due to the limited information available. In this paper, we introduce FDGaussian, a novel two-stage framework for single-image 3D reconstruction. Recent methods typically utilize pre-trained 2D <b>diffusion</b> <b>models</b> to generate plausible novel views from the input image, yet they encounter issues with either multi-view inconsistency or lack of geometric fidelity. To overcome these challenges, we propose an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, enabling the generation of consistent multi-view images. Moreover, we further accelerate the state-of-the-art Gaussian Splatting incorporating epipolar attention to fuse images from different viewpoints. We demonstrate that FDGaussian generates images with high consistency across different views and reconstructs high-quality 3D objects, both qualitatively and quantitatively. More examples can be found at our website <a href=https://qjfeng.net/FDGaussian/>https://qjfeng.net/FDGaussian/</a>.</p></p class="citation"></blockquote><h3 id=89104--89298-a-fixed-point-approach-to-unified-prompt-based-counting-wei-lin-et-al-2024>(89/104 | 89/298) A Fixed-Point Approach to Unified Prompt-Based Counting (Wei Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Lin, Antoni B. Chan. (2024)<br><strong>A Fixed-Point Approach to Unified Prompt-Based Counting</strong><br><button class=copy-to-clipboard title="A Fixed-Point Approach to Unified Prompt-Based Counting" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10236v1.pdf filename=2403.10236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing class-agnostic counting models typically rely on a single type of <b>prompt,</b> e.g., box annotations. This paper aims to establish a comprehensive <b>prompt-based</b> counting framework capable of generating density maps for concerned objects indicated by various <b>prompt</b> types, such as box, point, and text. To achieve this goal, we begin by converting <b>prompts</b> from different modalities into <b>prompt</b> masks without requiring training. These masks are then integrated into a class-agnostic counting methodology for predicting density maps. Furthermore, we introduce a fixed-point inference along with an associated loss function to improve counting accuracy, all without introducing new parameters. The effectiveness of this method is substantiated both theoretically and experimentally. Additionally, a contrastive training scheme is implemented to mitigate dataset bias inherent in current class-agnostic counting datasets, a strategy whose effectiveness is confirmed by our ablation study. Our model excels in prominent class-agnostic datasets and exhibits superior performance in cross-dataset adaptation tasks.</p></p class="citation"></blockquote><h3 id=90104--90298-a-hybrid-snn-ann-network-for-event-based-object-detection-with-spatial-and-temporal-attention-soikat-hasan-ahmed-et-al-2024>(90/104 | 90/298) A Hybrid SNN-ANN Network for Event-based Object Detection with Spatial and Temporal Attention (Soikat Hasan Ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soikat Hasan Ahmed, Jan Finkbeiner, Emre Neftci. (2024)<br><strong>A Hybrid SNN-ANN Network for Event-based Object Detection with Spatial and Temporal Attention</strong><br><button class=copy-to-clipboard title="A Hybrid SNN-ANN Network for Event-based Object Detection with Spatial and Temporal Attention" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10173v1.pdf filename=2403.10173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for <b>object</b> <b>detection</b> tasks. While Spiking Neural Networks (SNNs) are a natural match for event-based sensory data and enable ultra-energy efficient and low latency inference on neuromorphic hardware, Artificial Neural Networks (ANNs) tend to display more stable training dynamics and faster convergence resulting in greater task performance. Hybrid SNN-ANN approaches are a promising alternative, enabling to leverage the strengths of both SNN and ANN architectures. In this work, we introduce the first Hybrid Attention-based SNN-ANN backbone for <b>object</b> <b>detection</b> using event cameras. We propose a novel Attention-based SNN-ANN bridge module to capture sparse spatial and temporal relations from the SNN layer and convert them into dense feature maps for the ANN part of the backbone. Experimental results demonstrate that our proposed method surpasses baseline hybrid and SNN-based approaches by significant margins, with results comparable to existing ANN-based methods. Extensive ablation studies confirm the effectiveness of our proposed modules and architectural choices. These results pave the way toward a hybrid SNN-ANN architecture that achieves ANN like performance at a drastically reduced parameter budget. We implemented the SNN blocks on digital neuromorphic hardware to investigate latency and power consumption and demonstrate the feasibility of our approach.</p></p class="citation"></blockquote><h3 id=91104--91298-computer-user-interface-understanding-a-new-dataset-and-a-learning-framework-andrés-muñoz-et-al-2024>(91/104 | 91/298) Computer User Interface Understanding. A New Dataset and a Learning Framework (Andrés Muñoz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrés Muñoz, Daniel Borrajo. (2024)<br><strong>Computer User Interface Understanding. A New Dataset and a Learning Framework</strong><br><button class=copy-to-clipboard title="Computer User Interface Understanding. A New Dataset and a Learning Framework" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10170v1.pdf filename=2403.10170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>User Interface (UI) understanding has been an increasingly popular topic over the last few years. So far, there has been a vast focus solely on web and mobile applications. In this paper, we introduce the harder task of computer UI understanding. With the goal of enabling research in this field, we have generated a dataset with a set of videos where a user is performing a sequence of actions and each image shows the desktop contents at that time point. We also present a framework that is composed of a synthetic sample generation pipeline to augment the dataset with relevant characteristics, and a <b>contrastive</b> <b>learning</b> method to classify images in the videos. We take advantage of the natural conditional, tree-like, relationship of the images&rsquo; characteristics to regularize the learning of the representations by dealing with multiple partial tasks simultaneously. Experimental results show that the proposed framework outperforms previously proposed hierarchical multi-label <b>contrastive</b> <b>losses</b> in fine-grain UI classification.</p></p class="citation"></blockquote><h3 id=92104--92298-semantichuman-hd-high-resolution-semantic-disentangled-3d-human-generation-peng-zheng-et-al-2024>(92/104 | 92/298) SemanticHuman-HD: High-Resolution Semantic Disentangled 3D Human Generation (Peng Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Zheng, Tao Liu, Zili Yi, Rui Ma. (2024)<br><strong>SemanticHuman-HD: High-Resolution Semantic Disentangled 3D Human Generation</strong><br><button class=copy-to-clipboard title="SemanticHuman-HD: High-Resolution Semantic Disentangled 3D Human Generation" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2-10, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10166v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10166v1.pdf filename=2403.10166v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of neural radiance fields and generative models, numerous methods have been proposed for learning 3D human generation from 2D images. These methods allow control over the pose of the generated 3D human and enable rendering from different viewpoints. However, none of these methods explore semantic disentanglement in human image synthesis, i.e., they can not disentangle the generation of different semantic parts, such as the body, tops, and bottoms. Furthermore, existing methods are limited to synthesize images at $512^2$ resolution due to the high computational cost of neural radiance fields. To address these limitations, we introduce SemanticHuman-HD, the first method to achieve semantic disentangled human image synthesis. Notably, SemanticHuman-HD is also the first method to achieve 3D-aware image synthesis at $1024^2$ resolution, benefiting from our proposed 3D-aware super-resolution module. By leveraging the depth maps and semantic masks as guidance for the 3D-aware super-resolution, we significantly reduce the number of sampling points during volume rendering, thereby reducing the computational cost. Our comparative experiments demonstrate the superiority of our method. The effectiveness of each proposed component is also verified through ablation studies. Moreover, our method opens up exciting possibilities for various applications, including 3D garment generation, semantic-aware image synthesis, controllable image synthesis, and <b>out-of-domain</b> image synthesis.</p></p class="citation"></blockquote><h3 id=93104--93298-monkeypox-disease-recognition-model-based-on-improved-se-inceptionv3-junzhuo-chen-et-al-2024>(93/104 | 93/298) Monkeypox disease recognition model based on improved SE-InceptionV3 (Junzhuo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junzhuo Chen, Zonghan Lu, Shitong Kang. (2024)<br><strong>Monkeypox disease recognition model based on improved SE-InceptionV3</strong><br><button class=copy-to-clipboard title="Monkeypox disease recognition model based on improved SE-InceptionV3" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10087v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10087v1.pdf filename=2403.10087v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the wake of the global spread of monkeypox, accurate disease recognition has become crucial. This study introduces an improved SE-InceptionV3 model, embedding the SENet module and incorporating L2 regularization into the InceptionV3 framework to enhance monkeypox disease detection. Utilizing the Kaggle monkeypox dataset, which includes images of monkeypox and similar skin conditions, our model demonstrates a noteworthy accuracy of 96.71% on the test set, outperforming conventional methods and deep learning models. The SENet modules channel attention mechanism significantly elevates feature representation, while L2 regularization ensures robust generalization. Extensive experiments validate the models superiority in precision, recall, and F1 score, highlighting its effectiveness in differentiating monkeypox lesions in diverse and complex cases. The study not only provides insights into the application of advanced <b>CNN</b> architectures in medical diagnostics but also opens avenues for further research in model optimization and hyperparameter tuning for enhanced disease recognition. <a href=https://github.com/jzc777/SE-inceptionV3-L2>https://github.com/jzc777/SE-inceptionV3-L2</a></p></p class="citation"></blockquote><h3 id=94104--94298-learning-physical-dynamics-for-object-centric-visual-prediction-huilin-xu-et-al-2024>(94/104 | 94/298) Learning Physical Dynamics for Object-centric Visual Prediction (Huilin Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huilin Xu, Tao Chen, Feng Xu. (2024)<br><strong>Learning Physical Dynamics for Object-centric Visual Prediction</strong><br><button class=copy-to-clipboard title="Learning Physical Dynamics for Object-centric Visual Prediction" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10079v1.pdf filename=2403.10079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to model the underlying dynamics of visual scenes and reason about the future is central to human intelligence. Many attempts have been made to empower intelligent systems with such physical understanding and prediction abilities. However, most existing methods focus on pixel-to-pixel prediction, which suffers from heavy computational costs while lacking a deep understanding of the physical dynamics behind videos. Recently, object-centric prediction methods have emerged and attracted increasing interest. Inspired by it, this paper proposes an <b>unsupervised</b> object-centric prediction model that makes future predictions by learning visual dynamics between objects. Our model consists of two modules, perceptual, and dynamic module. The perceptual module is utilized to decompose images into several objects and synthesize images with a set of object-centric representations. The dynamic module fuses contextual information, takes environment-object and object-object interaction into account, and predicts the future trajectory of objects. Extensive experiments are conducted to validate the effectiveness of the proposed method. Both quantitative and qualitative experimental results demonstrate that our model generates higher visual quality and more physically reliable predictions compared to the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=95104--95298-lifelong-person-re-identification-with-backward-compatibility-minyoung-oh-et-al-2024>(95/104 | 95/298) Lifelong Person Re-Identification with Backward-Compatibility (Minyoung Oh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minyoung Oh, Jae-Young Sim. (2024)<br><strong>Lifelong Person Re-Identification with Backward-Compatibility</strong><br><button class=copy-to-clipboard title="Lifelong Person Re-Identification with Backward-Compatibility" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10022v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10022v2.pdf filename=2403.10022v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lifelong person re-identification (LReID) assumes a practical scenario where the model is sequentially trained on continuously incoming datasets while alleviating the catastrophic forgetting in the old datasets. However, not only the training datasets but also the gallery images are incrementally accumulated, that requires a huge amount of computational complexity and storage space to extract the features at the inference phase. In this paper, we address the above mentioned problem by incorporating the backward-compatibility to LReID for the first time. We train the model using the continuously incoming datasets while maintaining the model&rsquo;s compatibility toward the previously trained old models without re-computing the features of the old gallery images. To this end, we devise the cross-model compatibility loss based on the <b>contrastive</b> <b>learning</b> with respect to the replay features across all the old datasets. Moreover, we also develop the knowledge consolidation method based on the part classification to learn the shared representation across different datasets for the backward-compatibility. We suggest a more practical methodology for performance evaluation as well where all the gallery and query images are considered together. Experimental results demonstrate that the proposed method achieves a significantly higher performance of the backward-compatibility compared with the existing methods. It is a promising tool for more practical scenarios of LReID.</p></p class="citation"></blockquote><h3 id=96104--96298-linear-optimal-transport-subspaces-for-point-set-classification-mohammad-shifat-e-rabbi-et-al-2024>(96/104 | 96/298) Linear optimal transport subspaces for point set classification (Mohammad Shifat E Rabbi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Shifat E Rabbi, Naqib Sad Pathan, Shiying Li, Yan Zhuang, Abu Hasnat Mohammad Rubaiyat, Gustavo K Rohde. (2024)<br><strong>Linear optimal transport subspaces for point set classification</strong><br><button class=copy-to-clipboard title="Linear optimal transport subspaces for point set classification" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10015v1.pdf filename=2403.10015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning from point sets is an essential component in many computer vision and machine learning applications. Native, unordered, and permutation invariant set structure space is challenging to model, particularly for point set classification under spatial deformations. Here we propose a framework for classifying point sets experiencing certain types of spatial deformations, with a particular emphasis on datasets featuring affine deformations. Our approach employs the Linear Optimal Transport (LOT) transform to obtain a linear embedding of set-structured data. Utilizing the mathematical properties of the LOT transform, we demonstrate its capacity to accommodate variations in point sets by constructing a convex data space, effectively simplifying point set classification problems. Our method, which employs a nearest-subspace algorithm in the LOT space, demonstrates label efficiency, non-iterative behavior, and requires no hyper-parameter tuning. It achieves competitive accuracies compared to state-of-the-art methods across various point set classification tasks. Furthermore, our approach exhibits robustness in <b>out-of-distribution</b> scenarios where training and test distributions vary in terms of deformation magnitudes.</p></p class="citation"></blockquote><h3 id=97104--97298-medpnet-achieving-high-precision-adaptive-registration-for-complex-die-castings-yu-du-et-al-2024>(97/104 | 97/298) MEDPNet: Achieving High-Precision Adaptive Registration for Complex Die Castings (Yu Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Du, Yu Song, Ce Guo, Xiaojing Tian, Dong Liu, Ming Cong. (2024)<br><strong>MEDPNet: Achieving High-Precision Adaptive Registration for Complex Die Castings</strong><br><button class=copy-to-clipboard title="MEDPNet: Achieving High-Precision Adaptive Registration for Complex Die Castings" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09996v1.pdf filename=2403.09996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to their complex spatial structure and diverse geometric features, achieving high-precision and robust point cloud registration for complex Die Castings has been a significant challenge in the die-casting industry. Existing point cloud registration methods primarily optimize network models using well-established high-quality datasets, often neglecting practical application in real scenarios. To address this gap, this paper proposes a high-precision adaptive registration method called Multiscale Efficient Deep Closest Point (MEDPNet) and introduces a die-casting point cloud dataset, DieCastCloud, specifically designed to tackle the challenges of point cloud registration in the die-casting industry. The MEDPNet method performs coarse die-casting point cloud data registration using the Efficient-DCP method, followed by precision registration using the Multiscale feature fusion dual-channel registration (MDR) method. We enhance the modeling capability and computational efficiency of the model by replacing the attention mechanism of the <b>Transformer</b> in DCP with Efficient Attention and implementing a collaborative scale mechanism through the combination of serial and parallel blocks. Additionally, we propose the MDR method, which utilizes multilayer perceptrons (MLP), Normal Distributions Transform (NDT), and Iterative Closest Point (ICP) to achieve learnable adaptive fusion, enabling high-precision, scalable, and noise-resistant global point cloud registration. Our proposed method demonstrates excellent performance compared to state-of-the-art geometric and learning-based registration methods when applied to complex die-casting point cloud data.</p></p class="citation"></blockquote><h3 id=98104--98298-thermal-nerf-neural-radiance-fields-from-an-infrared-camera-tianxiang-ye-et-al-2024>(98/104 | 98/298) Thermal-NeRF: Neural Radiance Fields from an Infrared Camera (Tianxiang Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianxiang Ye, Qi Wu, Junyuan Deng, Guoqing Liu, Liu Liu, Songpengcheng Xia, Liang Pang, Wenxian Yu, Ling Pei. (2024)<br><strong>Thermal-NeRF: Neural Radiance Fields from an Infrared Camera</strong><br><button class=copy-to-clipboard title="Thermal-NeRF: Neural Radiance Fields from an Infrared Camera" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10340v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10340v1.pdf filename=2403.10340v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, Neural Radiance Fields (NeRFs) have demonstrated significant potential in encoding highly-detailed 3D <b>geometry</b> and environmental appearance, positioning themselves as a promising alternative to traditional explicit representation for 3D scene reconstruction. However, the predominant reliance on RGB imaging presupposes ideal lighting conditions: a premise frequently unmet in robotic applications plagued by poor lighting or visual obstructions. This limitation overlooks the capabilities of infrared (IR) cameras, which excel in low-light detection and present a robust alternative under such adverse scenarios. To tackle these issues, we introduce Thermal-NeRF, the first method that estimates a volumetric scene representation in the form of a NeRF solely from IR imaging. By leveraging a thermal mapping and structural thermal constraint derived from the thermal characteristics of IR imaging, our method showcasing unparalleled proficiency in recovering NeRFs in visually degraded scenes where RGB-based methods fall short. We conduct extensive experiments to demonstrate that Thermal-NeRF can achieve superior quality compared to existing methods. Furthermore, we contribute a dataset for IR-based NeRF applications, paving the way for future research in IR NeRF reconstruction.</p></p class="citation"></blockquote><h3 id=99104--99298-neca-neural-customizable-human-avatar-junjin-xiao-et-al-2024>(99/104 | 99/298) NECA: Neural Customizable Human Avatar (Junjin Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjin Xiao, Qing Zhang, Zhan Xu, Wei-Shi Zheng. (2024)<br><strong>NECA: Neural Customizable Human Avatar</strong><br><button class=copy-to-clipboard title="NECA: Neural Customizable Human Avatar" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10335v1.pdf filename=2403.10335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human avatar has become a novel type of 3D asset with various applications. Ideally, a human avatar should be fully customizable to accommodate different settings and environments. In this work, we introduce NECA, an approach capable of learning versatile human representation from monocular or sparse-view videos, enabling granular customization across aspects such as pose, shadow, shape, lighting and texture. The core of our approach is to represent humans in complementary dual spaces and predict disentangled neural fields of <b>geometry,</b> albedo, shadow, as well as an external lighting, from which we are able to derive realistic rendering with high-frequency details via volumetric rendering. Extensive experiments demonstrate the advantage of our method over the state-of-the-art methods in photorealistic rendering, as well as various editing tasks such as novel pose synthesis and relighting. The code is available at <a href=https://github.com/iSEE-Laboratory/NECA>https://github.com/iSEE-Laboratory/NECA</a>.</p></p class="citation"></blockquote><h3 id=100104--100298-coreecho-continuous-representation-learning-for-2dtime-echocardiography-analysis-fadillah-adamsyah-maani-et-al-2024>(100/104 | 100/298) CoReEcho: Continuous Representation Learning for 2D+time Echocardiography Analysis (Fadillah Adamsyah Maani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fadillah Adamsyah Maani, Numan Saeed, Aleksandr Matsun, Mohammad Yaqub. (2024)<br><strong>CoReEcho: Continuous Representation Learning for 2D+time Echocardiography Analysis</strong><br><button class=copy-to-clipboard title="CoReEcho: Continuous Representation Learning for 2D+time Echocardiography Analysis" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10164v1.pdf filename=2403.10164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning (DL) models have been advancing automatic medical image analysis on various modalities, including echocardiography, by offering a comprehensive end-to-end training pipeline. This approach enables DL models to regress ejection fraction (EF) directly from 2D+time echocardiograms, resulting in superior performance. However, the end-to-end training pipeline makes the learned <b>representations</b> <b>less</b> explainable. The <b>representations</b> <b>may</b> also fail to capture the continuous relation among echocardiogram clips, indicating the existence of spurious correlations, which can negatively affect the generalization. To mitigate this issue, we propose CoReEcho, a novel training framework emphasizing continuous <b>representations</b> <b>tailored</b> for direct EF regression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms the current state-of-the-art (SOTA) on the largest echocardiography dataset (EchoNet-Dynamic) with MAE of 3.90 & R2 of 82.44, and 2) provides robust and generalizable features that transfer more effectively in related downstream tasks. The code is publicly available at <a href=https://github.com/fadamsyah/CoReEcho>https://github.com/fadamsyah/CoReEcho</a>.</p></p class="citation"></blockquote><h3 id=101104--101298-kp-red-exploiting-semantic-keypoints-for-joint-3d-shape-retrieval-and-deformation-ruida-zhang-et-al-2024>(101/104 | 101/298) KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and Deformation (Ruida Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruida Zhang, Chenyangguang Zhang, Yan Di, Fabian Manhardt, Xingyu Liu, Federico Tombari, Xiangyang Ji. (2024)<br><strong>KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and Deformation</strong><br><button class=copy-to-clipboard title="KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and Deformation" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10099v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10099v2.pdf filename=2403.10099v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present KP-RED, a unified KeyPoint-driven REtrieval and Deformation framework that takes object scans as input and jointly retrieves and deforms the most geometrically similar CAD models from a pre-processed database to tightly match the target. Unlike existing dense matching based methods that typically struggle with noisy partial scans, we propose to leverage category-consistent sparse keypoints to naturally handle both full and partial object scans. Specifically, we first employ a lightweight retrieval module to establish a keypoint-based embedding space, measuring the similarity among objects by dynamically aggregating deformation-aware local-global features around extracted keypoints. Objects that are close in the embedding space are considered similar in <b>geometry.</b> Then we introduce the neural cage-based deformation module that estimates the influence vector of each keypoint upon cage vertices inside its local support region to control the deformation of the retrieved shape. Extensive experiments on the synthetic dataset PartNet and the real-world dataset Scan2CAD demonstrate that KP-RED surpasses existing state-of-the-art approaches by a large margin. Codes and trained models will be released in <a href=https://github.com/lolrudy/KP-RED>https://github.com/lolrudy/KP-RED</a>.</p></p class="citation"></blockquote><h3 id=102104--102298-texture-gs-disentangling-the-geometry-and-texture-for-3d-gaussian-splatting-editing-tian-xing-xu-et-al-2024>(102/104 | 102/298) Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing (Tian-Xing Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian-Xing Xu, Wenbo Hu, Yu-Kun Lai, Ying Shan, Song-Hai Zhang. (2024)<br><strong>Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing</strong><br><button class=copy-to-clipboard title="Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10050v1.pdf filename=2403.10050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D Gaussian splatting, emerging as a groundbreaking approach, has drawn increasing attention for its capabilities of high-fidelity reconstruction and real-time rendering. However, it couples the appearance and <b>geometry</b> of the scene within the Gaussian attributes, which hinders the flexibility of editing operations, such as texture swapping. To address this issue, we propose a novel approach, namely Texture-GS, to disentangle the appearance from the <b>geometry</b> by representing it as a 2D texture mapped onto the 3D surface, thereby facilitating appearance editing. Technically, the disentanglement is achieved by our proposed texture mapping module, which consists of a UV mapping MLP to learn the UV coordinates for the 3D Gaussian centers, a local Taylor expansion of the MLP to efficiently approximate the UV coordinates for the ray-Gaussian intersections, and a learnable texture to capture the fine-grained appearance. Extensive experiments on the DTU dataset demonstrate that our method not only facilitates high-fidelity appearance editing but also achieves real-time rendering on consumer-level devices, e.g. a single RTX 2080 Ti GPU.</p></p class="citation"></blockquote><h3 id=103104--103298-urs-nerf-unordered-rolling-shutter-bundle-adjustment-for-neural-radiance-fields-bo-xu-et-al-2024>(103/104 | 103/298) URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural Radiance Fields (Bo Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Xu, Ziao Liu, Mengqi Guo, Jiancheng Li, Gim Hee Li. (2024)<br><strong>URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural Radiance Fields</strong><br><button class=copy-to-clipboard title="URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural Radiance Fields" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10119v1.pdf filename=2403.10119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel rolling shutter bundle adjustment method for neural radiance fields (NeRF), which utilizes the unordered rolling shutter (RS) images to obtain the implicit 3D representation. Existing NeRF methods suffer from low-quality images and inaccurate initial camera poses due to the RS effect in the image, whereas, the previous method that incorporates the RS into NeRF requires strict sequential data input, limiting its widespread applicability. In constant, our method recovers the physical formation of RS images by estimating camera poses and velocities, thereby removing the input constraints on sequential data. Moreover, we adopt a coarse-to-fine training strategy, in which the RS epipolar constraints of the pairwise frames in the scene <b>graph</b> are used to detect the camera poses that fall into local minima. The poses detected as outliers are corrected by the interpolation method with neighboring poses. The experimental results validate the effectiveness of our method over state-of-the-art works and demonstrate that the reconstruction of 3D representations is not constrained by the requirement of video sequence input.</p></p class="citation"></blockquote><h3 id=104104--104298-skeleton-based-human-action-recognition-with-noisy-labels-yi-xu-et-al-2024>(104/104 | 104/298) Skeleton-Based Human Action Recognition with Noisy Labels (Yi Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Xu, Kunyu Peng, Di Wen, Ruiping Liu, Junwei Zheng, Yufan Chen, Jiaming Zhang, Alina Roitberg, Kailun Yang, Rainer Stiefelhagen. (2024)<br><strong>Skeleton-Based Human Action Recognition with Noisy Labels</strong><br><button class=copy-to-clipboard title="Skeleton-Based Human Action Recognition with Noisy Labels" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV, eess-IV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09975v1.pdf filename=2403.09975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding human actions from body poses is critical for assistive robots sharing space with humans in order to make informed and safe decisions about the next interaction. However, precise temporal localization and annotation of activity sequences is time-consuming and the resulting labels are often noisy. If not effectively addressed, label noise negatively affects the model&rsquo;s training, resulting in lower recognition quality. Despite its importance, addressing label noise for skeleton-based action recognition has been overlooked so far. In this study, we bridge this gap by implementing a framework that augments well-established skeleton-based human action recognition methods with label-denoising strategies from various research areas to serve as the initial <b>benchmark.</b> Observations reveal that these baselines yield only marginal performance when dealing with sparse skeleton data. Consequently, we introduce a novel methodology, NoiseEraSAR, which integrates global sample selection, co-teaching, and Cross-Modal Mixture-of-Experts (CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise. Our proposed approach demonstrates better performance on the established <b>benchmark,</b> setting new state-of-the-art standards. The source code for this study will be made accessible at <a href=https://github.com/xuyizdby/NoiseEraSAR>https://github.com/xuyizdby/NoiseEraSAR</a>.</p></p class="citation"></blockquote><h2 id=cscl-28>cs.CL (28)</h2><h3 id=128--105298-team-trifecta-at-factify5wqa-setting-the-standard-in-fact-verification-with-fine-tuning-shang-hsuan-chiang-et-al-2024>(1/28 | 105/298) Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning (Shang-Hsuan Chiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shang-Hsuan Chiang, Ming-Chih Lo, Lin-Wei Chao, Wen-Chih Peng. (2024)<br><strong>Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning</strong><br><button class=copy-to-clipboard title="Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, Fact Verification, Question Answering, Text Classification, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10281v1.pdf filename=2403.10281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of <b>Question</b> <b>Answering</b> and <b>Text</b> <b>Classification</b> components for <b>fact</b> <b>verification.</b> Leveraging <b>In-Context</b> <b>Learning,</b> <b>Fine-tuned</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> and the FakeNet model, we address the challenges of <b>fact</b> <b>verification.</b> Our experiments explore diverse approaches, comparing different Pre-trained <b>LLMs,</b> introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing <b>fact</b> <b>verification</b> research.</p></p class="citation"></blockquote><h3 id=228--106298-whose-side-are-you-on-investigating-the-political-stance-of-large-language-models-pagnarasmey-pit-et-al-2024>(2/28 | 106/298) Whose Side Are You On? Investigating the Political Stance of Large Language Models (Pagnarasmey Pit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pagnarasmey Pit, Xingjun Ma, Mike Conway, Qingyu Chen, James Bailey, Henry Pit, Putrasmey Keo, Watey Diep, Yu-Gang Jiang. (2024)<br><strong>Whose Side Are You On? Investigating the Political Stance of Large Language Models</strong><br><button class=copy-to-clipboard title="Whose Side Are You On? Investigating the Political Stance of Large Language Models" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-SI, cs.CL<br>Keyword Score: 80<br>Keywords: Fairness, Recommendation, Information Retrieval, Text Generation, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13840v1.pdf filename=2403.13840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have gained significant popularity for their application in various everyday tasks such as <b>text</b> <b>generation,</b> <b>summarization,</b> and <b>information</b> <b>retrieval.</b> As the widespread adoption of <b>LLMs</b> continues to surge, it becomes increasingly crucial to ensure that these models yield responses that are politically impartial, with the aim of preventing <b>information</b> <b>bubbles,</b> upholding <b>fairness</b> in representation, and mitigating confirmation bias. In this paper, we propose a quantitative framework and pipeline designed to systematically investigate the political orientation of <b>LLMs.</b> Our investigation delves into the political alignment of <b>LLMs</b> across a spectrum of eight polarizing topics, spanning from abortion to LGBTQ issues. Across topics, the results indicate that <b>LLMs</b> exhibit a tendency to provide responses that closely align with liberal or left-leaning perspectives rather than conservative or right-leaning ones when user queries include details pertaining to occupation, race, or political affiliation. The findings presented in this study not only reaffirm earlier observations regarding the left-leaning characteristics of <b>LLMs</b> but also surface particular attributes, such as occupation, that are particularly susceptible to such inclinations even when directly steered towards conservatism. As a <b>recommendation</b> to avoid these models providing politicised responses, users should be mindful when crafting queries, and exercise caution in selecting neutral <b>prompt</b> language.</p></p class="citation"></blockquote><h3 id=328--107298-read-between-the-lines----functionality-extraction-from-readmes-prince-kumar-et-al-2024>(3/28 | 107/298) Read between the lines &ndash; Functionality Extraction From READMEs (Prince Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prince Kumar, Srikanth Tamilselvam, Dinesh Garg. (2024)<br><strong>Read between the lines &ndash; Functionality Extraction From READMEs</strong><br><button class=copy-to-clipboard title="Read between the lines -- Functionality Extraction From READMEs" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 75<br>Keywords: Black Box, Fine-tuning, Bard, ChatGPT, Text Summarization, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10205v1.pdf filename=2403.10205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>text</b> <b>summarization</b> is a well-known NLP task, in this paper, we introduce a novel and useful variant of it called functionality extraction from Git README files. Though this task is a text2text generation at an abstract level, it involves its own peculiarities and challenges making existing text2text generation systems not very useful. The motivation behind this task stems from a recent surge in research and development activities around the use of <b>large</b> <b>language</b> <b>models</b> for code-related tasks, such as code refactoring, code <b>summarization,</b> etc. We also release a human-annotated dataset called FuncRead, and develop a battery of models for the task. Our exhaustive experimentation shows that small size <b>fine-tuned</b> models beat any baseline models that can be designed using popular <b>black-box</b> <b>or</b> white-box <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>ChatGPT</b> and <b>Bard.</b> Our best <b>fine-tuned</b> 7 Billion CodeLlama model exhibit 70% and 20% gain on the F1 score against <b>ChatGPT</b> and <b>Bard</b> respectively.</p></p class="citation"></blockquote><h3 id=428--108298-triple-gnns-introducing-syntactic-and-semantic-information-for-conversational-aspect-based-quadruple-sentiment-analysis-binbin-li-et-al-2024>(4/28 | 108/298) Triple GNNs: Introducing Syntactic and Semantic Information for Conversational Aspect-Based Quadruple Sentiment Analysis (Binbin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binbin Li, Yuqing Li, Siyu Jia, Bingnan Ma, Yu Ding, Zisen Qi, Xingbang Tan, Menghan Guo, Shenghui Liu. (2024)<br><strong>Triple GNNs: Introducing Syntactic and Semantic Information for Conversational Aspect-Based Quadruple Sentiment Analysis</strong><br><button class=copy-to-clipboard title="Triple GNNs: Introducing Syntactic and Semantic Information for Conversational Aspect-Based Quadruple Sentiment Analysis" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Graph Neural Network, Convolution, Convolutional Neural Network, Aspect-based Sentiment Analysis, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10065v1.pdf filename=2403.10065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational <b>Aspect-Based</b> <b>Sentiment</b> <b>Analysis</b> (DiaASQ) aims to detect quadruples {target, aspect, opinion, <b>sentiment</b> <b>polarity}</b> from given dialogues. In DiaASQ, elements constituting these quadruples are not necessarily confined to individual sentences but may span across multiple utterances within a dialogue. This necessitates a dual focus on both the syntactic information of individual utterances and the semantic interaction among them. However, previous studies have primarily focused on coarse-grained relationships between utterances, thus overlooking the potential benefits of detailed intra-utterance syntactic information and the granularity of inter-utterance relationships. This paper introduces the Triple <b>GNNs</b> network to enhance DiaAsQ. It employs a <b>Graph</b> <b>Convolutional</b> <b>Network</b> <b>(GCN)</b> for modeling syntactic dependencies within utterances and a Dual <b>Graph</b> <b>Attention</b> <b>Network</b> (DualGATs) to construct interactions between utterances. Experiments on two standard datasets reveal that our model significantly outperforms state-of-the-art baselines. The code is available at \url{https://github.com/nlperi2b/Triple-GNNs-}.</p></p class="citation"></blockquote><h3 id=528--109298-enhancing-llm-factual-accuracy-with-rag-to-counter-hallucinations-a-case-study-on-domain-specific-queries-in-private-knowledge-bases-jiarui-li-et-al-2024>(5/28 | 109/298) Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases (Jiarui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiarui Li, Ye Yuan, Zehua Zhang. (2024)<br><strong>Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases</strong><br><button class=copy-to-clipboard title="Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10446v1.pdf filename=2403.10446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We proposed an end-to-end system design towards utilizing <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)</b> to improve the factual accuracy of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates <b>RAG</b> pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of <b>LLM</b> hallucinations, we <b>finetune</b> models with a curated dataset which originates from CMU&rsquo;s extensive resources and annotated with the teacher model. Our experiments demonstrate the system&rsquo;s effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of <b>fine-tuning</b> <b>LLMs</b> with small-scale and skewed datasets. This research highlights the potential of <b>RAG</b> systems in augmenting <b>LLMs</b> with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available on Github.</p></p class="citation"></blockquote><h3 id=628--110298-trisum-learning-summarization-ability-from-large-language-models-with-structured-rationale-pengcheng-jiang-et-al-2024>(6/28 | 110/298) TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale (Pengcheng Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengcheng Jiang, Cao Xiao, Zifeng Wang, Parminder Bhatia, Jimeng Sun, Jiawei Han. (2024)<br><strong>TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale</strong><br><button class=copy-to-clipboard title="TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Curriculum Learning, Knowledge Distillation, Text Summarization, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10351v1.pdf filename=2403.10351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has significantly advanced natural language processing tasks like <b>text</b> <b>summarization.</b> However, their <b>large</b> <b>size</b> <b>and</b> computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings. To overcome this, we introduce TriSum, a framework for <b>distilling</b> <b>LLMs&rsquo;</b> <b>text</b> <b>summarization</b> abilities into a compact, local model. Initially, <b>LLMs</b> extract a set of aspect-triple rationales and summaries, which are refined using a dual-scoring method for quality. Next, a smaller local model is trained with these tasks, employing a <b>curriculum</b> <b>learning</b> strategy that evolves from simple to complex tasks. Our method enhances local model performance on various <b>benchmarks</b> (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by 4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by providing insights into the <b>summarization</b> rationale.</p></p class="citation"></blockquote><h3 id=728--111298-dragin-dynamic-retrieval-augmented-generation-based-on-the-real-time-information-needs-of-large-language-models-weihang-su-et-al-2024>(7/28 | 111/298) DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models (Weihang Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, Yiqun Liu. (2024)<br><strong>DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models</strong><br><button class=copy-to-clipboard title="DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 60<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10081v1.pdf filename=2403.10081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)</b> paradigm actively decides when and what to retrieve during the <b>text</b> <b>generation</b> process of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> There are two key elements of this paradigm: identifying the optimal moment to activate the <b>retrieval</b> <b>module</b> <b>(deciding</b> when to retrieve) and crafting the appropriate query once <b>retrieval</b> <b>is</b> <b>triggered</b> (determining what to retrieve). However, current dynamic <b>RAG</b> methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the <b>LLM&rsquo;s</b> most recent sentence or the last few tokens, while the <b>LLM&rsquo;s</b> real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> based on the real-time Information Needs of <b>LLMs.</b> Our framework is specifically designed to make decisions on when and what to retrieve based on the <b>LLM&rsquo;s</b> real-time information needs during the <b>text</b> <b>generation</b> process. We evaluate DRAGIN along with existing methods comprehensively over 4 knowledge-intensive generation datasets. Experimental results show that DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of our method. We have open-sourced all the code, data, and models in GitHub: <a href=https://github.com/oneal2000/DRAGIN/tree/main>https://github.com/oneal2000/DRAGIN/tree/main</a></p></p class="citation"></blockquote><h3 id=828--112298-intent-conditioned-and-non-toxic-counterspeech-generation-using-multi-task-instruction-tuning-with-rlaif-amey-hengle-et-al-2024>(8/28 | 112/298) Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF (Amey Hengle et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amey Hengle, Aswini Kumar, Sahajpreet Singh, Anil Bandhakavi, Md Shad Akhtar, Tanmoy Chakroborty. (2024)<br><strong>Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF</strong><br><button class=copy-to-clipboard title="Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Reinforcement Learning, ChatGPT, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10088v1.pdf filename=2403.10088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Counterspeech, defined as a response to mitigate online hate speech, is increasingly used as a non-censorial solution. Addressing hate speech effectively involves dispelling the stereotypes, prejudices, and biases often subtly implied in brief, single-sentence statements or abuses. These implicit expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts. Our study introduces CoARL, a novel framework enhancing counterspeech generation by modeling the pragmatic implications underlying social biases in hateful statements. CoARL&rsquo;s first two phases involve sequential multi-instruction tuning, teaching the model to understand intents, reactions, and harms of offensive statements, and then learning task-specific low-rank adapter weights for generating intent-conditioned counterspeech. The final phase uses <b>reinforcement</b> <b>learning</b> to <b>fine-tune</b> outputs for effectiveness and non-toxicity. CoARL outperforms existing <b>benchmarks</b> in intent-conditioned counterspeech generation, showing an average improvement of 3 points in intent-conformity and 4 points in argument-quality metrics. Extensive human evaluation supports CoARL&rsquo;s efficacy in generating superior and more context-appropriate responses compared to existing systems, including prominent <b>LLMs</b> like <b>ChatGPT.</b></p></p class="citation"></blockquote><h3 id=928--113298-investigating-grammatical-abstraction-in-language-models-using-few-shot-learning-of-novel-noun-gender-priyanka-sukumaran-et-al-2024>(9/28 | 113/298) Investigating grammatical abstraction in language models using few-shot learning of novel noun gender (Priyanka Sukumaran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Priyanka Sukumaran, Conor Houghton, Nina Kazanina. (2024)<br><strong>Investigating grammatical abstraction in language models using few-shot learning of novel noun gender</strong><br><button class=copy-to-clipboard title="Investigating grammatical abstraction in language models using few-shot learning of novel noun gender" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Few-shot, Few-shot Learning, LSTM, Transformer, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10338v1.pdf filename=2403.10338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans can learn a new <b>word</b> <b>and</b> infer its grammatical properties from very few examples. They have an abstract notion of linguistic properties like grammatical gender and agreement rules that can be applied to novel syntactic contexts and <b>words.</b> <b>Drawing</b> inspiration from psycholinguistics, we conduct a noun learning experiment to assess whether an <b>LSTM</b> and a decoder-only <b>transformer</b> can achieve human-like abstraction of grammatical gender in French. Language models were tasked with learning the gender of a novel noun embedding from a few examples in one grammatical agreement context and predicting agreement in another, unseen context. We find that both language models effectively generalise novel noun gender from one to two learning examples and apply the learnt gender across agreement contexts, albeit with a bias for the masculine gender category. Importantly, the <b>few-shot</b> <b>updates</b> were only applied to the embedding layers, demonstrating that models encode sufficient gender information within the <b>word</b> <b>embedding</b> space. While the generalisation behaviour of models suggests that they represent grammatical gender as an abstract category, like humans, further work is needed to explore the details of how exactly this is implemented. For a comparative perspective with human behaviour, we conducted an analogous one-shot novel noun gender learning experiment, which revealed that native French speakers, like language models, also exhibited a masculine gender bias and are not excellent one-shot learners either.</p></p class="citation"></blockquote><h3 id=1028--114298-exams-v-a-multi-discipline-multilingual-multimodal-exam-benchmark-for-evaluating-vision-language-models-rocktim-jyoti-das-et-al-2024>(10/28 | 114/298) EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models (Rocktim Jyoti Das et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rocktim Jyoti Das, Simeon Emilov Hristov, Haonan Li, Dimitar Iliyanov Dimitrov, Ivan Koychev, Preslav Nakov. (2024)<br><strong>EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models</strong><br><button class=copy-to-clipboard title="EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, GPT, Gemini, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10378v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10378v1.pdf filename=2403.10378v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce EXAMS-V, a new challenging multi-discipline <b>multimodal</b> multilingual exam <b>benchmark</b> for evaluating vision language models. It consists of 20,932 multiple-choice questions across 20 school disciplines covering natural science, social science, and other miscellaneous studies, e.g., religion, fine arts, business, etc. EXAMS-V includes a variety of <b>multimodal</b> features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in 11 languages from 7 language families. Unlike existing <b>benchmarks,</b> EXAMS-V is uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate <b>reasoning</b> across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint <b>reasoning</b> over the text and the visual content of the image. Our evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision-text models such as <b>GPT-4V</b> and <b>Gemini;</b> this underscores the inherent complexity of the dataset and its significance as a future <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=1128--115298-uncovering-latent-themes-of-messaging-on-social-media-by-integrating-llms-a-case-study-on-climate-campaigns-tunazzina-islam-et-al-2024>(11/28 | 115/298) Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns (Tunazzina Islam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tunazzina Islam, Dan Goldwasser. (2024)<br><strong>Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns</strong><br><button class=copy-to-clipboard title="Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs-SI, cs.CL<br>Keyword Score: 40<br>Keywords: Topic Model, human-in-the-loop, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10707v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10707v1.pdf filename=2403.10707v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel approach to uncovering and analyzing themes in social media messaging. Recognizing the limitations of traditional <b>topic-level</b> <b>analysis,</b> which tends to capture only the overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Conventional methods of theme discovery, involving manual processes and a <b>human-in-the-loop</b> approach, are valuable but face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> This approach allows for a deeper investigation into the thematic aspects of social media discourse, enabling us to uncover a diverse array of themes, each with unique characteristics and relevance, thereby offering a comprehensive understanding of the nuances present within broader <b>topics.</b> <b>Furthermore,</b> this method efficiently maps the text and the newly discovered themes, enhancing our understanding of the thematic nuances in social media messaging. We employ climate campaigns as a case study and demonstrate that our methodology yields more accurate and interpretable results compared to traditional <b>topic</b> <b>models.</b> Our results not only demonstrate the effectiveness of our approach in uncovering latent themes but also illuminate how these themes are tailored for demographic targeting in social media contexts. Additionally, our work sheds light on the dynamic nature of social media, revealing the shifts in the thematic focus of messaging in response to real-world events.</p></p class="citation"></blockquote><h3 id=1228--116298-explorer-exploration-guided-reasoning-for-textual-reinforcement-learning-kinjal-basu-et-al-2024>(12/28 | 116/298) EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning (Kinjal Basu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kinjal Basu, Keerthiram Murugesan, Subhajit Chaudhury, Murray Campbell, Kartik Talamadupula, Tim Klinger. (2024)<br><strong>EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning</strong><br><button class=copy-to-clipboard title="EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LO, cs.CL<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Natural Language Understanding, Reasoning, Textual Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10692v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10692v1.pdf filename=2403.10692v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-based games (TBGs) have emerged as an important collection of NLP tasks, requiring <b>reinforcement</b> <b>learning</b> (RL) agents to combine <b>natural</b> <b>language</b> <b>understanding</b> with <b>reasoning.</b> A key challenge for agents attempting to solve such tasks is to generalize across multiple games and demonstrate good performance on both seen and unseen objects. Purely deep-RL-based approaches may perform well on seen objects; however, they fail to showcase the same performance on unseen objects. Commonsense-infused deep-RL agents may work better on unseen data; unfortunately, their policies are often not interpretable or easily transferable. To tackle these issues, in this paper, we present EXPLORER which is an exploration-guided <b>reasoning</b> agent for <b>textual</b> <b>reinforcement</b> <b>learning.</b> EXPLORER is neurosymbolic in nature, as it relies on a neural module for exploration and a symbolic module for exploitation. It can also learn generalized symbolic policies and perform well over unseen data. Our experiments show that EXPLORER outperforms the baseline agents on Text-World cooking (TW-Cooking) and Text-World Commonsense (TWC) games.</p></p class="citation"></blockquote><h3 id=1328--117298-neural-erosion-emulating-controlled-neurodegeneration-and-aging-in-ai-systems-antonios-alexos-et-al-2024>(13/28 | 117/298) Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems (Antonios Alexos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonios Alexos, Yu-Dai Tsai, Ian Domingo, Maryam Pishgar, Pierre Baldi. (2024)<br><strong>Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems</strong><br><button class=copy-to-clipboard title="Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL, q-bio-NC<br>Keyword Score: 40<br>Keywords: LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10596v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10596v1.pdf filename=2403.10596v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Creating controlled methods to simulate neurodegeneration in artificial intelligence (AI) is crucial for applications that emulate brain function decline and cognitive disorders. We use IQ tests performed by <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and, more specifically, the <b>LLaMA</b> 2 to introduce the concept of ``neural erosion." This deliberate erosion involves ablating synapses or neurons, or adding Gaussian noise during or after training, resulting in a controlled progressive decline in the <b>LLMs&rsquo;</b> performance. We are able to describe the neurodegeneration in the IQ tests and show that the <b>LLM</b> first loses its mathematical abilities and then its linguistic abilities, while further losing its ability to understand the questions. To the best of our knowledge, this is the first work that models neurodegeneration with text data, compared to other works that operate in the computer vision domain. Finally, we draw similarities between our study and cognitive decline clinical studies involving test subjects. We find that with the application of neurodegenerative methods, <b>LLMs</b> lose abstract thinking abilities, followed by mathematical degradation, and ultimately, a loss in linguistic ability, responding to <b>prompts</b> incoherently. These findings are in accordance with human studies.</p></p class="citation"></blockquote><h3 id=1428--118298-raft-adapting-language-model-to-domain-specific-rag-tianjun-zhang-et-al-2024>(14/28 | 118/298) RAFT: Adapting Language Model to Domain Specific RAG (Tianjun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, Joseph E. Gonzalez. (2024)<br><strong>RAFT: Adapting Language Model to Domain Specific RAG</strong><br><button class=copy-to-clipboard title="RAFT: Adapting Language Model to Domain Specific RAG" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10131v1.pdf filename=2403.10131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretraining <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> on <b>large</b> <b>corpora</b> <b>of</b> textual data is now a standard paradigm. When using these <b>LLMs</b> for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through <b>RAG-based-prompting,</b> or <b>fine-tuning.</b> However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented <b>FineTuning</b> (RAFT), a training recipe that improves the model&rsquo;s ability to answer questions in a &ldquo;open-book&rdquo; in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don&rsquo;t help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT&rsquo;s chain-of-thought-style response helps improve the model&rsquo;s ability to reason. In domain-specific <b>RAG,</b> RAFT consistently improves the model&rsquo;s performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained <b>LLMs</b> to in-domain <b>RAG.</b> RAFT&rsquo;s code and demo are open-sourced at github.com/ShishirPatil/gorilla.</p></p class="citation"></blockquote><h3 id=1528--119298-dont-half-listen-capturing-key-part-information-in-continual-instruction-tuning-yongquan-he-et-al-2024>(15/28 | 119/298) Don&rsquo;t Half-listen: Capturing Key-part Information in Continual Instruction Tuning (Yongquan He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongquan He, Xuancheng Huang, Minghao Tang, Lingxun Meng, Xiang Li, Wei Lin, Wenyuan Zhang, Yifu Gao. (2024)<br><strong>Don&rsquo;t Half-listen: Capturing Key-part Information in Continual Instruction Tuning</strong><br><button class=copy-to-clipboard title="Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Instruction Following, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10056v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10056v1.pdf filename=2403.10056v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>tuning</b> for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual <b>instruction</b> <b>tuning</b> (CIT) for <b>LLMs</b> may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of <b>instructions</b> <b>and</b> get confused on held-out tasks. In this paper, we propose a novel continual <b>instruction</b> <b>tuning</b> method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables <b>LLMs</b> to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in <b>instructions.</b> <b>In</b> addition, we propose two metrics, P-score and V-score, to measure the generalization and <b>instruction-following</b> <b>abilities</b> of <b>LLMs.</b> Experiments demonstrate our method achieves superior performance on both seen and held-out tasks.</p></p class="citation"></blockquote><h3 id=1628--120298-uni-smart-universal-science-multimodal-analysis-and-research-transformer-hengxing-cai-et-al-2024>(16/28 | 120/298) Uni-SMART: Universal Science Multimodal Analysis and Research Transformer (Hengxing Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hengxing Cai, Xiaochen Cai, Shuwen Yang, Jiankun Wang, Lin Yao, Zhifeng Gao, Junhan Chang, Sihang Li, Mingjun Xu, Changxin Wang, Hongshuai Wang, Yongge Li, Mujie Lin, Yaqi Li, Yuqi Yin, Linfeng Zhang, Guolin Ke. (2024)<br><strong>Uni-SMART: Universal Science Multimodal Analysis and Research Transformer</strong><br><button class=copy-to-clipboard title="Uni-SMART: Universal Science Multimodal Analysis and Research Transformer" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10301v1.pdf filename=2403.10301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In scientific research and its application, scientific literature analysis is crucial as it allows researchers to build on the work of others. However, the fast growth of scientific knowledge has led to a massive increase in scholarly articles, making in-depth literature analysis increasingly challenging and time-consuming. The emergence of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has offered a new way to address this challenge. Known for their strong abilities in summarizing texts, <b>LLMs</b> are seen as a potential tool to improve the analysis of scientific literature. However, existing <b>LLMs</b> have their own limits. Scientific literature often includes a wide range of <b>multimodal</b> elements, such as molecular structure, tables, and charts, which are hard for text-focused <b>LLMs</b> to understand and analyze. This issue points to the urgent need for new solutions that can fully understand and analyze <b>multimodal</b> content in scientific literature. To answer this demand, we present Uni-SMART (Universal Science <b>Multimodal</b> Analysis and Research <b>Transformer),</b> an innovative model designed for in-depth understanding of <b>multimodal</b> scientific literature. Through rigorous quantitative evaluation across several domains, Uni-SMART demonstrates superior performance over leading text-focused <b>LLMs.</b> Furthermore, our exploration extends to practical applications, including patent infringement detection and nuanced analysis of charts. These applications not only highlight Uni-SMART&rsquo;s adaptability but also its potential to revolutionize how we interact with scientific literature.</p></p class="citation"></blockquote><h3 id=1728--121298-take-care-of-your-prompt-bias-investigating-and-mitigating-prompt-bias-in-factual-knowledge-extraction-ziyang-xu-et-al-2024>(17/28 | 121/298) Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction (Ziyang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyang Xu, Keqin Peng, Liang Ding, Dacheng Tao, Xiliang Lu. (2024)<br><strong>Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction</strong><br><button class=copy-to-clipboard title="Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09963v1.pdf filename=2403.09963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research shows that <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> suffer from <b>&ldquo;prompt</b> bias&rdquo; in factual knowledge extraction, i.e., <b>prompts</b> tend to introduce biases toward specific labels. However, the extent and impact of <b>prompt</b> bias within the model remain underexplored. In response, this paper quantifies the bias with various types of <b>prompts</b> and assesses their impact on different <b>benchmarks.</b> We show that: 1) all <b>prompts</b> in the experiments exhibit non-negligible bias, with gradient-based <b>prompts</b> like AutoPrompt and OptiPrompt displaying significantly higher levels of bias; 2) <b>prompt</b> bias can amplify <b>benchmark</b> accuracy unreasonably by overfitting the test datasets, especially on imbalanced datasets like LAMA. Based on these findings, we propose a representation-based approach to mitigate the <b>prompt</b> bias during inference time. Specifically, we first estimate the biased representation using <b>prompt-only</b> querying, and then remove it from the model&rsquo;s internal representations to generate the debiased representations, which are used to produce the final debiased outputs. Experiments across various <b>prompts,</b> <b>PLMs,</b> and <b>benchmarks</b> show that our approach can not only correct the overfitted performance caused by <b>prompt</b> bias, but also significantly improve the <b>prompt</b> retrieval capability (up to 10% absolute performance gain). Our findings shed new light on the underlying predicting mechanisms of <b>prompt-based</b> queries in <b>PLMs.</b> Hopefully, our plug-and-play approach can be a golden standard to strengthen <b>PLMs</b> toward reliable knowledge bases. Code and data are released in <a href=https://github.com/FelliYang/PromptBias>https://github.com/FelliYang/PromptBias</a>.</p></p class="citation"></blockquote><h3 id=1828--122298-is-translation-all-you-need-a-study-on-solving-multilingual-tasks-with-large-language-models-chaoqun-liu-et-al-2024>(18/28 | 122/298) Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models (Chaoqun Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoqun Liu, Wenxuan Zhang, Yiran Zhao, Anh Tuan Luu, Lidong Bing. (2024)<br><strong>Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models</strong><br><button class=copy-to-clipboard title="Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10258v1.pdf filename=2403.10258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated strong multilingual capabilities; yet, they are mostly English-centric due to the imbalanced training corpora. Existing works leverage this phenomenon to improve their multilingual performances on NLP tasks. In this work, we extend the evaluation from NLP tasks to real user queries. We find that even though translation into English can help improve the performance of multilingual NLP tasks for English-centric <b>LLMs,</b> it may not be optimal for all scenarios. For culture-related tasks that need deep language understanding, <b>prompting</b> in the native language proves to be more promising since it can capture the nuances related to culture and language. Therefore, we advocate for more efforts towards the development of strong multilingual <b>LLMs</b> instead of just English-centric <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1928--123298-lost-in-overlap-exploring-watermark-collision-in-llms-yiyang-luo-et-al-2024>(19/28 | 123/298) Lost in Overlap: Exploring Watermark Collision in LLMs (Yiyang Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyang Luo, Ke Lin, Chao Gu. (2024)<br><strong>Lost in Overlap: Exploring Watermark Collision in LLMs</strong><br><button class=copy-to-clipboard title="Lost in Overlap: Exploring Watermark Collision in LLMs" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-MM, cs.CL<br>Keyword Score: 30<br>Keywords: Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10020v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10020v1.pdf filename=2403.10020v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread use of watermarking across diverse <b>LLMs</b> has led to an inevitable issue known as watermark collision during common tasks like <b>question</b> <b>answering</b> and paraphrasing. This study focuses on dual watermark collisions, where two watermarks are present simultaneously in the same text. The research demonstrates that watermark collision poses a threat to detection performance for detectors of both upstream and downstream watermark algorithms.</p></p class="citation"></blockquote><h3 id=2028--124298-think-twice-before-assure-confidence-estimation-for-large-language-models-through-reflection-on-multiple-answers-moxin-li-et-al-2024>(20/28 | 124/298) Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers (Moxin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Moxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, Tat-Seng Chua. (2024)<br><strong>Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers</strong><br><button class=copy-to-clipboard title="Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 25<br>Keywords: Black Box, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09972v1.pdf filename=2403.09972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Confidence estimation aiming to evaluate output trustability is crucial for the application of <b>large</b> <b>language</b> <b>models</b> <b>(LLM),</b> especially the <b>black-box</b> <b>ones.</b> Existing confidence estimation of <b>LLM</b> is typically not calibrated due to the overconfidence of <b>LLM</b> on its generated incorrect answers. Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by <b>LLM.</b> To tackle this limitation, we propose a novel paradigm that thoroughly evaluates the trustability of multiple candidate answers to mitigate the overconfidence on incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs <b>LLM</b> to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation. This framework can be integrated with existing confidence estimation approaches for superior calibration. Experimental results on six datasets of three tasks demonstrate the rationality and effectiveness of the proposed framework.</p></p class="citation"></blockquote><h3 id=2128--125298-myte-morphology-driven-byte-encoding-for-better-and-fairer-multilingual-language-modeling-tomasz-limisiewicz-et-al-2024>(21/28 | 125/298) MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling (Tomasz Limisiewicz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tomasz Limisiewicz, Terra Blevins, Hila Gonen, Orevaoghene Ahia, Luke Zettlemoyer. (2024)<br><strong>MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling</strong><br><button class=copy-to-clipboard title="MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: High-Resource, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10691v1.pdf filename=2403.10691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the world&rsquo;s writing systems, they exhibit bias towards the <b>high-resource</b> languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and diminishes the <b>perplexity</b> gap throughout diverse languages.</p></p class="citation"></blockquote><h3 id=2228--126298-cdgp-automatic-cloze-distractor-generation-based-on-pre-trained-language-model-shang-hsuan-chiang-et-al-2024>(22/28 | 126/298) CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model (Shang-Hsuan Chiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shang-Hsuan Chiang, Ssu-Cheng Wang, Yao-Chung Fan. (2024)<br><strong>CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model</strong><br><button class=copy-to-clipboard title="CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10326v1.pdf filename=2403.10326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Manually designing cloze test consumes enormous time and efforts. The major challenge lies in wrong option (distractor) selection. Having carefully-design distractors improves the effectiveness of learner ability assessment. As a result, the idea of automatically generating cloze distractor is motivated. In this paper, we investigate cloze distractor generation by exploring the employment of <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> as an alternative for candidate distractor generation. Experiments show that the <b>PLM-enhanced</b> model brings a substantial performance improvement. Our best performing model advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our code and dataset is available at <a href=https://github.com/AndyChiangSH/CDGP>https://github.com/AndyChiangSH/CDGP</a>.</p></p class="citation"></blockquote><h3 id=2328--127298-a-question-on-the-explainability-of-large-language-models-and-the-word-level-univariate-first-order-plausibility-assumption-jeremie-bogaert-et-al-2024>(23/28 | 127/298) A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption (Jeremie Bogaert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeremie Bogaert, Francois-Xavier Standaert. (2024)<br><strong>A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption</strong><br><button class=copy-to-clipboard title="A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10275v1.pdf filename=2403.10275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The explanations of <b>large</b> <b>language</b> <b>models</b> have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity. In this paper, we propose a characterization that questions the possibility to provide simple and informative explanations for such models. To this end, we give statistical definitions for the explanations&rsquo; signal, noise and signal-to-noise ratio. We highlight that, in a typical case study where word-level univariate explanations are analyzed with first-order statistical tools, the explanations of simple feature-based models carry more signal and less noise than those of <b>transformer</b> ones. We then discuss the possibility to improve these results with alternative definitions of signal and noise that would capture more complex explanations and analysis methods, while also questioning the tradeoff with their plausibility for readers.</p></p class="citation"></blockquote><h3 id=2428--128298-a-big-data-approach-to-understand-sub-national-determinants-of-fdi-in-africa-a-fronzetti-colladon-et-al-2024>(24/28 | 128/298) A Big Data Approach to Understand Sub-national Determinants of FDI in Africa (A. Fronzetti Colladon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>A. Fronzetti Colladon, R. Vestrelli, S. Bait, M. M. Schiraldi. (2024)<br><strong>A Big Data Approach to Understand Sub-national Determinants of FDI in Africa</strong><br><button class=copy-to-clipboard title="A Big Data Approach to Understand Sub-national Determinants of FDI in Africa" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; J-4; H-4-0, cs-CL, cs.CL, econ-EM, physics-soc-ph<br>Keyword Score: 10<br>Keywords: Text Mining<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10239v1.pdf filename=2403.10239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Various macroeconomic and institutional factors hinder FDI inflows, including corruption, trade openness, access to finance, and political instability. Existing research mostly focuses on country-level data, with limited exploration of firm-level data, especially in developing countries. Recognizing this gap, recent calls for research emphasize the need for qualitative data analysis to delve into FDI determinants, particularly at the regional level. This paper proposes a novel methodology, based on <b>text</b> <b>mining</b> and social network analysis, to get information from more than 167,000 online news articles to quantify regional-level (sub-national) attributes affecting FDI ownership in African companies. Our analysis extends information on obstacles to industrial development as mapped by the World Bank Enterprise Surveys. Findings suggest that regional (sub-national) structural and institutional characteristics can play an important role in determining foreign ownership.</p></p class="citation"></blockquote><h3 id=2528--129298-enhanced-coherence-aware-network-with-hierarchical-disentanglement-for-aspect-category-sentiment-analysis-jin-cui-et-al-2024>(25/28 | 129/298) Enhanced Coherence-Aware Network with Hierarchical Disentanglement for Aspect-Category Sentiment Analysis (Jin Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jin Cui, Fumiyo Fukumoto, Xinfeng Wang, Yoshimi Suzuki, Jiyi Li, Noriko Tomuro, Wanzeng Kong. (2024)<br><strong>Enhanced Coherence-Aware Network with Hierarchical Disentanglement for Aspect-Category Sentiment Analysis</strong><br><button class=copy-to-clipboard title="Enhanced Coherence-Aware Network with Hierarchical Disentanglement for Aspect-Category Sentiment Analysis" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10214v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10214v1.pdf filename=2403.10214v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aspect-category-based <b>sentiment</b> <b>analysis</b> (ACSA), which aims to identify aspect categories and predict their <b>sentiments</b> <b>has</b> been intensively studied due to its wide range of NLP applications. Most approaches mainly utilize intrasentential features. However, a review often includes multiple different aspect categories, and some of them do not explicitly appear in the review. Even in a sentence, there is more than one aspect category with its <b>sentiments,</b> <b>and</b> they are entangled intra-sentence, which makes the model fail to discriminately preserve all <b>sentiment</b> <b>characteristics.</b> In this paper, we propose an enhanced coherence-aware network with hierarchical disentanglement (ECAN) for ACSA tasks. Specifically, we explore coherence modeling to capture the contexts across the whole review and to help the implicit aspect and <b>sentiment</b> <b>identification.</b> To address the issue of multiple aspect categories and <b>sentiment</b> <b>entanglement,</b> we propose a hierarchical disentanglement module to extract distinct categories and <b>sentiment</b> <b>features.</b> Extensive experimental and visualization results show that our ECAN effectively decouples multiple categories and <b>sentiments</b> <b>entangled</b> in the coherence representations and achieves state-of-the-art (SOTA) performance. Our codes and data are available online: \url{https://github.com/cuijin-23/ECAN}.</p></p class="citation"></blockquote><h3 id=2628--130298-identifying-health-risks-from-family-history-a-survey-of-natural-language-processing-techniques-xiang-dai-et-al-2024>(26/28 | 130/298) Identifying Health Risks from Family History: A Survey of Natural Language Processing Techniques (Xiang Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Dai, Sarvnaz Karimi, Nathan O&rsquo;Callaghan. (2024)<br><strong>Identifying Health Risks from Family History: A Survey of Natural Language Processing Techniques</strong><br><button class=copy-to-clipboard title="Identifying Health Risks from Family History: A Survey of Natural Language Processing Techniques" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09997v1.pdf filename=2403.09997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electronic health records include information on patients&rsquo; status and medical history, which could cover the history of diseases and disorders that could be hereditary. One important use of family history information is in precision health, where the goal is to keep the population healthy with preventative measures. Natural Language Processing (NLP) and machine learning techniques can assist with identifying information that could assist health professionals in identifying health risks before a condition is developed in their later years, saving lives and reducing healthcare costs. We survey the literature on the techniques from the NLP field that have been developed to utilise digital health records to identify risks of familial diseases. We highlight that rule-based methods are heavily investigated and are still actively used for family history extraction. Still, more recent efforts have been put into building neural models based on large-scale <b>pre-trained</b> <b>language</b> <b>models.</b> In addition to the areas where NLP has successfully been utilised, we also identify the areas where more research is needed to unlock the value of patients&rsquo; records regarding data collection, task formulation and downstream applications.</p></p class="citation"></blockquote><h3 id=2728--131298-maibaam-a-multi-dialectal-bavarian-universal-dependency-treebank-verena-blaschke-et-al-2024>(27/28 | 131/298) MaiBaam: A Multi-Dialectal Bavarian Universal Dependency Treebank (Verena Blaschke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Verena Blaschke, Barbara Kovačić, Siyao Peng, Hinrich Schütze, Barbara Plank. (2024)<br><strong>MaiBaam: A Multi-Dialectal Bavarian Universal Dependency Treebank</strong><br><button class=copy-to-clipboard title="MaiBaam: A Multi-Dialectal Bavarian Universal Dependency Treebank" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10293v1.pdf filename=2403.10293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the success of the Universal Dependencies (UD) project exemplified by its impressive language breadth, there is still a lack in `within-language breadth&rsquo;: most treebanks focus on standard languages. Even for German, the language with the most annotations in UD, so far no treebank exists for one of its language varieties spoken by over 10M people: Bavarian. To contribute to closing this gap, we present the first multi-dialect Bavarian treebank (MaiBaam) manually annotated with part-of-speech and syntactic dependency information in UD, covering multiple text genres (wiki, fiction, grammar examples, social, non-fiction). We highlight the morphosyntactic differences between the closely-related Bavarian and German and showcase the rich variability of speakers&rsquo; orthographies. Our corpus includes 15k tokens, covering dialects from all Bavarian-speaking areas spanning three countries. We provide baseline parsing and POS tagging results, which are lower than results obtained on German and vary substantially between different <b>graph-based</b> parsers. To support further research on Bavarian syntax, we make our dataset, language-specific guidelines and code publicly available.</p></p class="citation"></blockquote><h3 id=2828--132298-a-comprehensive-study-on-frequent-pattern-mining-and-clustering-categories-for-topic-detection-in-persian-text-stream-elnaz-zafarani-moattar-et-al-2024>(28/28 | 132/298) A comprehensive study on Frequent Pattern Mining and Clustering categories for topic detection in Persian text stream (Elnaz Zafarani-Moattar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elnaz Zafarani-Moattar, Mohammad Reza Kangavari, Amir Masoud Rahmani. (2024)<br><strong>A comprehensive study on Frequent Pattern Mining and Clustering categories for topic detection in Persian text stream</strong><br><button class=copy-to-clipboard title="A comprehensive study on Frequent Pattern Mining and Clustering categories for topic detection in Persian text stream" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10237v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10237v1.pdf filename=2403.10237v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Topic detection is a complex process and depends on language because it somehow needs to analyze text. There have been few studies on topic detection in Persian, and the existing algorithms are not remarkable. Therefore, we aimed to study topic detection in Persian. The objectives of this study are: 1) to conduct an extensive study on the best algorithms for topic detection, 2) to identify necessary adaptations to make these algorithms suitable for the Persian language, and 3) to evaluate their performance on Persian social network texts. To achieve these objectives, we have formulated two research questions: First, considering the lack of research in Persian, what modifications should be made to existing frameworks, especially those developed in English, to make them compatible with Persian? Second, how do these algorithms perform, and which one is superior? There are various topic detection methods that can be categorized into different categories. Frequent pattern and <b>clustering</b> are selected for this research, and a hybrid of both is proposed as a new category. Then, ten methods from these three categories are selected. All of them are re-implemented from scratch, changed, and adapted with Persian. These ten methods encompass different types of topic detection methods and have shown good performance in English. The text of Persian social network posts is used as the dataset. Additionally, a new multiclass evaluation criterion, called FS, is used in this paper for the first time in the field of topic detection. Approximately 1.4 billion tokens are processed during experiments. The results indicate that if we are searching for keyword-topics that are easily understandable by humans, the hybrid category is better. However, if the aim is to cluster posts for further analysis, the frequent pattern category is more suitable.</p></p class="citation"></blockquote><h2 id=csir-5>cs.IR (5)</h2><h3 id=15--133298-the-whole-is-better-than-the-sum-using-aggregated-demonstrations-in-in-context-learning-for-sequential-recommendation-lei-wang-et-al-2024>(1/5 | 133/298) The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation (Lei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Wang, Ee-Peng Lim. (2024)<br><strong>The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation</strong><br><button class=copy-to-clipboard title="The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CL, cs-IR, cs.IR<br>Keyword Score: 90<br>Keywords: Recommendation, Supervised Learning, Supervised Learning, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10135v1.pdf filename=2403.10135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown excellent performance on various NLP tasks. To use <b>LLMs</b> as strong sequential recommenders, we explore the <b>in-context</b> <b>learning</b> approach to sequential <b>recommendation.</b> We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in <b>ICL</b> does not improve accuracy despite using a long <b>prompt,</b> we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three <b>recommendation</b> datasets show that LLMSRec-Syn outperforms state-of-the-art <b>LLM-based</b> sequential <b>recommendation</b> methods. In some cases, LLMSRec-Syn can perform on par with or even better than <b>supervised</b> <b>learning</b> methods. Our code is publicly available at <a href=https://github.com/demoleiwang/LLMSRec_Syn>https://github.com/demoleiwang/LLMSRec_Syn</a>.</p></p class="citation"></blockquote><h3 id=25--134298-a-thorough-comparison-of-cross-encoders-and-llms-for-reranking-splade-hervé-déjean-et-al-2024>(2/5 | 134/298) A Thorough Comparison of Cross-Encoders and LLMs for Reranking SPLADE (Hervé Déjean et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hervé Déjean, Stéphane Clinchant, Thibault Formal. (2024)<br><strong>A Thorough Comparison of Cross-Encoders and LLMs for Reranking SPLADE</strong><br><button class=copy-to-clipboard title="A Thorough Comparison of Cross-Encoders and LLMs for Reranking SPLADE" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 70<br>Keywords: Out-of-domain, Rerank, Zero-shot, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10407v1.pdf filename=2403.10407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a comparative study between cross-encoder and <b>LLMs</b> rerankers in the context of re-ranking effective SPLADE retrievers. We conduct a <b>large</b> <b>evaluation</b> <b>on</b> TREC Deep Learning datasets and <b>out-of-domain</b> datasets such as BEIR and LoTTE. In the first set of experiments, we show how cross-encoder rerankers are hard to distinguish when it comes to re-rerank SPLADE on MS MARCO. Observations shift in the <b>out-of-domain</b> scenario, where both the type of model and the number of documents to re-rank have an impact on effectiveness. Then, we focus on listwise rerankers based on <b>Large</b> <b>Language</b> <b>Models</b> &ndash; especially <b>GPT-4.</b> While <b>GPT-4</b> demonstrates impressive <b>(zero-shot)</b> performance, we show that traditional cross-encoders remain very competitive. Overall, our findings aim to to provide a more nuanced perspective on the recent excitement surrounding <b>LLM-based</b> re-rankers &ndash; by positioning them as another factor to consider in balancing effectiveness and efficiency in search systems.</p></p class="citation"></blockquote><h3 id=35--135298-towards-unified-multi-modal-personalization-large-vision-language-models-for-generative-recommendation-and-beyond-tianxin-wei-et-al-2024>(3/5 | 135/298) Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond (Tianxin Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianxin Wei, Bowen Jin, Ruirui Li, Hansi Zeng, Zhengyang Wang, Jianhui Sun, Qingyu Yin, Hanqing Lu, Suhang Wang, Jingrui He, Xianfeng Tang. (2024)<br><strong>Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond</strong><br><button class=copy-to-clipboard title="Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CL, cs-IR, cs-MM, cs.IR<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Recommendation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10667v1.pdf filename=2403.10667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by <b>multi-modal</b> data, such as pictures and textual descriptions. These modalities not only offer intuitive guidance but also cater to personalized user preferences. However, the predominant personalization approaches mainly focus on the ID or text-based <b>recommendation</b> problem, failing to comprehend the information spanning various tasks or modalities. In this paper, our goal is to establish a Unified paradigm for <b>Multi-modal</b> Personalization systems (UniMP), which effectively leverages <b>multi-modal</b> data while eliminating the complexities associated with task- and modality-specific customization. We argue that the advancements in foundational generative modeling have provided the flexibility and effectiveness necessary to achieve the objective. In light of this, we develop a generic and extensible personalization generative framework, that can handle a wide range of personalized needs including item <b>recommendation,</b> product search, preference prediction, explanation generation, and further user-guided image generation. Our methodology enhances the capabilities of foundational language models for personalized tasks by seamlessly ingesting interleaved cross-modal user history information, ensuring a more precise and customized experience for users. To train and evaluate the proposed <b>multi-modal</b> personalized tasks, we also introduce a novel and comprehensive <b>benchmark</b> covering a variety of user requirements. Our experiments on the real-world <b>benchmark</b> showcase the model&rsquo;s potential, outperforming competitive methods specialized for each task.</p></p class="citation"></blockquote><h3 id=45--136298-ppm--a-pre-trained-plug-in-model-for-click-through-rate-prediction-yuanbo-gao-et-al-2024>(4/5 | 136/298) PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction (Yuanbo Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanbo Gao, Peng Lin, Dongyue Wang, Feng Mei, Xiwei Zhao, Sulong Xu, Jinghe Hu. (2024)<br><strong>PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction</strong><br><button class=copy-to-clipboard title="PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 13<br>Keywords: Multi-modal, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10049v1.pdf filename=2403.10049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Click-through rate (CTR) prediction is a core task in <b>recommender</b> <b>systems.</b> Existing methods (IDRec for short) rely on unique identities to represent distinct users and items that have prevailed for decades. On one hand, IDRec often faces significant performance degradation on cold-start problem; on the other hand, IDRec cannot use longer training data due to constraints imposed by iteration efficiency. Most prior studies alleviate the above problems by introducing pre-trained knowledge(e.g. pre-trained user model or <b>multi-modal</b> embeddings). However, the explosive growth of online latency can be attributed to the huge parameters in the pre-trained model. Therefore, most of them cannot employ the unified model of end-to-end training with IDRec in industrial <b>recommender</b> <b>systems,</b> thus limiting the potential of the pre-trained model. To this end, we propose a $\textbf{P}$re-trained $\textbf{P}$lug-in CTR $\textbf{M}$odel, namely PPM. PPM employs <b>multi-modal</b> features as input and utilizes large-scale data for pre-training. Then, PPM is plugged in IDRec model to enhance unified model&rsquo;s performance and iteration efficiency. Upon incorporating IDRec model, certain intermediate results within the network are cached, with only a subset of the parameters participating in training and serving. Hence, our approach can successfully deploy an end-to-end model without causing huge latency increases. Comprehensive offline experiments and online A/B testing at JD E-commerce demonstrate the efficiency and effectiveness of PPM.</p></p class="citation"></blockquote><h3 id=55--137298-enriching-user-shopping-history-empowering-e-commerce-with-a-hierarchical-recommendation-system-irem-islek-et-al-2024>(5/5 | 137/298) Enriching User Shopping History: Empowering E-commerce with a Hierarchical Recommendation System (Irem Islek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Irem Islek, Sule Gunduz Oguducu. (2024)<br><strong>Enriching User Shopping History: Empowering E-commerce with a Hierarchical Recommendation System</strong><br><button class=copy-to-clipboard title="Enriching User Shopping History: Empowering E-commerce with a Hierarchical Recommendation System" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12096v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12096v1.pdf filename=2403.12096v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommendation</b> systems can provide accurate <b>recommendations</b> by analyzing user shopping history. A richer user history results in more accurate <b>recommendations.</b> However, in real applications, users prefer e-commerce platforms where the item they seek is at the lowest price. In other words, most users shop from multiple e-commerce platforms simultaneously; different parts of the user&rsquo;s shopping history are shared between different e-commerce platforms. Consequently, we assume in this study that any e-commerce platform has a complete record of the user&rsquo;s history but can only access some parts of it. If a <b>recommendation</b> system is able to predict the missing parts first and enrich the user&rsquo;s shopping history properly, it will be possible to recommend the next item more accurately. Our <b>recommendation</b> system leverages user shopping history to improve prediction accuracy. The proposed approach shows significant improvements in both NDCG@10 and HR@10.</p></p class="citation"></blockquote><h2 id=cslg-36>cs.LG (36)</h2><h3 id=136--138298-generation-is-better-than-modification-combating-high-class-homophily-variance-in-graph-anomaly-detection-rui-zhang-et-al-2024>(1/36 | 138/298) Generation is better than Modification: Combating High Class Homophily Variance in Graph Anomaly Detection (Rui Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Zhang, Dawei Cheng, Xin Liu, Jie Yang, Yi Ouyang, Xian Wu, Yefeng Zheng. (2024)<br><strong>Generation is better than Modification: Combating High Class Homophily Variance in Graph Anomaly Detection</strong><br><button class=copy-to-clipboard title="Generation is better than Modification: Combating High Class Homophily Variance in Graph Anomaly Detection" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 86<br>Keywords: Graph Anomaly Detection, Graph Classification, Node Classification, Graph, Graph Neural Network, Graph Neural Network, Anomaly Detection, Benchmarking, Pruning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10339v1.pdf filename=2403.10339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph-based</b> <b>anomaly</b> <b>detection</b> is currently an important research topic in the field of <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs).</b> We find that in <b>graph</b> <b>anomaly</b> <b>detection,</b> the homophily distribution differences between different classes are significantly greater than those in homophilic and heterophilic <b>graphs.</b> <b>For</b> <b>the</b> first time, we introduce a new metric called Class Homophily Variance, which quantitatively describes this phenomenon. To mitigate its impact, we propose a novel <b>GNN</b> model named Homophily Edge Generation <b>Graph</b> <b>Neural</b> <b>Network</b> (HedGe). Previous works typically focused on <b>pruning,</b> selecting or connecting on original relationships, and we refer to these methods as modifications. Different from these works, our method emphasizes generating new relationships with low class homophily variance, using the original relationships as an auxiliary. HedGe samples homophily adjacency matrices from scratch using a <b>self-attention</b> mechanism, and leverages <b>nodes</b> <b>that</b> are relevant in the feature space but not directly connected in the original <b>graph.</b> <b>Additionally,</b> <b>we</b> modify the loss function to punish the generation of unnecessary heterophilic edges by the model. Extensive comparison experiments demonstrate that HedGe achieved the best performance across multiple <b>benchmark</b> datasets, including <b>anomaly</b> <b>detection</b> and edgeless <b>node</b> <b>classification.</b> The proposed model also improves the robustness under the novel Heterophily Attack with increased class homophily variance on other <b>graph</b> <b>classification</b> <b>tasks.</b></p></p class="citation"></blockquote><h3 id=236--139298-functional-graph-convolutional-networks-a-unified-multi-task-and-multi-modal-learning-framework-to-facilitate-health-and-social-care-insights-tobia-boschi-et-al-2024>(2/36 | 139/298) Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights (Tobia Boschi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, Cécile Rosseau, Alessandra Pascale, John Dinsmore. (2024)<br><strong>Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights</strong><br><button class=copy-to-clipboard title="Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 64<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Knowledge Graph, Multi-modal, Sample Size, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10158v1.pdf filename=2403.10158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel Functional <b>Graph</b> <b>Convolutional</b> <b>Network</b> (funGCN) framework that combines Functional Data Analysis and <b>Graph</b> <b>Convolutional</b> <b>Networks</b> to address the complexities of multi-task and <b>multi-modal</b> learning in digital health and longitudinal studies. With the growing importance of health solutions to improve health care and social support, ensure healthy lives, and promote well-being at all ages, funGCN offers a unified approach to handle multivariate longitudinal data for multiple entities and ensures interpretability even with small <b>sample</b> <b>sizes.</b> Key innovations include task-specific embedding components that manage different data types, the ability to perform classification, regression, and forecasting, and the creation of a <b>knowledge</b> <b>graph</b> <b>for</b> <b>insightful</b> data interpretation. The efficacy of funGCN is validated through <b>simulation</b> experiments and a real-data application.</p></p class="citation"></blockquote><h3 id=336--140298-perl-parameter-efficient-reinforcement-learning-from-human-feedback-hakim-sidahmed-et-al-2024>(3/36 | 140/298) PERL: Parameter Efficient Reinforcement Learning from Human Feedback (Hakim Sidahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hakim Sidahmed, Samrat Phatale, Alex Hutcheson, Zhuonan Lin, Zhang Chen, Zac Yu, Jarvis Jin, Roman Komarytsia, Christiane Ahlheim, Yonghao Zhu, Simral Chaudhary, Bowen Li, Saravanan Ganesh, Bill Byrne, Jessica Hoffmann, Hassan Mansoor, Wei Li, Abhinav Rastogi, Lucas Dixon. (2024)<br><strong>PERL: Parameter Efficient Reinforcement Learning from Human Feedback</strong><br><button class=copy-to-clipboard title="PERL: Parameter Efficient Reinforcement Learning from Human Feedback" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10704v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10704v1.pdf filename=2403.10704v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> has proven to be a strong method to align Pretrained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with human preferences. But training models with <b>RLHF</b> is computationally expensive, and an overall complex process. In this work, we study <b>RLHF</b> where the underlying models are trained using the parameter efficient method of Low-Rank Adaptation (LoRA) introduced by Hu et al. [2021]. We investigate the setup of &ldquo;Parameter Efficient <b>Reinforcement</b> <b>Learning&rdquo;</b> <b>(PERL),</b> <b>in</b> <b>which</b> we perform reward model training and <b>reinforcement</b> <b>learning</b> <b>using</b> <b>LoRA.</b> <b>We</b> compare PERL to conventional <b>fine-tuning</b> (full-tuning) across various configurations for 7 <b>benchmarks,</b> including 2 novel datasets, of reward modeling and <b>reinforcement</b> <b>learning.</b> <b>We</b> <b>find</b> <b>that</b> PERL performs on par with the conventional <b>RLHF</b> setting, while training faster, and with less memory. This enables the high performance of <b>RLHF,</b> while reducing the computational burden that limits its adoption as an alignment technique for <b>Large</b> <b>Language</b> <b>Models.</b> We also release 2 novel thumbs up/down preference datasets: &ldquo;Taskmaster Coffee&rdquo;, and &ldquo;Taskmaster Ticketing&rdquo; to promote research around <b>RLHF.</b></p></p class="citation"></blockquote><h3 id=436--141298-benchmarking-zero-shot-robustness-of-multimodal-foundation-models-a-pilot-study-chenguang-wang-et-al-2024>(4/36 | 141/298) Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study (Chenguang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenguang Wang, Ruoxi Jia, Xin Liu, Dawn Song. (2024)<br><strong>Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study</strong><br><button class=copy-to-clipboard title="Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 62<br>Keywords: Benchmarking, Benchmarking, Distribution Shift, Distribution Shift, Foundation Model, Multi-modal, Multi-modal, Supervised Learning, Zero-shot, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10499v1.pdf filename=2403.10499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-training image representations from the raw text about images enables <b>zero-shot</b> vision transfer to downstream tasks. Through pre-training on millions of samples collected from the internet, <b>multimodal</b> <b>foundation</b> <b>models,</b> such as CLIP, produce state-of-the-art <b>zero-shot</b> results that often reach competitiveness with fully <b>supervised</b> methods without the need for task-specific training. Besides the encouraging performance on classification accuracy, it is reported that these models close the robustness gap by matching the performance of <b>supervised</b> models trained on ImageNet under natural <b>distribution</b> <b>shift.</b> Because robustness is critical to real-world applications, especially safety-critical ones, in this paper, we present a comprehensive evaluation based on a large-scale robustness <b>benchmark</b> covering 7 natural, 3 synthetic <b>distribution</b> <b>shifts,</b> and 11 <b>adversarial</b> <b>attacks.</b> We use CLIP as a pilot study. We show that CLIP leads to a significant robustness drop compared to <b>supervised</b> ImageNet models on our <b>benchmark,</b> especially under synthetic <b>distribution</b> <b>shift</b> and <b>adversarial</b> <b>attacks.</b> Furthermore, data overlap analysis suggests that the observed robustness under natural <b>distribution</b> <b>shifts</b> could be attributed, at least in part, to data overlap. In summary, our evaluation shows a comprehensive evaluation of robustness is necessary; and there is a significant need to improve the robustness of <b>zero-shot</b> <b>multimodal</b> models.</p></p class="citation"></blockquote><h3 id=536--142298-counterfactual-analysis-of-neural-networks-used-to-create-fertilizer-management-zones-giorgio-morales-et-al-2024>(5/36 | 142/298) Counterfactual Analysis of Neural Networks Used to Create Fertilizer Management Zones (Giorgio Morales et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giorgio Morales, John Sheppard. (2024)<br><strong>Counterfactual Analysis of Neural Networks Used to Create Fertilizer Management Zones</strong><br><button class=copy-to-clipboard title="Counterfactual Analysis of Neural Networks Used to Create Fertilizer Management Zones" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Clustering, Convolution, Convolutional Neural Network, Convolutional Neural Network, Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10730v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10730v1.pdf filename=2403.10730v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Precision Agriculture, the utilization of management zones (MZs) that take into account within-field variability facilitates effective fertilizer management. This approach enables the optimization of nitrogen (N) rates to maximize crop yield production and enhance agronomic use efficiency. However, existing works often neglect the consideration of responsivity to fertilizer as a factor influencing MZ determination. In response to this gap, we present a MZ <b>clustering</b> method based on fertilizer responsivity. We build upon the statement that the responsivity of a given site to the fertilizer rate is described by the shape of its corresponding N fertilizer-yield response (N-response) curve. Thus, we generate N-response curves for all sites within the field using a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN).</b> The shape of the approximated N-response curves is then characterized using functional principal component analysis. Subsequently, a <b>counterfactual</b> explanation (CFE) method is applied to discern the impact of various variables on MZ membership. The genetic algorithm-based CFE solves a multi-objective optimization problem and aims to identify the minimum combination of features needed to alter a site&rsquo;s cluster assignment. Results from two yield prediction datasets indicate that the features with the greatest influence on MZ membership are associated with terrain characteristics that either facilitate or impede fertilizer runoff, such as terrain slope or topographic aspect.</p></p class="citation"></blockquote><h3 id=636--143298-from-chaos-to-clarity-time-series-anomaly-detection-in-astronomical-observations-xinli-hao-et-al-2024>(6/36 | 143/298) From Chaos to Clarity: Time Series Anomaly Detection in Astronomical Observations (Xinli Hao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinli Hao, Yile Chen, Chen Yang, Zhihui Du, Chaohong Ma, Chao Wu, Xiaofeng Meng. (2024)<br><strong>From Chaos to Clarity: Time Series Anomaly Detection in Astronomical Observations</strong><br><button class=copy-to-clipboard title="From Chaos to Clarity: Time Series Anomaly Detection in Astronomical Observations" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Anomaly Detection, Unsupervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10220v1.pdf filename=2403.10220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of astronomical facilities, large-scale time series data observed by these facilities is being collected. Analyzing anomalies in these astronomical observations is crucial for uncovering potential celestial events and physical phenomena, thus advancing the scientific research process. However, existing time series <b>anomaly</b> <b>detection</b> methods fall short in tackling the unique characteristics of astronomical observations where each star is inherently independent but interfered by random concurrent noise, resulting in a high rate of false alarms. To overcome the challenges, we propose AERO, a novel two-stage framework tailored for <b>unsupervised</b> <b>anomaly</b> <b>detection</b> in astronomical observations. In the first stage, we employ a <b>Transformer-based</b> encoder-decoder architecture to learn the normal temporal patterns on each variate (i.e., star) in alignment with the characteristic of variate independence. In the second stage, we enhance the <b>graph</b> <b>neural</b> <b>network</b> with a window-wise <b>graph</b> <b>structure</b> <b>learning</b> to tackle the occurrence of concurrent noise characterized by spatial and temporal randomness. In this way, AERO is not only capable of distinguishing normal temporal patterns from potential anomalies but also effectively differentiating concurrent noise, thus decreasing the number of false alarms. We conducted extensive experiments on three synthetic datasets and three real-world datasets. The results demonstrate that AERO outperforms the compared baselines. Notably, compared to the state-of-the-art model, AERO improves the F1-score by up to 8.76% and 2.63% on synthetic and real-world datasets respectively.</p></p class="citation"></blockquote><h3 id=736--144298-comprehensive-study-of-predictive-maintenance-in-industries-using-classification-models-and-lstm-model-saket-maheshwari-et-al-2024>(7/36 | 144/298) Comprehensive Study Of Predictive Maintenance In Industries Using Classification Models And LSTM Model (Saket Maheshwari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saket Maheshwari, Sambhav Tiwari, Shyam Rai, Satyam Vinayak Daman Pratap Singh. (2024)<br><strong>Comprehensive Study Of Predictive Maintenance In Industries Using Classification Models And LSTM Model</strong><br><button class=copy-to-clipboard title="Comprehensive Study Of Predictive Maintenance In Industries Using Classification Models And LSTM Model" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Logistic Regression, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10259v1.pdf filename=2403.10259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s technology-driven era, the imperative for predictive maintenance and advanced diagnostics extends beyond aviation to encompass the identification of damages, failures, and operational defects in rotating and moving machines. Implementing such services not only curtails maintenance costs but also extends machine lifespan, ensuring heightened operational efficiency. Moreover, it serves as a preventive measure against potential accidents or catastrophic events. The advent of Artificial Intelligence (AI) has revolutionized maintenance across industries, enabling more accurate and efficient prediction and analysis of machine failures, thereby conserving time and resources. Our proposed study aims to delve into various machine learning classification techniques, including Support Vector Machine (SVM), Random Forest, <b>Logistic</b> <b>Regression,</b> and <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>LSTM-Based,</b> for predicting and analyzing machine performance. SVM classifies data into different categories based on their positions in a multidimensional space, while Random Forest employs ensemble learning to create multiple decision trees for classification. <b>Logistic</b> <b>Regression</b> predicts the probability of binary outcomes using input data. The primary objective of the study is to assess these algorithms&rsquo; performance in predicting and analyzing machine performance, considering factors such as accuracy, precision, recall, and F1 score. The findings will aid maintenance experts in selecting the most suitable machine learning algorithm for effective prediction and analysis of machine performance.</p></p class="citation"></blockquote><h3 id=836--145298-a-short-survey-on-importance-weighting-for-machine-learning-masanari-kimura-et-al-2024>(8/36 | 145/298) A Short Survey on Importance Weighting for Machine Learning (Masanari Kimura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masanari Kimura, Hideitsu Hino. (2024)<br><strong>A Short Survey on Importance Weighting for Machine Learning</strong><br><button class=copy-to-clipboard title="A Short Survey on Importance Weighting for Machine Learning" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, Supervised Learning, Supervised Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10175v1.pdf filename=2403.10175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Importance weighting is a fundamental procedure in statistics and machine learning that weights the objective function or probability <b>distribution</b> <b>based</b> on the importance of the instance in some sense. The simplicity and usefulness of the idea has led to many applications of importance weighting. For example, it is known that <b>supervised</b> <b>learning</b> under an assumption about the difference between the training and test <b>distributions,</b> <b>called</b> <b>distribution</b> <b>shift,</b> can guarantee statistically desirable properties through importance weighting by their density ratio. This survey <b>summarizes</b> the broad applications of importance weighting in machine learning and related research.</p></p class="citation"></blockquote><h3 id=936--146298-towards-adversarially-robust-dataset-distillation-by-curvature-regularization-eric-xue-et-al-2024>(9/36 | 146/298) Towards Adversarially Robust Dataset Distillation by Curvature Regularization (Eric Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric Xue, Yijiang Li, Haoyang Liu, Yifan Shen, Haohan Wang. (2024)<br><strong>Towards Adversarially Robust Dataset Distillation by Curvature Regularization</strong><br><button class=copy-to-clipboard title="Towards Adversarially Robust Dataset Distillation by Curvature Regularization" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Knowledge Distillation, Knowledge Distillation, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10045v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10045v1.pdf filename=2403.10045v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dataset <b>distillation</b> (DD) allows datasets to be <b>distilled</b> to fractions of their original size while preserving the rich distributional information so that models trained on the <b>distilled</b> datasets can achieve a comparable accuracy while saving significant computational loads. Recent research in this area has been focusing on improving the accuracy of models trained on <b>distilled</b> datasets. In this paper, we aim to explore a new perspective of DD. We study how to embed <b>adversarial</b> <b>robustness</b> in <b>distilled</b> datasets, so that models trained on these datasets maintain the high accuracy and meanwhile acquire better <b>adversarial</b> <b>robustness.</b> We propose a new method that achieves this goal by incorporating curvature regularization into the <b>distillation</b> process with much less computational overhead than standard <b>adversarial</b> <b>training.</b> Extensive empirical experiments suggest that our method not only outperforms standard <b>adversarial</b> <b>training</b> on both accuracy and robustness with less computation overhead but is also capable of generating robust <b>distilled</b> datasets that can withstand various <b>adversarial</b> <b>attacks.</b></p></p class="citation"></blockquote><h3 id=1036--147298-regret-minimization-via-saddle-point-optimization-johannes-kirschner-et-al-2024>(10/36 | 147/298) Regret Minimization via Saddle Point Optimization (Johannes Kirschner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johannes Kirschner, Seyed Alireza Bakhtiari, Kushagra Chandak, Volodymyr Tkachuk, Csaba Szepesvári. (2024)<br><strong>Regret Minimization via Saddle Point Optimization</strong><br><button class=copy-to-clipboard title="Regret Minimization via Saddle Point Optimization" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Reinforcement Learning, Structured Bandit<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10379v1.pdf filename=2403.10379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A long line of works characterizes the sample complexity of regret minimization in sequential decision-making by min-max programs. In the corresponding saddle-point game, the min-player optimizes the sampling distribution against an adversarial max-player that chooses confusing models leading to large regret. The most recent instantiation of this idea is the decision-estimation coefficient (DEC), which was shown to provide nearly tight lower and upper bounds on the worst-case expected regret in <b>structured</b> <b>bandits</b> and <b>reinforcement</b> <b>learning.</b> By re-parametrizing the offset DEC with the confidence radius and solving the corresponding min-max program, we derive an anytime variant of the Estimation-To-Decisions (E2D) algorithm. Importantly, the algorithm optimizes the exploration-exploitation trade-off online instead of via the analysis. Our formulation leads to a practical algorithm for finite model classes and linear feedback models. We further point out connections to the information ratio, decoupling coefficient and PAC-DEC, and numerically evaluate the performance of E2D on simple examples.</p></p class="citation"></blockquote><h3 id=1136--148298-less-is-more-one-shot-subgraph-reasoning-on-large-scale-knowledge-graphs-zhanke-zhou-et-al-2024>(11/36 | 148/298) Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs (Zhanke Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanke Zhou, Yongqi Zhang, Jiangchao Yao, Quanming Yao, Bo Han. (2024)<br><strong>Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs</strong><br><button class=copy-to-clipboard title="Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SI, cs.LG<br>Keyword Score: 26<br>Keywords: Graph, Benchmarking, Knowledge Graph, Knowledge Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10231v1.pdf filename=2403.10231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To deduce new facts on a <b>knowledge</b> <b>graph</b> <b>(KG),</b> a link predictor learns from the <b>graph</b> structure and collects local evidence to find the answer to a given query. However, existing methods suffer from a severe scalability problem due to the utilization of the whole <b>KG</b> for prediction, which hinders their promise on large scale <b>KGs</b> and cannot be directly addressed by vanilla sampling methods. In this work, we propose the one-shot-subgraph link prediction to achieve efficient and adaptive prediction. The design principle is that, instead of directly acting on the whole <b>KG,</b> the prediction procedure is decoupled into two steps, i.e., (i) extracting only one subgraph according to the query and (ii) predicting on this single, query dependent subgraph. We reveal that the non-parametric and computation-efficient heuristics Personalized PageRank (PPR) can effectively identify the potential answers and supporting evidence. With efficient subgraph-based prediction, we further introduce the automated searching of the optimal configurations in both data and model spaces. Empirically, we achieve promoted efficiency and leading performances on five large-scale <b>benchmarks.</b> The code is publicly available at: <a href=https://github.com/tmlr-group/one-shot-subgraph>https://github.com/tmlr-group/one-shot-subgraph</a>.</p></p class="citation"></blockquote><h3 id=1236--149298-dipaco-distributed-path-composition-arthur-douillard-et-al-2024>(12/36 | 149/298) DiPaCo: Distributed Path Composition (Arthur Douillard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arthur Douillard, Qixuan Feng, Andrei A. Rusu, Adhiguna Kuncoro, Yani Donchev, Rachita Chhaparia, Ionel Gog, Marc&rsquo;Aurelio Ranzato, Jiajun Shen, Arthur Szlam. (2024)<br><strong>DiPaCo: Distributed Path Composition</strong><br><button class=copy-to-clipboard title="DiPaCo: Distributed Path Composition" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Model Compression, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10616v1.pdf filename=2403.10616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Progress in machine learning (ML) has been fueled by scaling neural network <b>models.</b> <b>This</b> scaling has been enabled by ever more heroic feats of engineering, necessary for accommodating ML approaches that require high bandwidth communication between devices working in parallel. In this work, we propose a co-designed modular architecture and training approach for ML <b>models,</b> <b>dubbed</b> DIstributed PAth COmposition (DiPaCo). During training, DiPaCo distributes computation by paths through a set of shared modules. Together with a Local-SGD inspired optimization (DiLoCo) that keeps modules in sync with drastically reduced communication, Our approach facilitates training across poorly connected and heterogeneous workers, with a design that ensures robustness to worker failures and preemptions. At inference time, only a single path needs to be executed for each input, without the need for any <b>model</b> <b>compression.</b> We consider this approach as a first prototype towards a new paradigm of large-scale learning, one that is less synchronous and more modular. Our experiments on the widely used C4 <b>benchmark</b> show that, for the same amount of training steps but less wall-clock time, DiPaCo exceeds the performance of a 1 billion-parameter dense <b>transformer</b> language <b>model</b> <b>by</b> choosing one of 256 possible paths, each with a size of 150 million parameters.</p></p class="citation"></blockquote><h3 id=1336--150298-open-continual-feature-selection-via-granular-ball-knowledge-transfer-xuemei-cao-et-al-2024>(13/36 | 150/298) Open Continual Feature Selection via Granular-Ball Knowledge Transfer (Xuemei Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuemei Cao, Xin Yang, Shuyin Xia, Guoyin Wang, Tianrui Li. (2024)<br><strong>Open Continual Feature Selection via Granular-Ball Knowledge Transfer</strong><br><button class=copy-to-clipboard title="Open Continual Feature Selection via Granular-Ball Knowledge Transfer" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Continual Learning, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10253v1.pdf filename=2403.10253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel framework for <b>continual</b> <b>feature</b> selection (CFS) in data preprocessing, particularly in the context of an open and dynamic environment where unknown classes may emerge. CFS encounters two primary challenges: the discovery of unknown <b>knowledge</b> <b>and</b> the transfer of known <b>knowledge.</b> <b>To</b> this end, the proposed CFS method combines the strengths of <b>continual</b> <b>learning</b> (CL) with granular-ball computing (GBC), which focuses on constructing a granular-ball <b>knowledge</b> <b>base</b> to detect unknown classes and facilitate the transfer of previously learned <b>knowledge</b> <b>for</b> further feature selection. CFS consists of two stages: initial learning and open learning. The former aims to establish an initial <b>knowledge</b> <b>base</b> through multi-granularity representation using granular-balls. The latter utilizes prior granular-ball <b>knowledge</b> <b>to</b> identify unknowns, updates the <b>knowledge</b> <b>base</b> for granular-ball <b>knowledge</b> <b>transfer,</b> reinforces old <b>knowledge,</b> <b>and</b> integrates new <b>knowledge.</b> <b>Subsequently,</b> we devise an optimal feature subset mechanism that incorporates minimal new features into the existing optimal subset, often yielding superior results during each period. Extensive experimental results on public <b>benchmark</b> datasets demonstrate our method&rsquo;s superiority in terms of both effectiveness and efficiency compared to state-of-the-art feature selection methods.</p></p class="citation"></blockquote><h3 id=1436--151298-discovering-invariant-neighborhood-patterns-for-heterophilic-graphs-ruihao-zhang-et-al-2024>(14/36 | 151/298) Discovering Invariant Neighborhood Patterns for Heterophilic Graphs (Ruihao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruihao Zhang, Zhengyu Chen, Teng Xiao, Yueyang Wang, Kun Kuang. (2024)<br><strong>Discovering Invariant Neighborhood Patterns for Heterophilic Graphs</strong><br><button class=copy-to-clipboard title="Discovering Invariant Neighborhood Patterns for Heterophilic Graphs" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10572v1.pdf filename=2403.10572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies the problem of <b>distribution</b> <b>shifts</b> on non-homophilous <b>graphs</b> <b>Mosting</b> <b>existing</b> <b>graph</b> <b>neural</b> <b>network</b> methods rely on the homophilous assumption that nodes from the same class are more likely to be linked. However, such assumptions of homophily do not always hold in real-world <b>graphs,</b> <b>which</b> <b>leads</b> to more complex <b>distribution</b> <b>shifts</b> unaccounted for in previous methods. The <b>distribution</b> <b>shifts</b> of neighborhood patterns are much more diverse on non-homophilous <b>graphs.</b> <b>We</b> <b>propose</b> a novel Invariant Neighborhood Pattern Learning (INPL) to alleviate the <b>distribution</b> <b>shifts</b> problem on non-homophilous <b>graphs.</b> <b>Specifically,</b> <b>we</b> propose the Adaptive Neighborhood Propagation (ANP) module to capture the adaptive neighborhood information, which could alleviate the neighborhood pattern <b>distribution</b> <b>shifts</b> problem on non-homophilous <b>graphs.</b> <b>We</b> <b>propose</b> Invariant Non-Homophilous <b>Graph</b> <b>Learning</b> <b>(INHGL)</b> module to constrain the ANP and learn invariant <b>graph</b> <b>representation</b> <b>on</b> non-homophilous <b>graphs.</b> <b>Extensive</b> <b>experimental</b> results on real-world non-homophilous <b>graphs</b> <b>show</b> <b>that</b> INPL could achieve state-of-the-art performance for learning on large non-homophilous <b>graphs.</b></p></p class="citation"></blockquote><h3 id=1536--152298-prediction-of-vessel-arrival-time-to-pilotage-area-using-multi-data-fusion-and-deep-learning-xiaocai-zhang-et-al-2024>(15/36 | 152/298) Prediction of Vessel Arrival Time to Pilotage Area Using Multi-Data Fusion and Deep Learning (Xiaocai Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaocai Zhang, Xiuju Fu, Zhe Xiao, Haiyan Xu, Xiaoyang Wei, Jimmy Koh, Daichi Ogawa, Zheng Qin. (2024)<br><strong>Prediction of Vessel Arrival Time to Pilotage Area Using Multi-Data Fusion and Deep Learning</strong><br><button class=copy-to-clipboard title="Prediction of Vessel Arrival Time to Pilotage Area Using Multi-Data Fusion and Deep Learning" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Clustering, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09969v1.pdf filename=2403.09969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the prediction of vessels&rsquo; arrival time to the pilotage area using multi-data fusion and deep learning approaches. Firstly, the vessel arrival contour is extracted based on Multivariate Kernel Density Estimation (MKDE) and <b>clustering.</b> Secondly, multiple data sources, including Automatic Identification System (AIS), pilotage booking information, and meteorological data, are fused before latent feature extraction. Thirdly, a Temporal <b>Convolutional</b> <b>Network</b> (TCN) framework that incorporates a residual mechanism is constructed to learn the hidden arrival patterns of the vessels. Extensive tests on two real-world data sets from Singapore have been conducted and the following promising results have been obtained: 1) fusion of pilotage booking information and meteorological data improves the prediction accuracy, with pilotage booking information having a more significant impact; 2) using discrete embedding for the meteorological data performs better than using continuous embedding; 3) the TCN outperforms the state-of-the-art baseline methods in regression tasks, exhibiting Mean Absolute Error (MAE) ranging from 4.58 min to 4.86 min; and 4) approximately 89.41% to 90.61% of the absolute prediction residuals fall within a time frame of 10 min.</p></p class="citation"></blockquote><h3 id=1636--153298-online-gnn-evaluation-under-test-time-graph-distribution-shifts-xin-zheng-et-al-2024>(16/36 | 153/298) Online GNN Evaluation Under Test-time Graph Distribution Shifts (Xin Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Zheng, Dongjin Song, Qingsong Wen, Bo Du, Shirui Pan. (2024)<br><strong>Online GNN Evaluation Under Test-time Graph Distribution Shifts</strong><br><button class=copy-to-clipboard title="Online GNN Evaluation Under Test-time Graph Distribution Shifts" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09953v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09953v1.pdf filename=2403.09953v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluating the performance of a well-trained <b>GNN</b> model on real-world <b>graphs</b> is a pivotal step for reliable <b>GNN</b> online deployment and serving. Due to a lack of test node labels and unknown potential training-test <b>graph</b> data <b>distribution</b> <b>shifts,</b> conventional model evaluation encounters limitations in calculating performance metrics (e.g., test error) and measuring <b>graph</b> data-level discrepancies, particularly when the training <b>graph</b> used for developing <b>GNNs</b> remains unobserved during test time. In this paper, we study a new research problem, online <b>GNN</b> evaluation, which aims to provide valuable insights into the well-trained <b>GNNs&rsquo;s</b> ability to effectively generalize to real-world unlabeled <b>graphs</b> under the test-time <b>graph</b> <b>distribution</b> <b>shifts.</b> Concretely, we develop an effective learning behavior discrepancy score, dubbed LeBeD, to estimate the test-time generalization errors of well-trained <b>GNN</b> models. Through a novel <b>GNN</b> re-training strategy with a parameter-free optimality criterion, the proposed LeBeD comprehensively integrates learning behavior discrepancies from both node prediction and structure reconstruction perspectives. This enables the effective evaluation of the well-trained <b>GNNs&rsquo;</b> ability to capture test node semantics and structural representations, making it an expressive metric for estimating the generalization error in online <b>GNN</b> evaluation. Extensive experiments on real-world test <b>graphs</b> under diverse <b>graph</b> <b>distribution</b> <b>shifts</b> could verify the effectiveness of the proposed method, revealing its strong correlation with ground-truth test errors on various well-trained <b>GNN</b> models.</p></p class="citation"></blockquote><h3 id=1736--154298-horizon-free-regret-for-linear-markov-decision-processes-zihan-zhang-et-al-2024>(17/36 | 154/298) Horizon-Free Regret for Linear Markov Decision Processes (Zihan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Zhang, Jason D. Lee, Yuxin Chen, Simon S. Du. (2024)<br><strong>Horizon-Free Regret for Linear Markov Decision Processes</strong><br><button class=copy-to-clipboard title="Horizon-Free Regret for Linear Markov Decision Processes" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10738v1.pdf filename=2403.10738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A recent line of works showed regret bounds in <b>reinforcement</b> <b>learning</b> (RL) can be (nearly) independent of planning horizon, a.k.a.~the horizon-free bounds. However, these regret bounds only apply to settings where a polynomial dependency on the size of transition model is allowed, such as tabular <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP) and linear mixture MDP. We give the first horizon-free bound for the popular linear MDP setting where the size of the transition model can be exponentially large or even uncountable. In contrast to prior works which explicitly estimate the transition model and compute the inhomogeneous value functions at different time steps, we directly estimate the value functions and confidence sets. We obtain the horizon-free bound by: (1) maintaining multiple weighted least square estimators for the value functions; and (2) a structural lemma which shows the maximal total variation of the inhomogeneous value functions is bounded by a polynomial factor of the feature dimension.</p></p class="citation"></blockquote><h3 id=1836--155298-introducing-adaptive-continuous-adversarial-training-acat-to-enhance-ml-robustness-mohamed-elshehaby-et-al-2024>(18/36 | 155/298) Introducing Adaptive Continuous Adversarial Training (ACAT) to Enhance ML Robustness (Mohamed elShehaby et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed elShehaby, Aditya Kotha, Ashraf Matrawy. (2024)<br><strong>Introducing Adaptive Continuous Adversarial Training (ACAT) to Enhance ML Robustness</strong><br><button class=copy-to-clipboard title="Introducing Adaptive Continuous Adversarial Training (ACAT) to Enhance ML Robustness" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs-NI, cs.LG<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10461v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10461v1.pdf filename=2403.10461v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine Learning (ML) is susceptible to <b>adversarial</b> <b>attacks</b> that aim to trick ML models, making them produce faulty predictions. <b>Adversarial</b> <b>training</b> was found to increase the robustness of ML models against these attacks. However, in network and cybersecurity, obtaining labeled training and <b>adversarial</b> <b>training</b> data is challenging and costly. Furthermore, concept drift deepens the challenge, particularly in dynamic domains like network and cybersecurity, and requires various models to conduct periodic retraining. This letter introduces Adaptive Continuous <b>Adversarial</b> <b>Training</b> (ACAT) to continuously integrate <b>adversarial</b> <b>training</b> samples into the model during ongoing learning sessions, using real-world detected <b>adversarial</b> <b>data,</b> to enhance model resilience against evolving <b>adversarial</b> <b>threats.</b> ACAT is an adaptive defense mechanism that utilizes periodic retraining to effectively counter <b>adversarial</b> <b>attacks</b> while mitigating catastrophic forgetting. Our approach also reduces the total time required for <b>adversarial</b> <b>sample</b> detection, especially in environments such as network security where the rate of attacks could be very high. Traditional detection processes that involve two stages may result in lengthy procedures. Experimental results using a SPAM detection dataset demonstrate that with ACAT, the accuracy of the SPAM filter increased from 69% to over 88% after just three retraining sessions. Furthermore, ACAT outperforms conventional <b>adversarial</b> <b>sample</b> detectors, providing faster decision times, up to four times faster in some cases.</p></p class="citation"></blockquote><h3 id=1936--156298-optimal-block-level-draft-verification-for-accelerating-speculative-decoding-ziteng-sun-et-al-2024>(19/36 | 156/298) Optimal Block-Level Draft Verification for Accelerating Speculative Decoding (Ziteng Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziteng Sun, Jae Hun Ro, Ahmad Beirami, Ananda Theertha Suresh. (2024)<br><strong>Optimal Block-Level Draft Verification for Accelerating Speculative Decoding</strong><br><button class=copy-to-clipboard title="Optimal Block-Level Draft Verification for Accelerating Speculative Decoding" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-DS, cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10444v1.pdf filename=2403.10444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speculative decoding has shown to be an effective method for lossless acceleration of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> during inference. In each iteration, the algorithm first uses a smaller model to draft a block of tokens. The tokens are then verified by the <b>large</b> <b>model</b> <b>in</b> parallel and only a subset of tokens will be kept to guarantee that the final output follows the distribution of the <b>large</b> <b>model.</b> <b>In</b> all of the prior speculative decoding works, the draft verification is performed token-by-token independently. In this work, we propose a better draft verification algorithm that provides additional wall-clock speedup without incurring additional computation cost and draft tokens. We first formulate the draft verification step as a block-level optimal transport problem. The block-level formulation allows us to consider a wider range of draft verification algorithms and obtain a higher number of accepted tokens in expectation in one draft block. We propose a verification algorithm that achieves the optimal accepted length for the block-level transport problem. We empirically evaluate our proposed block-level verification algorithm in a wide range of tasks and datasets, and observe consistent improvements in wall-clock speedup when compared to token-level verification algorithm. To the best of our knowledge, our work is the first to establish improvement over speculative decoding through a better draft verification algorithm.</p></p class="citation"></blockquote><h3 id=2036--157298-meta-operator-for-complex-query-answering-on-knowledge-graphs-hang-yin-et-al-2024>(20/36 | 157/298) Meta Operator for Complex Query Answering on Knowledge Graphs (Hang Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Yin, Zihao Wang, Yangqiu Song. (2024)<br><strong>Meta Operator for Complex Query Answering on Knowledge Graphs</strong><br><button class=copy-to-clipboard title="Meta Operator for Complex Query Answering on Knowledge Graphs" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-LO, cs.LG<br>Keyword Score: 18<br>Keywords: Graph, Knowledge Graph, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10110v1.pdf filename=2403.10110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>graphs</b> contain informative factual <b>knowledge</b> <b>but</b> are considered incomplete. To answer complex queries under incomplete <b>knowledge,</b> <b>learning-based</b> Complex Query Answering (CQA) models are proposed to directly learn from the query-answer samples to avoid the direct traversal of incomplete <b>graph</b> data. Existing works formulate the training of complex query answering models as multi-task learning and require a large number of training samples. In this work, we explore the compositional structure of complex queries and argue that the different logical operator types, rather than the different complex query types, are the key to improving generalizability. Accordingly, we propose a <b>meta-learning</b> <b>algorithm</b> to learn the <b>meta-operators</b> <b>with</b> limited data and adapt them to different instances of operators under various complex queries. Empirical results show that learning <b>meta-operators</b> <b>is</b> more effective than learning original CQA or <b>meta-CQA</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=2136--158298-variance-dependent-regret-bounds-for-non-stationary-linear-bandits-zhiyong-wang-et-al-2024>(21/36 | 158/298) Variance-Dependent Regret Bounds for Non-stationary Linear Bandits (Zhiyong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyong Wang, Jize Xie, Yi Chen, John C. S. Lui, Dongruo Zhou. (2024)<br><strong>Variance-Dependent Regret Bounds for Non-stationary Linear Bandits</strong><br><button class=copy-to-clipboard title="Variance-Dependent Regret Bounds for Non-stationary Linear Bandits" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10732v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10732v1.pdf filename=2403.10732v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the non-stationary stochastic linear <b>bandit</b> problem where the reward distribution evolves each round. Existing algorithms characterize the non-stationarity by the total variation budget $B_K$, which is the summation of the change of the consecutive feature vectors of the linear <b>bandits</b> over $K$ rounds. However, such a quantity only measures the non-stationarity with respect to the expectation of the reward distribution, which makes existing algorithms sub-optimal under the general non-stationary distribution setting. In this work, we propose algorithms that utilize the variance of the reward distribution as well as the $B_K$, and show that they can achieve tighter regret upper bounds. Specifically, we introduce two novel algorithms: Restarted Weighted$\text{OFUL}^+$ and Restarted $\text{SAVE}^+$. These algorithms address cases where the variance information of the rewards is known and unknown, respectively. Notably, when the total variance $V_K$ is much smaller than $K$, our algorithms outperform previous state-of-the-art results on non-stationary stochastic linear <b>bandits</b> under different settings. Experimental evaluations further validate the superior performance of our proposed algorithms over existing works.</p></p class="citation"></blockquote><h3 id=2236--159298-improving-fairness-in-credit-lending-models-using-subgroup-threshold-optimization-cecilia-ying-et-al-2024>(22/36 | 159/298) Improving Fairness in Credit Lending Models using Subgroup Threshold Optimization (Cecilia Ying et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cecilia Ying, Stephen Thomas. (2024)<br><strong>Improving Fairness in Credit Lending Models using Subgroup Threshold Optimization</strong><br><button class=copy-to-clipboard title="Improving Fairness in Credit Lending Models using Subgroup Threshold Optimization" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-fin-RM<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10652v1.pdf filename=2403.10652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In an effort to improve the accuracy of credit lending decisions, many financial intuitions are now using predictions from machine learning models. While such predictions enjoy many advantages, recent research has shown that the predictions have the potential to be biased and unfair towards certain subgroups of the population. To combat this, several techniques have been introduced to help remove the bias and improve the overall <b>fairness</b> of the predictions. We introduce a new <b>fairness</b> technique, called \textit{Subgroup Threshold Optimizer} (\textit{STO}), that does not require any alternations to the input training data nor does it require any changes to the underlying machine learning algorithm, and thus can be used with any existing machine learning pipeline. STO works by optimizing the classification thresholds for individual subgroups in order to minimize the overall discrimination score between them. Our experiments on a real-world credit lending dataset show that STO can reduce gender discrimination by over 90%.</p></p class="citation"></blockquote><h3 id=2336--160298-using-uncertainty-quantification-to-characterize-and-improve-out-of-domain-learning-for-pdes-s-chandra-mouli-et-al-2024>(23/36 | 160/298) Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs (S. Chandra Mouli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S. Chandra Mouli, Danielle C. Maddix, Shima Alizadeh, Gaurav Gupta, Andrew Stuart, Michael W. Mahoney, Yuyang Wang. (2024)<br><strong>Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs</strong><br><button class=copy-to-clipboard title="Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 10<br>Keywords: Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10642v1.pdf filename=2403.10642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing work in scientific machine learning (SciML) has shown that data-driven learning of solution operators can provide a fast approximate alternative to classical numerical partial differential equation (PDE) solvers. Of these, Neural Operators (NOs) have emerged as particularly promising. We observe that several uncertainty quantification (UQ) methods for NOs fail for test inputs that are even moderately <b>out-of-domain</b> (OOD), even when the model approximates the solution well for in-domain tasks. To address this limitation, we show that ensembling several NOs can identify high-error regions and provide good uncertainty estimates that are well-correlated with prediction errors. Based on this, we propose a cost-effective alternative, DiverseNO, that mimics the properties of the ensemble by encouraging diverse predictions from its multiple heads in the last feed-forward layer. We then introduce Operator-ProbConserv, a method that uses these well-calibrated UQ estimates within the ProbConserv framework to update the model. Our empirical results show that Operator-ProbConserv enhances OOD model performance for a variety of challenging PDE problems and satisfies physical constraints such as conservation laws.</p></p class="citation"></blockquote><h3 id=2436--161298-a-resource-constrained-stochastic-scheduling-algorithm-for-homeless-street-outreach-and-gleaning-edible-food-conor-m-artman-et-al-2024>(24/36 | 161/298) A resource-constrained stochastic scheduling algorithm for homeless street outreach and gleaning edible food (Conor M. Artman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Conor M. Artman, Aditya Mate, Ezinne Nwankwo, Aliza Heching, Tsuyoshi Idé, Jiří Navrátil, Karthikeyan Shanmugam, Wei Sun, Kush R. Varshney, Lauri Goldkind, Gidi Kroch, Jaclyn Sawyer, Ian Watson. (2024)<br><strong>A resource-constrained stochastic scheduling algorithm for homeless street outreach and gleaning edible food</strong><br><button class=copy-to-clipboard title="A resource-constrained stochastic scheduling algorithm for homeless street outreach and gleaning edible food" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10638v1.pdf filename=2403.10638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We developed a common algorithmic solution addressing the problem of resource-constrained outreach encountered by social change organizations with different missions and operations: Breaking Ground &ndash; an organization that helps individuals experiencing homelessness in New York transition to permanent housing and Leket &ndash; the national food bank of Israel that rescues food from farms and elsewhere to feed the hungry. Specifically, we developed an estimation and optimization approach for partially-observed episodic restless <b>bandits</b> under $k$-step transitions. The results show that our Thompson sampling with Markov chain recovery (via Stein variational gradient descent) algorithm significantly outperforms baselines for the problems of both organizations. We carried out this work in a prospective manner with the express goal of devising a flexible-enough but also useful-enough solution that can help overcome a lack of sustainable impact in data science for social good.</p></p class="citation"></blockquote><h3 id=2536--162298-a-comparative-study-on-machine-learning-approaches-for-rock-mass-classification-using-drilling-data-tom-f-hansen-et-al-2024>(25/36 | 162/298) A comparative study on machine learning approaches for rock mass classification using drilling data (Tom F. Hansen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tom F. Hansen, Georg H. Erharter, Zhongqiang Liu, Jim Torresen. (2024)<br><strong>A comparative study on machine learning approaches for rock mass classification using drilling data</strong><br><button class=copy-to-clipboard title="A comparative study on machine learning approaches for rock mass classification using drilling data" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10404v1.pdf filename=2403.10404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current rock engineering design in drill and blast tunnelling primarily relies on engineers&rsquo; observational assessments. Measure While Drilling (MWD) data, a high-resolution sensor dataset collected during tunnel excavation, is underutilised, mainly serving for geological visualisation. This study aims to automate the translation of MWD data into actionable metrics for rock engineering. It seeks to link data to specific engineering actions, thus providing critical decision support for geological challenges ahead of the tunnel face. Leveraging a large and geologically diverse dataset of 500,000 drillholes from 15 tunnels, the research introduces models for accurate rock mass quality classification in a real-world tunnelling context. Both conventional machine learning and image-based deep learning are explored to classify MWD data into Q-classes and Q-values, examples of metrics describing the stability of the rock mass, using both tabular and image data. The results indicate that the K-nearest neighbours algorithm in an ensemble with tree-based models using tabular data, effectively classifies rock mass quality. It achieves a cross-validated balanced accuracy of 0.86 in classifying rock mass into the Q-classes A, B, C, D, E1, E2, and 0.95 for a binary classification with E versus the rest. Classification using a <b>CNN</b> with MWD-images for each blasting round resulted in a balanced accuracy of 0.82 for binary classification. Regressing the Q-value from tabular MWD-data achieved cross-validated R2 and MSE scores of 0.80 and 0.18 for a similar ensemble model as in classification. High performance in regression and classification boosts confidence in automated rock mass assessment. Applying advanced modelling on a unique dataset demonstrates MWD data&rsquo;s value in improving rock mass classification accuracy and advancing data-driven rock engineering design, reducing manual intervention.</p></p class="citation"></blockquote><h3 id=2636--163298-towards-non-adversarial-algorithmic-recourse-tobias-leemann-et-al-2024>(26/36 | 163/298) Towards Non-Adversarial Algorithmic Recourse (Tobias Leemann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Leemann, Martin Pawelczyk, Bardh Prenkaj, Gjergji Kasneci. (2024)<br><strong>Towards Non-Adversarial Algorithmic Recourse</strong><br><button class=copy-to-clipboard title="Towards Non-Adversarial Algorithmic Recourse" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10330v1.pdf filename=2403.10330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The streams of research on adversarial examples and <b>counterfactual</b> explanations have largely been growing independently. This has led to several recent works trying to elucidate their similarities and differences. Most prominently, it has been argued that adversarial examples, as opposed to <b>counterfactual</b> explanations, have a unique characteristic in that they lead to a misclassification compared to the ground truth. However, the computational goals and methodologies employed in existing <b>counterfactual</b> explanation and adversarial example generation methods often lack alignment with this requirement. Using formal definitions of adversarial examples and <b>counterfactual</b> explanations, we introduce non-adversarial algorithmic recourse and outline why in high-stakes situations, it is imperative to obtain <b>counterfactual</b> explanations that do not exhibit adversarial characteristics. We subsequently investigate how different components in the objective functions, e.g., the machine learning model or cost function used to measure distance, determine whether the outcome can be considered an adversarial example or not. Our experiments on common datasets highlight that these design choices are often more critical in deciding whether recourse is non-adversarial than whether recourse or attack algorithms are used. Furthermore, we show that choosing a robust and accurate machine learning model results in less adversarial recourse desired in practice.</p></p class="citation"></blockquote><h3 id=2736--164298-reliable-uncertainty-with-cheaper-neural-network-ensembles-a-case-study-in-industrial-parts-classification-arthur-thuy-et-al-2024>(27/36 | 164/298) Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification (Arthur Thuy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arthur Thuy, Dries F. Benoit. (2024)<br><strong>Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification</strong><br><button class=copy-to-clipboard title="Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10182v1.pdf filename=2403.10182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In operations research (OR), predictive models often encounter <b>out-of-distribution</b> (OOD) scenarios where the data distribution differs from the training data distribution. In recent years, neural networks (NNs) are gaining traction in OR for their exceptional performance in fields such as image classification. However, NNs tend to make confident yet incorrect predictions when confronted with OOD data. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Hence, reliable uncertainty quantification in NNs is crucial in the OR domain. Deep ensembles, composed of multiple independent NNs, have emerged as a promising approach, offering not only strong predictive accuracy but also reliable uncertainty estimation. However, their deployment is challenging due to substantial computational demands. Recent fundamental research has proposed more efficient NN ensembles, namely the snapshot, batch, and multi-input multi-output ensemble. This study is the first to provide a comprehensive comparison of a single NN, a deep ensemble, and the three efficient NN ensembles. In addition, we propose a Diversity Quality metric to quantify the ensembles&rsquo; performance on the in-distribution and OOD sets in one single metric. The OR case study discusses industrial parts classification to identify and manage spare parts, important for timely maintenance of industrial plants. The results highlight the batch ensemble as a cost-effective and competitive alternative to the deep ensemble. It outperforms the deep ensemble in both uncertainty and accuracy while exhibiting a training time speedup of 7x, a test time speedup of 8x, and 9x memory savings.</p></p class="citation"></blockquote><h3 id=2836--165298-explainability-through-uncertainty-trustworthy-decision-making-with-neural-networks-arthur-thuy-et-al-2024>(28/36 | 165/298) Explainability through uncertainty: Trustworthy decision-making with neural networks (Arthur Thuy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arthur Thuy, Dries F. Benoit. (2024)<br><strong>Explainability through uncertainty: Trustworthy decision-making with neural networks</strong><br><button class=copy-to-clipboard title="Explainability through uncertainty: Trustworthy decision-making with neural networks" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10168v1.pdf filename=2403.10168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uncertainty is a key feature of any machine learning model and is particularly important in neural networks, which tend to be overconfident. This overconfidence is worrying under <b>distribution</b> <b>shifts,</b> where the model performance silently degrades as the data <b>distribution</b> <b>diverges</b> from the training data <b>distribution.</b> <b>Uncertainty</b> estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Although methods for uncertainty estimation have been developed, they have not been explicitly linked to the field of explainable artificial intelligence (XAI). Furthermore, literature in operations research ignores the actionability component of uncertainty estimation and does not consider <b>distribution</b> <b>shifts.</b> This work proposes a general uncertainty framework, with contributions being threefold: (i) uncertainty estimation in ML models is positioned as an XAI technique, giving local and model-specific explanations; (ii) classification with rejection is used to reduce misclassifications by bringing a human expert in the loop for uncertain observations; (iii) the framework is applied to a case study on neural networks in educational data mining subject to <b>distribution</b> <b>shifts.</b> Uncertainty as XAI improves the model&rsquo;s trustworthiness in downstream decision-making tasks, giving rise to more actionable and robust machine learning systems in operations research.</p></p class="citation"></blockquote><h3 id=2936--166298-online-policy-learning-from-offline-preferences-guoxi-zhang-et-al-2024>(29/36 | 166/298) Online Policy Learning from Offline Preferences (Guoxi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoxi Zhang, Han Bao, Hisashi Kashima. (2024)<br><strong>Online Policy Learning from Offline Preferences</strong><br><button class=copy-to-clipboard title="Online Policy Learning from Offline Preferences" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10160v1.pdf filename=2403.10160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In preference-based <b>reinforcement</b> <b>learning</b> (PbRL), a reward function is learned from a type of human feedback called preference. To expedite preference collection, recent works have leveraged \emph{offline preferences}, which are preferences collected for some offline data. In this scenario, the learned reward function is fitted on the offline data. If a learning agent exhibits behaviors that do not overlap with the offline data, the learned reward function may encounter generalizability issues. To address this problem, the present study introduces a framework that consolidates offline preferences and \emph{virtual preferences} for PbRL, which are comparisons between the agent&rsquo;s behaviors and the offline data. Critically, the reward function can track the agent&rsquo;s behaviors using the virtual preferences, thereby offering well-aligned guidance to the agent. Through experiments on continuous control tasks, this study demonstrates the effectiveness of incorporating the virtual preferences in PbRL.</p></p class="citation"></blockquote><h3 id=3036--167298-regularization-based-efficient-continual-learning-in-deep-state-space-models-yuanhang-zhang-et-al-2024>(30/36 | 167/298) Regularization-Based Efficient Continual Learning in Deep State-Space Models (Yuanhang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanhang Zhang, Zhidi Lin, Yiyong Sun, Feng Yin, Carsten Fritsche. (2024)<br><strong>Regularization-Based Efficient Continual Learning in Deep State-Space Models</strong><br><button class=copy-to-clipboard title="Regularization-Based Efficient Continual Learning in Deep State-Space Models" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10123v1.pdf filename=2403.10123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep state-space models (DSSMs) have gained popularity in recent years due to their potent modeling capacity for dynamic systems. However, existing DSSM works are limited to single-task modeling, which requires retraining with historical task data upon revisiting a forepassed task. To address this limitation, we propose <b>continual</b> <b>learning</b> DSSMs (CLDSSMs), which are capable of adapting to evolving tasks without catastrophic forgetting. Our proposed CLDSSMs integrate mainstream regularization-based <b>continual</b> <b>learning</b> (CL) methods, ensuring efficient updates with constant computational and memory costs for modeling multiple dynamic systems. We also conduct a comprehensive cost analysis of each CL method applied to the respective CLDSSMs, and demonstrate the efficacy of CLDSSMs through experiments on real-world datasets. The results corroborate that while various competing CL methods exhibit different merits, the proposed CLDSSMs consistently outperform traditional DSSMs in terms of effectively addressing catastrophic forgetting, enabling swift and accurate parameter transfer to new tasks.</p></p class="citation"></blockquote><h3 id=3136--168298-adaptive-random-feature-regularization-on-fine-tuning-deep-neural-networks-shinya-yamaguchi-et-al-2024>(31/36 | 168/298) Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks (Shin&rsquo;ya Yamaguchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shin&rsquo;ya Yamaguchi, Sekitoshi Kanai, Kazuki Adachi, Daiki Chijiwa. (2024)<br><strong>Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks</strong><br><button class=copy-to-clipboard title="Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10097v1.pdf filename=2403.10097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>fine-tuning</b> is a de facto standard method for training deep neural networks, it still suffers from overfitting when using small target datasets. Previous methods improve <b>fine-tuning</b> performance by maintaining knowledge of the source datasets or introducing regularization terms such as contrastive loss. However, these methods require auxiliary source information (e.g., source labels or datasets) or heavy additional computations. In this paper, we propose a simple method called adaptive random feature regularization (AdaRand). AdaRand helps the feature extractors of training models to adaptively change the distribution of feature vectors for downstream classification tasks without auxiliary source information and with reasonable computation costs. To this end, AdaRand minimizes the gap between feature vectors and random reference vectors that are sampled from class conditional Gaussian distributions. Furthermore, AdaRand dynamically updates the conditional distribution to follow the currently updated feature extractors and balance the distance between classes in feature spaces. Our experiments show that AdaRand outperforms the other <b>fine-tuning</b> regularization, which requires auxiliary source information and heavy computation costs.</p></p class="citation"></blockquote><h3 id=3236--169298-unified-projection-free-algorithms-for-adversarial-dr-submodular-optimization-mohammad-pedramfar-et-al-2024>(32/36 | 169/298) Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization (Mohammad Pedramfar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Pedramfar, Yididiya Y. Nadew, Christopher J. Quinn, Vaneet Aggarwal. (2024)<br><strong>Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization</strong><br><button class=copy-to-clipboard title="Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CC, cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10063v1.pdf filename=2403.10063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces unified projection-free Frank-Wolfe type algorithms for adversarial continuous DR-submodular optimization, spanning scenarios such as full information and (semi-)bandit feedback, monotone and non-monotone functions, different constraints, and types of stochastic queries. For every problem considered in the non-monotone setting, the proposed algorithms are either the first with proven sub-linear $\alpha$-regret bounds or have better $\alpha$-regret bounds than the state of the art, where $\alpha$ is a corresponding approximation bound in the offline setting. In the monotone setting, the proposed approach gives state-of-the-art sub-linear $\alpha$-regret bounds among projection-free algorithms in 7 of the 8 considered cases while matching the result of the remaining case. Additionally, this paper addresses semi-bandit and <b>bandit</b> feedback for adversarial DR-submodular optimization, advancing the understanding of this optimization area.</p></p class="citation"></blockquote><h3 id=3336--170298-global-convergence-guarantees-for-federated-policy-gradient-methods-with-adversaries-swetha-ganesh-et-al-2024>(33/36 | 170/298) Global Convergence Guarantees for Federated Policy Gradient Methods with Adversaries (Swetha Ganesh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Swetha Ganesh, Jiayu Chen, Gugan Thoppe, Vaneet Aggarwal. (2024)<br><strong>Global Convergence Guarantees for Federated Policy Gradient Methods with Adversaries</strong><br><button class=copy-to-clipboard title="Global Convergence Guarantees for Federated Policy Gradient Methods with Adversaries" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09940v1.pdf filename=2403.09940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Federated <b>Reinforcement</b> <b>Learning</b> (FRL) allows multiple agents to collaboratively build a decision making policy without sharing raw trajectories. However, if a small fraction of these agents are adversarial, it can lead to catastrophic results. We propose a policy gradient based approach that is robust to adversarial agents which can send arbitrary values to the server. Under this setting, our results form the first global convergence guarantees with general parametrization. These results demonstrate resilience with adversaries, while achieving sample complexity of order $\tilde{\mathcal{O}}\left( \frac{1}{\epsilon^2} \left( \frac{1}{N-f} + \frac{f^2}{(N-f)^2}\right)\right)$, where $N$ is the total number of agents and $f$ is the number of adversarial agents.</p></p class="citation"></blockquote><h3 id=3436--171298-quality-diversity-actor-critic-learning-high-performing-and-diverse-behaviors-via-value-and-successor-features-critics-luca-grillotti-et-al-2024>(34/36 | 171/298) Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics (Luca Grillotti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Grillotti, Maxence Faldor, Borja G. León, Antoine Cully. (2024)<br><strong>Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics</strong><br><button class=copy-to-clipboard title="Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09930v1.pdf filename=2403.09930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations. Over the past decade, advancements in deep <b>reinforcement</b> <b>learning</b> have led to groundbreaking achievements to solve complex continuous control tasks. However, most approaches return only one solution specialized for a specific problem. We introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic deep <b>reinforcement</b> <b>learning</b> algorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors. In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills. Compared with other Quality-Diversity methods, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion tasks. We also demonstrate that we can harness the learned skills to adapt better than other baselines to five perturbed environments. Finally, qualitative analyses showcase a range of remarkable behaviors, available at: <a href=http://bit.ly/qdac>http://bit.ly/qdac</a>.</p></p class="citation"></blockquote><h3 id=3536--172298-backdoor-secrets-unveiled-identifying-backdoor-data-with-optimized-scaled-prediction-consistency-soumyadeep-pal-et-al-2024>(35/36 | 172/298) Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency (Soumyadeep Pal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soumyadeep Pal, Yuguang Yao, Ren Wang, Bingquan Shen, Sijia Liu. (2024)<br><strong>Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency</strong><br><button class=copy-to-clipboard title="Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10717v1.pdf filename=2403.10717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern machine learning (ML) systems demand substantial training data, often resorting to external sources. Nevertheless, this practice renders them vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies have primarily focused on the identification of backdoored models or poisoned data characteristics, typically operating under the assumption of access to clean data. In this work, we delve into a relatively underexplored challenge: the automatic identification of backdoor data within a poisoned dataset, all under realistic conditions, i.e., without the need for additional clean data or without manually defining a threshold for backdoor detection. We draw an inspiration from the scaled prediction consistency (SPC) technique, which exploits the prediction invariance of poisoned data to an input scaling factor. Based on this, we pose the backdoor data identification problem as a hierarchical data splitting optimization problem, leveraging a novel SPC-based loss function as the primary optimization objective. Our innovation unfolds in several key aspects. First, we revisit the vanilla SPC method, unveiling its limitations in addressing the proposed backdoor identification problem. Subsequently, we develop a bi-level optimization-based approach to precisely identify backdoor data by minimizing the advanced SPC loss. Finally, we demonstrate the efficacy of our proposal against a spectrum of backdoor attacks, encompassing basic label-corrupted attacks as well as more sophisticated clean-label attacks, evaluated across various <b>benchmark</b> datasets. Experiment results show that our approach often surpasses the performance of current baselines in identifying backdoor data points, resulting in about 4%-36% improvement in average AUROC. Codes are available at <a href=https://github.com/OPTML-Group/BackdoorMSPC>https://github.com/OPTML-Group/BackdoorMSPC</a>.</p></p class="citation"></blockquote><h3 id=3636--173298-a-survey-of-source-code-representations-for-machine-learning-based-cybersecurity-tasks-beatrice-casey-et-al-2024>(36/36 | 173/298) A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks (Beatrice Casey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beatrice Casey, Joanna C. S. Santos, George Perry. (2024)<br><strong>A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks</strong><br><button class=copy-to-clipboard title="A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10646v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10646v1.pdf filename=2403.10646v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning techniques for cybersecurity-related software engineering tasks are becoming increasingly popular. The representation of source code is a key portion of the technique that can impact the way the model is able to learn the features of the source code. With an increasing number of these techniques being developed, it is valuable to see the current state of the field to better understand what exists and what&rsquo;s not there yet. This paper presents a study of these existing ML-based approaches and demonstrates what type of representations were used for different cybersecurity tasks and programming languages. Additionally, we study what types of models are used with different representations. We have found that <b>graph-based</b> representations are the most popular category of representation, and Tokenizers and Abstract Syntax Trees (ASTs) are the two most popular representations overall. We also found that the most popular cybersecurity task is vulnerability detection, and the language that is covered by the most techniques is C. Finally, we found that sequence-based models are the most popular category of models, and Support Vector Machines (SVMs) are the most popular model overall.</p></p class="citation"></blockquote><h2 id=csse-6>cs.SE (6)</h2><h3 id=16--174298-s3llm-large-scale-scientific-software-understanding-with-llms-using-source-metadata-and-document-kareem-shaik-et-al-2024>(1/6 | 174/298) S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document (Kareem Shaik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kareem Shaik, Dali Wang, Weijian Zheng, Qinglei Cao, Heng Fan, Peter Schwartz, Yunhe Feng. (2024)<br><strong>S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document</strong><br><button class=copy-to-clipboard title="S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 80<br>Keywords: Generative AI, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, LLaMA, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10588v1.pdf filename=2403.10588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The understanding of <b>large-scale</b> <b>scientific</b> <b>software</b> poses significant challenges due to its diverse codebase, extensive code length, and target computing architectures. The emergence of <b>generative</b> <b>AI,</b> specifically <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> provides novel pathways for understanding such complex scientific codes. This paper presents S3LLM, an <b>LLM-based</b> framework designed to enable the examination of source code, code metadata, and <b>summarized</b> information in conjunction with textual technical reports in an interactive, conversational manner through a user-friendly interface. S3LLM leverages open-source <b>LLaMA-2</b> models to enhance code analysis through the automatic transformation of natural language queries into domain-specific language (DSL) queries. Specifically, it translates these queries into Feature Query Language (FQL), enabling efficient scanning and parsing of entire code repositories. In addition, S3LLM is equipped to handle diverse metadata types, including DOT, SQL, and customized formats. Furthermore, S3LLM incorporates <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)</b> and LangChain technologies to directly query extensive documents. S3LLM demonstrates the potential of using locally deployed open-source <b>LLMs</b> for the rapid understanding of <b>large-scale</b> <b>scientific</b> <b>computing</b> software, eliminating the need for extensive coding expertise, and thereby making the process more efficient and effective. S3LLM is available at <a href=https://github.com/ResponsibleAILab/s3llm>https://github.com/ResponsibleAILab/s3llm</a>.</p></p class="citation"></blockquote><h3 id=26--175298-repoformer-selective-retrieval-for-repository-level-code-completion-di-wu-et-al-2024>(2/6 | 175/298) Repoformer: Selective Retrieval for Repository-Level Code Completion (Di Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Di Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Krishna Ramanathan, Xiaofei Ma. (2024)<br><strong>Repoformer: Selective Retrieval for Repository-Level Code Completion</strong><br><button class=copy-to-clipboard title="Repoformer: Selective Retrieval for Repository-Level Code Completion" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-SE, cs.SE<br>Keyword Score: 63<br>Keywords: Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Self-supervised Learning, Self-supervised Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10059v1.pdf filename=2403.10059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> have initiated a new era in repository-level code completion. However, the invariable use of <b>retrieval</b> <b>in</b> <b>existing</b> methods exposes issues in both efficiency and robustness, with a large proportion of the retrieved contexts proving unhelpful or harmful to code language models (code LMs). To tackle the challenges, this paper proposes a selective <b>RAG</b> framework where <b>retrieval</b> <b>is</b> <b>avoided</b> when unnecessary. To power this framework, we design a <b>self-supervised</b> <b>learning</b> approach that enables a code LM to accurately self-evaluate whether <b>retrieval</b> <b>can</b> <b>improve</b> its output quality and robustly leverage the potentially noisy retrieved contexts. Using this LM as both the selective <b>retrieval</b> <b>policy</b> <b>and</b> the generation model, our framework consistently outperforms the state-of-the-art <b>prompting</b> with an invariable <b>retrieval</b> <b>approach</b> <b>on</b> diverse <b>benchmarks</b> including RepoEval, CrossCodeEval, and a new <b>benchmark.</b> Meanwhile, our selective <b>retrieval</b> <b>strategy</b> <b>results</b> in strong efficiency improvements by as much as 70% inference speedup without harming the performance. We demonstrate that our framework effectively accommodates different generation models, retrievers, and programming languages. These advancements position our framework as an important step towards more accurate and efficient repository-level code completion.</p></p class="citation"></blockquote><h3 id=36--176298-large-language-models-to-generate-system-level-test-programs-targeting-non-functional-properties-denis-schwachhofer-et-al-2024>(3/6 | 176/298) Large Language Models to Generate System-Level Test Programs Targeting Non-functional Properties (Denis Schwachhofer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Denis Schwachhofer, Peter Domanski, Steffen Becker, Stefan Wagner, Matthias Sauer, Dirk Pflüger, Ilia Polian. (2024)<br><strong>Large Language Models to Generate System-Level Test Programs Targeting Non-functional Properties</strong><br><button class=copy-to-clipboard title="Large Language Models to Generate System-Level Test Programs Targeting Non-functional Properties" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-ET, cs-PL, cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10086v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10086v2.pdf filename=2403.10086v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>System-Level Test (SLT) has been a part of the test flow for integrated circuits for over a decade and still gains importance. However, no systematic approaches exist for test program generation, especially targeting non-functional properties of the Device under Test (DUT). Currently, test engineers manually compose test suites from off-the-shelf software, approximating the end-user environment of the DUT. This is a challenging and tedious task that does not guarantee sufficient control over non-functional properties. This paper proposes <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to generate test programs. We take a first glance at how pre-trained <b>LLMs</b> perform in test program generation to optimize non-functional properties of the DUT. Therefore, we write a <b>prompt</b> to generate C code snippets that maximize the instructions per cycle of a super-scalar, out-of-order architecture in <b>simulation.</b> Additionally, we apply <b>prompt</b> and hyperparameter optimization to achieve the best possible results without further training.</p></p class="citation"></blockquote><h3 id=46--177298-demystifying-faulty-code-with-llm-step-by-step-reasoning-for-explainable-fault-localization-ratnadira-widyasari-et-al-2024>(4/6 | 177/298) Demystifying Faulty Code with LLM: Step-by-Step Reasoning for Explainable Fault Localization (Ratnadira Widyasari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ratnadira Widyasari, Jia Wei Ang, Truong Giang Nguyen, Neil Sharma, David Lo. (2024)<br><strong>Demystifying Faulty Code with LLM: Step-by-Step Reasoning for Explainable Fault Localization</strong><br><button class=copy-to-clipboard title="Demystifying Faulty Code with LLM: Step-by-Step Reasoning for Explainable Fault Localization" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10507v1.pdf filename=2403.10507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fault localization is a critical process that involves identifying specific program elements responsible for program failures. Manually pinpointing these elements, such as classes, methods, or statements, which are associated with a fault is laborious and time-consuming. To overcome this challenge, various fault localization tools have been developed. These tools typically generate a ranked list of suspicious program elements. However, this information alone is insufficient. A prior study emphasized that automated fault localization should offer a rationale. In this study, we investigate the step-by-step <b>reasoning</b> for explainable fault localization. We explore the potential of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> in assisting developers in <b>reasoning</b> about code. We proposed FuseFL that utilizes several combinations of information to enhance the <b>LLM</b> results which are spectrum-based fault localization results, test case execution outcomes, and code description (i.e., explanation of what the given code is intended to do). We conducted our investigation using faulty code from Refactory dataset. First, we evaluate the performance of the automated fault localization. Our results demonstrate a more than 30% increase in the number of successfully localized faults at Top-1 compared to the baseline. To evaluate the explanations generated by FuseFL, we create a dataset of human explanations that provide step-by-step <b>reasoning</b> as to why specific lines of code are considered faulty. This dataset consists of 324 faulty code files, along with explanations for 600 faulty lines. Furthermore, we also conducted human studies to evaluate the explanations. We found that for 22 out of the 30 randomly sampled cases, FuseFL generated correct explanations.</p></p class="citation"></blockquote><h3 id=56--178298-an-empirical-study-on-developers-shared-conversations-with-chatgpt-in-github-pull-requests-and-issues-huizi-hao-et-al-2024>(5/6 | 178/298) An Empirical Study on Developers Shared Conversations with ChatGPT in GitHub Pull Requests and Issues (Huizi Hao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huizi Hao, Kazi Amit Hasan, Hong Qin, Marcos Macedo, Yuan Tian, Steven H. H. Ding, Ahmed E. Hassan. (2024)<br><strong>An Empirical Study on Developers Shared Conversations with ChatGPT in GitHub Pull Requests and Issues</strong><br><button class=copy-to-clipboard title="An Empirical Study on Developers Shared Conversations with ChatGPT in GitHub Pull Requests and Issues" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: ChatGPT, Code Generation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10468v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10468v1.pdf filename=2403.10468v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>ChatGPT</b> has significantly impacted software development practices, providing substantial assistance to developers in a variety of tasks, including coding, testing, and debugging. Despite its widespread adoption, the impact of <b>ChatGPT</b> as an assistant in collaborative coding remains largely unexplored. In this paper, we analyze a dataset of 210 and 370 developers shared conversations with <b>ChatGPT</b> in GitHub pull requests (PRs) and issues. We manually examined the content of the conversations and characterized the dynamics of the sharing behavior, i.e., understanding the rationale behind the sharing, identifying the locations where the conversations were shared, and determining the roles of the developers who shared them. Our main observations are: (1) Developers seek <b>ChatGPT</b> assistance across 16 types of software engineering inquiries. In both conversations shared in PRs and issues, the most frequently encountered inquiry categories include <b>code</b> <b>generation,</b> conceptual questions, how-to guides, issue resolution, and <b>code</b> <b>review.</b> (2) Developers frequently engage with <b>ChatGPT</b> via multi-turn conversations where each <b>prompt</b> can fulfill various roles, such as unveiling initial or new tasks, iterative follow-up, and <b>prompt</b> refinement. Multi-turn conversations account for 33.2% of the conversations shared in PRs and 36.9% in issues. (3) In collaborative coding, developers leverage shared conversations with <b>ChatGPT</b> to facilitate their role-specific contributions, whether as authors of PRs or issues, <b>code</b> <b>reviewers,</b> or collaborators on issues. Our work serves as the first step towards understanding the dynamics between developers and <b>ChatGPT</b> in collaborative software development and opens up new directions for future research on the topic.</p></p class="citation"></blockquote><h3 id=66--179298-exploring-language-models-code-generation-ability-with-auxiliary-functions-seonghyeon-lee-et-al-2024>(6/6 | 179/298) Exploring Language Model&rsquo;s Code Generation Ability with Auxiliary Functions (Seonghyeon Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seonghyeon Lee, Sanghwan Jang, Seongbo Jang, Dongha Lee, Hwanjo Yu. (2024)<br><strong>Exploring Language Model&rsquo;s Code Generation Ability with Auxiliary Functions</strong><br><button class=copy-to-clipboard title="Exploring Language Model's Code Generation Ability with Auxiliary Functions" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Code Generation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10575v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10575v1.pdf filename=2403.10575v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Auxiliary function is a helpful component to improve language model&rsquo;s <b>code</b> <b>generation</b> ability. However, a systematic exploration of how they affect has yet to be done. In this work, we comprehensively evaluate the ability to utilize auxiliary functions encoded in recent <b>code-pretrained</b> <b>language</b> models. First, we construct a human-crafted evaluation set, called HumanExtension, which contains examples of two functions where one function assists the other. With HumanExtension, we design several experiments to examine their ability in a multifaceted way. Our evaluation processes enable a comprehensive understanding of including auxiliary functions in the <b>prompt</b> in terms of effectiveness and robustness. An additional implementation style analysis captures the models&rsquo; various implementation patterns when they access the auxiliary function. Through this analysis, we discover the models&rsquo; promising ability to utilize auxiliary functions including their self-improving behavior by implementing the two functions step-by-step. However, our analysis also reveals the model&rsquo;s underutilized behavior to call the auxiliary function, suggesting the future direction to enhance their implementation by eliciting the auxiliary function call ability encoded in the models. We release our <b>code</b> <b>and</b> dataset to facilitate this research direction.</p></p class="citation"></blockquote><h2 id=csdc-4>cs.DC (4)</h2><h3 id=14--180298-atom-asynchronous-training-of-massive-models-for-deep-learning-in-a-decentralized-environment-xiaofeng-wu-et-al-2024>(1/4 | 180/298) ATOM: Asynchronous Training of Massive Models for Deep Learning in a Decentralized Environment (Xiaofeng Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaofeng Wu, Jia Rao, Wei Chen. (2024)<br><strong>ATOM: Asynchronous Training of Massive Models for Deep Learning in a Decentralized Environment</strong><br><button class=copy-to-clipboard title="ATOM: Asynchronous Training of Massive Models for Deep Learning in a Decentralized Environment" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-SE, cs.DC<br>Keyword Score: 60<br>Keywords: Fine-tuning, GPT, GPT-3, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10504v1.pdf filename=2403.10504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of the <b>Transformer</b> architecture has propelled the growth of natural language processing (NLP) models, leading to remarkable achievements in numerous NLP tasks. Yet, the absence of specialized hardware like expansive GPU memory and high-speed interconnects poses challenges for training <b>large-scale</b> <b>models.</b> <b>This</b> makes it daunting for many users to experiment with pre-training and <b>fine-tuning</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> In this study, we introduce \atom, a resilient distributed training framework designed for asynchronous training of vast models in a decentralized setting using cost-effective hardware, including consumer-grade GPUs and Ethernet. Unlike conventional model partitioning methods that distribute sub-models across GPUs, \atom aims to accommodate a complete <b>LLM</b> on one host (peer) through seamlessly model swapping and concurrently trains multiple copies across various peers to optimize training throughput. Through static analysis, \atom identifies the best model partitioning strategy and flawlessly merges model execution with swapping. Key benefits of \atom include: Avoiding the central point of failure found in pipeline parallelism methods. Demonstrating superior performance and scalability compared to closely-integrated pipeline parallelism in slower networks. Our experiments using different <b>GPT-3</b> model configurations reveal that, in scenarios with suboptimal network connections, \atom can enhance training efficiency up to $20 \times$ when juxtaposed with the state-of-the-art decentralized pipeline parallelism approaches.</p></p class="citation"></blockquote><h3 id=24--181298-dsp-dynamic-sequence-parallelism-for-multi-dimensional-transformers-xuanlei-zhao-et-al-2024>(2/4 | 181/298) DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers (Xuanlei Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuanlei Zhao, Shenggan Cheng, Zangwei Zheng, Zheming Yang, Ziming Liu, Yang You. (2024)<br><strong>DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers</strong><br><button class=copy-to-clipboard title="DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-LG, cs.DC<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Transformer, Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10266v1.pdf filename=2403.10266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scaling large models with long sequences across applications like <b>language</b> <b>generation,</b> video generation and <b>multimodal</b> tasks requires efficient sequence parallelism. However, existing sequence parallelism methods all assume a single sequence dimension and fail to adapt to multi-dimensional <b>transformer</b> architectures that perform attention calculations across different dimensions. This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to enable efficient sequence parallelism for multi-dimensional <b>transformer</b> models. The key idea is to dynamically switch the parallelism dimension according to the current computation stage, leveraging the potential characteristics of multi-dimensional attention. This dynamic dimension switching allows sequence parallelism with minimal communication overhead compared to applying traditional single-dimension parallelism to multi-dimensional models. Experiments show DSP improves end-to-end throughput by 42.0% to 216.8% over prior sequence parallelism methods.</p></p class="citation"></blockquote><h3 id=34--182298-greedyml-a-parallel-algorithm-for-maximizing-submodular-functions-shivaram-gopal-et-al-2024>(3/4 | 182/298) GreedyML: A Parallel Algorithm for Maximizing Submodular Functions (Shivaram Gopal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shivaram Gopal, S M Ferdous, Hemanta K. Maji, Alex Pothen. (2024)<br><strong>GreedyML: A Parallel Algorithm for Maximizing Submodular Functions</strong><br><button class=copy-to-clipboard title="GreedyML: A Parallel Algorithm for Maximizing Submodular Functions" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-DS, cs-LG, cs.DC<br>Keyword Score: 13<br>Keywords: Graph, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10332v1.pdf filename=2403.10332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We describe a parallel approximation algorithm for maximizing monotone submodular functions subject to hereditary constraints on distributed memory multiprocessors. Our work is motivated by the need to solve submodular optimization problems on massive data sets, for practical applications in areas such as data <b>summarization,</b> machine learning, and <b>graph</b> sparsification. Our work builds on the randomized distributed RandGreedI algorithm, proposed by Barbosa, Ene, Nguyen, and Ward (2015). This algorithm computes a distributed solution by randomly partitioning the data among all the processors and then employing a single accumulation step in which all processors send their partial solutions to one processor. However, for large problems, the accumulation step could exceed the memory available on a processor, and the processor which performs the accumulation could become a computational bottleneck. Here, we propose a generalization of the RandGreedI algorithm that employs multiple accumulation steps to reduce the memory required. We analyze the approximation ratio and the time complexity of the algorithm (in the BSP model). We also evaluate the new GreedyML algorithm on three classes of problems, and report results from massive data sets with millions of elements. The results show that the GreedyML algorithm can solve problems where the sequential Greedy and distributed RandGreedI algorithms fail due to memory constraints. For certain computationally intensive problems, the GreedyML algorithm can be faster than the RandGreedI algorithm. The observed approximation quality of the solutions computed by the GreedyML algorithm closely matches those obtained by the RandGreedI algorithm on these problems.</p></p class="citation"></blockquote><h3 id=44--183298-strict-partitioning-for-sporadic-rigid-gang-tasks-binqi-sun-et-al-2024>(4/4 | 183/298) Strict Partitioning for Sporadic Rigid Gang Tasks (Binqi Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binqi Sun, Tomasz Kloda, Marco Caccamo. (2024)<br><strong>Strict Partitioning for Sporadic Rigid Gang Tasks</strong><br><button class=copy-to-clipboard title="Strict Partitioning for Sporadic Rigid Gang Tasks" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-AI, cs-AR, cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10726v1.pdf filename=2403.10726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rigid gang task model is based on the idea of executing multiple threads simultaneously on a fixed number of processors to increase efficiency and performance. Although there is extensive literature on global rigid gang scheduling, partitioned approaches have several practical advantages (e.g., task isolation and reduced scheduling overheads). In this paper, we propose a new partitioned scheduling strategy for rigid gang tasks, named strict partitioning. The method creates disjoint partitions of tasks and processors to avoid inter-partition interference. Moreover, it tries to assign tasks with similar volumes (i.e., parallelisms) to the same partition so that the intra-partition interference can be reduced. Within each partition, the tasks can be scheduled using any type of scheduler, which allows the use of a less pessimistic schedulability test. Extensive synthetic experiments and a case study based on Edge TPU <b>benchmarks</b> show that strict partitioning achieves better schedulability performance than state-of-the-art global gang schedulability analyses for both preemptive and non-preemptive rigid gang task sets.</p></p class="citation"></blockquote><h2 id=csro-25>cs.RO (25)</h2><h3 id=125--184298-grasp-anything-combining-teacher-augmented-policy-gradient-learning-with-instance-segmentation-to-grasp-arbitrary-objects-malte-mosbach-et-al-2024>(1/25 | 184/298) Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning with Instance Segmentation to Grasp Arbitrary Objects (Malte Mosbach et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Malte Mosbach, Sven Behnke. (2024)<br><strong>Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning with Instance Segmentation to Grasp Arbitrary Objects</strong><br><button class=copy-to-clipboard title="Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning with Instance Segmentation to Grasp Arbitrary Objects" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Knowledge Distillation, Reinforcement Learning, Simulation, Simulator, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10187v1.pdf filename=2403.10187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interactive grasping from clutter, akin to human dexterity, is one of the longest-standing problems in robot learning. Challenges stem from the intricacies of visual perception, the demand for precise motor skills, and the complex interplay between the two. In this work, we present Teacher-Augmented Policy Gradient (TAPG), a novel two-stage learning framework that synergizes <b>reinforcement</b> <b>learning</b> and policy <b>distillation.</b> After training a teacher policy to master the motor control based on object pose information, TAPG facilitates guided, yet adaptive, learning of a sensorimotor policy, based on object segmentation. We <b>zero-shot</b> transfer from <b>simulation</b> to a real robot by using Segment Anything Model for promptable object segmentation. Our trained policies adeptly grasp a wide variety of objects from cluttered scenarios in <b>simulation</b> and the real world based on human-understandable <b>prompts.</b> Furthermore, we show robust <b>zero-shot</b> transfer to novel objects. Videos of our experiments are available at \url{https://maltemosbach.github.io/grasp_anything}.</p></p class="citation"></blockquote><h3 id=225--185298-spiking-neural-networks-for-fast-moving-object-detection-on-neuromorphic-hardware-devices-using-an-event-based-camera-andreas-ziegler-et-al-2024>(2/25 | 185/298) Spiking Neural Networks for Fast-Moving Object Detection on Neuromorphic Hardware Devices Using an Event-Based Camera (Andreas Ziegler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas Ziegler, Karl Vetter, Thomas Gossard, Jonas Tebbe, Andreas Zell. (2024)<br><strong>Spiking Neural Networks for Fast-Moving Object Detection on Neuromorphic Hardware Devices Using an Event-Based Camera</strong><br><button class=copy-to-clipboard title="Spiking Neural Networks for Fast-Moving Object Detection on Neuromorphic Hardware Devices Using an Event-Based Camera" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 43<br>Keywords: Object Detection, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10677v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10677v1.pdf filename=2403.10677v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Table tennis is a fast-paced and exhilarating sport that demands agility, precision, and fast reflexes. In recent years, robotic table tennis has become a popular research challenge for robot perception algorithms. Fast and accurate ball detection is crucial for enabling a robotic arm to rally the ball back successfully. Previous approaches have employed conventional frame-based cameras with <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> or traditional computer vision methods. In this paper, we propose a novel solution that combines an event-based camera with Spiking Neural Networks (SNNs) for ball detection. We use multiple state-of-the-art SNN frameworks and develop a SNN architecture for each of them, complying with their corresponding constraints. Additionally, we implement the SNN solution across multiple neuromorphic edge devices, conducting comparisons of their accuracies and run-times. This furnishes robotics researchers with a <b>benchmark</b> illustrating the capabilities achievable with each SNN framework and a corresponding neuromorphic edge device. Next to this comparison of SNN solutions for robots, we also show that an SNN on a neuromorphic edge device is able to run in real-time in a closed loop robotic system, a table tennis robot in our use case.</p></p class="citation"></blockquote><h3 id=325--186298-language-to-map-topological-map-generation-from-natural-language-path-instructions-hideki-deguchi-et-al-2024>(3/25 | 186/298) Language to Map: Topological map generation from natural language path instructions (Hideki Deguchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hideki Deguchi, Kazuki Shibata, Shun Taguchi. (2024)<br><strong>Language to Map: Topological map generation from natural language path instructions</strong><br><button class=copy-to-clipboard title="Language to Map: Topological map generation from natural language path instructions" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10008v1.pdf filename=2403.10008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, a method for generating a map from path information described using natural language (textual path) is proposed. In recent years, robotics research mainly focus on <b>vision-and-language</b> navigation (VLN), a navigation task based on images and textual paths. Although VLN is expected to facilitate user instructions to robots, its current implementation requires users to explain the details of the path for each navigation session, which results in high explanation costs for users. To solve this problem, we proposed a method that creates a map as a topological map from a textual path and automatically creates a new path using this map. We believe that <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can be used to understand textual path. Therefore, we propose and evaluate two methods, one for storing implicit maps in <b>LLMs,</b> and the other for generating explicit maps using <b>LLMs.</b> The implicit map is in the <b>LLM&rsquo;s</b> memory. It is created using <b>prompts.</b> In the explicit map, a topological map composed of nodes and edges is constructed and the actions at each node are stored. This makes it possible to estimate the path and actions at waypoints on an undescribed path, if enough information is available. Experimental results on path instructions generated in a real environment demonstrate that generating explicit maps achieves significantly higher accuracy than storing implicit maps in the <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=425--187298-autonomous-monitoring-of-pharmaceutical-rd-laboratories-with-6-axis-arm-equipped-quadruped-robot-and-generative-ai-a-preliminary-study-shunichi-hato-et-al-2024>(4/25 | 187/298) Autonomous Monitoring of Pharmaceutical R&amp;D Laboratories with 6 Axis Arm Equipped Quadruped Robot and Generative AI: A Preliminary Study (Shunichi Hato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shunichi Hato, Nozomi Ogawa. (2024)<br><strong>Autonomous Monitoring of Pharmaceutical R&amp;D Laboratories with 6 Axis Arm Equipped Quadruped Robot and Generative AI: A Preliminary Study</strong><br><button class=copy-to-clipboard title="Autonomous Monitoring of Pharmaceutical R&D Laboratories with 6 Axis Arm Equipped Quadruped Robot and Generative AI: A Preliminary Study" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Anomaly Detection, Foundation Model, Generative AI, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10108v1.pdf filename=2403.10108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a proof-of-concept study that examines the utilization of <b>generative</b> <b>AI</b> and mobile robotics for autonomous laboratory monitoring in the pharmaceutical R&amp;D laboratory. The study investigates the potential advantages of <b>anomaly</b> <b>detection</b> and automated reporting by <b>multi-modal</b> model and Vision <b>Foundation</b> <b>Model</b> (VFM), which have the potential to enhance compliance and safety in laboratory environments. Additionally, the paper discusses the current limitations of the <b>generative</b> <b>AI</b> approach and proposes future directions for its application in lab monitoring.</p></p class="citation"></blockquote><h3 id=525--188298-her-drlheterogeneous-relational-deep-reinforcement-learning-for-decentralized-multi-robot-crowd-navigation-xinyu-zhou-et-al-2024>(5/25 | 188/298) HeR-DRL:Heterogeneous Relational Deep Reinforcement Learning for Decentralized Multi-Robot Crowd Navigation (Xinyu Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Zhou, Songhao Piao, Wenzheng Chi, Liguo Chen, Wei Li. (2024)<br><strong>HeR-DRL:Heterogeneous Relational Deep Reinforcement Learning for Decentralized Multi-Robot Crowd Navigation</strong><br><button class=copy-to-clipboard title="HeR-DRL:Heterogeneous Relational Deep Reinforcement Learning for Decentralized Multi-Robot Crowd Navigation" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10083v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10083v1.pdf filename=2403.10083v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crowd navigation has received significant research attention in recent years, especially DRL-based methods. While single-robot crowd scenarios have dominated research, they offer limited applicability to real-world complexities. The heterogeneity of interaction among multiple agent categories, like in decentralized multi-robot pedestrian scenarios, are frequently disregarded. This &ldquo;interaction blind spot&rdquo; hinders generalizability and restricts progress towards robust navigation algorithms. In this paper, we propose a heterogeneous relational deep <b>reinforcement</b> <b>learning(HeR-DRL),</b> based on customised heterogeneous <b>GNN,</b> in order to improve navigation strategies in decentralized multi-robot crowd navigation. Firstly, we devised a method for constructing robot-crowd heterogenous relation <b>graph</b> <b>that</b> <b>effectively</b> simulates the heterogeneous pair-wise interaction relationships. We proposed a new heterogeneous <b>graph</b> <b>neural</b> <b>network</b> for transferring and aggregating the heterogeneous state information. Finally, we incorporate the encoded information into deep <b>reinforcement</b> <b>learning</b> to explore the optimal policy. HeR-DRL are rigorously evaluated through comparing it to state-of-the-art algorithms in both single-robot and multi-robot circle crowssing scenario. The experimental results demonstrate that HeR-DRL surpasses the state-of-the-art approaches in overall performance, particularly excelling in safety and comfort metrics. This underscores the significance of interaction heterogeneity for crowd navigation. The source code will be publicly released in <a href=https://github.com/Zhouxy-Debugging-Den/HeR-DRL>https://github.com/Zhouxy-Debugging-Den/HeR-DRL</a>.</p></p class="citation"></blockquote><h3 id=625--189298-mind-the-error-detection-and-localization-of-instruction-errors-in-vision-and-language-navigation-francesco-taioli-et-al-2024>(6/25 | 189/298) Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation (Francesco Taioli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Taioli, Stefano Rosa, Alberto Castellini, Lorenzo Natale, Alessio Del Bue, Alessandro Farinelli, Marco Cristani, Yiming Wang. (2024)<br><strong>Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation</strong><br><button class=copy-to-clipboard title="Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CL, cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10700v1.pdf filename=2403.10700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-and-Language</b> Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE <b>benchmarks</b> do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel <b>benchmark</b> dataset that introduces various types of instruction errors considering potential human causes. This <b>benchmark</b> provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable performance drop (up to -25%) in Success Rate when evaluating the state-of-the-art VLN-CE methods on our <b>benchmark.</b> Moreover, we formally define the task of Instruction Error Detection and Localization, and establish an evaluation protocol on top of our <b>benchmark</b> dataset. We also propose an effective method, based on a cross-modal <b>transformer</b> architecture, that achieves the best performance in error detection and localization, compared to baselines. Surprisingly, our proposed method has revealed errors in the validation set of the two commonly used datasets for VLN-CE, i.e., R2R-CE and RxR-CE, demonstrating the utility of our technique in other tasks. Code and dataset will be made available upon acceptance at <a href=https://intelligolabs.github.io/R2RIE-CE>https://intelligolabs.github.io/R2RIE-CE</a></p></p class="citation"></blockquote><h3 id=725--190298-virtual-elastic-tether-a-new-approach-for-multi-agent-navigation-in-confined-aquatic-environments-kanzhong-yao-et-al-2024>(7/25 | 190/298) Virtual Elastic Tether: a New Approach for Multi-agent Navigation in Confined Aquatic Environments (Kanzhong Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kanzhong Yao, Xueliang Cheng, Keir Groves, Barry Lennox, Ognjen Marjanovic, Simon Watson. (2024)<br><strong>Virtual Elastic Tether: a New Approach for Multi-agent Navigation in Confined Aquatic Environments</strong><br><button class=copy-to-clipboard title="Virtual Elastic Tether: a New Approach for Multi-agent Navigation in Confined Aquatic Environments" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10629v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10629v1.pdf filename=2403.10629v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Underwater navigation is a challenging area in the field of mobile robotics due to inherent constraints in self-localisation and communication in underwater environments. Some of these challenges can be mitigated by using collaborative multi-agent teams. However, when applied underwater, the robustness of traditional multi-agent collaborative control approaches is highly limited due to the unavailability of reliable measurements. In this paper, the concept of a Virtual Elastic Tether (VET) is introduced in the context of incomplete state measurements, which represents an innovative approach to underwater navigation in confined spaces. The concept of VET is formulated and validated using the Cooperative Aquatic Vehicle Exploration System (CAVES), which is a sim-to-real multi-agent aquatic robotic platform. Within this framework, a vision-based Autonomous Underwater Vehicle-Autonomous Surface Vehicle leader-follower formulation is developed. Experiments were conducted in both <b>simulation</b> and on a physical platform, <b>benchmarked</b> against a traditional Image-Based Visual Servoing approach. Results indicate that the formation of the baseline approach fails under discrete disturbances, when induced distances between the robots exceeds 0.6 m in <b>simulation</b> and 0.3 m in the real world. In contrast, the VET-enhanced system recovers to pre-perturbation distances within 5 seconds. Furthermore, results illustrate the successful navigation of VET-enhanced CAVES in a confined water pond where the baseline approach fails to perform adequately.</p></p class="citation"></blockquote><h3 id=825--191298-humanoidbench-simulated-humanoid-benchmark-for-whole-body-locomotion-and-manipulation-carmelo-sferrazza-et-al-2024>(8/25 | 191/298) HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation (Carmelo Sferrazza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, Pieter Abbeel. (2024)<br><strong>HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation</strong><br><button class=copy-to-clipboard title="HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Benchmarking, Reinforcement Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10506v1.pdf filename=2403.10506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning <b>benchmark,</b> HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art <b>reinforcement</b> <b>learning</b> algorithms struggle with most tasks, whereas a hierarchical learning baseline achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitating <b>prompt</b> verification of algorithms and ideas. The open-source code is available at <a href=https://sferrazza.cc/humanoidbench_site>https://sferrazza.cc/humanoidbench_site</a>.</p></p class="citation"></blockquote><h3 id=925--192298-online-concurrent-multi-robot-coverage-path-planning-ratijit-mitra-et-al-2024>(9/25 | 192/298) Online Concurrent Multi-Robot Coverage Path Planning (Ratijit Mitra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ratijit Mitra, Indranil Saha. (2024)<br><strong>Online Concurrent Multi-Robot Coverage Path Planning</strong><br><button class=copy-to-clipboard title="Online Concurrent Multi-Robot Coverage Path Planning" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10460v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10460v1.pdf filename=2403.10460v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, centralized receding horizon online multi-robot coverage path planning algorithms have shown remarkable scalability in thoroughly exploring large, complex, unknown workspaces with many robots. In a horizon, the path planning and the path execution interleave, meaning when the path planning occurs for robots with no paths, the robots with outstanding paths do not execute, and subsequently, when the robots with new or outstanding paths execute to reach respective goals, path planning does not occur for those robots yet to get new paths, leading to wastage of both the robotic and the computation resources. As a remedy, we propose a centralized algorithm that is not horizon-based. It plans paths at any time for a subset of robots with no paths, i.e., who have reached their previously assigned goals, while the rest execute their outstanding paths, thereby enabling concurrent planning and execution. We formally prove that the proposed algorithm ensures complete coverage of an unknown workspace and analyze its time complexity. To demonstrate scalability, we evaluate our algorithm to cover eight large $2$D grid <b>benchmark</b> workspaces with up to 512 aerial and ground robots, respectively. A comparison with a state-of-the-art horizon-based algorithm shows its superiority in completing the coverage with up to 1.6x speedup. For validation, we perform ROS + Gazebo <b>simulations</b> in six 2D grid <b>benchmark</b> workspaces with 10 quadcopters and TurtleBots, respectively. We also successfully conducted one outdoor experiment with three quadcopters and one indoor with two TurtleBots.</p></p class="citation"></blockquote><h3 id=1025--193298-incentive-compatible-and-distributed-allocation-for-robotic-service-provision-through-contract-theory-yuhan-zhao-et-al-2024>(10/25 | 193/298) Incentive-Compatible and Distributed Allocation for Robotic Service Provision Through Contract Theory (Yuhan Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhan Zhao, Quanyan Zhu. (2024)<br><strong>Incentive-Compatible and Distributed Allocation for Robotic Service Provision Through Contract Theory</strong><br><button class=copy-to-clipboard title="Incentive-Compatible and Distributed Allocation for Robotic Service Provision Through Contract Theory" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10733v1.pdf filename=2403.10733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robot allocation plays an essential role in facilitating robotic service provision across various domains. Yet the increasing number of users and the uncertainties regarding the users&rsquo; true service requirements have posed challenges for the service provider in effectively allocating service robots to users to meet their needs. In this work, we first propose a contract-based approach to enable incentive-compatible service selection so that the service provider can effectively reduce the user&rsquo;s service uncertainties for precise service provision. Then, we develop a distributed allocation algorithm that incorporates robot dynamics and collision avoidance to allocate service robots and address scalability concerns associated with increasing numbers of service robots and users. We conduct <b>simulations</b> in eight scenarios to validate our approach. Comparative analysis against the robust allocation paradigm and two alternative uncertainty reduction strategies demonstrates that our approach achieves better allocation efficiency and accuracy.</p></p class="citation"></blockquote><h3 id=1125--194298-reconfigurable-robot-identification-from-motion-data-yuhang-hu-et-al-2024>(11/25 | 194/298) Reconfigurable Robot Identification from Motion Data (Yuhang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhang Hu, Yunzhe Wang, Ruibo Liu, Zhou Shen, Hod Lipson. (2024)<br><strong>Reconfigurable Robot Identification from Motion Data</strong><br><button class=copy-to-clipboard title="Reconfigurable Robot Identification from Motion Data" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10496v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10496v1.pdf filename=2403.10496v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrating <b>Large</b> <b>Language</b> <b>Models</b> (VLMs) and <b>Vision-Language</b> Models (VLMs) with robotic systems enables robots to process and understand complex natural language instructions and visual information. However, a fundamental challenge remains: for robots to fully capitalize on these advancements, they must have a deep understanding of their physical embodiment. The gap between AI models cognitive capabilities and the understanding of physical embodiment leads to the following question: Can a robot autonomously understand and adapt to its physical form and functionalities through interaction with its environment? This question underscores the transition towards developing self-modeling robots without reliance on external sensory or pre-programmed knowledge about their structure. Here, we propose a meta self modeling that can deduce robot morphology through proprioception (the internal sense of position and movement). Our study introduces a 12 DoF reconfigurable legged robot, accompanied by a diverse dataset of 200k unique configurations, to systematically investigate the relationship between robotic motion and robot morphology. Utilizing a deep neural network model comprising a robot signature encoder and a configuration decoder, we demonstrate the capability of our system to accurately predict robot configurations from proprioceptive signals. This research contributes to the field of robotic self-modeling, aiming to enhance understanding of their physical embodiment and adaptability in real world scenarios.</p></p class="citation"></blockquote><h3 id=1225--195298-partially-observable-task-and-motion-planning-with-uncertainty-and-risk-awareness-aidan-curtis-et-al-2024>(12/25 | 195/298) Partially Observable Task and Motion Planning with Uncertainty and Risk Awareness (Aidan Curtis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aidan Curtis, George Matheos, Nishad Gothoskar, Vikash Mansinghka, Joshua Tenenbaum, Tomás Lozano-Pérez, Leslie Pack Kaelbling. (2024)<br><strong>Partially Observable Task and Motion Planning with Uncertainty and Risk Awareness</strong><br><button class=copy-to-clipboard title="Partially Observable Task and Motion Planning with Uncertainty and Risk Awareness" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10454v1.pdf filename=2403.10454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrated task and motion planning (TAMP) has proven to be a valuable approach to generalizable long-horizon robotic manipulation and navigation problems. However, the typical TAMP problem formulation assumes full observability and deterministic action effects. These assumptions limit the ability of the planner to gather information and make decisions that are risk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness (TAMPURA) that is capable of efficiently solving long-horizon planning problems with initial-state and action outcome uncertainty, including problems that require information gathering and avoiding undesirable and irreversible outcomes. Our planner reasons under uncertainty at both the abstract task level and continuous controller level. Given a set of closed-loop goal-conditioned controllers operating in the primitive action space and a description of their preconditions and potential capabilities, we learn a high-level abstraction that can be solved efficiently and then refined to continuous actions for execution. We demonstrate our approach on several robotics problems where uncertainty is a crucial factor and show that <b>reasoning</b> under uncertainty in these problems outperforms previously proposed determinized planning, direct search, and <b>reinforcement</b> <b>learning</b> strategies. Lastly, we demonstrate our planner on two real-world robotics problems using recent advancements in probabilistic perception.</p></p class="citation"></blockquote><h3 id=1325--196298-collaborative-aquatic-positioning-system-utilising-multi-beam-sonar-and-depth-sensors-xueliang-cheng-et-al-2024>(13/25 | 196/298) Collaborative Aquatic Positioning System Utilising Multi-beam Sonar and Depth Sensors ({Xueliang Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>{Xueliang Cheng, Barry Lennox, Keir Groves. (2024)<br><strong>Collaborative Aquatic Positioning System Utilising Multi-beam Sonar and Depth Sensors</strong><br><button class=copy-to-clipboard title="Collaborative Aquatic Positioning System Utilising Multi-beam Sonar and Depth Sensors" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10397v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10397v2.pdf filename=2403.10397v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate positioning of remotely operated underwater vehicles (ROVs) in confined environments is crucial for inspection and mapping tasks and is also a prerequisite for autonomous operations. Presently, there are no positioning systems available that are suited for real-world use in confined underwater environments, unconstrained by environmental lighting and water turbidity levels and have sufficient accuracy for long-term, reliable and repeatable navigation. This shortage presents a significant barrier to enhancing the capabilities of ROVs in such scenarios. This paper introduces an innovative positioning system for ROVs operating in confined, cluttered underwater settings, achieved through the collaboration of an omnidirectional surface vehicle and an ROV. A formulation is proposed and evaluated in the <b>simulation</b> against ground truth. The experimental results from the <b>simulation</b> form a proof of principle of the proposed system and also demonstrate its deployability. Unlike many previous approaches, the system does not rely on fixed infrastructure or tracking of features in the environment and can cover large enclosed areas without additional equipment.</p></p class="citation"></blockquote><h3 id=1425--197298-offline-goal-conditioned-reinforcement-learning-for-shape-control-of-deformable-linear-objects-rita-laezza-et-al-2024>(14/25 | 197/298) Offline Goal-Conditioned Reinforcement Learning for Shape Control of Deformable Linear Objects (Rita Laezza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rita Laezza, Mohammadreza Shetab-Bushehri, Gabriel Arslan Waltersson, Erol Özgür, Youcef Mezouar, Yiannis Karayiannidis. (2024)<br><strong>Offline Goal-Conditioned Reinforcement Learning for Shape Control of Deformable Linear Objects</strong><br><button class=copy-to-clipboard title="Offline Goal-Conditioned Reinforcement Learning for Shape Control of Deformable Linear Objects" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10290v1.pdf filename=2403.10290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deformable objects present several challenges to the field of robotic manipulation. One of the tasks that best encapsulates the difficulties arising due to non-rigid behavior is shape control, which requires driving an object to a desired shape. While shape-servoing methods have been shown successful in contexts with approximately linear behavior, they can fail in tasks with more complex dynamics. We investigate an alternative approach, using offline RL to solve a planar shape control problem of a Deformable Linear Object (DLO). To evaluate the effect of material properties, two DLOs are tested namely a soft rope and an elastic cord. We frame this task as a goal-conditioned offline RL problem, and aim to learn to generalize to unseen goal shapes. Data collection and augmentation procedures are proposed to limit the amount of experimental data which needs to be collected with the real robot. We evaluate the amount of augmentation needed to achieve the best results, and test the effect of regularization through behavior cloning on the TD3+BC algorithm. Finally, we show that the proposed approach is able to outperform a shape-servoing baseline in a curvature inversion experiment.</p></p class="citation"></blockquote><h3 id=1525--198298-belief-aided-navigation-using-bayesian-reinforcement-learning-for-avoiding-humans-in-blind-spots-jinyeob-kim-et-al-2024>(15/25 | 198/298) Belief Aided Navigation using Bayesian Reinforcement Learning for Avoiding Humans in Blind Spots (Jinyeob Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyeob Kim, Daewon Kwak, Hyunwoo Rim, Donghan Kim. (2024)<br><strong>Belief Aided Navigation using Bayesian Reinforcement Learning for Avoiding Humans in Blind Spots</strong><br><button class=copy-to-clipboard title="Belief Aided Navigation using Bayesian Reinforcement Learning for Avoiding Humans in Blind Spots" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10105v1.pdf filename=2403.10105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research on mobile robot navigation has focused on socially aware navigation in crowded environments. However, existing methods do not adequately account for human robot interactions and demand accurate location information from omnidirectional sensors, rendering them unsuitable for practical applications. In response to this need, this study introduces a novel algorithm, BNBRL+, predicated on the partially observable <b>Markov</b> <b>decision</b> <b>process</b> framework to assess risks in unobservable areas and formulate movement strategies under uncertainty. BNBRL+ consolidates belief algorithms with Bayesian neural networks to probabilistically infer beliefs based on the positional data of humans. It further integrates the dynamics between the robot, humans, and inferred beliefs to determine the navigation paths and embeds social norms within the reward function, thereby facilitating socially aware navigation. Through experiments in various risk laden scenarios, this study validates the effectiveness of BNBRL+ in navigating crowded environments with blind spots. The model&rsquo;s ability to navigate effectively in spaces with limited visibility and avoid obstacles dynamically can significantly improve the safety and reliability of autonomous vehicles.</p></p class="citation"></blockquote><h3 id=1625--199298-agile-and-safe-trajectory-planning-for-quadruped-navigation-with-motion-anisotropy-awareness-wentao-zhang-et-al-2024>(16/25 | 199/298) Agile and Safe Trajectory Planning for Quadruped Navigation with Motion Anisotropy Awareness (Wentao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wentao Zhang, Shaohang Xu, Peiyuan Cai, Lijun Zhu. (2024)<br><strong>Agile and Safe Trajectory Planning for Quadruped Navigation with Motion Anisotropy Awareness</strong><br><button class=copy-to-clipboard title="Agile and Safe Trajectory Planning for Quadruped Navigation with Motion Anisotropy Awareness" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10101v1.pdf filename=2403.10101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quadruped robots demonstrate robust and agile movements in various terrains; however, their navigation autonomy is still insufficient. One of the challenges is that the motion capabilities of the quadruped robot are anisotropic along different directions, which significantly affects the safety of quadruped robot navigation. This paper proposes a navigation framework that takes into account the motion anisotropy of quadruped robots including kinodynamic trajectory generation, nonlinear trajectory optimization, and nonlinear model predictive control. In <b>simulation</b> and real robot tests, we demonstrate that our motion-anisotropy-aware navigation framework could: (1) generate more efficient trajectories and realize more agile quadruped navigation; (2) significantly improve the navigation safety in challenging scenarios. The implementation is realized as an open-source package at <a href=https://github.com/ZWT006/agile_navigation>https://github.com/ZWT006/agile_navigation</a>.</p></p class="citation"></blockquote><h3 id=1725--200298-geopro-vo-dynamic-obstacle-avoidance-with-geometric-projector-based-on-velocity-obstacle-jihao-huang-et-al-2024>(17/25 | 200/298) GeoPro-VO: Dynamic Obstacle Avoidance with Geometric Projector Based on Velocity Obstacle (Jihao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jihao Huang, Xuemin Chi, Jun Zeng, Zhitao Liu, Hongye Su. (2024)<br><strong>GeoPro-VO: Dynamic Obstacle Avoidance with Geometric Projector Based on Velocity Obstacle</strong><br><button class=copy-to-clipboard title="GeoPro-VO: Dynamic Obstacle Avoidance with Geometric Projector Based on Velocity Obstacle" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10043v1.pdf filename=2403.10043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimization-based approaches are widely employed to generate optimal robot motions while considering various constraints, such as robot dynamics, collision avoidance, and physical limitations. It is crucial to efficiently solve the optimization problems in practice, yet achieving rapid computations remains a great challenge for optimization-based approaches with nonlinear constraints. In this paper, we propose a geometric projector for dynamic obstacle avoidance based on velocity obstacle (GeoPro-VO) by leveraging the projection feature of the velocity cone set represented by VO. Furthermore, with the proposed GeoPro-VO and the augmented Lagrangian spectral projected gradient descent (ALSPG) algorithm, we transform an initial mixed integer nonlinear programming problem (MINLP) in the form of constrained model predictive control (MPC) into a sub-optimization problem and solve it efficiently. Numerical <b>simulations</b> are conducted to validate the fast computing speed of our approach and its capability for reliable dynamic obstacle avoidance.</p></p class="citation"></blockquote><h3 id=1825--201298-advancing-object-goal-navigation-through-llm-enhanced-object-affinities-transfer-mengying-lin-et-al-2024>(18/25 | 201/298) Advancing Object Goal Navigation Through LLM-enhanced Object Affinities Transfer (Mengying Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengying Lin, Yaran Chen, Dongbin Zhao, Zhaoran Wang. (2024)<br><strong>Advancing Object Goal Navigation Through LLM-enhanced Object Affinities Transfer</strong><br><button class=copy-to-clipboard title="Advancing Object Goal Navigation Through LLM-enhanced Object Affinities Transfer" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09971v1.pdf filename=2403.09971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In object goal navigation, agents navigate towards objects identified by category labels using visual and spatial information. Previously, solely network-based methods typically rely on historical data for object affinities estimation, lacking adaptability to new environments and unseen targets. Simultaneously, employing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for navigation as either planners or agents, though offering a broad knowledge base, is cost-inefficient and lacks targeted historical experience. Addressing these challenges, we present the <b>LLM-enhanced</b> Object Affinities Transfer (LOAT) framework, integrating <b>LLM-derived</b> object semantics with network-based approaches to leverage experiential object affinities, thus improving adaptability in unfamiliar settings. LOAT employs a dual-module strategy: a generalized affinities module for accessing <b>LLMs&rsquo;</b> vast knowledge and an experiential affinities module for applying learned object semantic relationships, complemented by a dynamic fusion module harmonizing these information sources based on temporal context. The resulting scores activate semantic maps before feeding into downstream policies, enhancing navigation systems with context-aware inputs. Our evaluations in AI2-THOR and Habitat simulators demonstrate improvements in both navigation success rates and efficiency, validating the LOAT&rsquo;s efficacy in integrating <b>LLM</b> insights for improved object goal navigation.</p></p class="citation"></blockquote><h3 id=1925--202298-design-and-control-co-optimization-for-automated-design-iteration-of-dexterous-anthropomorphic-soft-robotic-hands-pragna-mannam-et-al-2024>(19/25 | 202/298) Design and Control Co-Optimization for Automated Design Iteration of Dexterous Anthropomorphic Soft Robotic Hands (Pragna Mannam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pragna Mannam, Xingyu Liu, Ding Zhao, Jean Oh, Nancy Pollard. (2024)<br><strong>Design and Control Co-Optimization for Automated Design Iteration of Dexterous Anthropomorphic Soft Robotic Hands</strong><br><button class=copy-to-clipboard title="Design and Control Co-Optimization for Automated Design Iteration of Dexterous Anthropomorphic Soft Robotic Hands" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09933v1.pdf filename=2403.09933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We automate soft robotic hand design iteration by co-optimizing design and control policy for dexterous manipulation skills in <b>simulation.</b> Our design iteration pipeline combines genetic algorithms and policy transfer to learn control policies for nearly 400 hand designs, testing grasp quality under external force disturbances. We validate the optimized designs in the real world through teleoperation of pickup and reorient manipulation tasks. Our real world evaluation, from over 900 teleoperated tasks, shows that the trend in design performance in <b>simulation</b> resembles that of the real world. Furthermore, we show that optimized hand designs from our approach outperform existing soft robot hands from prior work in the real world. The results highlight the usefulness of <b>simulation</b> in guiding parameter choices for anthropomorphic soft robotic hand systems, and the effectiveness of our automated design iteration approach, despite the sim-to-real gap.</p></p class="citation"></blockquote><h3 id=2025--203298-stackelberg-meta-learning-based-shared-control-for-assistive-driving-yuhan-zhao-et-al-2024>(20/25 | 203/298) Stackelberg Meta-Learning Based Shared Control for Assistive Driving (Yuhan Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhan Zhao, Quanyan Zhu. (2024)<br><strong>Stackelberg Meta-Learning Based Shared Control for Assistive Driving</strong><br><button class=copy-to-clipboard title="Stackelberg Meta-Learning Based Shared Control for Assistive Driving" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10736v1.pdf filename=2403.10736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Shared control allows the human driver to collaborate with an assistive driving system while retaining the ability to make decisions and take control if necessary. However, human-vehicle teaming and planning are challenging due to environmental uncertainties, the human&rsquo;s bounded rationality, and the variability in human behaviors. An effective collaboration plan needs to learn and adapt to these uncertainties. To this end, we develop a Stackelberg <b>meta-learning</b> <b>algorithm</b> to create automated learning-based planning for shared control. The Stackelberg games are used to capture the leader-follower structure in the asymmetric interactions between the human driver and the assistive driving system. The <b>meta-learning</b> <b>algorithm</b> generates a common behavioral model, which is capable of fast adaptation using a small amount of driving data to assist optimal decision-making. We use a case study of an obstacle avoidance driving scenario to corroborate that the adapted human behavioral model can successfully assist the human driver in reaching the target destination. Besides, it saves driving time compared with a driver-only scheme and is also robust to drivers&rsquo; bounded rationality and errors.</p></p class="citation"></blockquote><h3 id=2125--204298-latent-object-characteristics-recognition-with-visual-to-haptic-audio-cross-modal-transfer-learning-namiko-saito-et-al-2024>(21/25 | 204/298) Latent Object Characteristics Recognition with Visual to Haptic-Audio Cross-modal Transfer Learning (Namiko Saito et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Namiko Saito, Joao Moura, Hiroki Uchida, Sethu Vijayakumar. (2024)<br><strong>Latent Object Characteristics Recognition with Visual to Haptic-Audio Cross-modal Transfer Learning</strong><br><button class=copy-to-clipboard title="Latent Object Characteristics Recognition with Visual to Haptic-Audio Cross-modal Transfer Learning" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10689v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10689v1.pdf filename=2403.10689v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recognising the characteristics of objects while a robot handles them is crucial for adjusting motions that ensure stable and efficient interactions with containers. Ahead of realising stable and efficient robot motions for handling/transferring the containers, this work aims to recognise the latent unobservable object characteristics. While vision is commonly used for object recognition by robots, it is ineffective for detecting hidden objects. However, recognising objects indirectly using other sensors is a challenging task. To address this challenge, we propose a cross-modal <b>transfer</b> <b>learning</b> approach from vision to haptic-audio. We initially train the model with vision, directly observing the target object. Subsequently, we <b>transfer</b> <b>the</b> latent space learned from vision to a second module, trained only with haptic-audio and motor data. This <b>transfer</b> <b>learning</b> framework facilitates the representation of object characteristics using indirect sensor data, thereby improving recognition accuracy. For evaluating the recognition accuracy of our proposed learning framework we selected shape, position, and orientation as the object characteristics. Finally, we demonstrate online recognition of both trained and untrained objects using the humanoid robot Nextage Open.</p></p class="citation"></blockquote><h3 id=2225--205298-towards-embedding-dynamic-personas-in-interactive-robots-masquerading-animated-social-kinematics-mask-jeongeun-park-et-al-2024>(22/25 | 205/298) Towards Embedding Dynamic Personas in Interactive Robots: Masquerading Animated Social Kinematics (MASK) (Jeongeun Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeongeun Park, Taemoon Jeong, Hyeonseong Kim, Taehyun Byun, Seungyoon Shin, Keunjun Choi, Jaewoon Kwon, Taeyoon Lee, Matthew Pan, Sungjoon Choi. (2024)<br><strong>Towards Embedding Dynamic Personas in Interactive Robots: Masquerading Animated Social Kinematics (MASK)</strong><br><button class=copy-to-clipboard title="Towards Embedding Dynamic Personas in Interactive Robots: Masquerading Animated Social Kinematics (MASK)" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10041v1.pdf filename=2403.10041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the design and development of an innovative interactive robotic system to enhance audience engagement using character-like personas. Built upon the foundations of persona-driven dialog agents, this work extends the agent application to the physical realm, employing robots to provide a more immersive and interactive experience. The proposed system, named the Masquerading Animated Social Kinematics (MASK), leverages an anthropomorphic robot which interacts with guests using non-verbal interactions, including facial expressions and gestures. A behavior generation system based upon a finite-state machine structure effectively conditions robotic behavior to convey distinct personas. The MASK framework integrates a perception engine, a behavior selection engine, and a comprehensive action library to enable real-time, dynamic interactions with minimal <b>human</b> <b>intervention</b> in behavior design. Throughout the user subject studies, we examined whether the users could recognize the intended character in film-character-based persona conditions. We conclude by discussing the role of personas in interactive agents and the factors to consider for creating an engaging user experience.</p></p class="citation"></blockquote><h3 id=2325--206298-interactive-distance-field-mapping-and-planning-to-enable-human-robot-collaboration-usama-ali-et-al-2024>(23/25 | 206/298) Interactive Distance Field Mapping and Planning to Enable Human-Robot Collaboration (Usama Ali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Usama Ali, Lan Wu, Adrian Mueller, Fouad Sukkar, Tobias Kaupp, Teresa Vidal-Calleja. (2024)<br><strong>Interactive Distance Field Mapping and Planning to Enable Human-Robot Collaboration</strong><br><button class=copy-to-clipboard title="Interactive Distance Field Mapping and Planning to Enable Human-Robot Collaboration" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09988v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09988v1.pdf filename=2403.09988v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human-robot collaborative applications require scene representations that are kept up-to-date and facilitate safe motions in dynamic scenes. In this letter, we present an interactive distance field mapping and planning (IDMP) framework that handles dynamic objects and collision avoidance through an efficient representation. We define \textit{interactive} mapping and planning as the process of creating and updating the representation of the scene online while simultaneously planning and adapting the robot&rsquo;s actions based on that representation. Given depth sensor data, our framework builds a continuous field that allows to query the distance and gradient to the closest obstacle at any required position in 3D space. The key aspect of this work is an efficient <b>Gaussian</b> <b>Process</b> field that performs incremental updates and implicitly handles dynamic objects with a simple and elegant formulation based on a temporary latent model. In terms of mapping, IDMP is able to fuse point cloud data from single and multiple sensors, query the free space at any spatial resolution, and deal with moving objects without semantics. In terms of planning, IDMP allows seamless integration with gradient-based motion planners facilitating fast re-planning for collision-free navigation. The framework is evaluated on both real and synthetic datasets. A comparison with similar state-of-the-art frameworks shows superior performance when handling dynamic objects and comparable or better performance in the accuracy of the computed distance and gradient field. Finally, we show how the framework can be used for fast motion planning in the presence of moving objects. An accompanying video, code, and datasets are made publicly available <a href=https://uts-ri.github.io/IDMP>https://uts-ri.github.io/IDMP</a>.</p></p class="citation"></blockquote><h3 id=2425--207298-riemannian-flow-matching-policy-for-robot-motion-learning-max-braun-et-al-2024>(24/25 | 207/298) Riemannian Flow Matching Policy for Robot Motion Learning (Max Braun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Max Braun, Noémie Jaquier, Leonel Rozo, Tamim Asfour. (2024)<br><strong>Riemannian Flow Matching Policy for Robot Motion Learning</strong><br><button class=copy-to-clipboard title="Riemannian Flow Matching Policy for Robot Motion Learning" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10672v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10672v1.pdf filename=2403.10672v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Riemannian Flow Matching Policies (RFMP), a novel model for learning and synthesizing robot visuomotor policies. RFMP leverages the efficient training and inference capabilities of flow matching methods. By design, RFMP inherits the strengths of flow matching: the ability to encode high-dimensional <b>multimodal</b> distributions, commonly encountered in robotic tasks, and a very simple and fast inference process. We demonstrate the applicability of RFMP to both state-based and vision-conditioned robot motion policies. Notably, as the robot state resides on a Riemannian manifold, RFMP inherently incorporates geometric awareness, which is crucial for realistic robotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments, comparing its performance against Diffusion Policies. Although both approaches successfully learn the considered tasks, our results show that RFMP provides smoother action trajectories with significantly lower inference times.</p></p class="citation"></blockquote><h3 id=2525--208298-do-visual-language-maps-capture-latent-semantics-matti-pekkanen-et-al-2024>(25/25 | 208/298) Do Visual-Language Maps Capture Latent Semantics? (Matti Pekkanen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matti Pekkanen, Tsvetomila Mihaylova, Francesco Verdoja, Ville Kyrki. (2024)<br><strong>Do Visual-Language Maps Capture Latent Semantics?</strong><br><button class=copy-to-clipboard title="Do Visual-Language Maps Capture Latent Semantics?" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10117v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10117v1.pdf filename=2403.10117v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual-language models (VLMs) have recently been introduced in robotic mapping by using the latent representations, i.e., embeddings, of the VLMs to represent the natural language semantics in the map. The main benefit is moving beyond a small set of human-created labels toward open-vocabulary scene understanding. While there is anecdotal evidence that maps built this way support downstream tasks, such as navigation, rigorous analysis of the quality of the maps using these embeddings is lacking. We investigate two critical properties of map quality: queryability and consistency. The evaluation of queryability addresses the ability to retrieve information from the embeddings. We investigate two aspects of consistency: intra-map consistency and inter-map consistency. Intra-map consistency captures the ability of the embeddings to represent abstract semantic classes, and inter-map consistency captures the generalization properties of the representation. In this paper, we propose a way to analyze the quality of maps created using VLMs, which forms an open-source <b>benchmark</b> to be used when proposing new open-vocabulary map representations. We demonstrate the <b>benchmark</b> by evaluating the maps created by two state-of-the-art methods, VLMaps and OpenScene, using two encoders, LSeg and OpenSeg, using real-world data from the Matterport3D data set. We find that OpenScene outperforms VLMaps with both encoders, and LSeg outperforms OpenSeg with both methods.</p></p class="citation"></blockquote><h2 id=q-fincp-1>q-fin.CP (1)</h2><h3 id=11--209298-can-a-gpt4-powered-ai-agent-be-a-good-enough-performance-attribution-analyst-bruno-de-melo-2024>(1/1 | 209/298) Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst? (Bruno de Melo, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bruno de Melo. (2024)<br><strong>Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?</strong><br><button class=copy-to-clipboard title="Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.CP<br>Categories: cs-AI, q-fin-CP, q-fin-PM, q-fin.CP<br>Keyword Score: 53<br>Keywords: Benchmarking, GPT-4, Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10482v1.pdf filename=2403.10482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Performance attribution analysis, defined as the process of explaining the drivers of the excess performance of an investment portfolio against a <b>benchmark,</b> stands as a significant aspect of portfolio management and plays a crucial role in the investment decision-making process, particularly within the fund management industry. Rooted in a solid financial and mathematical framework, the importance and methodologies of this analytical technique are extensively documented across numerous academic research papers and books. The integration of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and AI agents marks a groundbreaking development in this field. These agents are designed to automate and enhance the performance attribution analysis by accurately calculating and analyzing portfolio performances against <b>benchmarks.</b> In this study, we introduce the application of an AI Agent for a variety of essential performance attribution tasks, including the analysis of performance drivers and utilizing <b>LLMs</b> as calculation engine for multi-level attribution analysis and question-answer <b>(QA)</b> exercises. Leveraging advanced <b>prompt</b> engineering techniques such as Chain-of-Thought (CoT) and Plan and Solve (PS), and employing a standard agent framework from LangChain, the research achieves promising results: it achieves accuracy rates exceeding 93% in analyzing performance drivers, attains 100% in multi-level attribution calculations, and surpasses 84% accuracy in <b>QA</b> exercises that simulate official examination standards. These findings affirm the impactful role of AI agents, <b>prompt</b> engineering and evaluation in advancing portfolio management processes, highlighting a significant advancement in the practical application and evaluation of AI technologies within the domain.</p></p class="citation"></blockquote><h2 id=eesssp-3>eess.SP (3)</h2><h3 id=13--210298-process-and-forward-deep-joint-source-channel-coding-over-cooperative-relay-networks-chenghong-bian-et-al-2024>(1/3 | 210/298) Process-and-Forward: Deep Joint Source-Channel Coding Over Cooperative Relay Networks (Chenghong Bian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenghong Bian, Yulin Shao, Haotian Wu, Emre Ozfatura, Deniz Gunduz. (2024)<br><strong>Process-and-Forward: Deep Joint Source-Channel Coding Over Cooperative Relay Networks</strong><br><button class=copy-to-clipboard title="Process-and-Forward: Deep Joint Source-Channel Coding Over Cooperative Relay Networks" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 50<br>Keywords: Vision Transformer, Simulation, Simulator, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10613v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10613v1.pdf filename=2403.10613v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces an innovative deep joint source-channel coding (DeepJSCC) approach to image transmission over a cooperative relay channel. The relay either amplifies and forwards a scaled version of its received signal, referred to as DeepJSCC-AF, or leverages neural networks to extract relevant features about the source signal before forwarding it to the destination, which we call DeepJSCC-PF (Process-and-Forward). In the full-duplex scheme, inspired by the block Markov coding (BMC) concept, we introduce a novel block transmission strategy built upon novel <b>vision</b> <b>transformer</b> architecture. In the proposed scheme, the source transmits information in blocks, and the relay updates its knowledge about the input signal after each block and generates its own signal to be conveyed to the destination. To enhance practicality, we introduce an adaptive transmission model, which allows a single trained DeepJSCC model to adapt seamlessly to various channel qualities, making it a versatile solution. <b>Simulation</b> results demonstrate the superior performance of our proposed DeepJSCC compared to the state-of-the-art BPG image compression algorithm, even when operating at the maximum achievable rate of conventional decode-and-forward and compress-and-forward protocols, for both half-duplex and full-duplex relay scenarios.</p></p class="citation"></blockquote><h3 id=23--211298-multi-source-localization-and-data-association-for-time-difference-of-arrival-measurements-gabrielle-flood-et-al-2024>(2/3 | 211/298) Multi-Source Localization and Data Association for Time-Difference of Arrival Measurements (Gabrielle Flood et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabrielle Flood, Filip Elvander. (2024)<br><strong>Multi-Source Localization and Data Association for Time-Difference of Arrival Measurements</strong><br><button class=copy-to-clipboard title="Multi-Source Localization and Data Association for Time-Difference of Arrival Measurements" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-SD, eess-AS, eess-SP, eess.SP, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10329v1.pdf filename=2403.10329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we consider the problem of localizing multiple signal sources based on time-difference of arrival (TDOA) measurements. In the blind setting, in which the source signals are not known, the localization task is challenging due to the data association problem. That is, it is not known which of the TDOA measurements correspond to the same source. Herein, we propose to perform joint localization and data association by means of an optimal transport formulation. The method operates by finding optimal groupings of TDOA measurements and associating these with candidate source locations. To allow for computationally feasible localization in three-dimensional space, an efficient set of candidate locations is constructed using a minimal multilateration solver based on minimal sets of receiver pairs. In numerical <b>simulations,</b> we demonstrate that the proposed method is robust both to measurement noise and TDOA detection errors. Furthermore, it is shown that the data association provided by the proposed method allows for statistically efficient estimates of the source locations.</p></p class="citation"></blockquote><h3 id=33--212298-decentralizing-coherent-joint-transmission-precoding-via-deterministic-equivalents-yuhao-liu-et-al-2024>(3/3 | 212/298) Decentralizing Coherent Joint Transmission Precoding via Deterministic Equivalents (Yuhao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhao Liu, Xinyu Bian, Yizhou Xu, Tianqi Hou, Wenjie Wang, Yuyi Mao, Jun Zhang. (2024)<br><strong>Decentralizing Coherent Joint Transmission Precoding via Deterministic Equivalents</strong><br><button class=copy-to-clipboard title="Decentralizing Coherent Joint Transmission Precoding via Deterministic Equivalents" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09958v1.pdf filename=2403.09958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In order to control the inter-cell interference for a multi-cell multi-user multiple-input multiple-output network, we consider the precoder design for coordinated multi-point with downlink coherent joint transmission. To avoid costly information exchange among the cooperating base stations in a centralized precoding scheme, we propose a decentralized one by considering the power minimization problem. By approximating the inter-cell interference using the deterministic equivalents, this problem is decoupled to sub-problems which are solved in a decentralized manner at different base stations. <b>Simulation</b> results demonstrate the effectiveness of our proposed decentralized precoding scheme, where only 2 ~ 7% more transmit power is needed compared with the optimal centralized precoder.</p></p class="citation"></blockquote><h2 id=csai-8>cs.AI (8)</h2><h3 id=18--213298-are-llms-good-cryptic-crossword-solvers-abdelrahman-boda-sadallah-et-al-2024>(1/8 | 213/298) Are LLMs Good Cryptic Crossword Solvers? (Abdelrahman &lsquo;Boda&rsquo; Sadallah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdelrahman &ldquo;Boda&rdquo; Sadallah, Daria Kotova, Ekaterina Kochmar. (2024)<br><strong>Are LLMs Good Cryptic Crossword Solvers?</strong><br><button class=copy-to-clipboard title="Are LLMs Good Cryptic Crossword Solvers?" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 43<br>Keywords: Benchmarking, ChatGPT, Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12094v1.pdf filename=2403.12094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cryptic crosswords are puzzles that rely not only on general knowledge but also on the solver&rsquo;s ability to manipulate language on different levels and deal with various types of wordplay. Previous research suggests that solving such puzzles is a challenge even for modern NLP models. However, the abilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have not yet been tested on this task. In this paper, we establish the <b>benchmark</b> results for three popular <b>LLMs</b> &ndash; LLaMA2, <b>Mistral,</b> and <b>ChatGPT</b> &ndash; showing that their performance on this task is still far from that of humans.</p></p class="citation"></blockquote><h3 id=28--214298-autonode-a-neuro-graphic-self-learnable-engine-for-cognitive-gui-automation-arkajit-datta-et-al-2024>(2/8 | 214/298) AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation (Arkajit Datta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arkajit Datta, Tushar Verma, Rajat Chawla. (2024)<br><strong>AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation</strong><br><button class=copy-to-clipboard title="AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CV, cs.AI<br>Keyword Score: 38<br>Keywords: Graph, Knowledge Graph, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10171v1.pdf filename=2403.10171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent advancements within the domain of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> there has been a notable emergence of agents capable of addressing Robotic Process Automation (RPA) challenges through enhanced cognitive capabilities and sophisticated <b>reasoning.</b> This development heralds a new era of scalability and human-like adaptability in goal attainment. In this context, we introduce AUTONODE (Autonomous User-interface Transformation through Online Neuro-graphic Operations and Deep Exploration). AUTONODE employs advanced neuro-graphical techniques to facilitate autonomous navigation and task execution on web interfaces, thereby obviating the necessity for predefined scripts or manual intervention. Our engine empowers agents to comprehend and implement complex workflows, adapting to dynamic web environments with unparalleled efficiency. Our methodology synergizes cognitive functionalities with robotic automation, endowing AUTONODE with the ability to learn from experience. We have integrated an exploratory module, DoRA (Discovery and mapping Operation for <b>graph</b> Retrieval Agent), which is instrumental in constructing a <b>knowledge</b> <b>graph</b> that the engine utilizes to optimize its actions and achieve objectives with minimal supervision. The versatility and efficacy of AUTONODE are demonstrated through a series of experiments, highlighting its proficiency in managing a diverse array of web-based tasks, ranging from data extraction to transaction processing.</p></p class="citation"></blockquote><h3 id=38--215298-gradient-based-feature-attribution-in-explainable-ai-a-technical-review-yongjie-wang-et-al-2024>(3/8 | 215/298) Gradient based Feature Attribution in Explainable AI: A Technical Review (Yongjie Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongjie Wang, Tong Zhang, Xu Guo, Zhiqi Shen. (2024)<br><strong>Gradient based Feature Attribution in Explainable AI: A Technical Review</strong><br><button class=copy-to-clipboard title="Gradient based Feature Attribution in Explainable AI: A Technical Review" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 25<br>Keywords: Black Box, Explainable AI, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10415v1.pdf filename=2403.10415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The surge in <b>black-box</b> <b>AI</b> models has <b>prompted</b> the need to explain the internal mechanism and justify their reliability, especially in high-stakes applications, such as healthcare and autonomous driving. Due to the lack of a rigorous definition of <b>explainable</b> <b>AI</b> (XAI), a plethora of research related to explainability, interpretability, and transparency has been developed to explain and analyze the model from various perspectives. Consequently, with an exhaustive list of papers, it becomes challenging to have a comprehensive overview of XAI research from all aspects. Considering the popularity of neural networks in AI research, we narrow our focus to a specific area of XAI research: gradient based explanations, which can be directly adopted for neural network models. In this review, we systematically explore gradient based explanation methods to date and introduce a novel taxonomy to categorize them into four distinct classes. Then, we present the essence of technique details in chronological order and underscore the evolution of algorithms. Next, we introduce both human and quantitative evaluations to measure algorithm performance. More importantly, we demonstrate the general challenges in XAI and specific challenges in gradient based explanations. We hope that this survey can help researchers understand state-of-the-art progress and their corresponding disadvantages, which could spark their interest in addressing these issues in future work.</p></p class="citation"></blockquote><h3 id=48--216298-development-and-application-of-a-monte-carlo-tree-search-algorithm-for-simulating-da-vinci-code-game-strategies-ye-zhang-et-al-2024>(4/8 | 216/298) Development and Application of a Monte Carlo Tree Search Algorithm for Simulating Da Vinci Code Game Strategies (Ye Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Zhang, Mengran Zhu, Kailin Gui, Jiayue Yu, Yong Hao, Haozhan Sun. (2024)<br><strong>Development and Application of a Monte Carlo Tree Search Algorithm for Simulating Da Vinci Code Game Strategies</strong><br><button class=copy-to-clipboard title="Development and Application of a Monte Carlo Tree Search Algorithm for Simulating Da Vinci Code Game Strategies" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10720v1.pdf filename=2403.10720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we explore the efficiency of the Monte Carlo Tree Search (MCTS), a prominent decision-making algorithm renowned for its effectiveness in complex decision environments, contingent upon the volume of <b>simulations</b> conducted. Notwithstanding its broad applicability, the algorithm&rsquo;s performance can be adversely impacted in certain scenarios, particularly within the domain of game strategy development. This research posits that the inherent branch divergence within the Da Vinci Code board game significantly impedes parallelism when executed on Graphics Processing Units (GPUs). To investigate this hypothesis, we implemented and meticulously evaluated two variants of the MCTS algorithm, specifically designed to assess the impact of branch divergence on computational performance. Our comparative analysis reveals a linear improvement in performance with the CPU-based implementation, in stark contrast to the GPU implementation, which exhibits a non-linear enhancement pattern and discernible performance troughs. These findings contribute to a deeper understanding of the MCTS algorithm&rsquo;s behavior in divergent branch scenarios, highlighting critical considerations for optimizing game strategy algorithms on parallel computing architectures.</p></p class="citation"></blockquote><h3 id=58--217298-a-survey-on-game-playing-agents-and-large-models-methods-applications-and-challenges-xinrun-xu-et-al-2024>(5/8 | 217/298) A Survey on Game Playing Agents and Large Models: Methods, Applications, and Challenges (Xinrun Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinrun Xu, Yuxin Wang, Chaoyi Xu, Ziluo Ding, Jiechuan Jiang, Zhiming Ding, Börje F. Karlsson. (2024)<br><strong>A Survey on Game Playing Agents and Large Models: Methods, Applications, and Challenges</strong><br><button class=copy-to-clipboard title="A Survey on Game Playing Agents and Large Models: Methods, Applications, and Challenges" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 13<br>Keywords: Multi-modal, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10249v1.pdf filename=2403.10249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The swift evolution of Large-scale Models (LMs), either language-focused or <b>multi-modal,</b> has garnered extensive attention in both academy and industry. But despite the surge in interest in this rapidly evolving area, there are scarce systematic reviews on their capabilities and potential in distinct impactful scenarios. This paper endeavours to help bridge this gap, offering a thorough examination of the current landscape of LM usage in regards to complex game playing scenarios and the challenges still open. Here, we seek to systematically review the existing architectures of LM-based Agents (LMAs) for games and <b>summarize</b> their commonalities, challenges, and any other insights. Furthermore, we present our perspective on promising future research avenues for the advancement of LMs in games. We hope to assist researchers in gaining a clear understanding of the field and to generate more interest in this highly impactful research direction. A corresponding resource, continuously updated, can be found in our GitHub repository.</p></p class="citation"></blockquote><h3 id=68--218298-single--and-multi-agent-private-active-sensing-a-deep-neuroevolution-approach-george-stamatelis-et-al-2024>(6/8 | 218/298) Single- and Multi-Agent Private Active Sensing: A Deep Neuroevolution Approach (George Stamatelis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>George Stamatelis, Angelos-Nikolaos Kanatas, Ioannis Asprogerakas, George C. Alexandropoulos. (2024)<br><strong>Single- and Multi-Agent Private Active Sensing: A Deep Neuroevolution Approach</strong><br><button class=copy-to-clipboard title="Single- and Multi-Agent Private Active Sensing: A Deep Neuroevolution Approach" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CR, cs-MA, cs-NE, cs.AI<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10112v1.pdf filename=2403.10112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we focus on one centralized and one decentralized problem of active hypothesis testing in the presence of an eavesdropper. For the centralized problem including a single legitimate agent, we present a new framework based on NeuroEvolution (NE), whereas, for the decentralized problem, we develop a novel NE-based method for solving collaborative multi-agent tasks, which interestingly maintains all computational benefits of single-agent NE. The superiority of the proposed EAHT approaches over conventional active hypothesis testing policies, as well as learning-based methods, is validated through numerical investigations in an example use case of <b>anomaly</b> <b>detection</b> over wireless sensor networks.</p></p class="citation"></blockquote><h3 id=78--219298-lifted-causal-inference-in-relational-domains-malte-luttermann-et-al-2024>(7/8 | 219/298) Lifted Causal Inference in Relational Domains (Malte Luttermann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Malte Luttermann, Mattis Hartwig, Tanya Braun, Ralf Möller, Marcel Gehrke. (2024)<br><strong>Lifted Causal Inference in Relational Domains</strong><br><button class=copy-to-clipboard title="Lifted Causal Inference in Relational Domains" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-DS, cs.AI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10184v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10184v1.pdf filename=2403.10184v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lifted inference exploits symmetries in probabilistic graphical models by using a representative for indistinguishable objects, thereby speeding up query answering while maintaining exact answers. Even though lifting is a well-established technique for the task of probabilistic inference in relational domains, it has not yet been applied to the task of causal inference. In this paper, we show how lifting can be applied to efficiently compute causal effects in relational domains. More specifically, we introduce parametric causal factor <b>graphs</b> as an extension of parametric factor <b>graphs</b> incorporating causal knowledge and give a formal semantics of interventions therein. We further present the lifted causal inference algorithm to compute causal effects on a lifted level, thereby drastically speeding up causal inference compared to propositional inference, e.g., in causal Bayesian networks. In our empirical evaluation, we demonstrate the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=88--220298-efficient-detection-of-exchangeable-factors-in-factor-graphs-malte-luttermann-et-al-2024>(8/8 | 220/298) Efficient Detection of Exchangeable Factors in Factor Graphs (Malte Luttermann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Malte Luttermann, Johann Machemer, Marcel Gehrke. (2024)<br><strong>Efficient Detection of Exchangeable Factors in Factor Graphs</strong><br><button class=copy-to-clipboard title="Efficient Detection of Exchangeable Factors in Factor Graphs" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-DS, cs.AI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10167v1.pdf filename=2403.10167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To allow for tractable probabilistic inference with respect to domain sizes, lifted probabilistic inference exploits symmetries in probabilistic graphical models. However, checking whether two factors encode equivalent semantics and hence are exchangeable is computationally expensive. In this paper, we efficiently solve the problem of detecting exchangeable factors in a factor <b>graph.</b> In particular, we introduce the detection of exchangeable factors (DEFT) algorithm, which allows us to drastically reduce the computational effort for checking whether two factors are exchangeable in practice. While previous approaches iterate all $O(n!)$ permutations of a factor&rsquo;s argument list in the worst case (where $n$ is the number of arguments of the factor), we prove that DEFT efficiently identifies restrictions to drastically reduce the number of permutations and validate the efficiency of DEFT in our empirical evaluation.</p></p class="citation"></blockquote><h2 id=cscr-11>cs.CR (11)</h2><h3 id=111--221298-federated-learning-with-anomaly-detection-via-gradient-and-reconstruction-analysis-zahir-alsulaimawi-2024>(1/11 | 221/298) Federated Learning with Anomaly Detection via Gradient and Reconstruction Analysis (Zahir Alsulaimawi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zahir Alsulaimawi. (2024)<br><strong>Federated Learning with Anomaly Detection via Gradient and Reconstruction Analysis</strong><br><button class=copy-to-clipboard title="Federated Learning with Anomaly Detection via Gradient and Reconstruction Analysis" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 43<br>Keywords: MNIST, Anomaly Detection, Autoencoder, Benchmarking, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10000v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10000v1.pdf filename=2403.10000v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving landscape of <b>Federated</b> <b>Learning</b> (FL), the challenge of ensuring data integrity against poisoning attacks is paramount, particularly for applications demanding stringent privacy preservation. Traditional <b>anomaly</b> <b>detection</b> strategies often struggle to adapt to the distributed nature of FL, leaving a gap our research aims to bridge. We introduce a novel framework that synergizes gradient-based analysis with <b>autoencoder-driven</b> data reconstruction to detect and mitigate poisoned data with unprecedented precision. Our approach uniquely combines detecting anomalous gradient patterns with identifying reconstruction errors, significantly enhancing FL model security. Validated through extensive experiments on <b>MNIST</b> and CIFAR-10 datasets, our method outperforms existing solutions by 15% in <b>anomaly</b> <b>detection</b> accuracy while maintaining a minimal false positive rate. This robust performance, consistent across varied data types and network sizes, underscores our framework&rsquo;s potential in securing FL deployments in critical domains such as healthcare and finance. By setting new <b>benchmarks</b> for <b>anomaly</b> <b>detection</b> within FL, our work paves the way for future advancements in distributed learning security.</p></p class="citation"></blockquote><h3 id=211--222298-socialgenpod-privacy-friendly-generative-ai-social-web-applications-with-decentralised-personal-data-stores-vidminas-vizgirda-et-al-2024>(2/11 | 222/298) SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores (Vidminas Vizgirda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vidminas Vizgirda, Rui Zhao, Naman Goel. (2024)<br><strong>SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores</strong><br><button class=copy-to-clipboard title="SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: H-3-4; H-3-5; C-2-4; I-2-1; K-8-1, cs-CR, cs-CY, cs-IR, cs-LG, cs-SI, cs.CR<br>Keyword Score: 40<br>Keywords: Generative AI, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10408v1.pdf filename=2403.10408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present SocialGenPod, a decentralised and privacy-friendly way of deploying <b>generative</b> <b>AI</b> Web applications. Unlike centralised Web and data architectures that keep user data tied to application and service providers, we show how one can use Solid &ndash; a decentralised Web specification &ndash; to decouple user data from <b>generative</b> <b>AI</b> applications. We demonstrate SocialGenPod using a prototype that allows users to converse with different <b>Large</b> <b>Language</b> <b>Models,</b> optionally leveraging <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> to generate answers grounded in private documents stored in any Solid Pod that the user is allowed to access, directly or indirectly. SocialGenPod makes use of Solid access control mechanisms to give users full control of determining who has access to data stored in their Pods. SocialGenPod keeps all user data (chat history, app configuration, personal documents, etc) securely in the user&rsquo;s personal Pod; separate from specific model or application providers. Besides better privacy controls, this approach also enables portability across different services and applications. Finally, we discuss challenges, posed by the <b>large</b> <b>compute</b> <b>requirements</b> of state-of-the-art models, that future research in this area should address. Our prototype is open-source and available at: <a href=https://github.com/Vidminas/socialgenpod/>https://github.com/Vidminas/socialgenpod/</a>.</p></p class="citation"></blockquote><h3 id=311--223298-securing-federated-learning-with-control-flow-attestation-a-novel-framework-for-enhanced-integrity-and-resilience-against-adversarial-attacks-zahir-alsulaimawi-2024>(3/11 | 223/298) Securing Federated Learning with Control-Flow Attestation: A Novel Framework for Enhanced Integrity and Resilience against Adversarial Attacks (Zahir Alsulaimawi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zahir Alsulaimawi. (2024)<br><strong>Securing Federated Learning with Control-Flow Attestation: A Novel Framework for Enhanced Integrity and Resilience against Adversarial Attacks</strong><br><button class=copy-to-clipboard title="Securing Federated Learning with Control-Flow Attestation: A Novel Framework for Enhanced Integrity and Resilience against Adversarial Attacks" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 33<br>Keywords: MNIST, Benchmarking, Federated Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10005v1.pdf filename=2403.10005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>Federated</b> <b>Learning</b> (FL) as a distributed machine learning paradigm has introduced new cybersecurity challenges, notably <b>adversarial</b> <b>attacks</b> that threaten model integrity and participant privacy. This study proposes an innovative security framework inspired by Control-Flow Attestation (CFA) mechanisms, traditionally used in cybersecurity, to ensure software execution integrity. By integrating digital signatures and cryptographic hashing within the FL framework, we authenticate and verify the integrity of model updates across the network, effectively mitigating risks associated with model poisoning and <b>adversarial</b> <b>interference.</b> Our approach, novel in its application of CFA principles to FL, ensures contributions from participating nodes are authentic and untampered, thereby enhancing system resilience without compromising computational efficiency or model performance. Empirical evaluations on <b>benchmark</b> datasets, <b>MNIST</b> and CIFAR-10, demonstrate our framework&rsquo;s effectiveness, achieving a 100% success rate in integrity verification and authentication and notable resilience against <b>adversarial</b> <b>attacks.</b> These results validate the proposed security enhancements and open avenues for more secure, reliable, and privacy-conscious distributed machine learning solutions. Our work bridges a critical gap between cybersecurity and distributed machine learning, offering a foundation for future advancements in secure FL.</p></p class="citation"></blockquote><h3 id=411--224298-ignore-me-but-dont-replace-me-utilizing-non-linguistic-elements-for-pretraining-on-the-cybersecurity-domain-eugene-jang-et-al-2024>(4/11 | 224/298) Ignore Me But Don&rsquo;t Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain (Eugene Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eugene Jang, Jian Cui, Dayeon Yim, Youngjin Jin, Jin-Woo Chung, Seungwon Shin, Yongjae Lee. (2024)<br><strong>Ignore Me But Don&rsquo;t Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain</strong><br><button class=copy-to-clipboard title="Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Masked Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10576v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10576v1.pdf filename=2403.10576v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cybersecurity information is often technically complex and relayed through unstructured text, making automation of cyber threat intelligence highly challenging. For such text domains that involve high levels of expertise, pretraining on in-domain corpora has been a popular method for language models to obtain domain expertise. However, cybersecurity texts often contain non-linguistic elements (such as URLs and hash values) that could be unsuitable with the established pretraining methodologies. Previous work in other domains have removed or filtered such text as noise, but the effectiveness of these methods have not been investigated, especially in the cybersecurity domain. We propose different pretraining methodologies and evaluate their effectiveness through downstream tasks and probing tasks. Our proposed strategy (selective <b>MLM</b> and jointly training NLE token classification) outperforms the commonly taken approach of replacing non-linguistic elements (NLEs). We use our domain-customized methodology to train CyBERTuned, a cybersecurity domain language model that outperforms other cybersecurity <b>PLMs</b> on most tasks.</p></p class="citation"></blockquote><h3 id=511--225298-not-just-change-the-labels-learn-the-features-watermarking-deep-neural-networks-with-multi-view-data-yuxuan-li-et-al-2024>(5/11 | 225/298) Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data (Yuxuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Li, Sarthak Kumar Maharana, Yunhui Guo. (2024)<br><strong>Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data</strong><br><button class=copy-to-clipboard title="Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 13<br>Keywords: Benchmarking, Model Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10663v1.pdf filename=2403.10663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN <b>model</b> <b>to</b> protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source <b>model.</b> <b>Unfortunately,</b> existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source <b>model</b> <b>without</b> a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as $\textit{multi-view data}$, it becomes feasible to effectively defend functionality stealing attacks. Based on this perspective, we introduce a novel watermarking technique based on Multi-view dATa, called MAT, for efficiently embedding watermarks within DNNs. This approach involves constructing a trigger set with multi-view data and incorporating a simple feature-based regularization method for training the source <b>model.</b> <b>We</b> validate our method across various <b>benchmarks</b> and demonstrate its efficacy in defending against <b>model</b> <b>extraction</b> attacks, surpassing relevant baselines by a significant margin.</p></p class="citation"></blockquote><h3 id=611--226298-unsupervised-threat-hunting-using-continuous-bag-of-terms-and-time-cbott-varol-kayhan-et-al-2024>(6/11 | 226/298) Unsupervised Threat Hunting using Continuous Bag-of-Terms-and-Time (CBoTT) (Varol Kayhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Varol Kayhan, Shivendu Shivendu, Rouzbeh Behnia, Clinton Daniel, Manish Agrawal. (2024)<br><strong>Unsupervised Threat Hunting using Continuous Bag-of-Terms-and-Time (CBoTT)</strong><br><button class=copy-to-clipboard title="Unsupervised Threat Hunting using Continuous Bag-of-Terms-and-Time (CBoTT)" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 13<br>Keywords: Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10327v1.pdf filename=2403.10327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Threat hunting is sifting through system logs to detect malicious activities that might have bypassed existing security measures. It can be performed in several ways, one of which is based on detecting anomalies. We propose an <b>unsupervised</b> framework, called continuous bag-of-terms-and-time (CBoTT), and publish its application programming interface (API) to help researchers and cybersecurity analysts perform anomaly-based threat hunting among SIEM logs geared toward process auditing on endpoint devices. Analyses show that our framework consistently outperforms <b>benchmark</b> approaches. When logs are sorted by likelihood of being an anomaly (from most likely to least), our approach identifies anomalies at higher percentiles (between 1.82-6.46) while <b>benchmark</b> approaches identify the same anomalies at lower percentiles (between 3.25-80.92). This framework can be used by other researchers to conduct <b>benchmark</b> analyses and cybersecurity analysts to find anomalies in SIEM logs.</p></p class="citation"></blockquote><h3 id=711--227298-interactive-trimming-against-evasive-online-data-manipulation-attacks-a-game-theoretic-approach-yue-fu-et-al-2024>(7/11 | 227/298) Interactive Trimming against Evasive Online Data Manipulation Attacks: A Game-Theoretic Approach (Yue Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Fu, Qingqing Ye, Rong Du, Haibo Hu. (2024)<br><strong>Interactive Trimming against Evasive Online Data Manipulation Attacks: A Game-Theoretic Approach</strong><br><button class=copy-to-clipboard title="Interactive Trimming against Evasive Online Data Manipulation Attacks: A Game-Theoretic Approach" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DB, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10313v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10313v1.pdf filename=2403.10313v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the exponential growth of data and its crucial impact on our lives and decision-making, the integrity of data has become a significant concern. Malicious data poisoning attacks, where false values are injected into the data, can disrupt machine learning processes and lead to severe consequences. To mitigate these attacks, distance-based defenses, such as trimming, have been proposed, but they can be easily evaded by white-box attackers. The evasiveness and effectiveness of poisoning attack strategies are two sides of the same coin, making game theory a promising approach. However, existing game-theoretical models often overlook the complexities of online data poisoning attacks, where strategies must adapt to the dynamic process of data collection. In this paper, we present an interactive game-theoretical model to defend online data manipulation attacks using the trimming strategy. Our model accommodates a complete strategy space, making it applicable to strong evasive and colluding adversaries. Leveraging the principle of least action and the Euler-Lagrange equation from theoretical physics, we derive an analytical model for the game-theoretic process. To demonstrate its practical usage, we present a case study in a privacy-preserving data collection system under local <b>differential</b> <b>privacy</b> where a non-deterministic utility function is adopted. Two strategies are devised from this analytical model, namely, Tit-for-tat and Elastic. We conduct extensive experiments on real-world datasets, which showcase the effectiveness and accuracy of these two strategies.</p></p class="citation"></blockquote><h3 id=811--228298-instance-optimal-clipping-for-summation-problems-in-the-shuffle-model-of-differential-privacy-wei-dong-et-al-2024>(8/11 | 228/298) Instance-optimal Clipping for Summation Problems in the Shuffle Model of Differential Privacy (Wei Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Dong, Qiyao Luo, Giulia Fanti, Elaine Shi, Ke Yi. (2024)<br><strong>Instance-optimal Clipping for Summation Problems in the Shuffle Model of Differential Privacy</strong><br><button class=copy-to-clipboard title="Instance-optimal Clipping for Summation Problems in the Shuffle Model of Differential Privacy" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DS, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10116v1.pdf filename=2403.10116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Differentially private mechanisms achieving worst-case optimal error bounds (e.g., the classical Laplace mechanism) are well-studied in the literature. However, when typical data are far from the worst case, \emph{instance-specific} error bounds &ndash; which depend on the largest value in the dataset &ndash; are more meaningful. For example, consider the sum estimation problem, where each user has an integer $x_i$ from the domain ${0,1,\dots,U}$ and we wish to estimate $\sum_i x_i$. This has a worst-case optimal error of $O(U/\varepsilon)$, while recent work has shown that the clipping mechanism can achieve an instance-optimal error of $O(\max_i x_i \cdot \log\log U /\varepsilon)$. Under the shuffle model, known instance-optimal protocols are less communication-efficient. The clipping mechanism also works in the shuffle model, but requires two rounds: Round one finds the clipping threshold, and round two does the clipping and computes the noisy sum of the clipped data. In this paper, we show how these two seemingly sequential steps can be done simultaneously in one round using just $1+o(1)$ messages per user, while maintaining the instance-optimal error bound. We also extend our technique to the high-dimensional sum estimation problem and sparse vector aggregation (a.k.a. frequency estimation under user-level <b>differential</b> <b>privacy).</b> Our experiments show order-of-magnitude improvements of our protocols in terms of error compared with prior work.</p></p class="citation"></blockquote><h3 id=911--229298-specification-and-enforcement-of-activity-dependency-policies-using-xacml-tanjila-mawla-et-al-2024>(9/11 | 229/298) Specification and Enforcement of Activity Dependency Policies using XACML (Tanjila Mawla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanjila Mawla, Maanak Gupta, Ravi Sandhu. (2024)<br><strong>Specification and Enforcement of Activity Dependency Policies using XACML</strong><br><button class=copy-to-clipboard title="Specification and Enforcement of Activity Dependency Policies using XACML" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10092v1.pdf filename=2403.10092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolving smart and interconnected systems are designed to operate with minimal <b>human</b> <b>intervention.</b> Devices within these smart systems often engage in prolonged operations based on sensor data and contextual factors. Recently, an Activity-Centric Access Control (ACAC) model has been introduced to regulate these prolonged operations, referred to as activities, which undergo state changes over extended duration of time. Dependencies among different activities can influence and restrict the execution of one another, necessitating active and real-time monitoring of the dependencies between activities to prevent security violation. In the ACAC model, the activity dependencies, denoted as &ldquo;D&rdquo;, is considered as a decision parameter for controlling a requested activity. These dependencies must be evaluated throughout all phases of an activity&rsquo;s life cycle. To ensure the consistency of access control rules across diverse domains and applications, a standard policy language is essential. We propose a policy framework adapting the widely-used eXtensible Access Control Markup Language (XACML) , referred to as $\mathrm{XACML_{AD}}$, to specify the activity dependency policies. This work involves extending the syntax and semantics of XACML by introducing new elements to check dependent activities&rsquo; states and handle state updates on dependent activities. In addition to the language extension, we present the enforcement architecture and data flow model of evaluating policies for activity dependencies. The integration of the proposed $\mathrm{XACML_{AD}}$ policy framework and the enforcement of the policies supports dependency evaluation, necessary updates and continuous enforcement of policies to control an activity throughout its life cycle. We implement the enforcement architecture exploiting the $\mathrm{XACML_{AD}}$ policy framework and discuss the performance evaluation results.</p></p class="citation"></blockquote><h3 id=1011--230298-time-frequency-jointed-imperceptible-adversarial-attack-to-brainprint-recognition-with-deep-learning-models-hangjie-yi-et-al-2024>(10/11 | 230/298) Time-Frequency Jointed Imperceptible Adversarial Attack to Brainprint Recognition with Deep Learning Models (Hangjie Yi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hangjie Yi, Yuhang Ming, Dongjun Liu, Wanzeng Kong. (2024)<br><strong>Time-Frequency Jointed Imperceptible Adversarial Attack to Brainprint Recognition with Deep Learning Models</strong><br><button class=copy-to-clipboard title="Time-Frequency Jointed Imperceptible Adversarial Attack to Brainprint Recognition with Deep Learning Models" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10021v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10021v1.pdf filename=2403.10021v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>EEG-based brainprint recognition with deep learning models has garnered much attention in biometric identification. Yet, studies have indicated vulnerability to <b>adversarial</b> <b>attacks</b> in deep learning models with EEG inputs. In this paper, we introduce a novel <b>adversarial</b> <b>attack</b> method that jointly attacks time-domain and frequency-domain EEG signals by employing wavelet transform. Different from most existing methods which only target time-domain EEG signals, our method not only takes advantage of the time-domain attack&rsquo;s potent <b>adversarial</b> <b>strength</b> but also benefits from the imperceptibility inherent in frequency-domain attack, achieving a better balance between attack performance and imperceptibility. Extensive experiments are conducted in both white- and grey-box scenarios and the results demonstrate that our attack method achieves state-of-the-art attack performance on three datasets and three deep-learning models. In the meanwhile, the perturbations in the signals attacked by our method are barely perceptible to the human visual system.</p></p class="citation"></blockquote><h3 id=1111--231298-search-based-ordered-password-generation-of-autoregressive-neural-networks-min-jin-et-al-2024>(11/11 | 231/298) Search-based Ordered Password Generation of Autoregressive Neural Networks (Min Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Min Jin, Junbin Ye, Rongxuan Shen, Huaxing Lu. (2024)<br><strong>Search-based Ordered Password Generation of Autoregressive Neural Networks</strong><br><button class=copy-to-clipboard title="Search-based Ordered Password Generation of Autoregressive Neural Networks" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09954v1.pdf filename=2403.09954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Passwords are the most widely used method of authentication and password guessing is the essential part of password cracking and password security research. The progress of deep learning technology provides a promising way to improve the efficiency of password guessing. However, current research on neural network password guessing methods mostly focuses on model structure and has overlooked the generation method. Due to the randomness of sampling, not only the generated passwords have a large number of duplicates, but also the order in which passwords generated is random, leading to inefficient password attacks. In this paper, we propose SOPG, a search-based ordered password generation method, which enables the password guessing model based on autoregressive neural network to generate passwords in approximately descending order of probability. Experiment on comparison of SOPG and Random sampling shows passwords generated by SOPG do not repeat, and when they reach the same cover rate, SOPG requires fewer inferences and far fewer generated passwords than Random sampling, which brings great efficiency improvement to subsequent password attacks. We build SOPGesGPT, a password guessing model based on <b>GPT,</b> using SOPG to generate passwords. Compared with the most influential models OMEN, FLA, PassGAN, VAEPass and the latest model PassGPT in one-site test, experiments show that SOPGesGPT is far ahead in terms of both effective rate and cover rate. As to cover rate that everyone recognizes, SOPGesGPT reaches 35.06%, which is 254%, 298%, 421%, 380%, 81% higher than OMEN, FLA, PassGAN, VAEPass, and PassGPT respectively.</p></p class="citation"></blockquote><h2 id=eessiv-12>eess.IV (12)</h2><h3 id=112--232298-d-net-dynamic-large-kernel-with-dynamic-feature-fusion-for-volumetric-medical-image-segmentation-jin-yang-et-al-2024>(1/12 | 232/298) D-Net: Dynamic Large Kernel with Dynamic Feature Fusion for Volumetric Medical Image Segmentation (Jin Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jin Yang, Peijie Qiu, Yichi Zhang, Daniel S. Marcus, Aristeidis Sotiras. (2024)<br><strong>D-Net: Dynamic Large Kernel with Dynamic Feature Fusion for Volumetric Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="D-Net: Dynamic Large Kernel with Dynamic Feature Fusion for Volumetric Medical Image Segmentation" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10674v1.pdf filename=2403.10674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hierarchical <b>transformers</b> have achieved significant success in medical image segmentation due to their large receptive field and capabilities of effectively leveraging global long-range contextual information. <b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> can also deliver a large receptive field by using large kernels, enabling them to achieve competitive performance with fewer model parameters. However, <b>CNNs</b> incorporated with large <b>convolutional</b> <b>kernels</b> <b>remain</b> constrained in adaptively capturing multi-scale features from organs with large variations in shape and size due to the employment of fixed-sized kernels. Additionally, they are unable to utilize global contextual information efficiently. To address these limitations, we propose Dynamic Large Kernel (DLK) and Dynamic Feature Fusion (DFF) modules. The DLK module employs multiple large kernels with varying kernel sizes and dilation rates to capture multi-scale features. Subsequently, a dynamic selection mechanism is utilized to adaptively highlight the most important spatial features based on global information. Additionally, the DFF module is proposed to adaptively fuse multi-scale local feature maps based on their global information. We integrate DLK and DFF in a hierarchical <b>transformer</b> architecture to develop a novel architecture, termed D-Net. D-Net is able to effectively utilize a multi-scale large receptive field and adaptively harness global contextual information. Extensive experimental results demonstrate that D-Net outperforms other state-of-the-art models in the two volumetric segmentation tasks, including abdominal multi-organ segmentation and multi-modality brain tumor segmentation. Our code is available at <a href=https://github.com/sotiraslab/DLK>https://github.com/sotiraslab/DLK</a>.</p></p class="citation"></blockquote><h3 id=212--233298-overcoming-distribution-shifts-in-plug-and-play-methods-with-test-time-training-edward-p-chandler-et-al-2024>(2/12 | 233/298) Overcoming Distribution Shifts in Plug-and-Play Methods with Test-Time Training (Edward P. Chandler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edward P. Chandler, Shirin Shoushtari, Jiaming Liu, M. Salman Asif, Ulugbek S. Kamilov. (2024)<br><strong>Overcoming Distribution Shifts in Plug-and-Play Methods with Test-Time Training</strong><br><button class=copy-to-clipboard title="Overcoming Distribution Shifts in Plug-and-Play Methods with Test-Time Training" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, Self-supervised Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10374v1.pdf filename=2403.10374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Plug-and-Play Priors (PnP) is a well-known class of methods for solving inverse problems in computational imaging. PnP methods combine physical forward models with learned prior models specified as image denoisers. A common issue with the learned models is that of a performance drop when there is a <b>distribution</b> <b>shift</b> between the training and testing data. Test-time training (TTT) was recently proposed as a general strategy for improving the performance of learned models when training and testing data come from different <b>distributions.</b> <b>In</b> this paper, we propose PnP-TTT as a new method for overcoming <b>distribution</b> <b>shifts</b> in PnP. PnP-TTT uses deep equilibrium learning (DEQ) for optimizing a <b>self-supervised</b> loss at the fixed points of PnP iterations. PnP-TTT can be directly applied on a single test sample to improve the generalization of PnP. We show through <b>simulations</b> that given a sufficient number of measurements, PnP-TTT enables the use of image priors trained on natural images for image reconstruction in magnetic resonance imaging (MRI).</p></p class="citation"></blockquote><h3 id=312--234298-histo-genomic-knowledge-distillation-for-cancer-prognosis-from-histopathology-whole-slide-images-zhikang-wang-et-al-2024>(3/12 | 234/298) Histo-Genomic Knowledge Distillation For Cancer Prognosis From Histopathology Whole Slide Images (Zhikang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhikang Wang, Yumeng Zhang, Yingxue Xu, Seiya Imoto, Hao Chen, Jiangning Song. (2024)<br><strong>Histo-Genomic Knowledge Distillation For Cancer Prognosis From Histopathology Whole Slide Images</strong><br><button class=copy-to-clipboard title="Histo-Genomic Knowledge Distillation For Cancer Prognosis From Histopathology Whole Slide Images" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 39<br>Keywords: Benchmarking, Benchmarking, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10040v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10040v2.pdf filename=2403.10040v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Histo-genomic <b>multi-modal</b> methods have recently emerged as a powerful paradigm, demonstrating significant potential for improving cancer prognosis. However, genome sequencing, unlike histopathology imaging, is still not widely accessible in underdeveloped regions, limiting the application of these <b>multi-modal</b> approaches in clinical settings. To address this, we propose a novel Genome-informed Hyper-Attention Network, termed G-HANet, which is capable of effectively <b>distilling</b> the histo-genomic <b>knowledge</b> <b>during</b> training to elevate uni-modal whole slide image (WSI)-based inference for the first time. Compared with traditional <b>knowledge</b> <b>distillation</b> methods (i.e., teacher-student architecture) in other tasks, our end-to-end model is superior in terms of training efficiency and learning cross-modal interactions. Specifically, the network comprises the cross-modal associating branch (CAB) and hyper-attention survival branch (HSB). Through the genomic data reconstruction from WSIs, CAB effectively <b>distills</b> the associations between functional genotypes and morphological phenotypes and offers insights into the gene expression profiles in the feature space. Subsequently, HSB leverages the <b>distilled</b> histo-genomic associations as well as the generated morphology-based weights to achieve the hyper-attention modeling of the patients from both histopathology and genomic perspectives to improve cancer prognosis. Extensive experiments are conducted on five TCGA <b>benchmarking</b> datasets and the results demonstrate that G-HANet significantly outperforms the state-of-the-art WSI-based methods and achieves competitive performance with genome-based and <b>multi-modal</b> methods. G-HANet is expected to be explored as a useful tool by the research community to address the current bottleneck of insufficient histo-genomic data pairing in the context of cancer prognosis and precision oncology.</p></p class="citation"></blockquote><h3 id=412--235298-learning-on-jpeg-ldpc-compressed-images-classifying-with-syndromes-ahcen-aliouat-et-al-2024>(4/12 | 235/298) Learning on JPEG-LDPC Compressed Images: Classifying with Syndromes (Ahcen Aliouat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahcen Aliouat, Elsa Dupraz. (2024)<br><strong>Learning on JPEG-LDPC Compressed Images: Classifying with Syndromes</strong><br><button class=copy-to-clipboard title="Learning on JPEG-LDPC Compressed Images: Classifying with Syndromes" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: 94A08, 94A29, 68P30, I-4-2; I-4-9; E-4; C-2-0; I-2-10, cs-AI, cs-CV, cs-IT, cs-LG, eess-IV, eess.IV, math-IT<br>Keyword Score: 30<br>Keywords: Graph Attention Networks, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10202v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10202v1.pdf filename=2403.10202v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In goal-oriented communications, the objective of the receiver is often to apply a Deep-Learning model, rather than reconstructing the original data. In this context, direct learning over compressed data, without any prior decoding, holds promise for enhancing the time-efficient execution of inference models at the receiver. However, conventional entropic-coding methods like Huffman and Arithmetic break data structure, rendering them unsuitable for learning without decoding. In this paper, we propose an alternative approach in which entropic coding is realized with Low-Density Parity Check (LDPC) codes. We hypothesize that Deep Learning models can more effectively exploit the internal code structure of LDPC codes. At the receiver, we leverage a specific class of <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNNs),</b> specifically <b>Gated</b> <b>Recurrent</b> <b>Unit</b> <b>(GRU),</b> trained for image classification. Our numerical results indicate that classification based on LDPC-coded bit-planes surpasses Huffman and Arithmetic coding, while necessitating a significantly smaller learning model. This demonstrates the efficiency of classification directly from LDPC-coded data, eliminating the need for any form of decompression, even partial, prior to applying the learning model.</p></p class="citation"></blockquote><h3 id=512--236298-hybrid-convolutional-and-attention-network-for-hyperspectral-image-denoising-shuai-hu-et-al-2024>(5/12 | 236/298) Hybrid Convolutional and Attention Network for Hyperspectral Image Denoising (Shuai Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Hu, Feng Gao, Xiaowei Zhou, Junyu Dong, Qian Du. (2024)<br><strong>Hybrid Convolutional and Attention Network for Hyperspectral Image Denoising</strong><br><button class=copy-to-clipboard title="Hybrid Convolutional and Attention Network for Hyperspectral Image Denoising" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10067v1.pdf filename=2403.10067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyperspectral image (HSI) denoising is critical for the effective analysis and interpretation of hyperspectral data. However, simultaneously modeling global and local features is rarely explored to enhance HSI denoising. In this letter, we propose a hybrid <b>convolution</b> and attention network (HCANet), which leverages both the strengths of <b>convolution</b> neural networks <b>(CNNs)</b> and <b>Transformers.</b> To enhance the modeling of both global and local features, we have devised a <b>convolution</b> and attention fusion module aimed at capturing long-range dependencies and neighborhood spectral correlations. Furthermore, to improve multi-scale information aggregation, we design a multi-scale feed-forward network to enhance denoising performance by extracting features at different scales. Experimental results on mainstream HSI datasets demonstrate the rationality and effectiveness of the proposed HCANet. The proposed model is effective in removing various types of complex noise. Our codes are available at \url{https://github.com/summitgao/HCANet}.</p></p class="citation"></blockquote><h3 id=612--237298-a-general-method-to-incorporate-spatial-information-into-loss-functions-for-gan-based-super-resolution-models-xijun-wang-et-al-2024>(6/12 | 237/298) A General Method to Incorporate Spatial Information into Loss Functions for GAN-based Super-resolution Models (Xijun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xijun Wang, Santiago López-Tapia, Alice Lucas, Xinyi Wu, Rafael Molina, Aggelos K. Katsaggelos. (2024)<br><strong>A General Method to Incorporate Spatial Information into Loss Functions for GAN-based Super-resolution Models</strong><br><button class=copy-to-clipboard title="A General Method to Incorporate Spatial Information into Loss Functions for GAN-based Super-resolution Models" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10589v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10589v1.pdf filename=2403.10589v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> have shown great performance on super-resolution problems since they can generate more visually realistic images and video frames. However, these models often introduce side effects into the outputs, such as unexpected artifacts and noises. To reduce these artifacts and enhance the perceptual quality of the results, in this paper, we propose a general method that can be effectively used in most <b>GAN-based</b> super-resolution (SR) models by introducing essential spatial information into the training process. We extract spatial information from the input data and incorporate it into the training loss, making the corresponding loss a spatially adaptive (SA) one. After that, we utilize it to guide the training process. We will show that the proposed approach is independent of the methods used to extract the spatial information and independent of the SR tasks and models. This method consistently guides the training process towards generating visually pleasing SR images and video frames, substantially mitigating artifacts and noise, ultimately leading to enhanced perceptual quality.</p></p class="citation"></blockquote><h3 id=712--238298-how-suboptimal-is-training-rppg-models-with-videos-and-targets-from-different-body-sites-björn-braun-et-al-2024>(7/12 | 238/298) How Suboptimal is Training rPPG Models with Videos and Targets from Different Body Sites? (Björn Braun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Björn Braun, Daniel McDuff, Christian Holz. (2024)<br><strong>How Suboptimal is Training rPPG Models with Videos and Targets from Different Body Sites?</strong><br><button class=copy-to-clipboard title="How Suboptimal is Training rPPG Models with Videos and Targets from Different Body Sites?" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10582v1.pdf filename=2403.10582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote camera measurement of the blood volume pulse via photoplethysmography (rPPG) is a compelling technology for scalable, low-cost, and accessible assessment of cardiovascular information. Neural networks currently provide the state-of-the-art for this task and <b>supervised</b> training or <b>fine-tuning</b> is an important step in creating these models. However, most current models are trained on facial videos using contact PPG measurements from the fingertip as targets/ labels. One of the reasons for this is that few public datasets to date have incorporated contact PPG measurements from the face. Yet there is copious evidence that the PPG signals at different sites on the body have very different morphological features. Is training a facial video rPPG model using contact measurements from another site on the body suboptimal? Using a recently released unique dataset with synchronized contact PPG and video measurements from both the hand and face, we can provide precise and quantitative answers to this question. We obtain up to 40 % lower mean squared errors between the waveforms of the predicted and the ground truth PPG signals using state-of-the-art neural models when using PPG signals from the forehead compared to using PPG signals from the fingertip. We also show qualitatively that the neural models learn to predict the morphology of the ground truth PPG signal better when trained on the forehead PPG signals. However, while models trained from the forehead PPG produce a more faithful waveform, models trained from a finger PPG do still learn the dominant frequency (i.e., the heart rate) well.</p></p class="citation"></blockquote><h3 id=812--239298-cardiac-magnetic-resonance-2dt-short--and-long-axis-segmentation-via-spatio-temporal-sam-adaptation-zhennong-chen-et-al-2024>(8/12 | 239/298) Cardiac Magnetic Resonance 2D+T Short- and Long-axis Segmentation via Spatio-temporal SAM Adaptation (Zhennong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhennong Chen, Sekeun Kim, Hui Ren, Quanzheng Li, Xiang Li. (2024)<br><strong>Cardiac Magnetic Resonance 2D+T Short- and Long-axis Segmentation via Spatio-temporal SAM Adaptation</strong><br><button class=copy-to-clipboard title="Cardiac Magnetic Resonance 2D+T Short- and Long-axis Segmentation via Spatio-temporal SAM Adaptation" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10009v1.pdf filename=2403.10009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate 2D+T myocardium segmentation in cine cardiac magnetic resonance (CMR) scans is essential to analyze LV motion throughout the cardiac cycle comprehensively. The Segment Anything Model (SAM), known for its accurate segmentation and <b>zero-shot</b> generalization, has not yet been tailored for CMR 2D+T segmentation. We therefore introduce CMR2D+T-SAM, a novel approach to adapt SAM for CMR 2D+T segmentation using spatio-temporal adaption. This approach also incorporates a U-Net framework for multi-scale feature extraction, as well as text <b>prompts</b> for accurate segmentation on both short-axis (SAX) and long-axis (LAX) views using a single model. CMR2D+T-SAM outperforms existing deep learning methods on the STACOM2011 dataset, achieving a myocardium Dice score of 0.885 and a Hausdorff distance (HD) of 2.900 pixels. It also demonstrates superior <b>zero-shot</b> generalization on the ACDC dataset with a Dice score of 0.840 and a HD of 4.076 pixels.</p></p class="citation"></blockquote><h3 id=912--240298-attention-enhanced-hybrid-feature-aggregation-network-for-3d-brain-tumor-segmentation-ziya-ata-yazıcı-et-al-2024>(9/12 | 240/298) Attention-Enhanced Hybrid Feature Aggregation Network for 3D Brain Tumor Segmentation (Ziya Ata Yazıcı et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziya Ata Yazıcı, İlkay Öksüz, Hazım Kemal Ekenel. (2024)<br><strong>Attention-Enhanced Hybrid Feature Aggregation Network for 3D Brain Tumor Segmentation</strong><br><button class=copy-to-clipboard title="Attention-Enhanced Hybrid Feature Aggregation Network for 3D Brain Tumor Segmentation" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09942v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09942v1.pdf filename=2403.09942v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Glioblastoma is a highly aggressive and malignant brain tumor type that requires early diagnosis and <b>prompt</b> intervention. Due to its heterogeneity in appearance, developing automated detection approaches is challenging. To address this challenge, Artificial Intelligence (AI)-driven approaches in healthcare have generated interest in efficiently diagnosing and evaluating brain tumors. The Brain Tumor Segmentation Challenge (BraTS) is a platform for developing and assessing automated techniques for tumor analysis using high-quality, clinically acquired MRI data. In our approach, we utilized a multi-scale, attention-guided and hybrid U-Net-shaped model &ndash; GLIMS &ndash; to perform 3D brain tumor segmentation in three regions: Enhancing Tumor (ET), Tumor Core (TC), and Whole Tumor (WT). The multi-scale feature extraction provides better contextual feature aggregation in high resolutions and the Swin <b>Transformer</b> blocks improve the global feature extraction at deeper levels of the model. The segmentation mask generation in the decoder branch is guided by the attention-refined features gathered from the encoder branch to enhance the important attributes. Moreover, hierarchical supervision is used to train the model efficiently. Our model&rsquo;s performance on the validation set resulted in 92.19, 87.75, and 83.18 Dice Scores and 89.09, 84.67, and 82.15 Lesion-wise Dice Scores in WT, TC, and ET, respectively. The code is publicly available at <a href=https://github.com/yaziciz/GLIMS>https://github.com/yaziciz/GLIMS</a>.</p></p class="citation"></blockquote><h3 id=1012--241298-solving-general-noisy-inverse-problem-via-posterior-sampling-a-policy-gradient-viewpoint-haoyue-tang-et-al-2024>(10/12 | 241/298) Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint (Haoyue Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyue Tang, Tian Xie, Aosong Feng, Hanyu Wang, Chenyang Zhang, Yang Bai. (2024)<br><strong>Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint</strong><br><button class=copy-to-clipboard title="Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10585v1.pdf filename=2403.10585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Solving image inverse problems (e.g., super-resolution and inpainting) requires generating a high fidelity image that matches the given input (the low-resolution image or the masked image). By using the input image as guidance, we can leverage a pretrained diffusion generative model to solve a wide range of image inverse tasks without task specific model <b>fine-tuning.</b> To precisely estimate the guidance score function of the input image, we propose Diffusion Policy Gradient (DPG), a tractable computation method by viewing the intermediate noisy images as policies and the target image as the states selected by the policy. Experiments show that our method is robust to both Gaussian and Poisson noise degradation on multiple linear and non-linear inverse tasks, resulting into a higher image restoration quality on FFHQ, ImageNet and LSUN datasets.</p></p class="citation"></blockquote><h3 id=1112--242298-cardiac-valve-event-timing-in-echocardiography-using-deep-learning-and-triplane-recordings-benjamin-strandli-fermann-et-al-2024>(11/12 | 242/298) Cardiac valve event timing in echocardiography using deep learning and triplane recordings (Benjamin Strandli Fermann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Strandli Fermann, John Nyberg, Espen W. Remme, Jahn Frederik Grue, Helén Grue, Roger Håland, Lasse Lovstakken, Håvard Dalen, Bjørnar Grenne, Svein Arne Aase, Sten Roar Snar, Andreas Østvik. (2024)<br><strong>Cardiac valve event timing in echocardiography using deep learning and triplane recordings</strong><br><button class=copy-to-clipboard title="Cardiac valve event timing in echocardiography using deep learning and triplane recordings" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10156v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10156v1.pdf filename=2403.10156v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cardiac valve <b>event</b> <b>timing</b> plays a crucial role when conducting clinical measurements using echocardiography. However, established automated approaches are limited by the need of external electrocardiogram sensors, and manual measurements often rely on timing from different cardiac cycles. Recent methods have applied deep learning to cardiac timing, but they have mainly been restricted to only detecting two key time points, namely end-diastole (ED) and end-systole (ES). In this work, we propose a deep learning approach that leverages triplane recordings to enhance detection of valve <b>events</b> <b>in</b> echocardiography. Our method demonstrates improved performance detecting six different <b>events,</b> <b>including</b> valve <b>events</b> <b>conventionally</b> associated with ED and ES. Of all <b>events,</b> <b>we</b> achieve an average absolute frame difference (aFD) of maximum 1.4 frames (29 ms) for start of diastasis, down to 0.6 frames (12 ms) for mitral valve opening when performing a ten-fold cross-validation with test splits on triplane data from 240 patients. On an external independent test consisting of apical long-axis data from 180 other patients, the worst performing <b>event</b> <b>detection</b> had an aFD of 1.8 (30 ms). The proposed approach has the potential to significantly impact clinical practice by enabling more accurate, rapid and comprehensive <b>event</b> <b>detection,</b> leading to improved clinical measurements.</p></p class="citation"></blockquote><h3 id=1212--243298-neuraloct-airway-oct-analysis-via-neural-fields-yining-jiao-et-al-2024>(12/12 | 243/298) NeuralOCT: Airway OCT Analysis via Neural Fields (Yining Jiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yining Jiao, Amy Oldenburg, Yinghan Xu, Srikamal Soundararajan, Carlton Zdanski, Julia Kimbell, Marc Niethammer. (2024)<br><strong>NeuralOCT: Airway OCT Analysis via Neural Fields</strong><br><button class=copy-to-clipboard title="NeuralOCT: Airway OCT Analysis via Neural Fields" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10622v1.pdf filename=2403.10622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optical coherence tomography (OCT) is a popular modality in ophthalmology and is also used intravascularly. Our interest in this work is OCT in the context of airway abnormalities in infants and children where the high resolution of OCT and the fact that it is radiation-free is important. The goal of airway OCT is to provide accurate estimates of airway <b>geometry</b> (in 2D and 3D) to assess airway abnormalities such as subglottic stenosis. We propose $\texttt{NeuralOCT}$, a learning-based approach to process airway OCT images. Specifically, $\texttt{NeuralOCT}$ extracts 3D geometries from OCT scans by robustly bridging two steps: point cloud extraction via 2D segmentation and 3D reconstruction from point clouds via neural fields. Our experiments show that $\texttt{NeuralOCT}$ produces accurate and robust 3D airway reconstructions with an average A-line error smaller than 70 micrometer. Our code will cbe available on GitHub.</p></p class="citation"></blockquote><h2 id=cshc-2>cs.HC (2)</h2><h3 id=12--244298-trusting-the-search-unraveling-human-trust-in-health-information-from-google-and-chatgpt-xin-sun-et-al-2024>(1/2 | 244/298) Trusting the Search: Unraveling Human Trust in Health Information from Google and ChatGPT (Xin Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Sun, Rongjun Ma, Xiaochang Zhao, Zhuying Li, Janne Lindqvist, Abdallah El Ali, Jos A. Bosch. (2024)<br><strong>Trusting the Search: Unraveling Human Trust in Health Information from Google and ChatGPT</strong><br><button class=copy-to-clipboard title="Trusting the Search: Unraveling Human Trust in Health Information from Google and ChatGPT" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: F-2-2, I-2-7, cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: Generative AI, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09987v1.pdf filename=2403.09987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>People increasingly rely on online sources for health information seeking due to their convenience and timeliness, traditionally using search engines like Google as the primary search agent. Recently, the emergence of <b>generative</b> <b>Artificial</b> Intelligence (AI) has made <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> powered conversational agents such as <b>ChatGPT</b> a viable alternative for health information search. However, while trust is crucial for adopting the online health advice, the factors influencing people&rsquo;s trust judgments in health information provided by <b>LLM-powered</b> conversational agents remain unclear. To address this, we conducted a mixed-methods, within-subjects lab study (N=21) to explore how interactions with different agents <b>(ChatGPT</b> vs. Google) across three health search tasks influence participants&rsquo; trust judgments of the search results as well as the search agents themselves. Our key findings showed that: (a) participants&rsquo; trust levels in <b>ChatGPT</b> were significantly higher than Google in the context of health information seeking; (b) there is a significant correlation between trust in health-related information and trust in the search agent, however only for Google; (c) the type of search tasks did not affect participants&rsquo; perceived trust; and (d) participants&rsquo; prior knowledge, the style of information presentation, and the interactive manner of using search agents were key determinants of trust in the health-related information. Our study taps into differences in trust perceptions when using traditional search engines compared to <b>LLM-powered</b> conversational agents. We highlight the potential role <b>LLMs</b> play in health-related information-seeking contexts, where they excel as stepping stones for further search. We contribute key factors and considerations for ensuring effective and reliable personal health information seeking in the age of <b>generative</b> <b>AI.</b></p></p class="citation"></blockquote><h3 id=22--245298-designing-user-centered-simulations-of-leadership-situations-for-cave-automatic-virtual-environments-development-and-usability-study-francesco-vona-et-al-2024>(2/2 | 245/298) Designing User-Centered Simulations of Leadership Situations for Cave Automatic Virtual Environments: Development and Usability Study (Francesco Vona et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Vona, Miladin Ćeranić, Irma Rybnikova, Jan-Niklas Voigt-Antons. (2024)<br><strong>Designing User-Centered Simulations of Leadership Situations for Cave Automatic Virtual Environments: Development and Usability Study</strong><br><button class=copy-to-clipboard title="Designing User-Centered Simulations of Leadership Situations for Cave Automatic Virtual Environments: Development and Usability Study" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10312v1.pdf filename=2403.10312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given that experience is a pivotal dimension of learning processes in the field of leadership, the ongoing and unresolved issue is how such experiential moments could be provided when developing leadership skills and competencies. Role-plays and business <b>simulations</b> are widely used in this context as they are said to teach relevant social leadership skills, like those required by everyday communication to followers, by decision-making on compensation, evaluating performance, dealing with conflicts, or terminating contracts. However, the effectiveness of <b>simulations</b> can highly vary depending on the counterpart&rsquo;s ability to act in the given scenarios. In our project, we deal with how immersive media could create experiential learning based on <b>simulations</b> for leadership development. In recent years different variations of extended reality got significant technological improvements. Head-mounted displays are an easy and cost-efficient way to present high-resolution virtual environments. For groups of people that are part of an immersive experience, cave automatic virtual environments offer an excellent trade-off between actual exchange with other humans and interaction with virtual content simultaneously. The work presented is based on developing a user-centered <b>simulation</b> of leadership situations for cave automatic virtual environments and includes the results of a first usability study. In the future, the presented results can help to support the development and evaluation of simulated situations for cave automatic virtual environments with an emphasis on leadership-related scenarios.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--246298-large-language-model-informed-ecg-dual-attention-network-for-heart-failure-risk-prediction-chen-chen-et-al-2024>(1/1 | 246/298) Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction (Chen Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Chen, Lei Li, Marcel Beetz, Abhirup Banerjee, Ramneek Gupta, Vicente Grau. (2024)<br><strong>Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction</strong><br><button class=copy-to-clipboard title="Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-AI, cs-CL, cs-LG, eess-SP, q-bio-QM, q-bio.QM<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10581v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10581v1.pdf filename=2403.10581v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Heart failure (HF) poses a significant public health challenge due to its rising global mortality rate. Addressing this issue through early diagnosis and prevention could significantly reduce the disease&rsquo;s impact. This work introduces a methodology for HF risk prediction using clinically acquired 12-lead electrocardiograms (ECGs). We present a novel, lightweight dual-attention ECG network designed to capture complex ECG features essential for early HF prediction, despite the notable imbalance between low and high-risk groups. The network features a cross-lead attention module and twelve lead-specific temporal attention modules to capture cross-lead interactions and local temporal dynamics within each lead. To prevent model overfitting from limited training data, we leverage a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> with a public ECG-Report dataset for pretraining on an ECG-report alignment task. The network is then <b>fine-tuned</b> for HF risk prediction using two specific cohorts from the UK Biobank study, focusing on patients with hypertension (UKB-HYP) and those who have had a myocardial infarction (UKB-MI). Our findings show that <b>LLM-informed</b> pretraining significantly improves the network&rsquo;s HF risk prediction capability in these cohorts. Moreover, the dual-attention mechanism enhances interpretability and predictive performance, ensuring a transparent and reliable prediction process. The method outperforms existing models, achieving average C-index scores of 0.6349 and 0.5805 on the UKB-HYP and UKB-MI test sets, respectively. This performance demonstrates our approach&rsquo;s effectiveness in managing complex clinical ECG data and its potential to improve HF risk assessment across various populations.</p></p class="citation"></blockquote><h2 id=cscy-3>cs.CY (3)</h2><h3 id=13--247298-emotion-aware-multimodal-fusion-for-meme-emotion-detection-shivam-sharma-et-al-2024>(1/3 | 247/298) Emotion-Aware Multimodal Fusion for Meme Emotion Detection (Shivam Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shivam Sharma, Ramaneswaran S, Md. Shad Akhtar, Tanmoy Chakraborty. (2024)<br><strong>Emotion-Aware Multimodal Fusion for Meme Emotion Detection</strong><br><button class=copy-to-clipboard title="Emotion-Aware Multimodal Fusion for Meme Emotion Detection" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 26<br>Keywords: Graph Attention Networks, Multi-modal, Multi-modal, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10279v1.pdf filename=2403.10279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ever-evolving social media discourse has witnessed an overwhelming use of memes to express opinions or dissent. Besides being misused for spreading malcontent, they are mined by corporations and political parties to glean the public&rsquo;s opinion. Therefore, memes predominantly offer affect-enriched insights towards ascertaining the societal psyche. However, the current approaches are yet to model the affective dimensions expressed in memes effectively. They rely extensively on large <b>multimodal</b> datasets for pre-training and do not generalize well due to constrained visual-linguistic <b>grounding.</b> In this paper, we introduce MOOD (Meme emOtiOns Dataset), which embodies six basic emotions. We then present ALFRED (emotion-Aware <b>muLtimodal</b> Fusion foR Emotion Detection), a novel <b>multimodal</b> neural framework that (i) explicitly models emotion-enriched visual cues, and (ii) employs an efficient cross-modal fusion via a <b>gating</b> mechanism. Our investigation establishes ALFRED&rsquo;s superiority over existing baselines by 4.94% F1. Additionally, ALFRED competes strongly with previous best approaches on the challenging Memotion task. We then discuss ALFRED&rsquo;s domain-agnostic generalizability by demonstrating its dominance on two recently-released datasets - HarMeme and Dank Memes, over other baselines. Further, we analyze ALFRED&rsquo;s interpretability using attention maps. Finally, we highlight the inherent challenges posed by the complex interplay of disparate modality-specific cues toward meme analysis.</p></p class="citation"></blockquote><h3 id=23--248298-graph-enhanced-reinforcement-learning-for-effective-group-formation-in-collaborative-problem-solving-zheng-fang-et-al-2024>(2/3 | 248/298) Graph Enhanced Reinforcement Learning for Effective Group Formation in Collaborative Problem Solving (Zheng Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Fang, Fucai Ke, Jae Young Han, Zhijie Feng, Toby Cai. (2024)<br><strong>Graph Enhanced Reinforcement Learning for Effective Group Formation in Collaborative Problem Solving</strong><br><button class=copy-to-clipboard title="Graph Enhanced Reinforcement Learning for Effective Group Formation in Collaborative Problem Solving" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-HC, cs-LG, cs-SI, cs.CY<br>Keyword Score: 16<br>Keywords: Graph, Clustering, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10006v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10006v1.pdf filename=2403.10006v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study addresses the challenge of forming effective groups in collaborative problem-solving environments. Recognizing the complexity of human interactions and the necessity for efficient collaboration, we propose a novel approach leveraging <b>graph</b> theory and <b>reinforcement</b> <b>learning.</b> Our methodology involves constructing a <b>graph</b> from a dataset where nodes represent participants, and edges signify the interactions between them. We conceptualize each participant as an agent within a <b>reinforcement</b> <b>learning</b> framework, aiming to learn an optimal <b>graph</b> structure that reflects effective group dynamics. <b>Clustering</b> techniques are employed to delineate clear group structures based on the learned <b>graph.</b> Our approach provides theoretical solutions based on evaluation metrics and <b>graph</b> measurements, offering insights into potential improvements in group effectiveness and reductions in conflict incidences. This research contributes to the fields of collaborative work and educational psychology by presenting a data-driven, analytical approach to group formation. It has practical implications for organizational team building, classroom settings, and any collaborative scenario where group dynamics are crucial. The study opens new avenues for exploring the application of <b>graph</b> theory and <b>reinforcement</b> <b>learning</b> in social and behavioral sciences, highlighting the potential for empirical validation in future work.</p></p class="citation"></blockquote><h3 id=33--249298-designing-sousveillance-tools-for-gig-workers-kimberly-do-et-al-2024>(3/3 | 249/298) Designing Sousveillance Tools for Gig Workers (Kimberly Do et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kimberly Do, Maya De Los Santos, Michael Muller, Saiph Savage. (2024)<br><strong>Designing Sousveillance Tools for Gig Workers</strong><br><button class=copy-to-clipboard title="Designing Sousveillance Tools for Gig Workers" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-HC, cs-SI, cs.CY<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09986v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09986v1.pdf filename=2403.09986v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As independently-contracted employees, gig workers disproportionately suffer the consequences of workplace surveillance, which include increased pressures to work, breaches of privacy, and decreased digital autonomy. Despite the negative impacts of workplace surveillance, gig workers lack the tools, strategies, and workplace social support to protect themselves against these harms. Meanwhile, some critical theorists have proposed sousveillance as a potential means of countering such abuses of power, whereby those under surveillance monitor those in positions of authority (e.g., gig workers collect data about requesters/platforms). To understand the benefits of sousveillance systems in the gig economy, we conducted semi-structured interviews and led co-design activities with gig workers. We use &ldquo;care ethics&rdquo; as a guiding concept to understand our interview and co-design data, while also focusing on empathic sousveillance technology design <b>recommendations.</b> Through our study, we identify gig workers&rsquo; attitudes towards and past experiences with sousveillance. We also uncover the type of sousveillance technologies imagined by workers, provide design <b>recommendations,</b> and finish by discussing how to create empowering, empathic spaces on gig platforms.</p></p class="citation"></blockquote><h2 id=csit-6>cs.IT (6)</h2><h3 id=16--250298-joint-group-scheduling-and-multicast-beamforming-for-downlink-large-scale-multi-group-multicast-chong-zhang-et-al-2024>(1/6 | 250/298) Joint Group Scheduling and Multicast Beamforming for Downlink Large-Scale Multi-Group Multicast (Chong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chong Zhang, Min Dong, Ben Liang, Ali Afana, Yahia Ahmed. (2024)<br><strong>Joint Group Scheduling and Multicast Beamforming for Downlink Large-Scale Multi-Group Multicast</strong><br><button class=copy-to-clipboard title="Joint Group Scheduling and Multicast Beamforming for Downlink Large-Scale Multi-Group Multicast" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 23<br>Keywords: Clustering, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10002v1.pdf filename=2403.10002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Next-generation wireless networks need to handle massive user access effectively. This paper addresses the problem of joint group scheduling and multicast beamforming for downlink multicast with many active groups. Aiming to maximize the minimum user throughput, we propose a three-phase approach to tackle this difficult joint optimization problem efficiently. In Phase 1, we utilize the optimal multicast beamforming structure obtained recently to find the group-channel directions for all groups. We propose two low-complexity scheduling algorithms in Phase 2, which determine the subset of groups in each time slot sequentially and the total number of time slots required for all groups. The first algorithm measures the level of spatial separation among groups and selects the dissimilar groups that maximize the minimum user rate into the same time slot. In contrast, the second algorithm first identifies the spatially correlated groups via a learning-based <b>clustering</b> method based on the group-channel directions, and then separates spatially similar groups into different time slots. Finally, the multicast beamformers for the scheduled groups are obtained in each time slot by a computationally efficient method. <b>Simulation</b> results show that our proposed approaches can effectively capture the level of spatial separation among groups for scheduling to improve the minimum user throughput over the conventional approach that serves all groups in a single time slot or one group per time slot, and can be executed with low computational complexity.</p></p class="citation"></blockquote><h3 id=26--251298-matrix-completion-via-nonsmooth-regularization-of-fully-connected-neural-networks-sajad-faramarzi-et-al-2024>(2/6 | 251/298) Matrix Completion via Nonsmooth Regularization of Fully Connected Neural Networks (Sajad Faramarzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sajad Faramarzi, Farzan Haddadi, Sajjad Amini, Masoud Ahookhosh. (2024)<br><strong>Matrix Completion via Nonsmooth Regularization of Fully Connected Neural Networks</strong><br><button class=copy-to-clipboard title="Matrix Completion via Nonsmooth Regularization of Fully Connected Neural Networks" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10232v1.pdf filename=2403.10232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional matrix completion methods approximate the missing values by assuming the matrix to be low-rank, which leads to a linear approximation of missing values. It has been shown that enhanced performance could be attained by using nonlinear estimators such as deep neural networks. Deep fully connected neural networks (FCNNs), one of the most suitable architectures for matrix completion, suffer from over-fitting due to their high capacity, which leads to low generalizability. In this paper, we control over-fitting by regularizing the FCNN model in terms of the $\ell_{1}$ norm of intermediate representations and nuclear norm of weight matrices. As such, the resulting regularized objective function becomes nonsmooth and nonconvex, i.e., existing gradient-based methods cannot be applied to our model. We propose a variant of the proximal gradient method and investigate its convergence to a critical point. In the initial epochs of FCNN training, the regularization terms are ignored, and through epochs, the effect of that increases. The gradual addition of nonsmooth regularization terms is the main reason for the better performance of the deep neural network with nonsmooth regularization terms (DNN-NSR) algorithm. Our <b>simulations</b> indicate the superiority of the proposed algorithm in comparison with existing linear and nonlinear algorithms.</p></p class="citation"></blockquote><h3 id=36--252298-fairness-optimization-for-intelligent-reflecting-surface-aided-uplink-rate-splitting-multiple-access-shanshan-zhang-et-al-2024>(3/6 | 252/298) Fairness Optimization for Intelligent Reflecting Surface Aided Uplink Rate-Splitting Multiple Access (Shanshan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanshan Zhang, Wen Chen, Qingqing Wu, Ziwei Liu, Shunqing Zhang, Jun Li. (2024)<br><strong>Fairness Optimization for Intelligent Reflecting Surface Aided Uplink Rate-Splitting Multiple Access</strong><br><button class=copy-to-clipboard title="Fairness Optimization for Intelligent Reflecting Surface Aided Uplink Rate-Splitting Multiple Access" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Fairness, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10230v1.pdf filename=2403.10230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies the fair transmission design for an intelligent reflecting surface (IRS) aided rate-splitting multiple access (RSMA). IRS is used to establish a good signal propagation environment and enhance the RSMA transmission performance. The fair rate adaption problem is constructed as a max-min optimization problem. To solve the optimization problem, we adopt an alternative optimization (AO) algorithm to optimize the power allocation, beamforming, and decoding order, respectively. A generalized power iteration (GPI) method is proposed to optimize the receive beamforming, which can improve the minimum rate of devices and reduce the optimization complexity. At the base station (BS), a successive group decoding <b>(SGD)</b> algorithm is proposed to tackle the uplink signal estimation, which trades off the <b>fairness</b> and complexity of decoding. At the same time, we also consider robust communication with imperfect channel state information at the transmitter (CSIT), which studies robust optimization by using lower bound expressions on the expected data rates. Extensive numerical results show that the proposed optimization algorithm can significantly improve the performance of <b>fairness.</b> It also provides reliable results for uplink communication with imperfect CSIT.</p></p class="citation"></blockquote><h3 id=46--253298-secure-distributed-storage-optimal-trade-off-between-storage-rate-and-privacy-leakage-remi-a-chou-et-al-2024>(4/6 | 253/298) Secure Distributed Storage: Optimal Trade-Off Between Storage Rate and Privacy Leakage (Remi A. Chou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Remi A. Chou, Joerg Kliewer. (2024)<br><strong>Secure Distributed Storage: Optimal Trade-Off Between Storage Rate and Privacy Leakage</strong><br><button class=copy-to-clipboard title="Secure Distributed Storage: Optimal Trade-Off Between Storage Rate and Privacy Leakage" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-CR, cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10676v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10676v1.pdf filename=2403.10676v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Consider the problem of storing data in a distributed manner over $T$ servers. Specifically, the data needs to (i) be recoverable from any $\tau$ servers, and (ii) remain private from any $z$ colluding servers, where privacy is quantified in terms of <b>mutual</b> <b>information</b> between the data and all the information available at any $z$ colluding servers. For this model, our main results are (i) the fundamental trade-off between storage size and the level of desired privacy, and (ii) the optimal amount of local randomness necessary at the encoder. As a byproduct, our results provide an optimal lower bound on the individual share size of ramp secret sharing schemes under a more general leakage symmetry condition than the ones previously considered in the literature.</p></p class="citation"></blockquote><h3 id=56--254298-chernoff-information-as-a-privacy-constraint-for-adversarial-classification-ayşe-ünsal-et-al-2024>(5/6 | 254/298) Chernoff Information as a Privacy Constraint for Adversarial Classification (Ayşe Ünsal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayşe Ünsal, Melek Önen. (2024)<br><strong>Chernoff Information as a Privacy Constraint for Adversarial Classification</strong><br><button class=copy-to-clipboard title="Chernoff Information as a Privacy Constraint for Adversarial Classification" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10307v1.pdf filename=2403.10307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work studies a privacy metric based on Chernoff information, \textit{Chernoff <b>differential</b> <b>privacy},</b> due to its significance in characterization of classifier performance. Adversarial classification, as any other classification problem is built around minimization of the (average or correct detection) probability of error in deciding on either of the classes in the case of binary classification. Unlike the classical hypothesis testing problem, where the false alarm and mis-detection probabilities are handled separately resulting in an asymmetric behavior of the best error exponent, in this work, we focus on the Bayesian setting and characterize the relationship between the best error exponent of the average error probability and $\varepsilon-$differential privacy. Accordingly, we re-derive Chernoff <b>differential</b> <b>privacy</b> in terms of $\varepsilon-$differential privacy using the Radon-Nikodym derivative and show that it satisfies the composition property. Subsequently, we present numerical evaluation results, which demonstrates that Chernoff information outperforms Kullback-Leibler divergence as a function of the privacy parameter $\varepsilon$, the impact of the adversary&rsquo;s attack and global sensitivity for the problem of adversarial classification in Laplace mechanisms.</p></p class="citation"></blockquote><h3 id=66--255298-approximation-and-bounding-techniques-for-the-fisher-rao-distances-frank-nielsen-2024>(6/6 | 255/298) Approximation and bounding techniques for the Fisher-Rao distances (Frank Nielsen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frank Nielsen. (2024)<br><strong>Approximation and bounding techniques for the Fisher-Rao distances</strong><br><button class=copy-to-clipboard title="Approximation and bounding techniques for the Fisher-Rao distances" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-CV, cs-IT, cs-LG, cs.IT, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10089v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10089v2.pdf filename=2403.10089v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Fisher-Rao distance between two probability distributions of a statistical model is defined as the Riemannian geodesic distance induced by the Fisher information metric. In order to calculate the Fisher-Rao distance in closed-form, we need (1) to elicit a formula for the Fisher-Rao geodesics, and (2) to integrate the Fisher length element along those geodesics. We consider several numerically robust approximation and bounding techniques for the Fisher-Rao distances: First, we report generic upper bounds on Fisher-Rao distances based on closed-form 1D Fisher-Rao distances of submodels. Second, we describe several generic approximation schemes depending on whether the Fisher-Rao geodesics or pregeodesics are available in closed-form or not. In particular, we obtain a generic method to guarantee an arbitrarily small additive error on the approximation provided that Fisher-Rao pregeodesics and tight lower and upper bounds are available. Third, we consider the case of Fisher metrics being Hessian metrics, and report generic tight upper bounds on the Fisher-Rao distances using techniques of information <b>geometry.</b> Uniparametric and biparametric statistical models always have Fisher Hessian metrics, and in general a simple test allows to check whether the Fisher information matrix yields a Hessian metric or not. Fourth, we consider elliptical distribution families and show how to apply the above techniques to these models. We also propose two new distances based either on the Fisher-Rao lengths of curves serving as proxies of Fisher-Rao geodesics, or based on the Birkhoff/Hilbert projective cone distance. Last, we consider an alternative group-theoretic approach for statistical transformation models based on the notion of maximal invariant which yields insights on the structures of the Fisher-Rao distance formula which may be used fruitfully in applications.</p></p class="citation"></blockquote><h2 id=csni-5>cs.NI (5)</h2><h3 id=15--256298-joint-optimization-of-star-ris-assisted-swipt-communication-systems-junlong-yang-2024>(1/5 | 256/298) Joint Optimization of STAR-RIS Assisted SWIPT Communication Systems (Junlong Yang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junlong Yang. (2024)<br><strong>Joint Optimization of STAR-RIS Assisted SWIPT Communication Systems</strong><br><button class=copy-to-clipboard title="Joint Optimization of STAR-RIS Assisted SWIPT Communication Systems" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09983v1.pdf filename=2403.09983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Simultaneous wireless information and power transfer (SWIPT) is an effective energy-saving technology, but its efficiency is hindered by environmental factors. The introduction of reconfigurable intelligent surfaces (RIS) has alleviated this issue, although it still faces significant constraints due to geographical limitations. This paper proposes a scheme that employs a simultaneously transmitting and reflecting (STAR)-RIS to assist SWIPT. It can overcome this limitation, achieve higher degrees of freedom (DoF), and provide better quality of service (QoS) for users on both sides of the ground. Meanwhile, we have considered a hybrid device based on a power splitting, which is capable of both energy harvesting and information decoding. We have proposed an efficient alternating optimization (AO) method to optimize the phase and amplitude vectors for reflection and transmission, beamforming and the optimal power splitting ratio, achieving an optimal balance between data rate and energy efficiency. Finally, <b>simulation</b> results demonstrate that the sum rate of the proposed model is superior to traditional RIS and other <b>benchmark</b> schemes.</p></p class="citation"></blockquote><h3 id=25--257298-rach-less-handover-with-early-timing-advance-acquisition-for-outage-reduction-subhyal-bin-iqbal-et-al-2024>(2/5 | 257/298) RACH-less Handover with Early Timing Advance Acquisition for Outage Reduction (Subhyal Bin Iqbal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subhyal Bin Iqbal, Umur Karabulut, Ahmad Awada, Philipp Schulz, Gerhard P. Fettweis. (2024)<br><strong>RACH-less Handover with Early Timing Advance Acquisition for Outage Reduction</strong><br><button class=copy-to-clipboard title="RACH-less Handover with Early Timing Advance Acquisition for Outage Reduction" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10286v1.pdf filename=2403.10286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For fifth-generation (5G) and 5G-Advanced networks, outage reduction within the context of reliability is a key objective since outage denotes the time period when a user equipment (UE) cannot communicate with the network. Earlier studies have shown that in the experimental high mobility scenario considered, outage is dominated by the interruption time that stems from the random access channel (RACH)-based handover process from the serving cell to the target cell. A handover by itself is a necessary mobility process to prevent mobility failures and their associated outage. This paper proposes a RACH-less handover signaling scheme for the 3rd Generation Partnership Project (3GPP) conditional handover (CHO) mechanism. The proposed scheme exploits the decoupling between the CHO preparation and execution phases to establish initial synchronization between the UE and the target cell through an early acquisition of the timing advance. This significantly curtails the RACH process and therefore the handover interruption time. Results based on a system-level <b>simulation-based</b> mobility study have shown that the proposed scheme significantly reduces the outage and its constituent handover interruption time relatively by 18.7% and 43.2%, respectively.</p></p class="citation"></blockquote><h3 id=35--258298-is-wireless-bad-for-consensus-in-blockchain-seungmo-kim-2024>(3/5 | 258/298) Is Wireless Bad for Consensus in Blockchain? (Seungmo Kim, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungmo Kim. (2024)<br><strong>Is Wireless Bad for Consensus in Blockchain?</strong><br><button class=copy-to-clipboard title="Is Wireless Bad for Consensus in Blockchain?" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10186v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10186v1.pdf filename=2403.10186v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper examines how wireless communication affects the performance of various blockchain consensus mechanisms, focusing on their scalability and decentralization. It introduces an analytical framework for quantifying these effects, backed by extensive <b>simulations,</b> underscoring its broad applicability to various consensus mechanisms despite wireless communication&rsquo;s unreliability.</p></p class="citation"></blockquote><h3 id=45--259298-netbench-a-large-scale-and-comprehensive-network-traffic-benchmark-dataset-for-foundation-models-chen-qian-et-al-2024>(4/5 | 259/298) NetBench: A Large-Scale and Comprehensive Network Traffic Benchmark Dataset for Foundation Models (Chen Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Qian, Xiaochang Li, Qineng Wang, Gang Zhou, Huajie Shao. (2024)<br><strong>NetBench: A Large-Scale and Comprehensive Network Traffic Benchmark Dataset for Foundation Models</strong><br><button class=copy-to-clipboard title="NetBench: A Large-Scale and Comprehensive Network Traffic Benchmark Dataset for Foundation Models" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-CR, cs-NI, cs.NI<br>Keyword Score: 13<br>Keywords: Benchmarking, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10319v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10319v2.pdf filename=2403.10319v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In computer networking, network traffic refers to the amount of data transmitted in the form of packets between internetworked computers or Cyber-Physical Systems. Monitoring and analyzing network traffic is crucial for ensuring the performance, security, and reliability of a network. However, a significant challenge in network traffic analysis is to process diverse data packets including both ciphertext and plaintext. While many methods have been adopted to analyze network traffic, they often rely on different datasets for performance evaluation. This inconsistency results in substantial manual data processing efforts and unfair comparisons. Moreover, some data processing methods may cause data leakage due to improper separation of training and testing data. To address these issues, we introduce the NetBench, a large-scale and comprehensive <b>benchmark</b> dataset for assessing machine learning models, especially <b>foundation</b> <b>models,</b> in both network traffic classification and generation tasks. NetBench is built upon seven publicly available datasets and encompasses a broad spectrum of 20 tasks, including 15 classification tasks and 5 generation tasks. Furthermore, we evaluate eight State-Of-The-Art (SOTA) classification models (including two <b>foundation</b> <b>models)</b> and two generative models using our <b>benchmark.</b> The results show that <b>foundation</b> <b>models</b> significantly outperform the traditional deep learning methods in traffic classification. We believe NetBench will facilitate fair comparisons among various approaches and advance the development of <b>foundation</b> <b>models</b> for network traffic. Our <b>benchmark</b> is available at <a href=https://github.com/WM-JayLab/NetBench>https://github.com/WM-JayLab/NetBench</a>.</p></p class="citation"></blockquote><h3 id=55--260298-cooperative-jamming-for-physical-layer-security-enhancement-using-deep-reinforcement-learning-sayed-amir-hoseini-et-al-2024>(5/5 | 260/298) Cooperative Jamming for Physical Layer Security Enhancement Using Deep Reinforcement Learning (Sayed Amir Hoseini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayed Amir Hoseini, Faycal Bouhafs, Neda Aboutorab, Parastoo Sadeghi, Frank den Hartog. (2024)<br><strong>Cooperative Jamming for Physical Layer Security Enhancement Using Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Cooperative Jamming for Physical Layer Security Enhancement Using Deep Reinforcement Learning" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10342v1.pdf filename=2403.10342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wireless data communications are always facing the risk of eavesdropping and interception. Conventional protection solutions which are based on encryption may not always be practical as is the case for wireless IoT networks or may soon become ineffective against quantum computers. In this regard, Physical Layer Security (PLS) presents a promising approach to secure wireless communications through the exploitation of the physical properties of the wireless channel. Cooperative Friendly Jamming (CFJ) is among the PLS techniques that have received attention in recent years. However, finding an optimal transmit power allocation that results in the highest secrecy is a complex problem that becomes more difficult to address as the size of the wireless network increases. In this paper, we propose an optimization approach to achieve CFJ in large Wi-Fi networks by using a <b>Reinforcement</b> <b>Learning</b> Algorithm. Obtained results show that our optimization approach offers better secrecy results and becomes more effective as the network size and the density of Wi-Fi access points increase.</p></p class="citation"></blockquote><h2 id=eesssy-5>eess.SY (5)</h2><h3 id=15--261298-time-robust-path-planning-with-piece-wise-linear-trajectory-for-signal-temporal-logic-specifications-nhan-khanh-le-et-al-2024>(1/5 | 261/298) Time-Robust Path Planning with Piece-Wise Linear Trajectory for Signal Temporal Logic Specifications (Nhan-Khanh Le et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nhan-Khanh Le, Erfaun Noorani, Sandra Hirche, John Baras. (2024)<br><strong>Time-Robust Path Planning with Piece-Wise Linear Trajectory for Signal Temporal Logic Specifications</strong><br><button class=copy-to-clipboard title="Time-Robust Path Planning with Piece-Wise Linear Trajectory for Signal Temporal Logic Specifications" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10735v1.pdf filename=2403.10735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world scenarios are characterized by timing uncertainties, e.g., delays, and disturbances. Algorithms with temporal robustness are crucial in guaranteeing the successful execution of tasks and missions in such scenarios. We study time-robust path planning for synthesizing robots&rsquo; trajectories that adhere to spatial-temporal specifications expressed in Signal Temporal Logic (STL). In contrast to prior approaches that rely on {discretize}d trajectories with fixed time steps, we leverage Piece-Wise Linear (PWL) signals for the synthesis. PWL signals represent a trajectory through a sequence of time-stamped waypoints. This allows us to encode the STL formula into a Mixed-Integer Linear Program (MILP) with fewer variables. This reduction is more pronounced for specifications with a long planning horizon. To that end, we define time-robustness for PWL signals. Subsequently, we propose quantitative semantics for PWL signals according to the recursive syntax of STL and prove their soundness. We then propose an encoding strategy to transform our semantics into a MILP. Our <b>simulations</b> showcase the soundness and the performance of our algorithm.</p></p class="citation"></blockquote><h3 id=25--262298-analysis-of-a-two-degree-of-freedom-beam-for-rotational-piezoelectric-energy-harvesting-xiang-yu-li-et-al-2024>(2/5 | 262/298) Analysis of a Two-degree-of-freedom Beam for Rotational Piezoelectric Energy Harvesting (Xiang-Yu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang-Yu Li, I-Chie Huang, Wei-Jiun Su. (2024)<br><strong>Analysis of a Two-degree-of-freedom Beam for Rotational Piezoelectric Energy Harvesting</strong><br><button class=copy-to-clipboard title="Analysis of a Two-degree-of-freedom Beam for Rotational Piezoelectric Energy Harvesting" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10269v1.pdf filename=2403.10269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces a two-degree-of-freedom piezoelectric energy harvester designed to harness rotational motion. The harvester is built using a cut-out beam which enables the first two resonant fre-quencies to be located close to each other. A distributed continuous model is developed and validated with experimental results. As the beam undergoes significant displacement due to rotational excitations, the geometric nonlinearity arising from longitudinal displacement is considered in the model. Com-pared to previous literature, the <b>simulation</b> results are much more precise for rotational energy harvest-ing. It is observed that as the rotating speed increases, the increased centrifugal force causes the first resonant frequency to rise while the second resonant frequency decreases. This study explores the po-tential to expand the bandwidth of the harvester using two types of nonlinear external force, namely mechanical stoppers and magnetic force. The results indicate that the proposed harvester can broaden the bandwidth by 1.17 Hz and 0.33 Hz at the first and second resonance frequencies, respectively, by using a stopper on the main beam.</p></p class="citation"></blockquote><h3 id=35--263298-leveraging-symmetries-in-gaits-for-reinforcement-learning-a-case-study-on-quadrupedal-gaits-jiayu-ding-et-al-2024>(3/5 | 263/298) Leveraging Symmetries in Gaits for Reinforcement Learning: A Case Study on Quadrupedal Gaits (Jiayu Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayu Ding, Xulin Chen, Garret E. Katz, Zhenyu Gan. (2024)<br><strong>Leveraging Symmetries in Gaits for Reinforcement Learning: A Case Study on Quadrupedal Gaits</strong><br><button class=copy-to-clipboard title="Leveraging Symmetries in Gaits for Reinforcement Learning: A Case Study on Quadrupedal Gaits" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10723v1.pdf filename=2403.10723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this research, we address the complex task of developing versatile and agile quadrupedal gaits for robotic platforms, a domain predominantly governed by model-based trajectory optimization methods. We propose an innovative, reference-free <b>reinforcement</b> <b>learning</b> framework that exploits the intrinsic symmetries of dynamic systems to synthesize a broad array of naturalistic quadrupedal locomotion patterns. By capitalizing on distinct symmetry characteristics - namely temporal, morphological, and time-reversal - our approach efficiently facilitates the generation and transition among diverse gaits such as pronking, bounding half-bounding and galloping, across a spectrum of velocities, circumventing the necessity for expert-generated trajectories or complex reward structures. Implemented on the Petoi Bittle robotic model, our methodology illustrates robust and adaptable gait generation capabilities, significantly broadening the scope for robotic mobility and speed adaptability. This contribution not only advances our comprehension of quadrupedal locomotion mechanisms but also underscores the pivotal role of symmetry in the development of scalable and effective robotic gait strategies. Our findings hold substantial implications for robotic design and control, potentially enhancing operational versatility and efficiency across a variety of deployment environments.</p></p class="citation"></blockquote><h3 id=45--264298-lyapunov-neural-network-with-region-of-attraction-search-zili-wang-et-al-2024>(4/5 | 264/298) Lyapunov Neural Network with Region of Attraction Search (Zili Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zili Wang, Sean B. Andersson, Roberto Tron. (2024)<br><strong>Lyapunov Neural Network with Region of Attraction Search</strong><br><button class=copy-to-clipboard title="Lyapunov Neural Network with Region of Attraction Search" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10621v1.pdf filename=2403.10621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning methods have been widely used in robotic applications, making learning-enabled control design for complex nonlinear systems a promising direction. Although deep <b>reinforcement</b> <b>learning</b> methods have demonstrated impressive empirical performance, they lack the stability guarantees that are important in safety-critical situations. One way to provide these guarantees is to learn Lyapunov certificates alongside control policies. There are three related problems: 1) verify that a given Lyapunov function candidate satisfies the conditions for a given controller on a region, 2) find a valid Lyapunov function and controller on a given region, and 3) find a valid Lyapunov function and a controller such that the region of attraction is as large as possible. Previous work has shown that if the dynamics are piecewise linear, it is possible to solve problems 1) and 2) by solving a Mixed-Integer Linear Program (MILP). In this work, we build upon this method by proposing a Lyapunov neural network that considers monotonicity over half spaces in different directions. We 1) propose a specific choice of Lyapunov function architecture that ensures non-negativity and a unique global minimum by construction, and 2) show that this can be leveraged to find the controller and Lyapunov certificates faster and with a larger valid region by maximizing the size of a square inscribed in a given level set. We apply our method to a 2D inverted pendulum, unicycle path following, a 3-D feedback system, and a 4-D cart pole system, and demonstrate it can shorten the training time by half compared to the baseline, as well as find a larger ROA.</p></p class="citation"></blockquote><h3 id=55--265298-data-driven-distributionally-robust-safety-verification-using-barrier-certificates-and-conditional-mean-embeddings-oliver-schön-et-al-2024>(5/5 | 265/298) Data-Driven Distributionally Robust Safety Verification Using Barrier Certificates and Conditional Mean Embeddings (Oliver Schön et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oliver Schön, Zhengang Zhong, Sadegh Soudjani. (2024)<br><strong>Data-Driven Distributionally Robust Safety Verification Using Barrier Certificates and Conditional Mean Embeddings</strong><br><button class=copy-to-clipboard title="Data-Driven Distributionally Robust Safety Verification Using Barrier Certificates and Conditional Mean Embeddings" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10497v1.pdf filename=2403.10497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Algorithmic verification of realistic systems to satisfy safety and other temporal requirements has suffered from poor scalability of the employed formal approaches. To design systems with rigorous guarantees, many approaches still rely on exact models of the underlying systems. Since this assumption can rarely be met in practice, models have to be inferred from measurement data or are bypassed completely. Whilst former usually requires the model structure to be known a-priori and immense amounts of data to be available, latter gives rise to a plethora of restrictive mathematical assumptions about the unknown dynamics. In a pursuit of developing scalable formal verification algorithms without shifting the problem to unrealistic assumptions, we employ the concept of barrier certificates, which can guarantee safety of the system, and learn the certificate directly from a compact set of system trajectories. We use conditional mean embeddings to embed data from the system into a reproducing kernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can be inflated to robustify the result w.r.t. a set of plausible transition kernels. We show how to solve the resulting program efficiently using sum-of-squares optimization and a <b>Gaussian</b> <b>process</b> envelope. Our approach lifts the need for restrictive assumptions on the system dynamics and uncertainty, and suggests an improvement in the sample complexity of verifying the safety of a system on a tested case study compared to a state-of-the-art approach.</p></p class="citation"></blockquote><h2 id=mathna-5>math.NA (5)</h2><h3 id=15--266298-computational-study-on-the-impact-of-gasoline-ethanol-blending-on-autoignition-and-sootnox-emissions-under-gasoline-compression-ignition-conditions-krishna-c-kalvakala-et-al-2024>(1/5 | 266/298) Computational Study on the Impact of Gasoline-Ethanol Blending on Autoignition and Soot/NOx Emissions under Gasoline Compression Ignition Conditions (Krishna C. Kalvakala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Krishna C. Kalvakala, Harsimran Singh, Pinaki Pal, Jorge P. Gonzalez, Christopher P. Kolodziej, Suresh K. Aggarwal. (2024)<br><strong>Computational Study on the Impact of Gasoline-Ethanol Blending on Autoignition and Soot/NOx Emissions under Gasoline Compression Ignition Conditions</strong><br><button class=copy-to-clipboard title="Computational Study on the Impact of Gasoline-Ethanol Blending on Autoignition and Soot/NOx Emissions under Gasoline Compression Ignition Conditions" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, physics-flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10687v1.pdf filename=2403.10687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computational fluid dynamics (CFD) <b>simulations</b> of a single-cylinder gasoline compression ignition engine are performed to investigate the impact of gasoline-ethanol blending on autoignition, nitrogen oxide (NOx), and soot emissions under low-load conditions. A four-component toluene primary reference fuel (TPRF) + ethanol (ETPRF) surrogate (with 10% ethanol by volume; E10) is employed to represent the test gasoline (RD5-87). A 3D engine CFD model employing finite-rate chemistry with a skeletal kinetic mechanism, adaptive mesh refinement (AMR), and hybrid method of moments (HMOM) is adopted to capture in-cylinder combustion and soot/NOx emissions. The engine CFD model is validated against experimental data for three gasoline-ethanol blends: E10, E30 and E100, with varying ethanol content by volume. Model validation is carried out for multiple start-of-injection (SOI) timings (-21, -27, -36, and -45 crank angle degrees after top-dead-center (aTDC)) with respect to in-cylinder pressure, heat release rate, combustion phasing, NOx and soot emissions. For late injection timings (-21 and -27oaTDC), E30 yields higher soot than E10; while the trend reverses for early injection cases (-36 and -45oaTDC). E100 yields the lowest amount of soot among all fuels irrespective of SOI timing. Further, E10 shows a non-monotonic trend in soot emissions with SOI timing: SOI-36>SOI-45>SOI-21>SOI-27, while soot emissions from E30 exhibit monotonic decrease with advancing SOI timing. NOx emissions from various fuels follow a trend of E10>E30>E100. NOx emissions increase as SOI timing is advanced for all fuels, with an anomaly for E10 and E100 where NOx decreases when SOI is advanced beyond -36oaTDC. Detailed analysis of the numerical results is performed to investigate the emission trends and elucidate the impact of chemical composition and physical properties on autoignition and emissions characteristics.</p></p class="citation"></blockquote><h3 id=25--267298-hessian-free-force-gradient-integrators-kevin-schäfers-et-al-2024>(2/5 | 267/298) Hessian-free force-gradient integrators (Kevin Schäfers et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin Schäfers, Jacob Finkenrath, Michael Günther, Francesco Knechtli. (2024)<br><strong>Hessian-free force-gradient integrators</strong><br><button class=copy-to-clipboard title="Hessian-free force-gradient integrators" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 81V05, 65P10, 65L05, 65L20 37N20, cs-NA, hep-lat, math-MP, math-NA, math-ph, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10370v1.pdf filename=2403.10370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a new framework of Hessian-free force-gradient integrators that do not require the analytical expression of the force-gradient term based on the Hessian of the potential. Due to that the new class of decomposition algorithms for separable Hamiltonian systems with quadratic kinetic energy may be particularly useful when applied to Hamiltonian systems where an evaluation of the Hessian is significantly more expensive than an evaluation of its gradient, e.g. in molecular dynamics <b>simulations</b> of classical systems. Numerical experiments of an N-body problem, as well as applications to the molecular dynamics step in the Hybrid Monte Carlo (HMC) algorithm for lattice <b>simulations</b> of the Schwinger model and Quantum Chromodynamics (QCD) verify these expectations.</p></p class="citation"></blockquote><h3 id=35--268298-boundary-parameter-matching-for-isogeometric-analysis-using-schwarz-christoffel-mapping-ye-ji-et-al-2024>(3/5 | 268/298) Boundary parameter matching for isogeometric analysis using Schwarz-Christoffel mapping (Ye Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Ji, Matthias Möller, Yingying Yu, Chungang Zhu. (2024)<br><strong>Boundary parameter matching for isogeometric analysis using Schwarz-Christoffel mapping</strong><br><button class=copy-to-clipboard title="Boundary parameter matching for isogeometric analysis using Schwarz-Christoffel mapping" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65D17, 65D07, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10284v1.pdf filename=2403.10284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Isogeometric analysis has brought a paradigm shift in integrating computational <b>simulations</b> with geometric designs across engineering disciplines. This technique necessitates analysis-suitable parameterization of physical domains to fully harness the synergy between Computer-Aided Design and Computer-Aided Engineering analyses. The existing methods often fix boundary parameters, leading to challenges in elongated geometries such as fluid channels and tubular reactors. This paper presents an innovative solution for the boundary parameter matching problem, specifically designed for analysis-suitable parameterizations. We employ a sophisticated Schwarz-Christoffel mapping technique, which is instrumental in computing boundary correspondences. A refined boundary curve reparameterization process complements this. Our dual-strategy approach maintains the geometric exactness and continuity of input physical domains, overcoming limitations often encountered with the existing reparameterization techniques. By employing our proposed boundary parameter method, we show that even a simple linear interpolation approach can effectively construct a satisfactory analysis-suitable parameterization. Our methodology offers significant improvements over traditional practices, enabling the generation of analysis-suitable and geometrically precise models, which is crucial for ensuring accurate <b>simulation</b> results. Numerical experiments show the capacity of the proposed method to enhance the quality and reliability of isogeometric analysis workflows.</p></p class="citation"></blockquote><h3 id=45--269298-effective-polygonal-mesh-generation-and-refinement-for-vem-stefano-berrone-et-al-2024>(4/5 | 269/298) Effective polygonal mesh generation and refinement for VEM (Stefano Berrone et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefano Berrone, Fabio Vicini. (2024)<br><strong>Effective polygonal mesh generation and refinement for VEM</strong><br><button class=copy-to-clipboard title="Effective polygonal mesh generation and refinement for VEM" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N30, 65N50, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10203v1.pdf filename=2403.10203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the present work we introduce a novel refinement algorithm for two-dimensional elliptic partial differential equations discretized with Virtual Element Method (VEM). The algorithm improves the numerical solution accuracy and the mesh quality through a controlled refinement strategy applied to the generic polygonal elements of the domain tessellation. The numerical results show that the outlined strategy proves to be versatile and applicable to any two-dimensional problem where polygonal meshes offer advantages. In particular, we focus on the <b>simulation</b> of flow in fractured media, specifically using the Discrete Fracture Network (DFN) model. A residual a-posteriori error estimator tailored for the DFN case is employed. We chose this particular application to emphasize the effectiveness of the algorithm in handling complex geometries. All the numerical tests demonstrate optimal convergence rates for all the tested VEM orders.</p></p class="citation"></blockquote><h3 id=55--270298-optimal-control-of-stationary-doubly-diffusive-flows-on-two-and-three-dimensional-bounded-lipschitz-domains-numerical-analysis-jai-tushar-et-al-2024>(5/5 | 270/298) Optimal Control of Stationary Doubly Diffusive Flows on Two and Three Dimensional Bounded Lipschitz Domains: Numerical Analysis (Jai Tushar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jai Tushar, Arbaz Khan, Manil T. Mohan. (2024)<br><strong>Optimal Control of Stationary Doubly Diffusive Flows on Two and Three Dimensional Bounded Lipschitz Domains: Numerical Analysis</strong><br><button class=copy-to-clipboard title="Optimal Control of Stationary Doubly Diffusive Flows on Two and Three Dimensional Bounded Lipschitz Domains: Numerical Analysis" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N30, 65N50, 74F99, 74A50, 76S05, cs-NA, math-NA, math-OC, math.NA<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10282v1.pdf filename=2403.10282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose fully nonconforming, locally exactly divergence-free discretizations based on lowest order Crouziex-Raviart finite element and piecewise constant spaces to study the optimal control of stationary double <b>diffusion</b> <b>model</b> presented in [B"urger, M'endez, Ruiz-Baier, SINUM (2019), 57:1318-1343]. The well-posedness of the discrete uncontrolled state and adjoint equations are discussed using discrete lifting and fixed point arguments, and convergence results are derived rigorously under minimal regularity. Building upon our recent work [Tushar, Khan, Mohan arXiv (2023)], we prove the local optimality of a reference control using second-order sufficient optimality condition for the control problem, and use it along with an optimize-then-discretize approach to prove optimal order a priori error estimates for the control, state and adjoint variables upto the regularity of the solution. The optimal control is computed using a primal-dual active set strategy as a semi-smooth Newton method and computational tests validate the predicted error decay rates and illustrate the proposed scheme&rsquo;s applicability to optimal control of thermohaline circulation problems.</p></p class="citation"></blockquote><h2 id=cssd-3>cs.SD (3)</h2><h3 id=13--271298-musichifi-fast-high-fidelity-stereo-vocoding-ge-zhu-et-al-2024>(1/3 | 271/298) MusicHiFi: Fast High-Fidelity Stereo Vocoding (Ge Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ge Zhu, Juan-Pablo Caceres, Zhiyao Duan, Nicholas J. Bryan. (2024)<br><strong>MusicHiFi: Fast High-Fidelity Stereo Vocoding</strong><br><button class=copy-to-clipboard title="MusicHiFi: Fast High-Fidelity Stereo Vocoding" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS, eess-SP<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10493v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10493v2.pdf filename=2403.10493v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion-based audio and music generation models commonly generate music by constructing an image representation of audio (e.g., a mel-spectrogram) and then converting it to audio using a phase reconstruction model or vocoder. Typical vocoders, however, produce monophonic audio at lower resolutions (e.g., 16-24 kHz), which limits their effectiveness. We propose MusicHiFi &ndash; an efficient high-fidelity stereophonic vocoder. Our method employs a cascade of three <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs)</b> that convert low-resolution mel-spectrograms to audio, upsamples to high-resolution audio via bandwidth expansion, and upmixes to stereophonic audio. Compared to previous work, we propose 1) a unified <b>GAN-based</b> generator and discriminator architecture and training procedure for each stage of our cascade, 2) a new fast, near downsampling-compatible bandwidth extension module, and 3) a new fast downmix-compatible mono-to-stereo upmixer that ensures the preservation of monophonic content in the output. We evaluate our approach using both objective and subjective listening tests and find our approach yields comparable or better audio quality, better spatialization control, and significantly faster inference speed compared to past work. Sound examples are at <a href=https://MusicHiFi.github.io/web/>https://MusicHiFi.github.io/web/</a>.</p></p class="citation"></blockquote><h3 id=23--272298-multiscale-matching-driven-by-cross-modal-similarity-consistency-for-audio-text-retrieval-qian-wang-et-al-2024>(2/3 | 272/298) Multiscale Matching Driven by Cross-Modal Similarity Consistency for Audio-Text Retrieval (Qian Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Wang, Jia-Chen Gu, Zhen-Hua Ling. (2024)<br><strong>Multiscale Matching Driven by Cross-Modal Similarity Consistency for Audio-Text Retrieval</strong><br><button class=copy-to-clipboard title="Multiscale Matching Driven by Cross-Modal Similarity Consistency for Audio-Text Retrieval" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-IR, cs-SD, cs.SD, eess-AS<br>Keyword Score: 16<br>Keywords: Contrastive Learning, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10146v1.pdf filename=2403.10146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Audio-text retrieval (ATR), which retrieves a relevant caption given an audio clip (A2T) and vice versa (T2A), has recently attracted much research attention. Existing methods typically aggregate information from each modality into a single vector for matching, but this sacrifices local details and can hardly capture intricate relationships within and between modalities. Furthermore, current ATR datasets lack comprehensive alignment information, and simple binary <b>contrastive</b> <b>learning</b> labels overlook the measurement of fine-grained semantic differences between samples. To counter these challenges, we present a novel ATR framework that comprehensively captures the matching relationships of <b>multimodal</b> information from different perspectives and finer granularities. Specifically, a fine-grained alignment method is introduced, achieving a more detail-oriented matching through a multiscale process from local to global levels to capture meticulous cross-modal relationships. In addition, we pioneer the application of cross-modal similarity consistency, leveraging intra-modal similarity relationships as soft supervision to boost more intricate alignment. Extensive experiments validate the effectiveness of our approach, outperforming previous methods by significant margins of at least 3.9% (T2A) / 6.9% (A2T) R@1 on the AudioCaps dataset and 2.9% (T2A) / 5.4% (A2T) R@1 on the Clotho dataset.</p></p class="citation"></blockquote><h3 id=33--273298-birdset-a-multi-task-benchmark-for-classification-in-avian-bioacoustics-lukas-rauch-et-al-2024>(3/3 | 273/298) BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics (Lukas Rauch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas Rauch, Raphael Schwinger, Moritz Wirth, René Heinrich, Jonas Lange, Stefan Kahl, Bernhard Sick, Sven Tomforde, Christoph Scholz. (2024)<br><strong>BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics</strong><br><button class=copy-to-clipboard title="BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10380v1.pdf filename=2403.10380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning (DL) models have emerged as a powerful tool in avian bioacoustics to diagnose environmental health and biodiversity. However, inconsistencies in research pose notable challenges hindering progress in this domain. Reliable DL models need to analyze bird calls flexibly across various species and environments to fully harness the potential of bioacoustics in a cost-effective passive acoustic monitoring scenario. Data fragmentation and opacity across studies complicate a comprehensive evaluation of general model performance. To overcome these challenges, we present the BirdSet <b>benchmark,</b> a unified framework consolidating research efforts with a holistic approach for classifying bird vocalizations in avian bioacoustics. BirdSet harmonizes open-source bird recordings into a curated dataset collection. This unified approach provides an in-depth understanding of model performance and identifies potential shortcomings across different tasks. By establishing baseline results of current models, BirdSet aims to facilitate comparability, guide subsequent data collection, and increase accessibility for newcomers to avian bioacoustics.</p></p class="citation"></blockquote><h2 id=physicschem-ph-1>physics.chem-ph (1)</h2><h3 id=11--274298-gradnav-accelerated-exploration-of-potential-energy-surfaces-with-gradient-based-navigation-janghoon-ock-et-al-2024>(1/1 | 274/298) GradNav: Accelerated Exploration of Potential Energy Surfaces with Gradient-Based Navigation (Janghoon Ock et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Janghoon Ock, Parisa Mollaei, Amir Barati Farimani. (2024)<br><strong>GradNav: Accelerated Exploration of Potential Energy Surfaces with Gradient-Based Navigation</strong><br><button class=copy-to-clipboard title="GradNav: Accelerated Exploration of Potential Energy Surfaces with Gradient-Based Navigation" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.chem-ph<br>Categories: cs-CE, physics-chem-ph, physics.chem-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10358v1.pdf filename=2403.10358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The exploration of molecular systems&rsquo; potential energy surface is important for comprehending their complex behaviors, particularly through identifying various metastable states. However, the transition between these states is often hindered by substantial energy barriers, demanding prolonged molecular <b>simulations</b> that consume considerable computational efforts. Our study introduces the GradNav algorithm, which enhances the exploration of the energy surface, accelerating the reconstruction of the potential energy surface (PES). This algorithm employs a strategy of initiating short <b>simulation</b> runs from updated starting points, derived from prior observations, to effectively navigate across potential barriers and explore new regions. To evaluate GradNav&rsquo;s performance, we introduce two metrics: the deepest well escape frame (DWEF) and the search success initialization ratio (SSIR). Through applications on Langevin dynamics within Mueller-type potential energy surfaces and molecular dynamics <b>simulations</b> of the Fs-Peptide protein, these metrics demonstrate GradNav&rsquo;s enhanced ability to escape deep energy wells, as shown by reduced DWEF values, and its reduced reliance on initial conditions, highlighted by increased SSIR values. Consequently, this improved exploration capability enables more precise energy estimations from <b>simulation</b> trajectories.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--275298-rough-transformers-for-continuous-and-efficient-time-series-modelling-fernando-moreno-pino-et-al-2024>(1/3 | 275/298) Rough Transformers for Continuous and Efficient Time-Series Modelling (Fernando Moreno-Pino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fernando Moreno-Pino, Álvaro Arroyo, Harrison Waldon, Xiaowen Dong, Álvaro Cartea. (2024)<br><strong>Rough Transformers for Continuous and Efficient Time-Series Modelling</strong><br><button class=copy-to-clipboard title="Rough Transformers for Continuous and Efficient Time-Series Modelling" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-AI, cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Continuous Time, Continuous Time, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10288v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10288v1.pdf filename=2403.10288v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time-series data in real-world medical settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In such contexts, traditional sequence-based recurrent models struggle. To overcome this, researchers replace recurrent architectures with Neural ODE-based models to model irregularly sampled data and use <b>Transformer-based</b> architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of moderate lengths and greater. To mitigate this, we introduce the Rough <b>Transformer,</b> a variation of the <b>Transformer</b> model which operates on <b>continuous-time</b> <b>representations</b> of input sequences and incurs significantly reduced computational costs, critical for addressing long-range dependencies common in medical contexts. In particular, we propose multi-view signature attention, which uses path signatures to augment vanilla attention and to capture both local and global dependencies in input data, while remaining robust to changes in the sequence length and sampling frequency. We find that Rough <b>Transformers</b> consistently outperform their vanilla attention counterparts while obtaining the benefits of Neural ODE-based models using a fraction of the computational time and memory resources on synthetic and real-world time-series tasks.</p></p class="citation"></blockquote><h3 id=23--276298-generative-modelling-of-stochastic-rotating-shallow-water-noise-dan-crisan-et-al-2024>(2/3 | 276/298) Generative Modelling of Stochastic Rotating Shallow Water Noise (Dan Crisan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dan Crisan, Oana Lang, Alexander Lobbe. (2024)<br><strong>Generative Modelling of Stochastic Rotating Shallow Water Noise</strong><br><button class=copy-to-clipboard title="Generative Modelling of Stochastic Rotating Shallow Water Noise" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: 68T05, 76M35, cs-LG, cs-NA, math-DS, math-NA, physics-flu-dyn, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10578v1.pdf filename=2403.10578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent work, the authors have developed a generic methodology for calibrating the noise in fluid dynamics stochastic partial differential equations where the stochasticity was introduced to parametrize subgrid-scale processes. The stochastic parameterization of sub-grid scale processes is required in the estimation of uncertainty in weather and climate predictions, to represent systematic model errors arising from subgrid-scale fluctuations. The previous methodology used a principal component analysis (PCA) technique based on the ansatz that the increments of the stochastic parametrization are normally distributed. In this paper, the PCA technique is replaced by a generative model technique. This enables us to avoid imposing additional constraints on the increments. The methodology is tested on a stochastic rotating shallow water model with the elevation variable of the model used as input data. The numerical <b>simulations</b> show that the noise is indeed non-Gaussian. The generative modelling technology gives good RMSE, CRPS score and forecast rank histogram results.</p></p class="citation"></blockquote><h3 id=33--277298-interpretable-machine-learning-for-survival-analysis-sophie-hanna-langbein-et-al-2024>(3/3 | 277/298) Interpretable Machine Learning for Survival Analysis (Sophie Hanna Langbein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sophie Hanna Langbein, Mateusz Krzyziński, Mikołaj Spytek, Hubert Baniecki, Przemysław Biecek, Marvin N. Wright. (2024)<br><strong>Interpretable Machine Learning for Survival Analysis</strong><br><button class=copy-to-clipboard title="Interpretable Machine Learning for Survival Analysis" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 15<br>Keywords: Black Box, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10250v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10250v1.pdf filename=2403.10250v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the spread and rapid advancement of <b>black</b> <b>box</b> machine learning models, the field of interpretable machine learning (IML) or explainable artificial intelligence (XAI) has become increasingly important over the last decade. This is particularly relevant for survival analysis, where the adoption of IML techniques promotes transparency, accountability and <b>fairness</b> in sensitive areas, such as clinical decision making processes, the development of targeted therapies, interventions or in other medical or healthcare related contexts. More specifically, explainability can uncover a survival model&rsquo;s potential biases and limitations and provide more mathematically sound ways to understand how and which features are influential for prediction or constitute risk factors. However, the lack of readily available IML methods may have deterred medical practitioners and policy makers in public health from leveraging the full potential of machine learning for predicting time-to-event data. We present a comprehensive review of the limited existing amount of work on IML methods for survival analysis within the context of the general IML taxonomy. In addition, we formally detail how commonly used IML methods, such as such as individual conditional expectation (ICE), partial dependence plots (PDP), accumulated local effects (ALE), different feature importance measures or Friedman&rsquo;s H-interaction statistics can be adapted to survival outcomes. An application of several IML methods to real data on data on under-5 year mortality of Ghanaian children from the Demographic and Health Surveys (DHS) Program serves as a tutorial or guide for researchers, on how to utilize the techniques in practice to facilitate understanding of model decisions or predictions.</p></p class="citation"></blockquote><h2 id=quant-ph-2>quant-ph (2)</h2><h3 id=12--278298-quantum-circuits-for-partial-differential-equations-via-schrödingerisation-junpeng-hu-et-al-2024>(1/2 | 278/298) Quantum Circuits for partial differential equations via Schrödingerisation (Junpeng Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junpeng Hu, Shi Jin, Nana Liu, Lei Zhang. (2024)<br><strong>Quantum Circuits for partial differential equations via Schrödingerisation</strong><br><button class=copy-to-clipboard title="Quantum Circuits for partial differential equations via Schrödingerisation" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-NA, math-NA, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10032v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10032v1.pdf filename=2403.10032v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum computing has emerged as a promising avenue for achieving significant speedup, particularly in large-scale PDE <b>simulations,</b> compared to classical computing. One of the main quantum approaches involves utilizing Hamiltonian <b>simulation,</b> which is directly applicable only to Schr"odinger-type equations. To address this limitation, Schr"odingerisation techniques have been developed, employing the warped transformation to convert general linear PDEs into Schr"odinger-type equations. However, despite the development of Schr"odingerisation techniques, the explicit implementation of the corresponding quantum circuit for solving general PDEs remains to be designed. In this paper, we present detailed implementation of a quantum algorithm for general PDEs using Schr"odingerisation techniques. We provide examples of the heat equation, and the advection equation approximated by the upwind scheme, to demonstrate the effectiveness of our approach. Complexity analysis is also carried out to demonstrate the quantum advantages of these algorithms in high dimensions over their classical counterparts.</p></p class="citation"></blockquote><h3 id=22--279298-evaluation-of-quantum-and-hybrid-solvers-for-combinatorial-optimization-amedeo-bertuzzi-et-al-2024>(2/2 | 279/298) Evaluation of Quantum and Hybrid Solvers for Combinatorial Optimization (Amedeo Bertuzzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amedeo Bertuzzi, Davide Ferrari, Antonio Manzalini, Michele Amoretti. (2024)<br><strong>Evaluation of Quantum and Hybrid Solvers for Combinatorial Optimization</strong><br><button class=copy-to-clipboard title="Evaluation of Quantum and Hybrid Solvers for Combinatorial Optimization" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-DC, cs-PF, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10455v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10455v1.pdf filename=2403.10455v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Academic and industrial sectors have been engaged in a fierce competition to develop quantum technologies, fueled by the explosive advancements in quantum hardware. While universal quantum computers have been shown to support up to hundreds of qubits, the scale of quantum annealers has reached three orders of magnitude (i.e., thousands of qubits). Therefore, quantum algorithms are becoming increasingly popular in a variety of fields, with optimization being one of the most prominent. This work aims to explore the topic of quantum optimization by comprehensively evaluating the technologies provided by D-Wave Systems. To do so, a model for the energy optimization of data centers is proposed as a <b>benchmark.</b> D-Wave quantum and hybrid solvers are compared, in order to identify the most suitable one for the considered application. To highlight its advantageous performance capabilities and associated solving potential, the selected D-Wave hybrid solver is then contrasted with CPLEX, a highly efficient classical solver.</p></p class="citation"></blockquote><h2 id=csgt-3>cs.GT (3)</h2><h3 id=13--280298-ddps-dynamic-differential-pricing-based-edge-offloading-system-with-energy-harvesting-devices-hai-xue-et-al-2024>(1/3 | 280/298) DDPS: Dynamic Differential Pricing-based Edge Offloading System with Energy Harvesting Devices (Hai Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hai Xue, Yun Xia, Neal N. Xiong, Di Zhang, Songwen Pei. (2024)<br><strong>DDPS: Dynamic Differential Pricing-based Edge Offloading System with Energy Harvesting Devices</strong><br><button class=copy-to-clipboard title="DDPS: Dynamic Differential Pricing-based Edge Offloading System with Energy Harvesting Devices" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09991v1.pdf filename=2403.09991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mobile edge computing (MEC) paves the way to alleviate the burden of energy and computation of mobile users (MUs) by offloading tasks to the network edge. To enhance the MEC server utilization by optimizing its resource allocation, a well-designed pricing strategy is indispensable. In this paper, we consider the edge offloading scenario with energy harvesting devices, and propose a dynamic differential pricing system (DDPS), which determines the price per unit time according to the usage of computing resources to improve the edge server utilization. Firstly, we propose an offloading decision algorithm to decide whether to conduct the offloading operation and how much data to be offloaded if conducted, the algorithm determines offloading operation by balancing the energy harvested with the energy consumed. Secondly, for the offloading case, we formulate the game between the MUs and the server as a Stackelberg game, and propose a differential pricing algorithm to determine the optimal computing resources required by MUs. Furthermore, the proposed algorithm also reallocates computing resources for delay-sensitive devices while server resources are surplus after the initial allocation, aiming to make full use of the server computing resources. Extensive <b>simulations</b> are conducted to demonstrate the effectiveness of the proposed DDPS scheme.</p></p class="citation"></blockquote><h3 id=23--281298-coordination-in-noncooperative-multiplayer-matrix-games-via-reduced-rank-correlated-equilibria-jaehan-im-et-al-2024>(2/3 | 281/298) Coordination in Noncooperative Multiplayer Matrix Games via Reduced Rank Correlated Equilibria (Jaehan Im et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaehan Im, Yue Yu, David Fridovich-Keil, Ufuk Topcu. (2024)<br><strong>Coordination in Noncooperative Multiplayer Matrix Games via Reduced Rank Correlated Equilibria</strong><br><button class=copy-to-clipboard title="Coordination in Noncooperative Multiplayer Matrix Games via Reduced Rank Correlated Equilibria" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-MA, cs-SY, cs.GT, eess-SY<br>Keyword Score: 13<br>Keywords: Benchmarking, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10384v1.pdf filename=2403.10384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Coordination in multiplayer games enables players to avoid the lose-lose outcome that often arises at Nash equilibria. However, designing a coordination mechanism typically requires the consideration of the joint actions of all players, which becomes intractable in large-scale games. We develop a novel coordination mechanism, termed reduced rank correlated equilibria, which reduces the number of joint actions to be considered and thereby mitigates computational complexity. The idea is to approximate the set of all joint actions with the actions used in a set of pre-computed Nash equilibria via a convex hull operation. In a game with n players and each player having m actions, the proposed mechanism reduces the number of joint actions considered from O(m^n) to O(mn). We demonstrate the application of the proposed mechanism to an air traffic queue management problem. Compared with the correlated equilibrium-a popular <b>benchmark</b> coordination mechanism-the proposed approach is capable of solving a queue management problem involving four thousand times more joint actions. In the meantime, it yields a solution that shows a 58.5% to 99.5% improvement in the <b>fairness</b> indicator and a 1.8% to 50.4% reduction in average delay cost compared to the Nash solution, which does not involve coordination.</p></p class="citation"></blockquote><h3 id=33--282298-scaling-game-theoretic-security-reasoning-sophie-rain-et-al-2024>(3/3 | 282/298) Scaling Game-Theoretic Security Reasoning (Sophie Rain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sophie Rain, Lea Salome Brugger, Anja Petkovic Komel, Laura Kovacs, Michael Rawson. (2024)<br><strong>Scaling Game-Theoretic Security Reasoning</strong><br><button class=copy-to-clipboard title="Scaling Game-Theoretic Security Reasoning" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-LO, cs.GT<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10310v1.pdf filename=2403.10310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the CheckMate tool for automated verification of game-theoretic security properties, with application to blockchain protocols. CheckMate applies automated <b>reasoning</b> techniques to determine whether a game-theoretic protocol model is game-theoretically secure, that is, Byzantine fault tolerant and incentive compatible. We describe CheckMate&rsquo;s input format and its various components, modes, and output. CheckMate is evaluated on 14 <b>benchmarks,</b> including models of decentralized protocols, board games, and game-theoretic examples.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--283298-complete-equational-theories-for-classical-and-quantum-gaussian-relations-robert-i-booth-et-al-2024>(1/1 | 283/298) Complete equational theories for classical and quantum Gaussian relations (Robert I. Booth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert I. Booth, Titouan Carette, Cole Comfort. (2024)<br><strong>Complete equational theories for classical and quantum Gaussian relations</strong><br><button class=copy-to-clipboard title="Complete equational theories for classical and quantum Gaussian relations" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO, math-CT, quant-ph<br>Keyword Score: 13<br>Keywords: Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10479v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10479v2.pdf filename=2403.10479v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We give generators and relations for the hypergraph props of Gaussian relations and positive affine Lagrangian relations. The former extends Gaussian probabilistic processes by completely-uninformative priors, and the latter extends Gaussian quantum mechanics with infinitely-squeezed states. These presentations are given by adding a generator to the presentation of real affine relations and of real affine Lagrangian relations which freely codiscards effects, as well as certain rotations. The presentation of positive affine Lagrangian relations provides a rigorous justification for many common yet informal calculations in the quantum physics literature involving infinite-squeezing. Our presentation naturally extends Menicucci et al.&rsquo;s <b>graph-theoretic</b> representation of Gaussian quantum states with a representation for Gaussian transformations. Using this graphical calculus, we also give a graphical proof of Braunstein and Kimble&rsquo;s continuous-variable quantum teleportation protocol. We also interpret the LOv-calculus, a diagrammatic calculus for <b>reasoning</b> about passive linear-optical quantum circuits in our graphical calculus. Moreover, we show how our presentation allows for additional optical operations such as active squeezing.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--284298-quantization-avoids-saddle-points-in-distributed-optimization-yanan-bo-et-al-2024>(1/1 | 284/298) Quantization Avoids Saddle Points in Distributed Optimization (Yanan Bo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanan Bo, Yongqiang Wang. (2024)<br><strong>Quantization Avoids Saddle Points in Distributed Optimization</strong><br><button class=copy-to-clipboard title="Quantization Avoids Saddle Points in Distributed Optimization" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 13<br>Keywords: Benchmarking, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10423v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10423v1.pdf filename=2403.10423v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed nonconvex optimization underpins key functionalities of numerous distributed systems, ranging from power systems, smart buildings, cooperative robots, vehicle networks to sensor networks. Recently, it has also merged as a promising solution to handle the enormous growth in data and model sizes in deep learning. A fundamental problem in distributed nonconvex optimization is avoiding convergence to saddle points, which significantly degrade optimization accuracy. We discover that the process of <b>quantization,</b> which is necessary for all digital communications, can be exploited to enable saddle-point avoidance. More specifically, we propose a stochastic <b>quantization</b> scheme and prove that it can effectively escape saddle points and ensure convergence to a second-order stationary point in distributed nonconvex optimization. With an easily adjustable <b>quantization</b> granularity, the approach allows a user to control the number of bits sent per iteration and, hence, to aggressively reduce the communication overhead. Numerical experimental results using distributed optimization and learning problems on <b>benchmark</b> datasets confirm the effectiveness of the approach.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--285298-scalable-algorithms-for-individual-preference-stable-clustering-ron-mosenzon-et-al-2024>(1/2 | 285/298) Scalable Algorithms for Individual Preference Stable Clustering (Ron Mosenzon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ron Mosenzon, Ali Vakilian. (2024)<br><strong>Scalable Algorithms for Individual Preference Stable Clustering</strong><br><button class=copy-to-clipboard title="Scalable Algorithms for Individual Preference Stable Clustering" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-AI, cs-CY, cs-DS, cs-LG, cs.DS<br>Keyword Score: 13<br>Keywords: Clustering, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10365v1.pdf filename=2403.10365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study the individual preference (IP) stability, which is an notion capturing individual <b>fairness</b> and stability in <b>clustering.</b> Within this setting, a <b>clustering</b> is $\alpha$-IP stable when each data point&rsquo;s average distance to its cluster is no more than $\alpha$ times its average distance to any other cluster. In this paper, we study the natural local search algorithm for IP stable <b>clustering.</b> Our analysis confirms a $O(\log n)$-IP stability guarantee for this algorithm, where $n$ denotes the number of points in the input. Furthermore, by refining the local search approach, we show it runs in an almost linear time, $\tilde{O}(nk)$.</p></p class="citation"></blockquote><h3 id=22--286298-first-passage-percolation-with-queried-hints-kritkorn-karntikoon-et-al-2024>(2/2 | 286/298) First Passage Percolation with Queried Hints (Kritkorn Karntikoon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kritkorn Karntikoon, Yiheng Shen, Sreenivas Gollapudi, Kostas Kollias, Aaron Schild, Ali Sinop. (2024)<br><strong>First Passage Percolation with Queried Hints</strong><br><button class=copy-to-clipboard title="First Passage Percolation with Queried Hints" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10640v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10640v1.pdf filename=2403.10640v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Solving optimization problems leads to elegant and practical solutions in a wide variety of real-world applications. In many of those real-world applications, some of the information required to specify the relevant optimization problem is noisy, uncertain, and expensive to obtain. In this work, we study how much of that information needs to be queried in order to obtain an approximately optimal solution to the relevant problem. In particular, we focus on the shortest path problem in <b>graphs</b> with dynamic edge costs. We adopt the $\textit{first passage percolation}$ model from probability theory wherein a <b>graph</b> $G&rsquo;$ is derived from a weighted base <b>graph</b> $G$ by multiplying each edge weight by an independently chosen random number in $[1, \rho]$. Mathematicians have studied this model extensively when $G$ is a $d$-dimensional grid <b>graph,</b> but the behavior of shortest paths in this model is still poorly understood in general <b>graphs.</b> We make progress in this direction for a class of <b>graphs</b> that resemble real-world road networks. Specifically, we prove that if $G$ has a constant continuous doubling dimension, then for a given $s-t$ pair, we only need to probe the weights on $((\rho \log n )/ \epsilon)^{O(1)}$ edges in $G&rsquo;$ in order to obtain a $(1 + \epsilon)$-approximation to the $s-t$ distance in $G&rsquo;$. We also generalize the result to a correlated setting and demonstrate experimentally that probing improves accuracy in estimating $s-t$ distances.</p></p class="citation"></blockquote><h2 id=physicsgeo-ph-1>physics.geo-ph (1)</h2><h3 id=11--287298-thermal-earth-model-for-the-conterminous-united-states-using-an-interpolative-physics-informed-graph-neural-network-interpignn-mohammad-j-aljubran-et-al-2024>(1/1 | 287/298) Thermal Earth Model for the Conterminous United States Using an Interpolative Physics-Informed Graph Neural Network (InterPIGNN) (Mohammad J. Aljubran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad J. Aljubran, Roland N. Horne. (2024)<br><strong>Thermal Earth Model for the Conterminous United States Using an Interpolative Physics-Informed Graph Neural Network (InterPIGNN)</strong><br><button class=copy-to-clipboard title="Thermal Earth Model for the Conterminous United States Using an Interpolative Physics-Informed Graph Neural Network (InterPIGNN)" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.geo-ph<br>Categories: cs-LG, physics-geo-ph, physics.geo-ph<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09961v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09961v1.pdf filename=2403.09961v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents a data-driven spatial interpolation algorithm based on physics-informed <b>graph</b> <b>neural</b> <b>networks</b> used to develop national temperature-at-depth maps for the conterminous United States. The model was trained to approximately satisfy the three-dimensional heat conduction law by simultaneously predicting subsurface temperature, surface heat flow, and rock thermal conductivity. In addition to bottomhole temperature measurements, we incorporated other physical quantities as model inputs, such as depth, geographic coordinates, elevation, sediment thickness, magnetic anomaly, gravity anomaly, gamma-ray flux of radioactive elements, seismicity, and electric conductivity. We constructed surface heat flow, and temperature and thermal conductivity predictions for depths of 0-7 km at an interval of 1 km with spatial resolution of 18 km$^2$ per grid cell. Our model showed superior temperature, surface heat flow and thermal conductivity mean absolute errors of 4.8{\deg} C, 5.817 mW/m$^2$ and 0.022 W/(C-m)$, respectively. The predictions were visualized in two-dimensional spatial maps across the modeled depths. This thorough modeling of the Earth&rsquo;s thermal processes is crucial to understanding subsurface phenomena and exploiting natural underground resources.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=11--288298-accurate-and-data-efficient-micro-xrd-phase-identification-using-multi-task-learning-application-to-hydrothermal-fluids-yanfei-li-et-al-2024>(1/1 | 288/298) Accurate and Data-Efficient Micro-XRD Phase Identification Using Multi-Task Learning: Application to Hydrothermal Fluids (Yanfei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanfei Li, Juejing Liu, Xiaodong Zhao, Wenjun Liu, Tong Geng, Ang Li, Xin Zhang. (2024)<br><strong>Accurate and Data-Efficient Micro-XRD Phase Identification Using Multi-Task Learning: Application to Hydrothermal Fluids</strong><br><button class=copy-to-clipboard title="Accurate and Data-Efficient Micro-XRD Phase Identification Using Multi-Task Learning: Application to Hydrothermal Fluids" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-LG<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10042v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10042v1.pdf filename=2403.10042v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional analysis of highly distorted micro-X-ray diffraction ({\mu}-XRD) patterns from hydrothermal fluid environments is a time-consuming process, often requiring substantial data preprocessing and labeled experimental data. This study demonstrates the potential of deep learning with a multitask learning (MTL) architecture to overcome these limitations. We trained MTL models to identify phase information in {\mu}-XRD patterns, minimizing the need for labeled experimental data and masking preprocessing steps. Notably, MTL models showed superior accuracy compared to binary classification <b>CNNs.</b> Additionally, introducing a tailored cross-entropy loss function improved MTL model performance. Most significantly, MTL models tuned to analyze raw and unmasked XRD patterns achieved close performance to models analyzing preprocessed data, with minimal accuracy differences. This work indicates that advanced deep learning architectures like MTL can automate arduous data handling tasks, streamline the analysis of distorted XRD patterns, and reduce the reliance on labor-intensive experimental datasets.</p></p class="citation"></blockquote><h2 id=cscg-2>cs.CG (2)</h2><h3 id=12--289298-ipelets-for-the-convex-polygonal-geometry-nithin-parepally-et-al-2024>(1/2 | 289/298) Ipelets for the Convex Polygonal Geometry (Nithin Parepally et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nithin Parepally, Ainesh Chatterjee, Auguste Gezalyan, Hongyang Du, Sukrit Mangla, Kenny Wu, Sarah Hwang, David Mount. (2024)<br><strong>Ipelets for the Convex Polygonal Geometry</strong><br><button class=copy-to-clipboard title="Ipelets for the Convex Polygonal Geometry" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10033v1.pdf filename=2403.10033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There are many structures, both classical and modern, involving convex polygonal geometries whose deeper understanding would be facilitated through interactive visualizations. The Ipe extensible drawing editor, developed by Otfried Cheong, is a widely used software system for generating geometric figures. One of its features is the capability to extend its functionality through programs called Ipelets. In this media submission, we showcase a collection of new Ipelets that construct a variety of geometric objects based on polygonal geometries. These include Macbeath regions, metric balls in the forward and reverse Funk distance, metric balls in the Hilbert metric, polar bodies, the minimum enclosing ball of a point set, and minimum spanning trees in both the Funk and Hilbert metrics. We also include a number of utilities on convex polygons, including union, intersection, subtraction, and Minkowski sum (previously implemented as a CGAL Ipelet). All of our Ipelets are programmed in Lua and are freely available.</p></p class="citation"></blockquote><h3 id=22--290298-a-canonical-tree-decomposition-for-order-types-and-some-applications-mathilde-bouvel-et-al-2024>(2/2 | 290/298) A canonical tree decomposition for order types, and some applications (Mathilde Bouvel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathilde Bouvel, Valentin Féray, Xavier Goaoc, Florent Koechlin. (2024)<br><strong>A canonical tree decomposition for order types, and some applications</strong><br><button class=copy-to-clipboard title="A canonical tree decomposition for order types, and some applications" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10311v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10311v1.pdf filename=2403.10311v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce and study a notion of decomposition of planar point sets (or rather of their chirotopes) as trees decorated by smaller chirotopes. This decomposition is based on the concept of mutually avoiding sets (which we rephrase as \emph{modules}), and adapts in some sense the modular decomposition of <b>graphs</b> in the world of chirotopes. The associated tree always exists and is unique up to some appropriate constraints. We also show how to compute the number of triangulations of a chirotope efficiently, starting from its tree and the (weighted) numbers of triangulations of its parts.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--291298-improved-discrete-particle-swarm-optimization-using-bee-algorithm-and-multi-parent-crossover-method-case-study-allocation-problem-and-benchmark-functions-hamed-zibaei-et-al-2024>(1/2 | 291/298) Improved discrete particle swarm optimization using Bee Algorithm and multi-parent crossover method (Case study: Allocation problem and benchmark functions) (Hamed Zibaei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamed Zibaei, Mohammad Saadi Mesgari. (2024)<br><strong>Improved discrete particle swarm optimization using Bee Algorithm and multi-parent crossover method (Case study: Allocation problem and benchmark functions)</strong><br><button class=copy-to-clipboard title="Improved discrete particle swarm optimization using Bee Algorithm and multi-parent crossover method (Case study: Allocation problem and benchmark functions)" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10684v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10684v1.pdf filename=2403.10684v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compared to other techniques, particle swarm optimization is more frequently utilized because of its ease of use and low variability. However, it is complicated to find the best possible solution in the search space in large-scale optimization problems. Moreover, changing algorithm variables does not influence algorithm convergence much. The PSO algorithm can be combined with other algorithms. It can use their advantages and operators to solve this problem. Therefore, this paper proposes the onlooker multi-parent crossover discrete particle swarm optimization (OMPCDPSO). To improve the efficiency of the DPSO algorithm, we utilized multi-parent crossover on the best solutions. We performed an independent and intensive neighborhood search using the onlooker bees of the bee algorithm. The algorithm uses onlooker bees and crossover. They do local search (exploitation) and global search (exploration). Each of these searches is among the best solutions (employed bees). The proposed algorithm was tested on the allocation problem, which is an NP-hard optimization problem. Also, we used two types of simulated data. They were used to test the scalability and complexity of the better algorithm. Also, fourteen 2D test functions and thirteen 30D test functions were used. They also used twenty IEEE CEC2005 <b>benchmark</b> functions to test the efficiency of OMPCDPSO. Also, to test OMPCDPSO&rsquo;s performance, we compared it to four new binary optimization algorithms and three classic ones. The results show that the OMPCDPSO version had high capability. It performed better than other algorithms. The developed algorithm in this research (OMCDPSO) in 36 test functions out of 47 (76.60%) is better than other algorithms. The Onlooker bees and multi-parent operators significantly impact the algorithm&rsquo;s performance.</p></p class="citation"></blockquote><h3 id=22--292298-efficient-multiplayer-battle-game-optimizer-for-adversarial-robust-neural-architecture-search-rui-zhong-et-al-2024>(2/2 | 292/298) Efficient Multiplayer Battle Game Optimizer for Adversarial Robust Neural Architecture Search (Rui Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Zhong, Yuefeng Xu, Chao Zhang, Jun Yu. (2024)<br><strong>Efficient Multiplayer Battle Game Optimizer for Adversarial Robust Neural Architecture Search</strong><br><button class=copy-to-clipboard title="Efficient Multiplayer Battle Game Optimizer for Adversarial Robust Neural Architecture Search" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10100v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10100v1.pdf filename=2403.10100v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel metaheuristic algorithm, known as the efficient multiplayer battle game optimizer (EMBGO), specifically designed for addressing complex numerical optimization tasks. The motivation behind this research stems from the need to rectify identified shortcomings in the original MBGO, particularly in search operators during the movement phase, as revealed through ablation experiments. EMBGO mitigates these limitations by integrating the movement and battle phases to simplify the original optimization framework and improve search efficiency. Besides, two efficient search operators: differential mutation and L'evy flight are introduced to increase the diversity of the population. To evaluate the performance of EMBGO comprehensively and fairly, numerical experiments are conducted on <b>benchmark</b> functions such as CEC2017, CEC2020, and CEC2022, as well as engineering problems. Twelve well-established MA approaches serve as competitor algorithms for comparison. Furthermore, we apply the proposed EMBGO to the complex adversarial robust neural architecture search (ARNAS) tasks and explore its robustness and scalability. The experimental results and statistical analyses confirm the efficiency and effectiveness of EMBGO across various optimization tasks. As a potential optimization technique, EMBGO holds promise for diverse applications in real-world problems and deep learning scenarios. The source code of EMBGO is made available in \url{https://github.com/RuiZhong961230/EMBGO}.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--293298-floodgenome-interpretable-machine-learning-for-decoding-features-shaping-property-flood-risk-predisposition-in-cities-chenyue-liu-et-al-2024>(1/1 | 293/298) FloodGenome: Interpretable Machine Learning for Decoding Features Shaping Property Flood Risk Predisposition in Cities (Chenyue Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyue Liu, Ali Mostafavi. (2024)<br><strong>FloodGenome: Interpretable Machine Learning for Decoding Features Shaping Property Flood Risk Predisposition in Cities</strong><br><button class=copy-to-clipboard title="FloodGenome: Interpretable Machine Learning for Decoding Features Shaping Property Flood Risk Predisposition in Cities" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10625v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10625v2.pdf filename=2403.10625v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the fundamental characteristics that shape the inherent flood risk disposition of urban areas is critical for integrated urban design strategies for flood risk reduction. Flood risk disposition specifies an inherent and event-independent magnitude of property flood risk and measures the extent to which urban areas are susceptible to property damage if exposed to a weather hazard. This study presents FloodGenome as an interpretable machine learning model for evaluation of the extent to which various hydrological, topographic, and built-environment features and their interactions shape flood risk disposition in urban areas. Using flood damage claims data from the U.S. National Flood Insurance Program covering the period 2003 through 2023 across four metropolitan statistical areas (MSAs), the analysis computes building damage ratios and flood claim counts by employing k-means <b>clustering</b> for classifying census block groups (CBGs) into distinct property flood risk disposition levels. Then a random forest model is created to specify property flood risk levels of CBGs based on various intertwined hydrological, topographic, and built-environment features. The model transferability analysis results show consistent performance across MSAs, revealing the universality of underlying features that shape city property flood risks. The FloodGenome model is then used to:(1) evaluate the extent to which future urban development would exacerbate flood risk disposition of urban areas; and (2) specify property flood risk levels at finer spatial resolution providing critical insights for flood risk management processes. The FloodGenome model and the findings provide novel tools and insights for improving the characterization and understanding of intertwined features that shape flood risk profiles of cities.</p></p class="citation"></blockquote><h2 id=physicscomp-ph-1>physics.comp-ph (1)</h2><h3 id=11--294298-model-free-collision-aggregation-for-the-computation-of-escape-distributions-laetitia-laguzet-et-al-2024>(1/1 | 294/298) Model free collision aggregation for the computation of escape distributions (Laetitia Laguzet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laetitia Laguzet, Gabriel Turinici. (2024)<br><strong>Model free collision aggregation for the computation of escape distributions</strong><br><button class=copy-to-clipboard title="Model free collision aggregation for the computation of escape distributions" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.comp-ph<br>Categories: cs-NA, math-NA, math-PR, physics-comp-ph, physics.comp-ph<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10432v1.pdf filename=2403.10432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by a heat radiative transport equation, we consider a particle undergoing collisions in a space-time domain and propose a method to sample its escape time, space and direction from the domain. The first step of the procedure is an estimation of how many elementary collisions is safe to take before chances of exiting the domain are too high; then these collisions are aggregated into a single movement. The method does not use any model nor any particular regime of parameters. We give theoretical results both under the normal approximation and without it and test the method on some <b>benchmarks</b> from the literature. The results confirm the theoretical predictions and show that the proposal is an efficient method to sample the escape distribution of the particle.</p></p class="citation"></blockquote><h2 id=mathfa-1>math.FA (1)</h2><h3 id=11--295298-discrete-functional-inequalities-on-lattice-graphs-shubham-gupta-2024>(1/1 | 295/298) Discrete functional inequalities on lattice graphs (Shubham Gupta, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubham Gupta. (2024)<br><strong>Discrete functional inequalities on lattice graphs</strong><br><button class=copy-to-clipboard title="Discrete functional inequalities on lattice graphs" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.FA<br>Categories: cs-DM, math-FA, math-MP, math-SP, math-ph, math.FA<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10270v1.pdf filename=2403.10270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this thesis, we study problems at the interface of analysis and discrete mathematics. We discuss analogues of well known Hardy-type inequalities and Rearrangement inequalities on the lattice <b>graphs</b> $\mathbb{Z}^d$, with a particular focus on behaviour of sharp constants and optimizers.In the first half of the thesis, we analyse Hardy inequalities on $\mathbb{Z}^d$, first for $d=1$ and then for $d \geq 3$. We prove a sharp weighted Hardy inequality on integers with power weights of the form $n^\alpha$. This is done via two different methods, namely super-solution and Fourier method. We also use Fourier method to prove a weighted Hardy type inequality for higher order operators. After discussing the one dimensional case, we study the Hardy inequality in higher dimensions ($d \geq 3$). In particular, we compute the asymptotic behaviour of the sharp constant in the discrete Hardy inequality, as $d \rightarrow \infty$. This is done by converting the inequality into a continuous Hardy-type inequality on a torus for functions having zero average. These continuous inequalities are new and interesting in themselves. In the second half, we focus our attention on analogues of Rearrangement inequalities on lattice <b>graphs.</b> We begin by analysing the situation in dimension one. We define various notions of rearrangements and prove the corresponding Polya-Szeg\H{o} inequality. These inequalities are also applied to prove some weighted Hardy inequalities on integers. Finally, we study Rearrangement inequalities (Polya-Szeg\H{o}) on general <b>graphs,</b> with a particular focus on lattice <b>graphs</b> $\mathbb{Z}^d$, for $d \geq 2$. We develop a framework to study these inequalities, using which we derive concrete results in dimension two. In particular, these results develop connections between Polya-Szeg\H{o} inequality and various isoperimetric inequalities on <b>graphs.</b></p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--296298-v2aix-a-multi-modal-real-world-dataset-of-etsi-its-v2x-messages-in-public-road-traffic-guido-kueppers-et-al-2024>(1/1 | 296/298) V2AIX: A Multi-Modal Real-World Dataset of ETSI ITS V2X Messages in Public Road Traffic (Guido Kueppers et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guido Kueppers, Jean-Pierre Busch, Lennart Reiher, Lutz Eckstein. (2024)<br><strong>V2AIX: A Multi-Modal Real-World Dataset of ETSI ITS V2X Messages in Public Road Traffic</strong><br><button class=copy-to-clipboard title="V2AIX: A Multi-Modal Real-World Dataset of ETSI ITS V2X Messages in Public Road Traffic" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10221v1.pdf filename=2403.10221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Connectivity is a main driver for the ongoing megatrend of automated mobility: future Cooperative Intelligent Transport Systems (C-ITS) will connect road vehicles, traffic signals, roadside infrastructure, and even vulnerable road users, sharing data and compute for safer, more efficient, and more comfortable mobility. In terms of communication technology for realizing such vehicle-to-everything (V2X) communication, the WLAN-based peer-to-peer approach (IEEE 802.11p, ITS-G5 in Europe) competes with C-V2X based on cellular technologies (4G and beyond). Irrespective of the underlying communication standard, common message interfaces are crucial for a common understanding between vehicles, especially from different manufacturers. Targeting this issue, the European Telecommunications Standards Institute (ETSI) has been standardizing V2X message formats such as the Cooperative Awareness Message (CAM). In this work, we present V2AIX, a <b>multi-modal</b> real-world dataset of ETSI ITS messages gathered in public road traffic, the first of its kind. Collected in measurement drives and with stationary infrastructure, we have recorded more than 230 000 V2X messages from more than 1800 vehicles and roadside units in public road traffic. Alongside a first analysis of the dataset, we present a way of integrating ETSI ITS V2X messages into the Robot Operating System (ROS). This enables researchers to not only thoroughly analyze real-world V2X data, but to also study and implement standardized V2X messages in ROS-based automated driving applications. The full dataset is publicly available for noncommercial use at v2aix.ika.rwth-aachen.de.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--297298-a-data-driven-approach-for-mitigating-dark-current-noise-and-bad-pixels-in-complementary-metal-oxide-semiconductor-cameras-for-space-based-telescopes-peng-jia-et-al-2024>(1/1 | 297/298) A Data-Driven Approach for Mitigating Dark Current Noise and Bad Pixels in Complementary Metal Oxide Semiconductor Cameras for Space-based Telescopes (Peng Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Jia, Chao Lv, Yushan Li, Yongyang Sun, Shu Niu, Zhuoxiao Wang. (2024)<br><strong>A Data-Driven Approach for Mitigating Dark Current Noise and Bad Pixels in Complementary Metal Oxide Semiconductor Cameras for Space-based Telescopes</strong><br><button class=copy-to-clipboard title="A Data-Driven Approach for Mitigating Dark Current Noise and Bad Pixels in Complementary Metal Oxide Semiconductor Cameras for Space-based Telescopes" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-IM, astro-ph-SR, astro-ph.IM, cs-CV, physics-ins-det, physics-optics<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10206v1.pdf filename=2403.10206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there has been a gradual increase in the performance of Complementary Metal Oxide Semiconductor (CMOS) cameras. These cameras have gained popularity as a viable alternative to charge-coupled device (CCD) cameras in a wide range of applications. One particular application is the CMOS camera installed in small space telescopes. However, the limited power and spatial resources available on satellites present challenges in maintaining ideal observation conditions, including temperature and radiation environment. Consequently, images captured by CMOS cameras are susceptible to issues such as dark current noise and defective pixels. In this paper, we introduce a data-driven framework for mitigating dark current noise and bad pixels for CMOS cameras. Our approach involves two key steps: pixel <b>clustering</b> and function fitting. During pixel <b>clustering</b> step, we identify and group pixels exhibiting similar dark current noise properties. Subsequently, in the function fitting step, we formulate functions that capture the relationship between dark current and temperature, as dictated by the Arrhenius law. Our framework leverages ground-based test data to establish distinct temperature-dark current relations for pixels within different clusters. The cluster results could then be utilized to estimate the dark current noise level and detect bad pixels from real observational data. To assess the effectiveness of our approach, we have conducted tests using real observation data obtained from the Yangwang-1 satellite, equipped with a near-ultraviolet telescope and an optical telescope. The results show a considerable improvement in the detection efficiency of space-based telescopes.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--298298-accelerating-regular-path-queries-over-graph-database-with-processing-in-memory-ruoyan-ma-et-al-2024>(1/1 | 298/298) Accelerating Regular Path Queries over Graph Database with Processing-in-Memory (Ruoyan Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruoyan Ma, Shengan Zheng, Guifeng Wang, Jin Pu, Yifan Hua, Wentao Wang, Linpeng Huang. (2024)<br><strong>Accelerating Regular Path Queries over Graph Database with Processing-in-Memory</strong><br><button class=copy-to-clipboard title="Accelerating Regular Path Queries over Graph Database with Processing-in-Memory" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10051v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10051v1.pdf filename=2403.10051v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Regular path queries (RPQs) in <b>graph</b> databases are bottlenecked by the memory wall. Emerging processing-in-memory (PIM) technologies offer a promising solution to dispatch and execute path matching tasks in parallel within PIM modules. We present Moctopus, a PIM-based data management system for <b>graph</b> databases that supports efficient batch RPQs and <b>graph</b> updates. Moctopus employs a PIM-friendly dynamic <b>graph</b> partitioning algorithm, which tackles <b>graph</b> skewness and preserves <b>graph</b> locality with low overhead for RPQ processing. Moctopus enables efficient <b>graph</b> update by amortizing the host CPU&rsquo;s update overhead to PIM modules. Evaluation of Moctopus demonstrates superiority over the state-of-the-art traditional <b>graph</b> database.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.16</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.18</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscv-104>cs.CV (104)</a><ul><li><a href=#1104--1298-few-shot-image-classification-and-segmentation-as-visual-question-answering-using-vision-language-models-tian-meng-et-al-2024>(1/104 | 1/298) Few-Shot Image Classification and Segmentation as Visual Question Answering Using Vision-Language Models (Tian Meng et al., 2024)</a></li><li><a href=#2104--2298-knowledge-condensation-and-reasoning-for-knowledge-based-vqa-dongze-hao-et-al-2024>(2/104 | 2/298) Knowledge Condensation and Reasoning for Knowledge-based VQA (Dongze Hao et al., 2024)</a></li><li><a href=#3104--3298-a-survey-of-synthetic-data-augmentation-methods-in-computer-vision-alhassan-mumuni-et-al-2024>(3/104 | 3/298) A survey of synthetic data augmentation methods in computer vision (Alhassan Mumuni et al., 2024)</a></li><li><a href=#4104--4298-medslip-medical-dual-stream-language-image-pre-training-for-fine-grained-alignment-wenrui-fan-et-al-2024>(4/104 | 4/298) MeDSLIP: Medical Dual-Stream Language-Image Pre-training for Fine-grained Alignment (Wenrui Fan et al., 2024)</a></li><li><a href=#5104--5298-improving-medical-multi-modal-contrastive-learning-with-expert-annotations-yogesh-kumar-et-al-2024>(5/104 | 5/298) Improving Medical Multi-modal Contrastive Learning with Expert Annotations (Yogesh Kumar et al., 2024)</a></li><li><a href=#6104--6298-vitcn-vision-transformer-contrastive-network-for-reasoning-bo-song-et-al-2024>(6/104 | 6/298) ViTCN: Vision Transformer Contrastive Network For Reasoning (Bo Song et al., 2024)</a></li><li><a href=#7104--7298-on-the-low-shot-transferability-of-v-mamba-diganta-misra-et-al-2024>(7/104 | 7/298) On the low-shot transferability of [V]-Mamba (Diganta Misra et al., 2024)</a></li><li><a href=#8104--8298-mitigating-dialogue-hallucination-for-large-multi-modal-models-via-adversarial-instruction-tuning-dongmin-park-et-al-2024>(8/104 | 8/298) Mitigating Dialogue Hallucination for Large Multi-modal Models via Adversarial Instruction Tuning (Dongmin Park et al., 2024)</a></li><li><a href=#9104--9298-textblockv2-towards-precise-detection-free-scene-text-spotting-with-pre-trained-language-model-jiahao-lyu-et-al-2024>(9/104 | 9/298) TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model (Jiahao Lyu et al., 2024)</a></li><li><a href=#10104--10298-videoagent-long-form-video-understanding-with-large-language-model-as-agent-xiaohan-wang-et-al-2024>(10/104 | 10/298) VideoAgent: Long-form Video Understanding with Large Language Model as Agent (Xiaohan Wang et al., 2024)</a></li><li><a href=#11104--11298-real-world-computational-aberration-correction-via-quantized-domain-mixing-representation-qi-jiang-et-al-2024>(11/104 | 11/298) Real-World Computational Aberration Correction via Quantized Domain-Mixing Representation (Qi Jiang et al., 2024)</a></li><li><a href=#12104--12298-visual-foundation-models-boost-cross-modal-unsupervised-domain-adaptation-for-3d-semantic-segmentation-jingyi-xu-et-al-2024>(12/104 | 12/298) Visual Foundation Models Boost Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation (Jingyi Xu et al., 2024)</a></li><li><a href=#13104--13298-multi-criteria-token-fusion-with-one-step-ahead-attention-for-efficient-vision-transformers-sanghyeok-lee-et-al-2024>(13/104 | 13/298) Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers (Sanghyeok Lee et al., 2024)</a></li><li><a href=#14104--14298-st-ldm-a-universal-framework-for-text-grounded-object-generation-in-real-images-xiangtian-xue-et-al-2024>(14/104 | 14/298) ST-LDM: A Universal Framework for Text-Grounded Object Generation in Real Images (Xiangtian Xue et al., 2024)</a></li><li><a href=#15104--15298-isotropic3d-image-to-3d-generation-based-on-a-single-clip-embedding-pengkun-liu-et-al-2024>(15/104 | 15/298) Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding (Pengkun Liu et al., 2024)</a></li><li><a href=#16104--16298-pame-self-supervised-masked-autoencoder-for-no-reference-point-cloud-quality-assessment-ziyu-shan-et-al-2024>(16/104 | 16/298) PAME: Self-Supervised Masked Autoencoder for No-Reference Point Cloud Quality Assessment (Ziyu Shan et al., 2024)</a></li><li><a href=#17104--17298-leveraging-synthetic-data-for-generalizable-and-fair-facial-action-unit-detection-liupei-lu-et-al-2024>(17/104 | 17/298) Leveraging Synthetic Data for Generalizable and Fair Facial Action Unit Detection (Liupei Lu et al., 2024)</a></li><li><a href=#18104--18298-approximate-nullspace-augmented-finetuning-for-robust-vision-transformers-haoyang-liu-et-al-2024>(18/104 | 18/298) Approximate Nullspace Augmented Finetuning for Robust Vision Transformers (Haoyang Liu et al., 2024)</a></li><li><a href=#19104--19298-deep-learning-for-multi-level-detection-and-localization-of-myocardial-scars-based-on-regional-strain-validated-on-virtual-patients-müjde-akdeniz-et-al-2024>(19/104 | 19/298) Deep Learning for Multi-Level Detection and Localization of Myocardial Scars Based on Regional Strain Validated on Virtual Patients (Müjde Akdeniz et al., 2024)</a></li><li><a href=#20104--20298-coleclip-open-domain-continual-learning-via-joint-task-prompt-and-vocabulary-learning-yukun-li-et-al-2024>(20/104 | 20/298) CoLeCLIP: Open-Domain Continual Learning via Joint Task Prompt and Vocabulary Learning (Yukun Li et al., 2024)</a></li><li><a href=#21104--21298-enhancing-human-centered-dynamic-scene-understanding-via-multiple-llms-collaborated-reasoning-hang-zhang-et-al-2024>(21/104 | 21/298) Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning (Hang Zhang et al., 2024)</a></li><li><a href=#22104--22298-efficientvmamba-atrous-selective-scan-for-light-weight-visual-mamba-xiaohuan-pei-et-al-2024>(22/104 | 22/298) EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba (Xiaohuan Pei et al., 2024)</a></li><li><a href=#23104--23298-leveraging-clip-for-inferring-sensitive-information-and-improving-model-fairness-miao-zhang-et-al-2024>(23/104 | 23/298) Leveraging CLIP for Inferring Sensitive Information and Improving Model Fairness (Miao Zhang et al., 2024)</a></li><li><a href=#24104--24298-magic-tokens-select-diverse-tokens-for-multi-modal-object-re-identification-pingping-zhang-et-al-2024>(24/104 | 24/298) Magic Tokens: Select Diverse Tokens for Multi-modal Object Re-Identification (Pingping Zhang et al., 2024)</a></li><li><a href=#25104--25298-controllable-text-to-3d-generation-via-surface-aligned-gaussian-splatting-zhiqi-li-et-al-2024>(25/104 | 25/298) Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting (Zhiqi Li et al., 2024)</a></li><li><a href=#26104--26298-hawkeye-training-video-text-llms-for-grounding-text-in-videos-yueqian-wang-et-al-2024>(26/104 | 26/298) HawkEye: Training Video-Text LLMs for Grounding Text in Videos (Yueqian Wang et al., 2024)</a></li><li><a href=#27104--27298-crossglg-llm-guides-one-shot-skeleton-based-3d-action-recognition-in-a-cross-level-manner-tingbing-yan-et-al-2024>(27/104 | 27/298) CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner (Tingbing Yan et al., 2024)</a></li><li><a href=#28104--28298-real-time-image-segmentation-via-hybrid-convolutional-transformer-architecture-search-hongyuan-yu-et-al-2024>(28/104 | 28/298) Real-Time Image Segmentation via Hybrid Convolutional-Transformer Architecture Search (Hongyuan Yu et al., 2024)</a></li><li><a href=#29104--29298-denoising-task-difficulty-based-curriculum-for-training-diffusion-models-jin-young-kim-et-al-2024>(29/104 | 29/298) Denoising Task Difficulty-based Curriculum for Training Diffusion Models (Jin-Young Kim et al., 2024)</a></li><li><a href=#30104--30298-how-powerful-potential-of-attention-on-image-restoration-cong-wang-et-al-2024>(30/104 | 30/298) How Powerful Potential of Attention on Image Restoration? (Cong Wang et al., 2024)</a></li><li><a href=#31104--31298-generative-region-language-pretraining-for-open-ended-object-detection-chuang-lin-et-al-2024>(31/104 | 31/298) Generative Region-Language Pretraining for Open-Ended Object Detection (Chuang Lin et al., 2024)</a></li><li><a href=#32104--32298-diffmac-diffusion-manifold-hallucination-correction-for-high-generalization-blind-face-restoration-nan-gao-et-al-2024>(32/104 | 32/298) DiffMAC: Diffusion Manifold Hallucination Correction for High Generalization Blind Face Restoration (Nan Gao et al., 2024)</a></li><li><a href=#33104--33298-rangeldm-fast-realistic-lidar-point-cloud-generation-qianjiang-hu-et-al-2024>(33/104 | 33/298) RangeLDM: Fast Realistic LiDAR Point Cloud Generation (Qianjiang Hu et al., 2024)</a></li><li><a href=#34104--34298-revisiting-adversarial-training-under-long-tailed-distributions-xinli-yue-et-al-2024>(34/104 | 34/298) Revisiting Adversarial Training under Long-Tailed Distributions (Xinli Yue et al., 2024)</a></li><li><a href=#35104--35298-rethinking-low-quality-optical-flow-in-unsupervised-surgical-instrument-segmentation-peiran-wu-et-al-2024>(35/104 | 35/298) Rethinking Low-quality Optical Flow in Unsupervised Surgical Instrument Segmentation (Peiran Wu et al., 2024)</a></li><li><a href=#36104--36298-fbpt-a-fully-binary-point-transformer-zhixing-hou-et-al-2024>(36/104 | 36/298) FBPT: A Fully Binary Point Transformer (Zhixing Hou et al., 2024)</a></li><li><a href=#37104--37298-interlude-interactions-between-labeled-and-unlabeled-data-to-enhance-semi-supervised-learning-zhe-huang-et-al-2024>(37/104 | 37/298) InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning (Zhe Huang et al., 2024)</a></li><li><a href=#38104--38298-joint-multimodal-transformer-for-dimensional-emotional-recognition-in-the-wild-paul-waligora-et-al-2024>(38/104 | 38/298) Joint Multimodal Transformer for Dimensional Emotional Recognition in the Wild (Paul Waligora et al., 2024)</a></li><li><a href=#39104--39298-get-unlocking-the-multi-modal-potential-of-clip-for-generalized-category-discovery-enguang-wang-et-al-2024>(39/104 | 39/298) GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery (Enguang Wang et al., 2024)</a></li><li><a href=#40104--40298-t4p-test-time-training-of-trajectory-prediction-via-masked-autoencoder-and-actor-specific-token-memory-daehee-park-et-al-2024>(40/104 | 40/298) T4P: Test-Time Training of Trajectory Prediction via Masked Autoencoder and Actor-specific Token Memory (Daehee Park et al., 2024)</a></li><li><a href=#41104--41298-local-positional-graphs-and-attentive-local-features-for-a-data-and-runtime-efficient-hierarchical-place-recognition-pipeline-fangming-yuan-et-al-2024>(41/104 | 41/298) Local positional graphs and attentive local features for a data and runtime-efficient hierarchical place recognition pipeline (Fangming Yuan et al., 2024)</a></li><li><a href=#42104--42298-region-aware-distribution-contrast-a-novel-approach-to-multi-task-partially-supervised-learning-meixuan-li-et-al-2024>(42/104 | 42/298) Region-aware Distribution Contrast: A Novel Approach to Multi-Task Partially Supervised Learning (Meixuan Li et al., 2024)</a></li><li><a href=#43104--43298-sparsefusion-efficient-sparse-multi-modal-fusion-framework-for-long-range-3d-perception-yiheng-li-et-al-2024>(43/104 | 43/298) SparseFusion: Efficient Sparse Multi-Modal Fusion Framework for Long-Range 3D Perception (Yiheng Li et al., 2024)</a></li><li><a href=#44104--44298-giving-a-hand-to-diffusion-models-a-two-stage-approach-to-improving-conditional-human-image-generation-anton-pelykh-et-al-2024>(44/104 | 44/298) Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving Conditional Human Image Generation (Anton Pelykh et al., 2024)</a></li><li><a href=#45104--45298-swinmtl-a-shared-architecture-for-simultaneous-depth-estimation-and-semantic-segmentation-from-monocular-camera-images-pardis-taghavi-et-al-2024>(45/104 | 45/298) SwinMTL: A Shared Architecture for Simultaneous Depth Estimation and Semantic Segmentation from Monocular Camera Images (Pardis Taghavi et al., 2024)</a></li><li><a href=#46104--46298-frozen-feature-augmentation-for-few-shot-image-classification-andreas-bär-et-al-2024>(46/104 | 46/298) Frozen Feature Augmentation for Few-Shot Image Classification (Andreas Bär et al., 2024)</a></li><li><a href=#47104--47298-featup-a-model-agnostic-framework-for-features-at-any-resolution-stephanie-fu-et-al-2024>(47/104 | 47/298) FeatUp: A Model-Agnostic Framework for Features at Any Resolution (Stephanie Fu et al., 2024)</a></li><li><a href=#48104--48298-using-an-llm-to-turn-sign-spottings-into-spoken-language-sentences-ozge-mercanoglu-sincan-et-al-2024>(48/104 | 48/298) Using an LLM to Turn Sign Spottings into Spoken Language Sentences (Ozge Mercanoglu Sincan et al., 2024)</a></li><li><a href=#49104--49298-pasta-towards-flexible-and-efficient-hdr-imaging-via-progressively-aggregated-spatio-temporal-aligment-xiaoning-liu-et-al-2024>(49/104 | 49/298) PASTA: Towards Flexible and Efficient HDR Imaging Via Progressively Aggregated Spatio-Temporal Aligment (Xiaoning Liu et al., 2024)</a></li><li><a href=#50104--50298-context-semantic-quality-awareness-network-for-fine-grained-visual-categorization-qin-xu-et-al-2024>(50/104 | 50/298) Context-Semantic Quality Awareness Network for Fine-Grained Visual Categorization (Qin Xu et al., 2024)</a></li><li><a href=#51104--51298-blinddiff-empowering-degradation-modelling-in-diffusion-models-for-blind-image-super-resolution-feng-li-et-al-2024>(51/104 | 51/298) BlindDiff: Empowering Degradation Modelling in Diffusion Models for Blind Image Super-Resolution (Feng Li et al., 2024)</a></li><li><a href=#52104--52298-e4c-enhance-editability-for-text-based-image-editing-by-harnessing-efficient-clip-guidance-tianrui-huang-et-al-2024>(52/104 | 52/298) E4C: Enhance Editability for Text-Based Image Editing by Harnessing Efficient CLIP Guidance (Tianrui Huang et al., 2024)</a></li><li><a href=#53104--53298-translandseg-a-transfer-learning-approach-for-landslide-semantic-segmentation-based-on-vision-foundation-model-changhong-hou-et-al-2024>(53/104 | 53/298) TransLandSeg: A Transfer Learning Approach for Landslide Semantic Segmentation Based on Vision Foundation Model (Changhong Hou et al., 2024)</a></li><li><a href=#54104--54298-codebook-transfer-with-part-of-speech-for-vector-quantized-image-modeling-baoquan-zhang-et-al-2024>(54/104 | 54/298) Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling (Baoquan Zhang et al., 2024)</a></li><li><a href=#55104--55298-boundary-matters-a-bi-level-active-finetuning-framework-han-lu-et-al-2024>(55/104 | 55/298) Boundary Matters: A Bi-Level Active Finetuning Framework (Han Lu et al., 2024)</a></li><li><a href=#56104--56298-what-makes-good-collaborative-views-contrastive-mutual-information-maximization-for-multi-agent-perception-wanfang-su-et-al-2024>(56/104 | 56/298) What Makes Good Collaborative Views? Contrastive Mutual Information Maximization for Multi-Agent Perception (Wanfang Su et al., 2024)</a></li><li><a href=#57104--57298-group-mix-sam-lightweight-solution-for-industrial-assembly-line-applications-wu-liang-et-al-2024>(57/104 | 57/298) Group-Mix SAM: Lightweight Solution for Industrial Assembly Line Applications (Wu Liang et al., 2024)</a></li><li><a href=#58104--58298-trg-net-an-interpretable-and-controllable-rain-generator-zhiqiang-pang-et-al-2024>(58/104 | 58/298) TRG-Net: An Interpretable and Controllable Rain Generator (Zhiqiang Pang et al., 2024)</a></li><li><a href=#59104--59298-radclip-enhancing-radiologic-image-analysis-through-contrastive-language-image-pre-training-zhixiu-lu-et-al-2024>(59/104 | 59/298) RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training (Zhixiu Lu et al., 2024)</a></li><li><a href=#60104--60298-quantization-effects-on-neural-networks-perception-how-would-quantization-change-the-perceptual-field-of-vision-models-mohamed-amine-kerkouri-et-al-2024>(60/104 | 60/298) Quantization Effects on Neural Networks Perception: How would quantization change the perceptual field of vision models? (Mohamed Amine Kerkouri et al., 2024)</a></li><li><a href=#61104--61298-csdnet-detect-salient-object-in-depth-thermal-via-a-lightweight-cross-shallow-and-deep-perception-network-xiaotong-yu-et-al-2024>(61/104 | 61/298) CSDNet: Detect Salient Object in Depth-Thermal via A Lightweight Cross Shallow and Deep Perception Network (Xiaotong Yu et al., 2024)</a></li><li><a href=#62104--62298-leveraging-neural-radiance-field-in-descriptor-synthesis-for-keypoints-scene-coordinate-regression-huy-hoang-bui-et-al-2024>(62/104 | 62/298) Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints Scene Coordinate Regression (Huy-Hoang Bui et al., 2024)</a></li><li><a href=#63104--63298-animate-your-motion-turning-still-images-into-dynamic-videos-mingxiao-li-et-al-2024>(63/104 | 63/298) Animate Your Motion: Turning Still Images into Dynamic Videos (Mingxiao Li et al., 2024)</a></li><li><a href=#64104--64298-benchmarking-adversarial-robustness-of-image-shadow-removal-with-shadow-adaptive-attacks-chong-wang-et-al-2024>(64/104 | 64/298) Benchmarking Adversarial Robustness of Image Shadow Removal with Shadow-adaptive Attacks (Chong Wang et al., 2024)</a></li><li><a href=#65104--65298-parapoint-learning-global-free-boundary-surface-parameterization-of-3d-point-clouds-qijian-zhang-et-al-2024>(65/104 | 65/298) ParaPoint: Learning Global Free-Boundary Surface Parameterization of 3D Point Clouds (Qijian Zhang et al., 2024)</a></li><li><a href=#66104--66298-spherediffusion-spherical-geometry-aware-distortion-resilient-diffusion-model-tao-wu-et-al-2024>(66/104 | 66/298) SphereDiffusion: Spherical Geometry-Aware Distortion Resilient Diffusion Model (Tao Wu et al., 2024)</a></li><li><a href=#67104--67298-palm-pushing-adaptive-learning-rate-mechanisms-for-continual-test-time-adaptation-sarthak-kumar-maharana-et-al-2024>(67/104 | 67/298) PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation (Sarthak Kumar Maharana et al., 2024)</a></li><li><a href=#68104--68298-p-mapnet-far-seeing-map-generator-enhanced-by-both-sdmap-and-hdmap-priors-zhou-jiang-et-al-2024>(68/104 | 68/298) P-MapNet: Far-seeing Map Generator Enhanced by both SDMap and HDMap Priors (Zhou Jiang et al., 2024)</a></li><li><a href=#69104--69298-energy-correction-model-in-the-feature-space-for-out-of-distribution-detection-marc-lafon-et-al-2024>(69/104 | 69/298) Energy Correction Model in the Feature Space for Out-of-Distribution Detection (Marc Lafon et al., 2024)</a></li><li><a href=#70104--70298-anim-accurate-neural-implicit-model-for-human-reconstruction-from-a-single-rgb-d-image-marco-pesavento-et-al-2024>(70/104 | 70/298) ANIM: Accurate Neural Implicit Model for Human Reconstruction from a single RGB-D image (Marco Pesavento et al., 2024)</a></li><li><a href=#71104--71298-learning-spatiotemporal-inconsistency-via-thumbnail-layout-for-face-deepfake-detection-yuting-xu-et-al-2024>(71/104 | 71/298) Learning Spatiotemporal Inconsistency via Thumbnail Layout for Face Deepfake Detection (Yuting Xu et al., 2024)</a></li><li><a href=#72104--72298-arbitrary-scale-image-generation-and-upsampling-using-latent-diffusion-model-and-implicit-neural-decoder-jinseok-kim-et-al-2024>(72/104 | 72/298) Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder (Jinseok Kim et al., 2024)</a></li><li><a href=#73104--73298-rcooper-a-real-world-large-scale-dataset-for-roadside-cooperative-perception-ruiyang-hao-et-al-2024>(73/104 | 73/298) RCooper: A Real-world Large-scale Dataset for Roadside Cooperative Perception (Ruiyang Hao et al., 2024)</a></li><li><a href=#74104--74298-contrastive-pre-training-with-multi-view-fusion-for-no-reference-point-cloud-quality-assessment-ziyu-shan-et-al-2024>(74/104 | 74/298) Contrastive Pre-Training with Multi-View Fusion for No-Reference Point Cloud Quality Assessment (Ziyu Shan et al., 2024)</a></li><li><a href=#75104--75298-autoregressive-queries-for-adaptive-tracking-with-spatio-temporaltransformers-jinxia-xie-et-al-2024>(75/104 | 75/298) Autoregressive Queries for Adaptive Tracking with Spatio-TemporalTransformers (Jinxia Xie et al., 2024)</a></li><li><a href=#76104--76298-shifting-focus-from-global-semantics-to-local-prominent-features-in-swin-transformer-for-knee-osteoarthritis-severity-assessment-aymen-sekhri-et-al-2024>(76/104 | 76/298) Shifting Focus: From Global Semantics to Local Prominent Features in Swin-Transformer for Knee Osteoarthritis Severity Assessment (Aymen Sekhri et al., 2024)</a></li><li><a href=#77104--77298-cannabis-seed-variant-detection-using-faster-r-cnn-toqi-tahamid-sarker-et-al-2024>(77/104 | 77/298) Cannabis Seed Variant Detection using Faster R-CNN (Toqi Tahamid Sarker et al., 2024)</a></li><li><a href=#78104--78298-robust-influence-based-training-methods-for-noisy-brain-mri-minh-hao-van-et-al-2024>(78/104 | 78/298) Robust Influence-based Training Methods for Noisy Brain MRI (Minh-Hao Van et al., 2024)</a></li><li><a href=#79104--79298-lightit-illumination-modeling-and-control-for-diffusion-models-peter-kocsis-et-al-2024>(79/104 | 79/298) LightIt: Illumination Modeling and Control for Diffusion Models (Peter Kocsis et al., 2024)</a></li><li><a href=#80104--80298-lodge-a-coarse-to-fine-diffusion-network-for-long-dance-generation-guided-by-the-characteristic-dance-primitives-ronghui-li-et-al-2024>(80/104 | 80/298) Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation Guided by the Characteristic Dance Primitives (Ronghui Li et al., 2024)</a></li><li><a href=#81104--81298-a-novel-framework-for-multi-person-temporal-gaze-following-and-social-gaze-prediction-anshul-gupta-et-al-2024>(81/104 | 81/298) A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction (Anshul Gupta et al., 2024)</a></li><li><a href=#82104--82298-robust-shape-fitting-for-3d-scene-abstraction-florian-kluger-et-al-2024>(82/104 | 82/298) Robust Shape Fitting for 3D Scene Abstraction (Florian Kluger et al., 2024)</a></li><li><a href=#83104--83298-swag-splatting-in-the-wild-images-with-appearance-conditioned-gaussians-hiba-dahmani-et-al-2024>(83/104 | 83/298) SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians (Hiba Dahmani et al., 2024)</a></li><li><a href=#84104--84298-neuflow-real-time-high-accuracy-optical-flow-estimation-on-robots-using-edge-devices-zhiyong-zhang-et-al-2024>(84/104 | 84/298) NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots Using Edge Devices (Zhiyong Zhang et al., 2024)</a></li><li><a href=#85104--85298-cdmad-class-distribution-mismatch-aware-debiasing-for-class-imbalanced-semi-supervised-learning-hyuck-lee-et-al-2024>(85/104 | 85/298) CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning (Hyuck Lee et al., 2024)</a></li><li><a href=#86104--86298-simpb-a-single-model-for-2d-and-3d-object-detection-from-multiple-cameras-yingqi-tang-et-al-2024>(86/104 | 86/298) SimPB: A Single Model for 2D and 3D Object Detection from Multiple Cameras (Yingqi Tang et al., 2024)</a></li><li><a href=#87104--87298-scilla-surface-implicit-learning-for-large-urban-area-a-volumetric-hybrid-solution-hala-djeghim-et-al-2024>(87/104 | 87/298) SCILLA: SurfaCe Implicit Learning for Large Urban Area, a volumetric hybrid solution (Hala Djeghim et al., 2024)</a></li><li><a href=#88104--88298-fdgaussian-fast-gaussian-splatting-from-single-image-via-geometric-aware-diffusion-model-qijun-feng-et-al-2024>(88/104 | 88/298) FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model (Qijun Feng et al., 2024)</a></li><li><a href=#89104--89298-a-fixed-point-approach-to-unified-prompt-based-counting-wei-lin-et-al-2024>(89/104 | 89/298) A Fixed-Point Approach to Unified Prompt-Based Counting (Wei Lin et al., 2024)</a></li><li><a href=#90104--90298-a-hybrid-snn-ann-network-for-event-based-object-detection-with-spatial-and-temporal-attention-soikat-hasan-ahmed-et-al-2024>(90/104 | 90/298) A Hybrid SNN-ANN Network for Event-based Object Detection with Spatial and Temporal Attention (Soikat Hasan Ahmed et al., 2024)</a></li><li><a href=#91104--91298-computer-user-interface-understanding-a-new-dataset-and-a-learning-framework-andrés-muñoz-et-al-2024>(91/104 | 91/298) Computer User Interface Understanding. A New Dataset and a Learning Framework (Andrés Muñoz et al., 2024)</a></li><li><a href=#92104--92298-semantichuman-hd-high-resolution-semantic-disentangled-3d-human-generation-peng-zheng-et-al-2024>(92/104 | 92/298) SemanticHuman-HD: High-Resolution Semantic Disentangled 3D Human Generation (Peng Zheng et al., 2024)</a></li><li><a href=#93104--93298-monkeypox-disease-recognition-model-based-on-improved-se-inceptionv3-junzhuo-chen-et-al-2024>(93/104 | 93/298) Monkeypox disease recognition model based on improved SE-InceptionV3 (Junzhuo Chen et al., 2024)</a></li><li><a href=#94104--94298-learning-physical-dynamics-for-object-centric-visual-prediction-huilin-xu-et-al-2024>(94/104 | 94/298) Learning Physical Dynamics for Object-centric Visual Prediction (Huilin Xu et al., 2024)</a></li><li><a href=#95104--95298-lifelong-person-re-identification-with-backward-compatibility-minyoung-oh-et-al-2024>(95/104 | 95/298) Lifelong Person Re-Identification with Backward-Compatibility (Minyoung Oh et al., 2024)</a></li><li><a href=#96104--96298-linear-optimal-transport-subspaces-for-point-set-classification-mohammad-shifat-e-rabbi-et-al-2024>(96/104 | 96/298) Linear optimal transport subspaces for point set classification (Mohammad Shifat E Rabbi et al., 2024)</a></li><li><a href=#97104--97298-medpnet-achieving-high-precision-adaptive-registration-for-complex-die-castings-yu-du-et-al-2024>(97/104 | 97/298) MEDPNet: Achieving High-Precision Adaptive Registration for Complex Die Castings (Yu Du et al., 2024)</a></li><li><a href=#98104--98298-thermal-nerf-neural-radiance-fields-from-an-infrared-camera-tianxiang-ye-et-al-2024>(98/104 | 98/298) Thermal-NeRF: Neural Radiance Fields from an Infrared Camera (Tianxiang Ye et al., 2024)</a></li><li><a href=#99104--99298-neca-neural-customizable-human-avatar-junjin-xiao-et-al-2024>(99/104 | 99/298) NECA: Neural Customizable Human Avatar (Junjin Xiao et al., 2024)</a></li><li><a href=#100104--100298-coreecho-continuous-representation-learning-for-2dtime-echocardiography-analysis-fadillah-adamsyah-maani-et-al-2024>(100/104 | 100/298) CoReEcho: Continuous Representation Learning for 2D+time Echocardiography Analysis (Fadillah Adamsyah Maani et al., 2024)</a></li><li><a href=#101104--101298-kp-red-exploiting-semantic-keypoints-for-joint-3d-shape-retrieval-and-deformation-ruida-zhang-et-al-2024>(101/104 | 101/298) KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and Deformation (Ruida Zhang et al., 2024)</a></li><li><a href=#102104--102298-texture-gs-disentangling-the-geometry-and-texture-for-3d-gaussian-splatting-editing-tian-xing-xu-et-al-2024>(102/104 | 102/298) Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing (Tian-Xing Xu et al., 2024)</a></li><li><a href=#103104--103298-urs-nerf-unordered-rolling-shutter-bundle-adjustment-for-neural-radiance-fields-bo-xu-et-al-2024>(103/104 | 103/298) URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural Radiance Fields (Bo Xu et al., 2024)</a></li><li><a href=#104104--104298-skeleton-based-human-action-recognition-with-noisy-labels-yi-xu-et-al-2024>(104/104 | 104/298) Skeleton-Based Human Action Recognition with Noisy Labels (Yi Xu et al., 2024)</a></li></ul></li><li><a href=#cscl-28>cs.CL (28)</a><ul><li><a href=#128--105298-team-trifecta-at-factify5wqa-setting-the-standard-in-fact-verification-with-fine-tuning-shang-hsuan-chiang-et-al-2024>(1/28 | 105/298) Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning (Shang-Hsuan Chiang et al., 2024)</a></li><li><a href=#228--106298-whose-side-are-you-on-investigating-the-political-stance-of-large-language-models-pagnarasmey-pit-et-al-2024>(2/28 | 106/298) Whose Side Are You On? Investigating the Political Stance of Large Language Models (Pagnarasmey Pit et al., 2024)</a></li><li><a href=#328--107298-read-between-the-lines----functionality-extraction-from-readmes-prince-kumar-et-al-2024>(3/28 | 107/298) Read between the lines &ndash; Functionality Extraction From READMEs (Prince Kumar et al., 2024)</a></li><li><a href=#428--108298-triple-gnns-introducing-syntactic-and-semantic-information-for-conversational-aspect-based-quadruple-sentiment-analysis-binbin-li-et-al-2024>(4/28 | 108/298) Triple GNNs: Introducing Syntactic and Semantic Information for Conversational Aspect-Based Quadruple Sentiment Analysis (Binbin Li et al., 2024)</a></li><li><a href=#528--109298-enhancing-llm-factual-accuracy-with-rag-to-counter-hallucinations-a-case-study-on-domain-specific-queries-in-private-knowledge-bases-jiarui-li-et-al-2024>(5/28 | 109/298) Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases (Jiarui Li et al., 2024)</a></li><li><a href=#628--110298-trisum-learning-summarization-ability-from-large-language-models-with-structured-rationale-pengcheng-jiang-et-al-2024>(6/28 | 110/298) TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale (Pengcheng Jiang et al., 2024)</a></li><li><a href=#728--111298-dragin-dynamic-retrieval-augmented-generation-based-on-the-real-time-information-needs-of-large-language-models-weihang-su-et-al-2024>(7/28 | 111/298) DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models (Weihang Su et al., 2024)</a></li><li><a href=#828--112298-intent-conditioned-and-non-toxic-counterspeech-generation-using-multi-task-instruction-tuning-with-rlaif-amey-hengle-et-al-2024>(8/28 | 112/298) Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF (Amey Hengle et al., 2024)</a></li><li><a href=#928--113298-investigating-grammatical-abstraction-in-language-models-using-few-shot-learning-of-novel-noun-gender-priyanka-sukumaran-et-al-2024>(9/28 | 113/298) Investigating grammatical abstraction in language models using few-shot learning of novel noun gender (Priyanka Sukumaran et al., 2024)</a></li><li><a href=#1028--114298-exams-v-a-multi-discipline-multilingual-multimodal-exam-benchmark-for-evaluating-vision-language-models-rocktim-jyoti-das-et-al-2024>(10/28 | 114/298) EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models (Rocktim Jyoti Das et al., 2024)</a></li><li><a href=#1128--115298-uncovering-latent-themes-of-messaging-on-social-media-by-integrating-llms-a-case-study-on-climate-campaigns-tunazzina-islam-et-al-2024>(11/28 | 115/298) Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns (Tunazzina Islam et al., 2024)</a></li><li><a href=#1228--116298-explorer-exploration-guided-reasoning-for-textual-reinforcement-learning-kinjal-basu-et-al-2024>(12/28 | 116/298) EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning (Kinjal Basu et al., 2024)</a></li><li><a href=#1328--117298-neural-erosion-emulating-controlled-neurodegeneration-and-aging-in-ai-systems-antonios-alexos-et-al-2024>(13/28 | 117/298) Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems (Antonios Alexos et al., 2024)</a></li><li><a href=#1428--118298-raft-adapting-language-model-to-domain-specific-rag-tianjun-zhang-et-al-2024>(14/28 | 118/298) RAFT: Adapting Language Model to Domain Specific RAG (Tianjun Zhang et al., 2024)</a></li><li><a href=#1528--119298-dont-half-listen-capturing-key-part-information-in-continual-instruction-tuning-yongquan-he-et-al-2024>(15/28 | 119/298) Don&rsquo;t Half-listen: Capturing Key-part Information in Continual Instruction Tuning (Yongquan He et al., 2024)</a></li><li><a href=#1628--120298-uni-smart-universal-science-multimodal-analysis-and-research-transformer-hengxing-cai-et-al-2024>(16/28 | 120/298) Uni-SMART: Universal Science Multimodal Analysis and Research Transformer (Hengxing Cai et al., 2024)</a></li><li><a href=#1728--121298-take-care-of-your-prompt-bias-investigating-and-mitigating-prompt-bias-in-factual-knowledge-extraction-ziyang-xu-et-al-2024>(17/28 | 121/298) Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction (Ziyang Xu et al., 2024)</a></li><li><a href=#1828--122298-is-translation-all-you-need-a-study-on-solving-multilingual-tasks-with-large-language-models-chaoqun-liu-et-al-2024>(18/28 | 122/298) Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models (Chaoqun Liu et al., 2024)</a></li><li><a href=#1928--123298-lost-in-overlap-exploring-watermark-collision-in-llms-yiyang-luo-et-al-2024>(19/28 | 123/298) Lost in Overlap: Exploring Watermark Collision in LLMs (Yiyang Luo et al., 2024)</a></li><li><a href=#2028--124298-think-twice-before-assure-confidence-estimation-for-large-language-models-through-reflection-on-multiple-answers-moxin-li-et-al-2024>(20/28 | 124/298) Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers (Moxin Li et al., 2024)</a></li><li><a href=#2128--125298-myte-morphology-driven-byte-encoding-for-better-and-fairer-multilingual-language-modeling-tomasz-limisiewicz-et-al-2024>(21/28 | 125/298) MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling (Tomasz Limisiewicz et al., 2024)</a></li><li><a href=#2228--126298-cdgp-automatic-cloze-distractor-generation-based-on-pre-trained-language-model-shang-hsuan-chiang-et-al-2024>(22/28 | 126/298) CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model (Shang-Hsuan Chiang et al., 2024)</a></li><li><a href=#2328--127298-a-question-on-the-explainability-of-large-language-models-and-the-word-level-univariate-first-order-plausibility-assumption-jeremie-bogaert-et-al-2024>(23/28 | 127/298) A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption (Jeremie Bogaert et al., 2024)</a></li><li><a href=#2428--128298-a-big-data-approach-to-understand-sub-national-determinants-of-fdi-in-africa-a-fronzetti-colladon-et-al-2024>(24/28 | 128/298) A Big Data Approach to Understand Sub-national Determinants of FDI in Africa (A. Fronzetti Colladon et al., 2024)</a></li><li><a href=#2528--129298-enhanced-coherence-aware-network-with-hierarchical-disentanglement-for-aspect-category-sentiment-analysis-jin-cui-et-al-2024>(25/28 | 129/298) Enhanced Coherence-Aware Network with Hierarchical Disentanglement for Aspect-Category Sentiment Analysis (Jin Cui et al., 2024)</a></li><li><a href=#2628--130298-identifying-health-risks-from-family-history-a-survey-of-natural-language-processing-techniques-xiang-dai-et-al-2024>(26/28 | 130/298) Identifying Health Risks from Family History: A Survey of Natural Language Processing Techniques (Xiang Dai et al., 2024)</a></li><li><a href=#2728--131298-maibaam-a-multi-dialectal-bavarian-universal-dependency-treebank-verena-blaschke-et-al-2024>(27/28 | 131/298) MaiBaam: A Multi-Dialectal Bavarian Universal Dependency Treebank (Verena Blaschke et al., 2024)</a></li><li><a href=#2828--132298-a-comprehensive-study-on-frequent-pattern-mining-and-clustering-categories-for-topic-detection-in-persian-text-stream-elnaz-zafarani-moattar-et-al-2024>(28/28 | 132/298) A comprehensive study on Frequent Pattern Mining and Clustering categories for topic detection in Persian text stream (Elnaz Zafarani-Moattar et al., 2024)</a></li></ul></li><li><a href=#csir-5>cs.IR (5)</a><ul><li><a href=#15--133298-the-whole-is-better-than-the-sum-using-aggregated-demonstrations-in-in-context-learning-for-sequential-recommendation-lei-wang-et-al-2024>(1/5 | 133/298) The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation (Lei Wang et al., 2024)</a></li><li><a href=#25--134298-a-thorough-comparison-of-cross-encoders-and-llms-for-reranking-splade-hervé-déjean-et-al-2024>(2/5 | 134/298) A Thorough Comparison of Cross-Encoders and LLMs for Reranking SPLADE (Hervé Déjean et al., 2024)</a></li><li><a href=#35--135298-towards-unified-multi-modal-personalization-large-vision-language-models-for-generative-recommendation-and-beyond-tianxin-wei-et-al-2024>(3/5 | 135/298) Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond (Tianxin Wei et al., 2024)</a></li><li><a href=#45--136298-ppm--a-pre-trained-plug-in-model-for-click-through-rate-prediction-yuanbo-gao-et-al-2024>(4/5 | 136/298) PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction (Yuanbo Gao et al., 2024)</a></li><li><a href=#55--137298-enriching-user-shopping-history-empowering-e-commerce-with-a-hierarchical-recommendation-system-irem-islek-et-al-2024>(5/5 | 137/298) Enriching User Shopping History: Empowering E-commerce with a Hierarchical Recommendation System (Irem Islek et al., 2024)</a></li></ul></li><li><a href=#cslg-36>cs.LG (36)</a><ul><li><a href=#136--138298-generation-is-better-than-modification-combating-high-class-homophily-variance-in-graph-anomaly-detection-rui-zhang-et-al-2024>(1/36 | 138/298) Generation is better than Modification: Combating High Class Homophily Variance in Graph Anomaly Detection (Rui Zhang et al., 2024)</a></li><li><a href=#236--139298-functional-graph-convolutional-networks-a-unified-multi-task-and-multi-modal-learning-framework-to-facilitate-health-and-social-care-insights-tobia-boschi-et-al-2024>(2/36 | 139/298) Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights (Tobia Boschi et al., 2024)</a></li><li><a href=#336--140298-perl-parameter-efficient-reinforcement-learning-from-human-feedback-hakim-sidahmed-et-al-2024>(3/36 | 140/298) PERL: Parameter Efficient Reinforcement Learning from Human Feedback (Hakim Sidahmed et al., 2024)</a></li><li><a href=#436--141298-benchmarking-zero-shot-robustness-of-multimodal-foundation-models-a-pilot-study-chenguang-wang-et-al-2024>(4/36 | 141/298) Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study (Chenguang Wang et al., 2024)</a></li><li><a href=#536--142298-counterfactual-analysis-of-neural-networks-used-to-create-fertilizer-management-zones-giorgio-morales-et-al-2024>(5/36 | 142/298) Counterfactual Analysis of Neural Networks Used to Create Fertilizer Management Zones (Giorgio Morales et al., 2024)</a></li><li><a href=#636--143298-from-chaos-to-clarity-time-series-anomaly-detection-in-astronomical-observations-xinli-hao-et-al-2024>(6/36 | 143/298) From Chaos to Clarity: Time Series Anomaly Detection in Astronomical Observations (Xinli Hao et al., 2024)</a></li><li><a href=#736--144298-comprehensive-study-of-predictive-maintenance-in-industries-using-classification-models-and-lstm-model-saket-maheshwari-et-al-2024>(7/36 | 144/298) Comprehensive Study Of Predictive Maintenance In Industries Using Classification Models And LSTM Model (Saket Maheshwari et al., 2024)</a></li><li><a href=#836--145298-a-short-survey-on-importance-weighting-for-machine-learning-masanari-kimura-et-al-2024>(8/36 | 145/298) A Short Survey on Importance Weighting for Machine Learning (Masanari Kimura et al., 2024)</a></li><li><a href=#936--146298-towards-adversarially-robust-dataset-distillation-by-curvature-regularization-eric-xue-et-al-2024>(9/36 | 146/298) Towards Adversarially Robust Dataset Distillation by Curvature Regularization (Eric Xue et al., 2024)</a></li><li><a href=#1036--147298-regret-minimization-via-saddle-point-optimization-johannes-kirschner-et-al-2024>(10/36 | 147/298) Regret Minimization via Saddle Point Optimization (Johannes Kirschner et al., 2024)</a></li><li><a href=#1136--148298-less-is-more-one-shot-subgraph-reasoning-on-large-scale-knowledge-graphs-zhanke-zhou-et-al-2024>(11/36 | 148/298) Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs (Zhanke Zhou et al., 2024)</a></li><li><a href=#1236--149298-dipaco-distributed-path-composition-arthur-douillard-et-al-2024>(12/36 | 149/298) DiPaCo: Distributed Path Composition (Arthur Douillard et al., 2024)</a></li><li><a href=#1336--150298-open-continual-feature-selection-via-granular-ball-knowledge-transfer-xuemei-cao-et-al-2024>(13/36 | 150/298) Open Continual Feature Selection via Granular-Ball Knowledge Transfer (Xuemei Cao et al., 2024)</a></li><li><a href=#1436--151298-discovering-invariant-neighborhood-patterns-for-heterophilic-graphs-ruihao-zhang-et-al-2024>(14/36 | 151/298) Discovering Invariant Neighborhood Patterns for Heterophilic Graphs (Ruihao Zhang et al., 2024)</a></li><li><a href=#1536--152298-prediction-of-vessel-arrival-time-to-pilotage-area-using-multi-data-fusion-and-deep-learning-xiaocai-zhang-et-al-2024>(15/36 | 152/298) Prediction of Vessel Arrival Time to Pilotage Area Using Multi-Data Fusion and Deep Learning (Xiaocai Zhang et al., 2024)</a></li><li><a href=#1636--153298-online-gnn-evaluation-under-test-time-graph-distribution-shifts-xin-zheng-et-al-2024>(16/36 | 153/298) Online GNN Evaluation Under Test-time Graph Distribution Shifts (Xin Zheng et al., 2024)</a></li><li><a href=#1736--154298-horizon-free-regret-for-linear-markov-decision-processes-zihan-zhang-et-al-2024>(17/36 | 154/298) Horizon-Free Regret for Linear Markov Decision Processes (Zihan Zhang et al., 2024)</a></li><li><a href=#1836--155298-introducing-adaptive-continuous-adversarial-training-acat-to-enhance-ml-robustness-mohamed-elshehaby-et-al-2024>(18/36 | 155/298) Introducing Adaptive Continuous Adversarial Training (ACAT) to Enhance ML Robustness (Mohamed elShehaby et al., 2024)</a></li><li><a href=#1936--156298-optimal-block-level-draft-verification-for-accelerating-speculative-decoding-ziteng-sun-et-al-2024>(19/36 | 156/298) Optimal Block-Level Draft Verification for Accelerating Speculative Decoding (Ziteng Sun et al., 2024)</a></li><li><a href=#2036--157298-meta-operator-for-complex-query-answering-on-knowledge-graphs-hang-yin-et-al-2024>(20/36 | 157/298) Meta Operator for Complex Query Answering on Knowledge Graphs (Hang Yin et al., 2024)</a></li><li><a href=#2136--158298-variance-dependent-regret-bounds-for-non-stationary-linear-bandits-zhiyong-wang-et-al-2024>(21/36 | 158/298) Variance-Dependent Regret Bounds for Non-stationary Linear Bandits (Zhiyong Wang et al., 2024)</a></li><li><a href=#2236--159298-improving-fairness-in-credit-lending-models-using-subgroup-threshold-optimization-cecilia-ying-et-al-2024>(22/36 | 159/298) Improving Fairness in Credit Lending Models using Subgroup Threshold Optimization (Cecilia Ying et al., 2024)</a></li><li><a href=#2336--160298-using-uncertainty-quantification-to-characterize-and-improve-out-of-domain-learning-for-pdes-s-chandra-mouli-et-al-2024>(23/36 | 160/298) Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs (S. Chandra Mouli et al., 2024)</a></li><li><a href=#2436--161298-a-resource-constrained-stochastic-scheduling-algorithm-for-homeless-street-outreach-and-gleaning-edible-food-conor-m-artman-et-al-2024>(24/36 | 161/298) A resource-constrained stochastic scheduling algorithm for homeless street outreach and gleaning edible food (Conor M. Artman et al., 2024)</a></li><li><a href=#2536--162298-a-comparative-study-on-machine-learning-approaches-for-rock-mass-classification-using-drilling-data-tom-f-hansen-et-al-2024>(25/36 | 162/298) A comparative study on machine learning approaches for rock mass classification using drilling data (Tom F. Hansen et al., 2024)</a></li><li><a href=#2636--163298-towards-non-adversarial-algorithmic-recourse-tobias-leemann-et-al-2024>(26/36 | 163/298) Towards Non-Adversarial Algorithmic Recourse (Tobias Leemann et al., 2024)</a></li><li><a href=#2736--164298-reliable-uncertainty-with-cheaper-neural-network-ensembles-a-case-study-in-industrial-parts-classification-arthur-thuy-et-al-2024>(27/36 | 164/298) Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification (Arthur Thuy et al., 2024)</a></li><li><a href=#2836--165298-explainability-through-uncertainty-trustworthy-decision-making-with-neural-networks-arthur-thuy-et-al-2024>(28/36 | 165/298) Explainability through uncertainty: Trustworthy decision-making with neural networks (Arthur Thuy et al., 2024)</a></li><li><a href=#2936--166298-online-policy-learning-from-offline-preferences-guoxi-zhang-et-al-2024>(29/36 | 166/298) Online Policy Learning from Offline Preferences (Guoxi Zhang et al., 2024)</a></li><li><a href=#3036--167298-regularization-based-efficient-continual-learning-in-deep-state-space-models-yuanhang-zhang-et-al-2024>(30/36 | 167/298) Regularization-Based Efficient Continual Learning in Deep State-Space Models (Yuanhang Zhang et al., 2024)</a></li><li><a href=#3136--168298-adaptive-random-feature-regularization-on-fine-tuning-deep-neural-networks-shinya-yamaguchi-et-al-2024>(31/36 | 168/298) Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks (Shin&rsquo;ya Yamaguchi et al., 2024)</a></li><li><a href=#3236--169298-unified-projection-free-algorithms-for-adversarial-dr-submodular-optimization-mohammad-pedramfar-et-al-2024>(32/36 | 169/298) Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization (Mohammad Pedramfar et al., 2024)</a></li><li><a href=#3336--170298-global-convergence-guarantees-for-federated-policy-gradient-methods-with-adversaries-swetha-ganesh-et-al-2024>(33/36 | 170/298) Global Convergence Guarantees for Federated Policy Gradient Methods with Adversaries (Swetha Ganesh et al., 2024)</a></li><li><a href=#3436--171298-quality-diversity-actor-critic-learning-high-performing-and-diverse-behaviors-via-value-and-successor-features-critics-luca-grillotti-et-al-2024>(34/36 | 171/298) Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics (Luca Grillotti et al., 2024)</a></li><li><a href=#3536--172298-backdoor-secrets-unveiled-identifying-backdoor-data-with-optimized-scaled-prediction-consistency-soumyadeep-pal-et-al-2024>(35/36 | 172/298) Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency (Soumyadeep Pal et al., 2024)</a></li><li><a href=#3636--173298-a-survey-of-source-code-representations-for-machine-learning-based-cybersecurity-tasks-beatrice-casey-et-al-2024>(36/36 | 173/298) A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks (Beatrice Casey et al., 2024)</a></li></ul></li><li><a href=#csse-6>cs.SE (6)</a><ul><li><a href=#16--174298-s3llm-large-scale-scientific-software-understanding-with-llms-using-source-metadata-and-document-kareem-shaik-et-al-2024>(1/6 | 174/298) S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document (Kareem Shaik et al., 2024)</a></li><li><a href=#26--175298-repoformer-selective-retrieval-for-repository-level-code-completion-di-wu-et-al-2024>(2/6 | 175/298) Repoformer: Selective Retrieval for Repository-Level Code Completion (Di Wu et al., 2024)</a></li><li><a href=#36--176298-large-language-models-to-generate-system-level-test-programs-targeting-non-functional-properties-denis-schwachhofer-et-al-2024>(3/6 | 176/298) Large Language Models to Generate System-Level Test Programs Targeting Non-functional Properties (Denis Schwachhofer et al., 2024)</a></li><li><a href=#46--177298-demystifying-faulty-code-with-llm-step-by-step-reasoning-for-explainable-fault-localization-ratnadira-widyasari-et-al-2024>(4/6 | 177/298) Demystifying Faulty Code with LLM: Step-by-Step Reasoning for Explainable Fault Localization (Ratnadira Widyasari et al., 2024)</a></li><li><a href=#56--178298-an-empirical-study-on-developers-shared-conversations-with-chatgpt-in-github-pull-requests-and-issues-huizi-hao-et-al-2024>(5/6 | 178/298) An Empirical Study on Developers Shared Conversations with ChatGPT in GitHub Pull Requests and Issues (Huizi Hao et al., 2024)</a></li><li><a href=#66--179298-exploring-language-models-code-generation-ability-with-auxiliary-functions-seonghyeon-lee-et-al-2024>(6/6 | 179/298) Exploring Language Model&rsquo;s Code Generation Ability with Auxiliary Functions (Seonghyeon Lee et al., 2024)</a></li></ul></li><li><a href=#csdc-4>cs.DC (4)</a><ul><li><a href=#14--180298-atom-asynchronous-training-of-massive-models-for-deep-learning-in-a-decentralized-environment-xiaofeng-wu-et-al-2024>(1/4 | 180/298) ATOM: Asynchronous Training of Massive Models for Deep Learning in a Decentralized Environment (Xiaofeng Wu et al., 2024)</a></li><li><a href=#24--181298-dsp-dynamic-sequence-parallelism-for-multi-dimensional-transformers-xuanlei-zhao-et-al-2024>(2/4 | 181/298) DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers (Xuanlei Zhao et al., 2024)</a></li><li><a href=#34--182298-greedyml-a-parallel-algorithm-for-maximizing-submodular-functions-shivaram-gopal-et-al-2024>(3/4 | 182/298) GreedyML: A Parallel Algorithm for Maximizing Submodular Functions (Shivaram Gopal et al., 2024)</a></li><li><a href=#44--183298-strict-partitioning-for-sporadic-rigid-gang-tasks-binqi-sun-et-al-2024>(4/4 | 183/298) Strict Partitioning for Sporadic Rigid Gang Tasks (Binqi Sun et al., 2024)</a></li></ul></li><li><a href=#csro-25>cs.RO (25)</a><ul><li><a href=#125--184298-grasp-anything-combining-teacher-augmented-policy-gradient-learning-with-instance-segmentation-to-grasp-arbitrary-objects-malte-mosbach-et-al-2024>(1/25 | 184/298) Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning with Instance Segmentation to Grasp Arbitrary Objects (Malte Mosbach et al., 2024)</a></li><li><a href=#225--185298-spiking-neural-networks-for-fast-moving-object-detection-on-neuromorphic-hardware-devices-using-an-event-based-camera-andreas-ziegler-et-al-2024>(2/25 | 185/298) Spiking Neural Networks for Fast-Moving Object Detection on Neuromorphic Hardware Devices Using an Event-Based Camera (Andreas Ziegler et al., 2024)</a></li><li><a href=#325--186298-language-to-map-topological-map-generation-from-natural-language-path-instructions-hideki-deguchi-et-al-2024>(3/25 | 186/298) Language to Map: Topological map generation from natural language path instructions (Hideki Deguchi et al., 2024)</a></li><li><a href=#425--187298-autonomous-monitoring-of-pharmaceutical-rd-laboratories-with-6-axis-arm-equipped-quadruped-robot-and-generative-ai-a-preliminary-study-shunichi-hato-et-al-2024>(4/25 | 187/298) Autonomous Monitoring of Pharmaceutical R&amp;D Laboratories with 6 Axis Arm Equipped Quadruped Robot and Generative AI: A Preliminary Study (Shunichi Hato et al., 2024)</a></li><li><a href=#525--188298-her-drlheterogeneous-relational-deep-reinforcement-learning-for-decentralized-multi-robot-crowd-navigation-xinyu-zhou-et-al-2024>(5/25 | 188/298) HeR-DRL:Heterogeneous Relational Deep Reinforcement Learning for Decentralized Multi-Robot Crowd Navigation (Xinyu Zhou et al., 2024)</a></li><li><a href=#625--189298-mind-the-error-detection-and-localization-of-instruction-errors-in-vision-and-language-navigation-francesco-taioli-et-al-2024>(6/25 | 189/298) Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation (Francesco Taioli et al., 2024)</a></li><li><a href=#725--190298-virtual-elastic-tether-a-new-approach-for-multi-agent-navigation-in-confined-aquatic-environments-kanzhong-yao-et-al-2024>(7/25 | 190/298) Virtual Elastic Tether: a New Approach for Multi-agent Navigation in Confined Aquatic Environments (Kanzhong Yao et al., 2024)</a></li><li><a href=#825--191298-humanoidbench-simulated-humanoid-benchmark-for-whole-body-locomotion-and-manipulation-carmelo-sferrazza-et-al-2024>(8/25 | 191/298) HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation (Carmelo Sferrazza et al., 2024)</a></li><li><a href=#925--192298-online-concurrent-multi-robot-coverage-path-planning-ratijit-mitra-et-al-2024>(9/25 | 192/298) Online Concurrent Multi-Robot Coverage Path Planning (Ratijit Mitra et al., 2024)</a></li><li><a href=#1025--193298-incentive-compatible-and-distributed-allocation-for-robotic-service-provision-through-contract-theory-yuhan-zhao-et-al-2024>(10/25 | 193/298) Incentive-Compatible and Distributed Allocation for Robotic Service Provision Through Contract Theory (Yuhan Zhao et al., 2024)</a></li><li><a href=#1125--194298-reconfigurable-robot-identification-from-motion-data-yuhang-hu-et-al-2024>(11/25 | 194/298) Reconfigurable Robot Identification from Motion Data (Yuhang Hu et al., 2024)</a></li><li><a href=#1225--195298-partially-observable-task-and-motion-planning-with-uncertainty-and-risk-awareness-aidan-curtis-et-al-2024>(12/25 | 195/298) Partially Observable Task and Motion Planning with Uncertainty and Risk Awareness (Aidan Curtis et al., 2024)</a></li><li><a href=#1325--196298-collaborative-aquatic-positioning-system-utilising-multi-beam-sonar-and-depth-sensors-xueliang-cheng-et-al-2024>(13/25 | 196/298) Collaborative Aquatic Positioning System Utilising Multi-beam Sonar and Depth Sensors ({Xueliang Cheng et al., 2024)</a></li><li><a href=#1425--197298-offline-goal-conditioned-reinforcement-learning-for-shape-control-of-deformable-linear-objects-rita-laezza-et-al-2024>(14/25 | 197/298) Offline Goal-Conditioned Reinforcement Learning for Shape Control of Deformable Linear Objects (Rita Laezza et al., 2024)</a></li><li><a href=#1525--198298-belief-aided-navigation-using-bayesian-reinforcement-learning-for-avoiding-humans-in-blind-spots-jinyeob-kim-et-al-2024>(15/25 | 198/298) Belief Aided Navigation using Bayesian Reinforcement Learning for Avoiding Humans in Blind Spots (Jinyeob Kim et al., 2024)</a></li><li><a href=#1625--199298-agile-and-safe-trajectory-planning-for-quadruped-navigation-with-motion-anisotropy-awareness-wentao-zhang-et-al-2024>(16/25 | 199/298) Agile and Safe Trajectory Planning for Quadruped Navigation with Motion Anisotropy Awareness (Wentao Zhang et al., 2024)</a></li><li><a href=#1725--200298-geopro-vo-dynamic-obstacle-avoidance-with-geometric-projector-based-on-velocity-obstacle-jihao-huang-et-al-2024>(17/25 | 200/298) GeoPro-VO: Dynamic Obstacle Avoidance with Geometric Projector Based on Velocity Obstacle (Jihao Huang et al., 2024)</a></li><li><a href=#1825--201298-advancing-object-goal-navigation-through-llm-enhanced-object-affinities-transfer-mengying-lin-et-al-2024>(18/25 | 201/298) Advancing Object Goal Navigation Through LLM-enhanced Object Affinities Transfer (Mengying Lin et al., 2024)</a></li><li><a href=#1925--202298-design-and-control-co-optimization-for-automated-design-iteration-of-dexterous-anthropomorphic-soft-robotic-hands-pragna-mannam-et-al-2024>(19/25 | 202/298) Design and Control Co-Optimization for Automated Design Iteration of Dexterous Anthropomorphic Soft Robotic Hands (Pragna Mannam et al., 2024)</a></li><li><a href=#2025--203298-stackelberg-meta-learning-based-shared-control-for-assistive-driving-yuhan-zhao-et-al-2024>(20/25 | 203/298) Stackelberg Meta-Learning Based Shared Control for Assistive Driving (Yuhan Zhao et al., 2024)</a></li><li><a href=#2125--204298-latent-object-characteristics-recognition-with-visual-to-haptic-audio-cross-modal-transfer-learning-namiko-saito-et-al-2024>(21/25 | 204/298) Latent Object Characteristics Recognition with Visual to Haptic-Audio Cross-modal Transfer Learning (Namiko Saito et al., 2024)</a></li><li><a href=#2225--205298-towards-embedding-dynamic-personas-in-interactive-robots-masquerading-animated-social-kinematics-mask-jeongeun-park-et-al-2024>(22/25 | 205/298) Towards Embedding Dynamic Personas in Interactive Robots: Masquerading Animated Social Kinematics (MASK) (Jeongeun Park et al., 2024)</a></li><li><a href=#2325--206298-interactive-distance-field-mapping-and-planning-to-enable-human-robot-collaboration-usama-ali-et-al-2024>(23/25 | 206/298) Interactive Distance Field Mapping and Planning to Enable Human-Robot Collaboration (Usama Ali et al., 2024)</a></li><li><a href=#2425--207298-riemannian-flow-matching-policy-for-robot-motion-learning-max-braun-et-al-2024>(24/25 | 207/298) Riemannian Flow Matching Policy for Robot Motion Learning (Max Braun et al., 2024)</a></li><li><a href=#2525--208298-do-visual-language-maps-capture-latent-semantics-matti-pekkanen-et-al-2024>(25/25 | 208/298) Do Visual-Language Maps Capture Latent Semantics? (Matti Pekkanen et al., 2024)</a></li></ul></li><li><a href=#q-fincp-1>q-fin.CP (1)</a><ul><li><a href=#11--209298-can-a-gpt4-powered-ai-agent-be-a-good-enough-performance-attribution-analyst-bruno-de-melo-2024>(1/1 | 209/298) Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst? (Bruno de Melo, 2024)</a></li></ul></li><li><a href=#eesssp-3>eess.SP (3)</a><ul><li><a href=#13--210298-process-and-forward-deep-joint-source-channel-coding-over-cooperative-relay-networks-chenghong-bian-et-al-2024>(1/3 | 210/298) Process-and-Forward: Deep Joint Source-Channel Coding Over Cooperative Relay Networks (Chenghong Bian et al., 2024)</a></li><li><a href=#23--211298-multi-source-localization-and-data-association-for-time-difference-of-arrival-measurements-gabrielle-flood-et-al-2024>(2/3 | 211/298) Multi-Source Localization and Data Association for Time-Difference of Arrival Measurements (Gabrielle Flood et al., 2024)</a></li><li><a href=#33--212298-decentralizing-coherent-joint-transmission-precoding-via-deterministic-equivalents-yuhao-liu-et-al-2024>(3/3 | 212/298) Decentralizing Coherent Joint Transmission Precoding via Deterministic Equivalents (Yuhao Liu et al., 2024)</a></li></ul></li><li><a href=#csai-8>cs.AI (8)</a><ul><li><a href=#18--213298-are-llms-good-cryptic-crossword-solvers-abdelrahman-boda-sadallah-et-al-2024>(1/8 | 213/298) Are LLMs Good Cryptic Crossword Solvers? (Abdelrahman &lsquo;Boda&rsquo; Sadallah et al., 2024)</a></li><li><a href=#28--214298-autonode-a-neuro-graphic-self-learnable-engine-for-cognitive-gui-automation-arkajit-datta-et-al-2024>(2/8 | 214/298) AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation (Arkajit Datta et al., 2024)</a></li><li><a href=#38--215298-gradient-based-feature-attribution-in-explainable-ai-a-technical-review-yongjie-wang-et-al-2024>(3/8 | 215/298) Gradient based Feature Attribution in Explainable AI: A Technical Review (Yongjie Wang et al., 2024)</a></li><li><a href=#48--216298-development-and-application-of-a-monte-carlo-tree-search-algorithm-for-simulating-da-vinci-code-game-strategies-ye-zhang-et-al-2024>(4/8 | 216/298) Development and Application of a Monte Carlo Tree Search Algorithm for Simulating Da Vinci Code Game Strategies (Ye Zhang et al., 2024)</a></li><li><a href=#58--217298-a-survey-on-game-playing-agents-and-large-models-methods-applications-and-challenges-xinrun-xu-et-al-2024>(5/8 | 217/298) A Survey on Game Playing Agents and Large Models: Methods, Applications, and Challenges (Xinrun Xu et al., 2024)</a></li><li><a href=#68--218298-single--and-multi-agent-private-active-sensing-a-deep-neuroevolution-approach-george-stamatelis-et-al-2024>(6/8 | 218/298) Single- and Multi-Agent Private Active Sensing: A Deep Neuroevolution Approach (George Stamatelis et al., 2024)</a></li><li><a href=#78--219298-lifted-causal-inference-in-relational-domains-malte-luttermann-et-al-2024>(7/8 | 219/298) Lifted Causal Inference in Relational Domains (Malte Luttermann et al., 2024)</a></li><li><a href=#88--220298-efficient-detection-of-exchangeable-factors-in-factor-graphs-malte-luttermann-et-al-2024>(8/8 | 220/298) Efficient Detection of Exchangeable Factors in Factor Graphs (Malte Luttermann et al., 2024)</a></li></ul></li><li><a href=#cscr-11>cs.CR (11)</a><ul><li><a href=#111--221298-federated-learning-with-anomaly-detection-via-gradient-and-reconstruction-analysis-zahir-alsulaimawi-2024>(1/11 | 221/298) Federated Learning with Anomaly Detection via Gradient and Reconstruction Analysis (Zahir Alsulaimawi, 2024)</a></li><li><a href=#211--222298-socialgenpod-privacy-friendly-generative-ai-social-web-applications-with-decentralised-personal-data-stores-vidminas-vizgirda-et-al-2024>(2/11 | 222/298) SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores (Vidminas Vizgirda et al., 2024)</a></li><li><a href=#311--223298-securing-federated-learning-with-control-flow-attestation-a-novel-framework-for-enhanced-integrity-and-resilience-against-adversarial-attacks-zahir-alsulaimawi-2024>(3/11 | 223/298) Securing Federated Learning with Control-Flow Attestation: A Novel Framework for Enhanced Integrity and Resilience against Adversarial Attacks (Zahir Alsulaimawi, 2024)</a></li><li><a href=#411--224298-ignore-me-but-dont-replace-me-utilizing-non-linguistic-elements-for-pretraining-on-the-cybersecurity-domain-eugene-jang-et-al-2024>(4/11 | 224/298) Ignore Me But Don&rsquo;t Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain (Eugene Jang et al., 2024)</a></li><li><a href=#511--225298-not-just-change-the-labels-learn-the-features-watermarking-deep-neural-networks-with-multi-view-data-yuxuan-li-et-al-2024>(5/11 | 225/298) Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data (Yuxuan Li et al., 2024)</a></li><li><a href=#611--226298-unsupervised-threat-hunting-using-continuous-bag-of-terms-and-time-cbott-varol-kayhan-et-al-2024>(6/11 | 226/298) Unsupervised Threat Hunting using Continuous Bag-of-Terms-and-Time (CBoTT) (Varol Kayhan et al., 2024)</a></li><li><a href=#711--227298-interactive-trimming-against-evasive-online-data-manipulation-attacks-a-game-theoretic-approach-yue-fu-et-al-2024>(7/11 | 227/298) Interactive Trimming against Evasive Online Data Manipulation Attacks: A Game-Theoretic Approach (Yue Fu et al., 2024)</a></li><li><a href=#811--228298-instance-optimal-clipping-for-summation-problems-in-the-shuffle-model-of-differential-privacy-wei-dong-et-al-2024>(8/11 | 228/298) Instance-optimal Clipping for Summation Problems in the Shuffle Model of Differential Privacy (Wei Dong et al., 2024)</a></li><li><a href=#911--229298-specification-and-enforcement-of-activity-dependency-policies-using-xacml-tanjila-mawla-et-al-2024>(9/11 | 229/298) Specification and Enforcement of Activity Dependency Policies using XACML (Tanjila Mawla et al., 2024)</a></li><li><a href=#1011--230298-time-frequency-jointed-imperceptible-adversarial-attack-to-brainprint-recognition-with-deep-learning-models-hangjie-yi-et-al-2024>(10/11 | 230/298) Time-Frequency Jointed Imperceptible Adversarial Attack to Brainprint Recognition with Deep Learning Models (Hangjie Yi et al., 2024)</a></li><li><a href=#1111--231298-search-based-ordered-password-generation-of-autoregressive-neural-networks-min-jin-et-al-2024>(11/11 | 231/298) Search-based Ordered Password Generation of Autoregressive Neural Networks (Min Jin et al., 2024)</a></li></ul></li><li><a href=#eessiv-12>eess.IV (12)</a><ul><li><a href=#112--232298-d-net-dynamic-large-kernel-with-dynamic-feature-fusion-for-volumetric-medical-image-segmentation-jin-yang-et-al-2024>(1/12 | 232/298) D-Net: Dynamic Large Kernel with Dynamic Feature Fusion for Volumetric Medical Image Segmentation (Jin Yang et al., 2024)</a></li><li><a href=#212--233298-overcoming-distribution-shifts-in-plug-and-play-methods-with-test-time-training-edward-p-chandler-et-al-2024>(2/12 | 233/298) Overcoming Distribution Shifts in Plug-and-Play Methods with Test-Time Training (Edward P. Chandler et al., 2024)</a></li><li><a href=#312--234298-histo-genomic-knowledge-distillation-for-cancer-prognosis-from-histopathology-whole-slide-images-zhikang-wang-et-al-2024>(3/12 | 234/298) Histo-Genomic Knowledge Distillation For Cancer Prognosis From Histopathology Whole Slide Images (Zhikang Wang et al., 2024)</a></li><li><a href=#412--235298-learning-on-jpeg-ldpc-compressed-images-classifying-with-syndromes-ahcen-aliouat-et-al-2024>(4/12 | 235/298) Learning on JPEG-LDPC Compressed Images: Classifying with Syndromes (Ahcen Aliouat et al., 2024)</a></li><li><a href=#512--236298-hybrid-convolutional-and-attention-network-for-hyperspectral-image-denoising-shuai-hu-et-al-2024>(5/12 | 236/298) Hybrid Convolutional and Attention Network for Hyperspectral Image Denoising (Shuai Hu et al., 2024)</a></li><li><a href=#612--237298-a-general-method-to-incorporate-spatial-information-into-loss-functions-for-gan-based-super-resolution-models-xijun-wang-et-al-2024>(6/12 | 237/298) A General Method to Incorporate Spatial Information into Loss Functions for GAN-based Super-resolution Models (Xijun Wang et al., 2024)</a></li><li><a href=#712--238298-how-suboptimal-is-training-rppg-models-with-videos-and-targets-from-different-body-sites-björn-braun-et-al-2024>(7/12 | 238/298) How Suboptimal is Training rPPG Models with Videos and Targets from Different Body Sites? (Björn Braun et al., 2024)</a></li><li><a href=#812--239298-cardiac-magnetic-resonance-2dt-short--and-long-axis-segmentation-via-spatio-temporal-sam-adaptation-zhennong-chen-et-al-2024>(8/12 | 239/298) Cardiac Magnetic Resonance 2D+T Short- and Long-axis Segmentation via Spatio-temporal SAM Adaptation (Zhennong Chen et al., 2024)</a></li><li><a href=#912--240298-attention-enhanced-hybrid-feature-aggregation-network-for-3d-brain-tumor-segmentation-ziya-ata-yazıcı-et-al-2024>(9/12 | 240/298) Attention-Enhanced Hybrid Feature Aggregation Network for 3D Brain Tumor Segmentation (Ziya Ata Yazıcı et al., 2024)</a></li><li><a href=#1012--241298-solving-general-noisy-inverse-problem-via-posterior-sampling-a-policy-gradient-viewpoint-haoyue-tang-et-al-2024>(10/12 | 241/298) Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint (Haoyue Tang et al., 2024)</a></li><li><a href=#1112--242298-cardiac-valve-event-timing-in-echocardiography-using-deep-learning-and-triplane-recordings-benjamin-strandli-fermann-et-al-2024>(11/12 | 242/298) Cardiac valve event timing in echocardiography using deep learning and triplane recordings (Benjamin Strandli Fermann et al., 2024)</a></li><li><a href=#1212--243298-neuraloct-airway-oct-analysis-via-neural-fields-yining-jiao-et-al-2024>(12/12 | 243/298) NeuralOCT: Airway OCT Analysis via Neural Fields (Yining Jiao et al., 2024)</a></li></ul></li><li><a href=#cshc-2>cs.HC (2)</a><ul><li><a href=#12--244298-trusting-the-search-unraveling-human-trust-in-health-information-from-google-and-chatgpt-xin-sun-et-al-2024>(1/2 | 244/298) Trusting the Search: Unraveling Human Trust in Health Information from Google and ChatGPT (Xin Sun et al., 2024)</a></li><li><a href=#22--245298-designing-user-centered-simulations-of-leadership-situations-for-cave-automatic-virtual-environments-development-and-usability-study-francesco-vona-et-al-2024>(2/2 | 245/298) Designing User-Centered Simulations of Leadership Situations for Cave Automatic Virtual Environments: Development and Usability Study (Francesco Vona et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--246298-large-language-model-informed-ecg-dual-attention-network-for-heart-failure-risk-prediction-chen-chen-et-al-2024>(1/1 | 246/298) Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction (Chen Chen et al., 2024)</a></li></ul></li><li><a href=#cscy-3>cs.CY (3)</a><ul><li><a href=#13--247298-emotion-aware-multimodal-fusion-for-meme-emotion-detection-shivam-sharma-et-al-2024>(1/3 | 247/298) Emotion-Aware Multimodal Fusion for Meme Emotion Detection (Shivam Sharma et al., 2024)</a></li><li><a href=#23--248298-graph-enhanced-reinforcement-learning-for-effective-group-formation-in-collaborative-problem-solving-zheng-fang-et-al-2024>(2/3 | 248/298) Graph Enhanced Reinforcement Learning for Effective Group Formation in Collaborative Problem Solving (Zheng Fang et al., 2024)</a></li><li><a href=#33--249298-designing-sousveillance-tools-for-gig-workers-kimberly-do-et-al-2024>(3/3 | 249/298) Designing Sousveillance Tools for Gig Workers (Kimberly Do et al., 2024)</a></li></ul></li><li><a href=#csit-6>cs.IT (6)</a><ul><li><a href=#16--250298-joint-group-scheduling-and-multicast-beamforming-for-downlink-large-scale-multi-group-multicast-chong-zhang-et-al-2024>(1/6 | 250/298) Joint Group Scheduling and Multicast Beamforming for Downlink Large-Scale Multi-Group Multicast (Chong Zhang et al., 2024)</a></li><li><a href=#26--251298-matrix-completion-via-nonsmooth-regularization-of-fully-connected-neural-networks-sajad-faramarzi-et-al-2024>(2/6 | 251/298) Matrix Completion via Nonsmooth Regularization of Fully Connected Neural Networks (Sajad Faramarzi et al., 2024)</a></li><li><a href=#36--252298-fairness-optimization-for-intelligent-reflecting-surface-aided-uplink-rate-splitting-multiple-access-shanshan-zhang-et-al-2024>(3/6 | 252/298) Fairness Optimization for Intelligent Reflecting Surface Aided Uplink Rate-Splitting Multiple Access (Shanshan Zhang et al., 2024)</a></li><li><a href=#46--253298-secure-distributed-storage-optimal-trade-off-between-storage-rate-and-privacy-leakage-remi-a-chou-et-al-2024>(4/6 | 253/298) Secure Distributed Storage: Optimal Trade-Off Between Storage Rate and Privacy Leakage (Remi A. Chou et al., 2024)</a></li><li><a href=#56--254298-chernoff-information-as-a-privacy-constraint-for-adversarial-classification-ayşe-ünsal-et-al-2024>(5/6 | 254/298) Chernoff Information as a Privacy Constraint for Adversarial Classification (Ayşe Ünsal et al., 2024)</a></li><li><a href=#66--255298-approximation-and-bounding-techniques-for-the-fisher-rao-distances-frank-nielsen-2024>(6/6 | 255/298) Approximation and bounding techniques for the Fisher-Rao distances (Frank Nielsen, 2024)</a></li></ul></li><li><a href=#csni-5>cs.NI (5)</a><ul><li><a href=#15--256298-joint-optimization-of-star-ris-assisted-swipt-communication-systems-junlong-yang-2024>(1/5 | 256/298) Joint Optimization of STAR-RIS Assisted SWIPT Communication Systems (Junlong Yang, 2024)</a></li><li><a href=#25--257298-rach-less-handover-with-early-timing-advance-acquisition-for-outage-reduction-subhyal-bin-iqbal-et-al-2024>(2/5 | 257/298) RACH-less Handover with Early Timing Advance Acquisition for Outage Reduction (Subhyal Bin Iqbal et al., 2024)</a></li><li><a href=#35--258298-is-wireless-bad-for-consensus-in-blockchain-seungmo-kim-2024>(3/5 | 258/298) Is Wireless Bad for Consensus in Blockchain? (Seungmo Kim, 2024)</a></li><li><a href=#45--259298-netbench-a-large-scale-and-comprehensive-network-traffic-benchmark-dataset-for-foundation-models-chen-qian-et-al-2024>(4/5 | 259/298) NetBench: A Large-Scale and Comprehensive Network Traffic Benchmark Dataset for Foundation Models (Chen Qian et al., 2024)</a></li><li><a href=#55--260298-cooperative-jamming-for-physical-layer-security-enhancement-using-deep-reinforcement-learning-sayed-amir-hoseini-et-al-2024>(5/5 | 260/298) Cooperative Jamming for Physical Layer Security Enhancement Using Deep Reinforcement Learning (Sayed Amir Hoseini et al., 2024)</a></li></ul></li><li><a href=#eesssy-5>eess.SY (5)</a><ul><li><a href=#15--261298-time-robust-path-planning-with-piece-wise-linear-trajectory-for-signal-temporal-logic-specifications-nhan-khanh-le-et-al-2024>(1/5 | 261/298) Time-Robust Path Planning with Piece-Wise Linear Trajectory for Signal Temporal Logic Specifications (Nhan-Khanh Le et al., 2024)</a></li><li><a href=#25--262298-analysis-of-a-two-degree-of-freedom-beam-for-rotational-piezoelectric-energy-harvesting-xiang-yu-li-et-al-2024>(2/5 | 262/298) Analysis of a Two-degree-of-freedom Beam for Rotational Piezoelectric Energy Harvesting (Xiang-Yu Li et al., 2024)</a></li><li><a href=#35--263298-leveraging-symmetries-in-gaits-for-reinforcement-learning-a-case-study-on-quadrupedal-gaits-jiayu-ding-et-al-2024>(3/5 | 263/298) Leveraging Symmetries in Gaits for Reinforcement Learning: A Case Study on Quadrupedal Gaits (Jiayu Ding et al., 2024)</a></li><li><a href=#45--264298-lyapunov-neural-network-with-region-of-attraction-search-zili-wang-et-al-2024>(4/5 | 264/298) Lyapunov Neural Network with Region of Attraction Search (Zili Wang et al., 2024)</a></li><li><a href=#55--265298-data-driven-distributionally-robust-safety-verification-using-barrier-certificates-and-conditional-mean-embeddings-oliver-schön-et-al-2024>(5/5 | 265/298) Data-Driven Distributionally Robust Safety Verification Using Barrier Certificates and Conditional Mean Embeddings (Oliver Schön et al., 2024)</a></li></ul></li><li><a href=#mathna-5>math.NA (5)</a><ul><li><a href=#15--266298-computational-study-on-the-impact-of-gasoline-ethanol-blending-on-autoignition-and-sootnox-emissions-under-gasoline-compression-ignition-conditions-krishna-c-kalvakala-et-al-2024>(1/5 | 266/298) Computational Study on the Impact of Gasoline-Ethanol Blending on Autoignition and Soot/NOx Emissions under Gasoline Compression Ignition Conditions (Krishna C. Kalvakala et al., 2024)</a></li><li><a href=#25--267298-hessian-free-force-gradient-integrators-kevin-schäfers-et-al-2024>(2/5 | 267/298) Hessian-free force-gradient integrators (Kevin Schäfers et al., 2024)</a></li><li><a href=#35--268298-boundary-parameter-matching-for-isogeometric-analysis-using-schwarz-christoffel-mapping-ye-ji-et-al-2024>(3/5 | 268/298) Boundary parameter matching for isogeometric analysis using Schwarz-Christoffel mapping (Ye Ji et al., 2024)</a></li><li><a href=#45--269298-effective-polygonal-mesh-generation-and-refinement-for-vem-stefano-berrone-et-al-2024>(4/5 | 269/298) Effective polygonal mesh generation and refinement for VEM (Stefano Berrone et al., 2024)</a></li><li><a href=#55--270298-optimal-control-of-stationary-doubly-diffusive-flows-on-two-and-three-dimensional-bounded-lipschitz-domains-numerical-analysis-jai-tushar-et-al-2024>(5/5 | 270/298) Optimal Control of Stationary Doubly Diffusive Flows on Two and Three Dimensional Bounded Lipschitz Domains: Numerical Analysis (Jai Tushar et al., 2024)</a></li></ul></li><li><a href=#cssd-3>cs.SD (3)</a><ul><li><a href=#13--271298-musichifi-fast-high-fidelity-stereo-vocoding-ge-zhu-et-al-2024>(1/3 | 271/298) MusicHiFi: Fast High-Fidelity Stereo Vocoding (Ge Zhu et al., 2024)</a></li><li><a href=#23--272298-multiscale-matching-driven-by-cross-modal-similarity-consistency-for-audio-text-retrieval-qian-wang-et-al-2024>(2/3 | 272/298) Multiscale Matching Driven by Cross-Modal Similarity Consistency for Audio-Text Retrieval (Qian Wang et al., 2024)</a></li><li><a href=#33--273298-birdset-a-multi-task-benchmark-for-classification-in-avian-bioacoustics-lukas-rauch-et-al-2024>(3/3 | 273/298) BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics (Lukas Rauch et al., 2024)</a></li></ul></li><li><a href=#physicschem-ph-1>physics.chem-ph (1)</a><ul><li><a href=#11--274298-gradnav-accelerated-exploration-of-potential-energy-surfaces-with-gradient-based-navigation-janghoon-ock-et-al-2024>(1/1 | 274/298) GradNav: Accelerated Exploration of Potential Energy Surfaces with Gradient-Based Navigation (Janghoon Ock et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--275298-rough-transformers-for-continuous-and-efficient-time-series-modelling-fernando-moreno-pino-et-al-2024>(1/3 | 275/298) Rough Transformers for Continuous and Efficient Time-Series Modelling (Fernando Moreno-Pino et al., 2024)</a></li><li><a href=#23--276298-generative-modelling-of-stochastic-rotating-shallow-water-noise-dan-crisan-et-al-2024>(2/3 | 276/298) Generative Modelling of Stochastic Rotating Shallow Water Noise (Dan Crisan et al., 2024)</a></li><li><a href=#33--277298-interpretable-machine-learning-for-survival-analysis-sophie-hanna-langbein-et-al-2024>(3/3 | 277/298) Interpretable Machine Learning for Survival Analysis (Sophie Hanna Langbein et al., 2024)</a></li></ul></li><li><a href=#quant-ph-2>quant-ph (2)</a><ul><li><a href=#12--278298-quantum-circuits-for-partial-differential-equations-via-schrödingerisation-junpeng-hu-et-al-2024>(1/2 | 278/298) Quantum Circuits for partial differential equations via Schrödingerisation (Junpeng Hu et al., 2024)</a></li><li><a href=#22--279298-evaluation-of-quantum-and-hybrid-solvers-for-combinatorial-optimization-amedeo-bertuzzi-et-al-2024>(2/2 | 279/298) Evaluation of Quantum and Hybrid Solvers for Combinatorial Optimization (Amedeo Bertuzzi et al., 2024)</a></li></ul></li><li><a href=#csgt-3>cs.GT (3)</a><ul><li><a href=#13--280298-ddps-dynamic-differential-pricing-based-edge-offloading-system-with-energy-harvesting-devices-hai-xue-et-al-2024>(1/3 | 280/298) DDPS: Dynamic Differential Pricing-based Edge Offloading System with Energy Harvesting Devices (Hai Xue et al., 2024)</a></li><li><a href=#23--281298-coordination-in-noncooperative-multiplayer-matrix-games-via-reduced-rank-correlated-equilibria-jaehan-im-et-al-2024>(2/3 | 281/298) Coordination in Noncooperative Multiplayer Matrix Games via Reduced Rank Correlated Equilibria (Jaehan Im et al., 2024)</a></li><li><a href=#33--282298-scaling-game-theoretic-security-reasoning-sophie-rain-et-al-2024>(3/3 | 282/298) Scaling Game-Theoretic Security Reasoning (Sophie Rain et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--283298-complete-equational-theories-for-classical-and-quantum-gaussian-relations-robert-i-booth-et-al-2024>(1/1 | 283/298) Complete equational theories for classical and quantum Gaussian relations (Robert I. Booth et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--284298-quantization-avoids-saddle-points-in-distributed-optimization-yanan-bo-et-al-2024>(1/1 | 284/298) Quantization Avoids Saddle Points in Distributed Optimization (Yanan Bo et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--285298-scalable-algorithms-for-individual-preference-stable-clustering-ron-mosenzon-et-al-2024>(1/2 | 285/298) Scalable Algorithms for Individual Preference Stable Clustering (Ron Mosenzon et al., 2024)</a></li><li><a href=#22--286298-first-passage-percolation-with-queried-hints-kritkorn-karntikoon-et-al-2024>(2/2 | 286/298) First Passage Percolation with Queried Hints (Kritkorn Karntikoon et al., 2024)</a></li></ul></li><li><a href=#physicsgeo-ph-1>physics.geo-ph (1)</a><ul><li><a href=#11--287298-thermal-earth-model-for-the-conterminous-united-states-using-an-interpolative-physics-informed-graph-neural-network-interpignn-mohammad-j-aljubran-et-al-2024>(1/1 | 287/298) Thermal Earth Model for the Conterminous United States Using an Interpolative Physics-Informed Graph Neural Network (InterPIGNN) (Mohammad J. Aljubran et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#11--288298-accurate-and-data-efficient-micro-xrd-phase-identification-using-multi-task-learning-application-to-hydrothermal-fluids-yanfei-li-et-al-2024>(1/1 | 288/298) Accurate and Data-Efficient Micro-XRD Phase Identification Using Multi-Task Learning: Application to Hydrothermal Fluids (Yanfei Li et al., 2024)</a></li></ul></li><li><a href=#cscg-2>cs.CG (2)</a><ul><li><a href=#12--289298-ipelets-for-the-convex-polygonal-geometry-nithin-parepally-et-al-2024>(1/2 | 289/298) Ipelets for the Convex Polygonal Geometry (Nithin Parepally et al., 2024)</a></li><li><a href=#22--290298-a-canonical-tree-decomposition-for-order-types-and-some-applications-mathilde-bouvel-et-al-2024>(2/2 | 290/298) A canonical tree decomposition for order types, and some applications (Mathilde Bouvel et al., 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--291298-improved-discrete-particle-swarm-optimization-using-bee-algorithm-and-multi-parent-crossover-method-case-study-allocation-problem-and-benchmark-functions-hamed-zibaei-et-al-2024>(1/2 | 291/298) Improved discrete particle swarm optimization using Bee Algorithm and multi-parent crossover method (Case study: Allocation problem and benchmark functions) (Hamed Zibaei et al., 2024)</a></li><li><a href=#22--292298-efficient-multiplayer-battle-game-optimizer-for-adversarial-robust-neural-architecture-search-rui-zhong-et-al-2024>(2/2 | 292/298) Efficient Multiplayer Battle Game Optimizer for Adversarial Robust Neural Architecture Search (Rui Zhong et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--293298-floodgenome-interpretable-machine-learning-for-decoding-features-shaping-property-flood-risk-predisposition-in-cities-chenyue-liu-et-al-2024>(1/1 | 293/298) FloodGenome: Interpretable Machine Learning for Decoding Features Shaping Property Flood Risk Predisposition in Cities (Chenyue Liu et al., 2024)</a></li></ul></li><li><a href=#physicscomp-ph-1>physics.comp-ph (1)</a><ul><li><a href=#11--294298-model-free-collision-aggregation-for-the-computation-of-escape-distributions-laetitia-laguzet-et-al-2024>(1/1 | 294/298) Model free collision aggregation for the computation of escape distributions (Laetitia Laguzet et al., 2024)</a></li></ul></li><li><a href=#mathfa-1>math.FA (1)</a><ul><li><a href=#11--295298-discrete-functional-inequalities-on-lattice-graphs-shubham-gupta-2024>(1/1 | 295/298) Discrete functional inequalities on lattice graphs (Shubham Gupta, 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--296298-v2aix-a-multi-modal-real-world-dataset-of-etsi-its-v2x-messages-in-public-road-traffic-guido-kueppers-et-al-2024>(1/1 | 296/298) V2AIX: A Multi-Modal Real-World Dataset of ETSI ITS V2X Messages in Public Road Traffic (Guido Kueppers et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--297298-a-data-driven-approach-for-mitigating-dark-current-noise-and-bad-pixels-in-complementary-metal-oxide-semiconductor-cameras-for-space-based-telescopes-peng-jia-et-al-2024>(1/1 | 297/298) A Data-Driven Approach for Mitigating Dark Current Noise and Bad Pixels in Complementary Metal Oxide Semiconductor Cameras for Space-based Telescopes (Peng Jia et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--298298-accelerating-regular-path-queries-over-graph-database-with-processing-in-memory-ruoyan-ma-et-al-2024>(1/1 | 298/298) Accelerating Regular Path Queries over Graph Database with Processing-in-Memory (Ruoyan Ma et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>