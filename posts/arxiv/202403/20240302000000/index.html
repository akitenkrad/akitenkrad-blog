<!doctype html><html><head><title>arXiv @ 2024.03.02</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.02"><meta property="og:description" content="Primary Categories cond-mat.mtrl-sci (2) cs.AI (4) cs.AR (2) cs.CC (1) cs.CE (2) cs.CL (36) cs.CR (10) cs.CV (65) cs.CY (5) cs.DC (2) cs.DM (1) cs.DS (1) cs.GR (1) cs.GT (2) cs.HC (4) cs.IR (5) cs.IT (3) cs.LG (52) cs.LO (4) cs.NE (2) cs.NI (3) cs.RO (9) cs.SD (2) cs.SE (5) cs.SI (2) eess.AS (2) eess.IV (12) eess.SP (1) eess.SY (4) math.AP (1) math.CO (3) math.NA (2) math.OC (1) q-bio.BM (1) quant-ph (1) stat."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240302000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-02T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-02T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.02"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240302000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Saturday, Mar 2, 2024</p></div><div class=title><h1>arXiv @ 2024.03.02</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#cond-matmtrl-sci-2>cond-mat.mtrl-sci (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csai-4>cs.AI (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csar-2>cs.AR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csce-2>cs.CE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#cscl-36>cs.CL (36)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#cscr-10>cs.CR (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#cscv-65>cs.CV (65)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#cscy-5>cs.CY (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csgt-2>cs.GT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#cshc-4>cs.HC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csir-5>cs.IR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csit-3>cs.IT (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#cslg-52>cs.LG (52)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#cslo-4>cs.LO (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csni-3>cs.NI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csro-9>cs.RO (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#csse-5>cs.SE (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#eessiv-12>eess.IV (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#eesssy-4>eess.SY (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#mathap-1>math.AP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#mathco-3>math.CO (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/#statml-1>stat.ML (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CR</th><th>cs.CV</th><th>cs.LG</th><th>eess.IV</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Adversarial Learning</td><td>1</td><td>1</td><td></td><td>1</td><td>1</td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Automatic Speech Recognition</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>BERTScore</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Benchmarking</td><td>5</td><td>1</td><td>19</td><td>10</td><td>1</td></tr><tr><td>Black Box</td><td>1</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Clustering</td><td></td><td>2</td><td>2</td><td>2</td><td></td></tr><tr><td>Common-sense Reasoning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td>1</td><td>4</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td>8</td><td>1</td><td>2</td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>4</td><td>1</td><td>2</td></tr><tr><td>Counter-factual</td><td>1</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Curriculum Learning</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Diffusion Model</td><td>1</td><td></td><td>5</td><td>3</td><td>3</td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td></td><td>1</td><td>1</td><td>5</td><td></td></tr><tr><td>Few-shot</td><td>1</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>6</td><td>1</td><td>8</td><td>8</td><td></td></tr><tr><td>Foundation Model</td><td></td><td></td><td>5</td><td>1</td><td></td></tr><tr><td>GPT</td><td>3</td><td></td><td>1</td><td></td><td></td></tr><tr><td>GPT-3</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>GPT-4</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Generative AI</td><td>1</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Generative Adversarial Network</td><td>1</td><td></td><td></td><td>4</td><td>4</td></tr><tr><td>Geometry</td><td></td><td></td><td>1</td><td>2</td><td>1</td></tr><tr><td>Graph</td><td></td><td>1</td><td>5</td><td>5</td><td>1</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td>2</td><td>2</td><td>2</td><td></td></tr><tr><td>Grounding</td><td>2</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Image2text</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>2</td><td>1</td><td></td><td>6</td><td></td></tr><tr><td>Information Retrieval</td><td>1</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td></td><td>6</td><td>1</td><td></td></tr><tr><td>Knowledge Graph</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>LLaMA</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Large Language Model</td><td>31</td><td>6</td><td>6</td><td>10</td><td></td></tr><tr><td>Low-Resource</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Mistral</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>4</td><td></td><td>21</td><td>1</td><td></td></tr><tr><td>Multiple Instance Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Mutual Information</td><td>3</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Natural Language Understanding</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>7</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td></td><td>2</td><td>3</td><td></td></tr><tr><td>Perplexity</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>5</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Prompt</td><td>5</td><td>1</td><td>7</td><td>2</td><td></td></tr><tr><td>Pruning</td><td></td><td>1</td><td>1</td><td>2</td><td></td></tr><tr><td>Quantization</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Question Answering</td><td>8</td><td></td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>6</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Recommendation</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Recommender System</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td>1</td><td>1</td><td>6</td><td></td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td>1</td><td>6</td><td>1</td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>RoBERTa</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Rouge</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Rouge-L</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Scaling Law</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td>1</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Self-Distillation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td>2</td><td></td><td>5</td><td></td><td>1</td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td>2</td><td>4</td><td></td></tr><tr><td>Simulator</td><td></td><td></td><td>2</td><td>4</td><td></td></tr><tr><td>Speech-to-Speech Translation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Stemming</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Summarization</td><td>4</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>2</td><td>4</td><td>1</td><td>2</td></tr><tr><td>Text Classification</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Tokenization</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Transformer</td><td>6</td><td></td><td>10</td><td>8</td><td>1</td></tr><tr><td>Unsupervised Learning</td><td>1</td><td></td><td>5</td><td></td><td>2</td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>10</td><td>2</td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>2</td><td></td><td>5</td><td>2</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-36>cs.CL (36)</h2><h3 id=136--1254-compact-speech-translation-models-via-discrete-speech-units-pretraining-tsz-kin-lam-et-al-2024>(1/36 | 1/254) Compact Speech Translation Models via Discrete Speech Units Pretraining (Tsz Kin Lam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tsz Kin Lam, Alexandra Birch, Barry Haddow. (2024)<br><strong>Compact Speech Translation Models via Discrete Speech Units Pretraining</strong><br><button class=copy-to-clipboard title="Compact Speech Translation Models via Discrete Speech Units Pretraining" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, Low-Resource, Self-supervised Learning, Self-supervised Learning, Automatic Speech Recognition, Speech-to-Speech Translation, Tokenization, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19333v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19333v1.pdf filename=2402.19333v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using <b>Self-Supervised</b> <b>Learning</b> (SSL) as model initialization is now common to obtain strong results in <b>Speech</b> <b>Translation</b> (ST). However, they also impose a large memory footprint, hindering on-device deployment. In this paper, we leverage the SSL models by pretraining smaller models on their Discrete <b>Speech</b> <b>Units</b> (DSU). We pretrain encoder-decoder models on 1) Filterbank-to-DSU and 2) DSU-to-Translation data, and take the encoder from 1) and the decoder from 2) to initialise a new model, <b>finetuning</b> this on limited <b>speech-translation</b> <b>data.</b> The final model becomes compact by using the DSU pretraining to distil the knowledge of the SSL model. Our method has several benefits over using DSU as model inputs, such as shorter inference pipeline and robustness over (DSU) <b>tokenization.</b> In contrast to <b>ASR</b> pretraining, it does not require transcripts, making it applicable to <b>low-resource</b> settings. Evaluation on CoVoST-2 X-En shows that our method is >$0.5$ <b>BLEU</b> better than a ST model that directly <b>finetune</b> the SSL model, given only half the model size, and on a par with <b>ASR</b> pretraining.</p></p class="citation"></blockquote><h3 id=236--2254-teaching-large-language-models-an-unseen-language-on-the-fly-chen-zhang-et-al-2024>(2/36 | 2/254) Teaching Large Language Models an Unseen Language on the Fly (Chen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Zhang, Xiao Liu, Jiuheng Lin, Yansong Feng. (2024)<br><strong>Teaching Large Language Models an Unseen Language on the Fly</strong><br><button class=copy-to-clipboard title="Teaching Large Language Models an Unseen Language on the Fly" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Low-Resource, GPT, GPT-4, BLEU, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19167v1.pdf filename=2402.19167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>large</b> <b>language</b> <b>models</b> struggle to support numerous <b>low-resource</b> languages, particularly the extremely <b>low-resource</b> ones where there is minimal training data available for effective parameter updating. We thus investigate whether <b>LLMs</b> can learn a new language on the fly solely through <b>prompting.</b> To study this question, we collect a research suite for Zhuang, a language supported by no <b>LLMs</b> currently. We introduce \textsc{DiPMT++}, a framework for adapting <b>LLMs</b> to unseen languages by <b>in-context</b> <b>learning.</b> Using a dictionary and only 5K parallel sentences, \textsc{DiPMT++} significantly enhances the performance of <b>GPT-4</b> from 0 to 16 <b>BLEU</b> for Chinese-to-Zhuang translation and achieves 32 <b>BLEU</b> for Zhuang-to-Chinese translation. Furthermore, we demonstrate the practical utility of this framework in aiding humans to translate completely unseen languages, which could contribute to the preservation of linguistic diversity.</p></p class="citation"></blockquote><h3 id=336--3254-openmedlm-prompt-engineering-can-out-perform-fine-tuning-in-medical-question-answering-with-open-source-large-language-models-jenish-maharjan-et-al-2024>(3/36 | 3/254) OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models (Jenish Maharjan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jenish Maharjan, Anurag Garikipati, Navan Preet Singh, Leo Cyrus, Mayank Sharma, Madalina Ciobanu, Gina Barnes, Rahul Thapa, Qingqing Mao, Ritankar Das. (2024)<br><strong>OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models</strong><br><button class=copy-to-clipboard title="OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Few-shot, Fine-tuning, Zero-shot, Massive Multitask Language Understanding (MMLU), Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19371v1.pdf filename=2402.19371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLMs</b> have become increasingly capable at accomplishing a range of specialized-tasks and can be utilized to expand equitable access to medical knowledge. Most medical <b>LLMs</b> have involved extensive <b>fine-tuning,</b> leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing <b>LLMs</b> are proprietary and their access is limited to very few research groups. However, open-source (OS) models represent a key area of growth for medical <b>LLMs</b> due to significant improvements in performance and an inherent ability to provide the transparency and compliance required in healthcare. We present OpenMedLM, a <b>prompting</b> platform which delivers state-of-the-art (SOTA) performance for OS <b>LLMs</b> on medical <b>benchmarks.</b> We evaluated a range of OS foundation <b>LLMs</b> (7B-70B) on four medical <b>benchmarks</b> (MedQA, MedMCQA, PubMedQA, <b>MMLU</b> medical-subset). We employed a series of <b>prompting</b> strategies, including <b>zero-shot,</b> <b>few-shot,</b> chain-of-thought (random selection and kNN selection), and ensemble/self-consistency voting. We found that OpenMedLM delivers OS SOTA results on three common medical <b>LLM</b> <b>benchmarks,</b> surpassing the previous best performing OS models that leveraged computationally costly extensive <b>fine-tuning.</b> The model delivers a 72.6% accuracy on the MedQA <b>benchmark,</b> outperforming the previous SOTA by 2.4%, and achieves 81.7% accuracy on the <b>MMLU</b> medical-subset, establishing itself as the first OS <b>LLM</b> to surpass 80% accuracy on this <b>benchmark.</b> Our results highlight medical-specific emergent properties in OS <b>LLMs</b> which have not yet been documented to date elsewhere, and showcase the benefits of further leveraging <b>prompt</b> engineering to improve the performance of accessible <b>LLMs</b> for medical applications.</p></p class="citation"></blockquote><h3 id=436--4254-exploring-the-efficacy-of-large-language-models-in-summarizing-mental-health-counseling-sessions-a-benchmark-study-prottay-kumar-adhikary-et-al-2024>(4/36 | 4/254) Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study (Prottay Kumar Adhikary et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prottay Kumar Adhikary, Aseem Srivastava, Shivani Kumar, Salam Michael Singh, Puneet Manuja, Jini K Gopinath, Vijay Krishnan, Swati Kedia, Koushik Sinha Deb, Tanmoy Chakraborty. (2024)<br><strong>Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study</strong><br><button class=copy-to-clipboard title="Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Mistral, BERTScore, Large Language Model, Large Language Model, Rouge, Rouge-L, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19052v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19052v1.pdf filename=2402.19052v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Comprehensive summaries of sessions enable an effective continuity in mental health counseling, facilitating informed therapy planning. Yet, manual <b>summarization</b> presents a significant challenge, diverting experts&rsquo; attention from the core counseling process. This study evaluates the effectiveness of state-of-the-art <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in selectively summarizing various components of therapy sessions through aspect-based <b>summarization,</b> aiming to <b>benchmark</b> their performance. We introduce MentalCLOUDS, a counseling-component guided <b>summarization</b> dataset consisting of 191 counseling sessions with summaries focused on three distinct counseling components (aka counseling aspects). Additionally, we assess the capabilities of 11 state-of-the-art <b>LLMs</b> in addressing the task of component-guided <b>summarization</b> in counseling. The generated summaries are evaluated quantitatively using standard <b>summarization</b> metrics and verified qualitatively by mental health professionals. Our findings demonstrate the superior performance of task-specific <b>LLMs</b> such as MentalLlama, <b>Mistral,</b> and MentalBART in terms of standard quantitative metrics such as <b>Rouge-1,</b> <b>Rouge-2,</b> <b>Rouge-L,</b> and <b>BERTScore</b> across all aspects of counseling components. Further, expert evaluation reveals that <b>Mistral</b> supersedes both MentalLlama and MentalBART based on six parameters &ndash; affective attitude, burden, ethicality, coherence, opportunity costs, and perceived effectiveness. However, these models share the same weakness by demonstrating a potential for improvement in the opportunity costs and perceived effectiveness metrics.</p></p class="citation"></blockquote><h3 id=536--5254-prompting-explicit-and-implicit-knowledge-for-multi-hop-question-answering-based-on-human-reading-process-guangming-huang-et-al-2024>(5/36 | 5/254) Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process (Guangming Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun. (2024)<br><strong>Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process</strong><br><button class=copy-to-clipboard title="Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Question Answering, Question Answering, Reasoning, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19350v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19350v2.pdf filename=2402.19350v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> leverage chains-of-thought (CoT) to simulate human <b>reasoning</b> and inference processes, achieving proficient performance in multi-hop <b>QA.</b> However, a gap persists between <b>PLMs&rsquo;</b> <b>reasoning</b> abilities and those of humans when tackling complex problems. Psychological studies suggest a vital connection between explicit information in passages and human prior knowledge during reading. Nevertheless, current research has given insufficient attention to linking input passages and <b>PLMs&rsquo;</b> pre-training-based knowledge from the perspective of human cognition studies. In this study, we introduce a <b>Prompting</b> Explicit and Implicit knowledge (PEI) framework, which uses <b>prompts</b> to connect explicit and implicit knowledge, aligning with human reading process for multi-hop <b>QA.</b> We consider the input passages as explicit knowledge, employing them to elicit implicit knowledge through unified <b>prompt</b> <b>reasoning.</b> Furthermore, our model incorporates type-specific <b>reasoning</b> via <b>prompts,</b> a form of implicit knowledge. Experimental results show that PEI performs comparably to the state-of-the-art on HotpotQA. Ablation studies confirm the efficacy of our model in bridging and integrating explicit and implicit knowledge.</p></p class="citation"></blockquote><h3 id=636--6254-on-the-decision-making-abilities-in-role-playing-using-large-language-models-chenglei-shen-et-al-2024>(6/36 | 6/254) On the Decision-Making Abilities in Role-Playing using Large Language Models (Chenglei Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenglei Shen, Guofu Xie, Xiao Zhang, Jun Xu. (2024)<br><strong>On the Decision-Making Abilities in Role-Playing using Large Language Models</strong><br><button class=copy-to-clipboard title="On the Decision-Making Abilities in Role-Playing using Large Language Models" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18807v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18807v1.pdf filename=2402.18807v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are now increasingly utilized for role-playing tasks, especially in impersonating domain-specific experts, primarily through role-playing <b>prompts.</b> When interacting in real-world scenarios, the decision-making abilities of a role significantly shape its behavioral patterns. In this paper, we concentrate on evaluating the decision-making abilities of <b>LLMs</b> post role-playing thereby validating the efficacy of role-playing. Our goal is to provide metrics and guidance for enhancing the decision-making abilities of <b>LLMs</b> in role-playing tasks. Specifically, we first use <b>LLMs</b> to generate virtual role descriptions corresponding to the 16 personality types of Myers-Briggs Type Indicator (abbreviated as MBTI) representing a segmentation of the population. Then we design specific quantitative operations to evaluate the decision-making abilities of <b>LLMs</b> post role-playing from four aspects: adaptability, exploration$&$exploitation trade-off ability, <b>reasoning</b> ability, and safety. Finally, we analyze the association between the performance of decision-making and the corresponding MBTI types through <b>GPT-4.</b> Extensive experiments demonstrate stable differences in the four aspects of decision-making abilities across distinct roles, signifying a robust correlation between decision-making abilities and the roles emulated by <b>LLMs.</b> These results underscore that <b>LLMs</b> can effectively impersonate varied roles while embodying their genuine sociological characteristics.</p></p class="citation"></blockquote><h3 id=736--7254-gsm-plus-a-comprehensive-benchmark-for-evaluating-the-robustness-of-llms-as-mathematical-problem-solvers-qintong-li-et-al-2024>(7/36 | 7/254) GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers (Qintong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, Wei Bi. (2024)<br><strong>GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers</strong><br><button class=copy-to-clipboard title="GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19255v1.pdf filename=2402.19255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved impressive performance across various <b>mathematical</b> <b>reasoning</b> <b>benchmarks.</b> However, there are increasing debates regarding whether these models truly understand and apply <b>mathematical</b> <b>knowledge</b> or merely rely on shortcuts for <b>mathematical</b> <b>reasoning.</b> One essential and frequently occurring evidence is that when the math questions are slightly changed, <b>LLMs</b> can behave incorrectly. This motivates us to evaluate the robustness of <b>LLMs&rsquo;</b> math <b>reasoning</b> capability by testing a wide range of question variations. We introduce the adversarial grade school math (\datasetname) dataset, an extension of GSM8K augmented with various <b>mathematical</b> <b>perturbations.</b> Our experiments on 25 <b>LLMs</b> and 4 <b>prompting</b> techniques show that while <b>LLMs</b> exhibit different levels of math <b>reasoning</b> abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, <b>LLMs</b> can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing <b>prompting</b> methods, in which we try an iterative method that generates and verifies each intermediate thought based on its <b>reasoning</b> goal and calculation result. Code and data are available at \url{https://github.com/qtli/GSM-Plus}.</p></p class="citation"></blockquote><h3 id=836--8254-how-to-understand-support-an-implicit-enhanced-causal-inference-approach-for-weakly-supervised-phrase-grounding-jiamin-luo-et-al-2024>(8/36 | 8/254) How to Understand &lsquo;Support&rsquo;? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding (Jiamin Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiamin Luo, Jianing Zhao, Jingjing Wang, Guodong Zhou. (2024)<br><strong>How to Understand &lsquo;Support&rsquo;? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding</strong><br><button class=copy-to-clipboard title="How to Understand 'Support'? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 46<br>Keywords: Counter-factual, Multi-modal, Multi-modal, Weakly-supervised Learning, Grounding, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19116v1.pdf filename=2402.19116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Weakly-supervised</b> Phrase <b>Grounding</b> (WPG) is an emerging task of inferring the fine-grained phrase-region matching, while merely leveraging the coarse-grained sentence-image pairs for training. However, existing studies on WPG largely ignore the implicit phrase-region matching relations, which are crucial for evaluating the capability of models in understanding the deep <b>multimodal</b> semantics. To this end, this paper proposes an Implicit-Enhanced Causal Inference (IECI) approach to address the challenges of modeling the implicit relations and highlighting them beyond the explicit. Specifically, this approach leverages both the intervention and <b>counterfactual</b> techniques to tackle the above two challenges respectively. Furthermore, a high-quality implicit-enhanced dataset is annotated to evaluate IECI and detailed evaluations show the great advantages of IECI over the state-of-the-art baselines. Particularly, we observe an interesting finding that IECI outperforms the advanced <b>multimodal</b> <b>LLMs</b> by a large margin on this implicit-enhanced dataset, which may facilitate more research to evaluate the <b>multimodal</b> <b>LLMs</b> in this direction.</p></p class="citation"></blockquote><h3 id=936--9254-let-llms-take-on-the-latest-challenges-a-chinese-dynamic-question-answering-benchmark-zhikun-xu-et-al-2024>(9/36 | 9/254) Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark (Zhikun Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhikun Xu, Yinghui Li, Ruixue Ding, Xinyu Wang, Boli Chen, Yong Jiang, Xiaodong Deng, Jianxin Ma, Hai-Tao Zheng, Wenlian Lu, Pengjun Xie, Chang Zhou, Fei Huang. (2024)<br><strong>Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark</strong><br><button class=copy-to-clipboard title="Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19248v1.pdf filename=2402.19248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How to better evaluate the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is the focal point and hot topic in current <b>LLMs</b> research. Previous work has noted that due to the extremely high cost of iterative updates of <b>LLMs,</b> they are often unable to answer the latest dynamic <b>questions</b> <b>well.</b> To promote the improvement of Chinese <b>LLMs&rsquo;</b> ability to answer dynamic <b>questions,</b> <b>in</b> this paper, we introduce CDQA, a Chinese Dynamic <b>QA</b> <b>benchmark</b> containing <b>question-answer</b> <b>pairs</b> related to the latest news on the Chinese Internet. We obtain high-quality data through a pipeline that combines humans and models, and carefully classify the samples according to the frequency of answer changes to facilitate a more fine-grained observation of <b>LLMs&rsquo;</b> capabilities. We have also evaluated and analyzed mainstream and advanced Chinese <b>LLMs</b> on CDQA. Extensive experiments and valuable insights suggest that our proposed CDQA is challenging and worthy of more further study. We believe that the <b>benchmark</b> we provide will become the key data resource for improving <b>LLMs&rsquo;</b> Chinese <b>question-answering</b> <b>ability</b> in the future.</p></p class="citation"></blockquote><h3 id=1036--10254-tv-trees-multimodal-entailment-trees-for-neuro-symbolic-video-reasoning-kate-sanders-et-al-2024>(10/36 | 10/254) TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning (Kate Sanders et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kate Sanders, Nathaniel Weir, Benjamin Van Durme. (2024)<br><strong>TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning</strong><br><button class=copy-to-clipboard title="TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; I-2-10, cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 41<br>Keywords: Black Box, Multi-modal, Multi-modal, Zero-shot, Question Answering, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19467v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19467v2.pdf filename=2402.19467v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is challenging to perform <b>question-answering</b> <b>over</b> complex, <b>multimodal</b> content such as television clips. This is in part because current video-language models rely on single-modality <b>reasoning,</b> have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first <b>multimodal</b> entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality <b>reasoning</b> by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of <b>multimodal</b> entailment tree generation to evaluate the <b>reasoning</b> quality of such methods. Our method&rsquo;s experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art <b>zero-shot</b> performance on full video clips, illustrating a best of both worlds contrast to <b>black-box</b> <b>methods.</b></p></p class="citation"></blockquote><h3 id=1136--11254-loose-lips-sink-ships-asking-questions-in-battleship-with-language-informed-program-sampling-gabriel-grand-et-al-2024>(11/36 | 11/254) Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling (Gabriel Grand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriel Grand, Valerio Pepe, Jacob Andreas, Joshua B. Tenenbaum. (2024)<br><strong>Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling</strong><br><button class=copy-to-clipboard title="Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: GPT, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19471v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19471v1.pdf filename=2402.19471v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Questions combine our mastery of language with our remarkable facility for <b>reasoning</b> about uncertainty. How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources? We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship. Our language-informed program sampling (LIPS) model uses <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios. In contrast, <b>LLM-only</b> baselines struggle to ground questions in the board state; notably, <b>GPT-4V</b> provides no improvement over non-visual baselines. Our results illustrate how Bayesian models of question-asking can leverage the statistics of language to capture human priors, while highlighting some shortcomings of pure <b>LLMs</b> as grounded reasoners.</p></p class="citation"></blockquote><h3 id=1236--12254-towards-tracing-trustworthiness-dynamics-revisiting-pre-training-period-of-large-language-models-chen-qian-et-al-2024>(12/36 | 12/254) Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models (Chen Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Qian, Jie Zhang, Wei Yao, Dongrui Liu, Zhenfei Yin, Yu Qiao, Yong Liu, Jing Shao. (2024)<br><strong>Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models</strong><br><button class=copy-to-clipboard title="Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fairness, Mutual Information, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19465v1.pdf filename=2402.19465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring the trustworthiness of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is crucial. Most studies concentrate on fully pre-trained <b>LLMs</b> to better understand and improve <b>LLMs&rsquo;</b> trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of <b>LLMs&rsquo;</b> trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, <b>fairness,</b> and robustness. To begin with, we apply linear probing to <b>LLMs.</b> The high probing accuracy suggests that \textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a <b>LLM&rsquo;s</b> pre-training checkpoints to enhance the <b>LLM&rsquo;s</b> trustworthiness. Finally, inspired by~\citet{choi2023understanding} that <b>mutual</b> <b>information</b> estimation is bounded by linear probing accuracy, we also probe <b>LLMs</b> with <b>mutual</b> <b>information</b> to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression~\citep{shwartz2017opening}. This research provides an initial exploration of trustworthiness modeling during <b>LLM</b> pre-training, seeking to unveil new insights and spur further developments in the field. We will make our code publicly accessible at \url{https://github.com/ChnQ/TracingLLM}.</p></p class="citation"></blockquote><h3 id=1336--13254-heres-a-free-lunch-sanitizing-backdoored-models-with-model-merge-ansh-arora-et-al-2024>(13/36 | 13/254) Here&rsquo;s a Free Lunch: Sanitizing Backdoored Models with Model Merge (Ansh Arora et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, Qiongkai Xu. (2024)<br><strong>Here&rsquo;s a Free Lunch: Sanitizing Backdoored Models with Model Merge</strong><br><button class=copy-to-clipboard title="Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: BERT, Mistral, RoBERTa, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19334v1.pdf filename=2402.19334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The democratization of <b>pre-trained</b> <b>language</b> <b>models</b> through open-source initiatives has rapidly advanced innovation and expanded access to cutting-edge technologies. However, this openness also brings significant security risks, including backdoor attacks, where hidden malicious behaviors are triggered by specific inputs, compromising natural language processing (NLP) system integrity and reliability. This paper suggests that merging a backdoored model with other homogeneous models can remediate backdoor vulnerabilities even if such models are not entirely secure. In our experiments, we explore various models <b>(BERT-Base,</b> <b>RoBERTa-Large,</b> Llama2-7B, and <b>Mistral-7B)</b> and datasets (SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensive approaches, our method offers an effective and efficient inference-stage defense against backdoor attacks without additional resources or specific knowledge. Our approach consistently outperforms the other advanced baselines, leading to an average of 75% reduction in the attack success rate. Since model merging has been an established approach for improving model performance, the extra advantage it provides regarding defense can be seen as a cost-free bonus.</p></p class="citation"></blockquote><h3 id=1436--14254-pelle-encoder-based-language-models-for-brazilian-portuguese-based-on-open-data-guilherme-lamartine-de-mello-et-al-2024>(14/36 | 14/254) PeLLE: Encoder-based language models for Brazilian Portuguese based on open data (Guilherme Lamartine de Mello et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guilherme Lamartine de Mello, Marcelo Finger, and Felipe Serras, Miguel de Mello Carpi, Marcos Menon Jose, Pedro Henrique Domingues, Paulo Cavalim. (2024)<br><strong>PeLLE: Encoder-based language models for Brazilian Portuguese based on open data</strong><br><button class=copy-to-clipboard title="PeLLE: Encoder-based language models for Brazilian Portuguese based on open data" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: RoBERTa, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19204v1.pdf filename=2402.19204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we present PeLLE, a family of <b>large</b> <b>language</b> <b>models</b> based on the <b>RoBERTa</b> architecture, for Brazilian Portuguese, trained on curated, open data from the Carolina corpus. Aiming at reproducible results, we describe details of the pretraining of the models. We also evaluate PeLLE models against a set of existing multilingual and PT-BR refined pretrained <b>Transformer-based</b> <b>LLM</b> encoders, contrasting performance of <b>large</b> <b>versus</b> <b>smaller-but-curated</b> pretrained models in several downstream tasks. We conclude that several tasks perform better with larger models, but some tasks benefit from smaller-but-curated data in its pretraining.</p></p class="citation"></blockquote><h3 id=1536--15254-tencdm-understanding-the-properties-of-diffusion-model-in-the-space-of-language-model-encodings-alexander-shabalin-et-al-2024>(15/36 | 15/254) TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings (Alexander Shabalin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Shabalin, Viacheslav Meshchaninov, Tingir Badmaev, Dmitry Molchanov, Grigory Bartosh, Sergey Markov, Dmitry Vetrov. (2024)<br><strong>TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings</strong><br><button class=copy-to-clipboard title="TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2; I-7, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Diffusion Model, Transformer, Text Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19097v1.pdf filename=2402.19097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Drawing inspiration from the success of <b>diffusion</b> <b>models</b> in various domains, numerous research papers proposed methods for adapting them to <b>text</b> <b>data.</b> Despite these efforts, none of them has managed to achieve the quality of the <b>large</b> <b>language</b> <b>models.</b> In this paper, we conduct a comprehensive analysis of key components of the <b>text</b> <b>diffusion</b> <b>models</b> and introduce a novel approach named <b>Text</b> <b>Encoding</b> <b>Diffusion</b> <b>Model</b> (TEncDM). Instead of the commonly used token embedding space, we train our model in the space of the language model encodings. Additionally, we propose to use a <b>Transformer-based</b> decoder that utilizes contextual information for <b>text</b> <b>reconstruction.</b> We also analyse self-conditioning and find that it increases the magnitude of the model outputs, allowing the reduction of the number of denoising steps at the inference stage. Evaluation of TEncDM on two downstream <b>text</b> <b>generation</b> tasks, QQP and XSum, demonstrates its superiority over existing non-autoregressive models.</p></p class="citation"></blockquote><h3 id=1636--16254-on-the-scaling-laws-of-geographical-representation-in-language-models-nathan-godey-et-al-2024>(16/36 | 16/254) On the Scaling Laws of Geographical Representation in Language Models (Nathan Godey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathan Godey, Éric de la Clergerie, Benoît Sagot. (2024)<br><strong>On the Scaling Laws of Geographical Representation in Language Models</strong><br><button class=copy-to-clipboard title="On the Scaling Laws of Geographical Representation in Language Models" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19406v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19406v1.pdf filename=2402.19406v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models have long been shown to embed geographical information in their hidden representations. This line of work has recently been revisited by extending this result to <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> In this paper, we propose to fill the gap between well-established and recent literature by observing how geographical knowledge evolves when <b>scaling</b> <b>language</b> models. We show that geographical knowledge is observable even for tiny models, and that it scales consistently as we increase the model size. Notably, we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.</p></p class="citation"></blockquote><h3 id=1736--17254-memory-augmented-generative-adversarial-transformers-stephan-raaijmakers-et-al-2024>(17/36 | 17/254) Memory-Augmented Generative Adversarial Transformers (Stephan Raaijmakers et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephan Raaijmakers, Roos Bakker, Anita Cremers, Roy de Kleijn, Tom Kouwenhoven, Tessa Verhoef. (2024)<br><strong>Memory-Augmented Generative Adversarial Transformers</strong><br><button class=copy-to-clipboard title="Memory-Augmented Generative Adversarial Transformers" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Generative Adversarial Network, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19218v1.pdf filename=2402.19218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational AI systems that rely on <b>Large</b> <b>Language</b> <b>Models,</b> like <b>Transformers,</b> have difficulty interweaving external data (like facts) with the language they generate. Vanilla <b>Transformer</b> architectures are not designed for answering factual questions with high accuracy. This paper investigates a possible route for addressing this problem. We propose to extend the standard <b>Transformer</b> architecture with an additional memory bank holding extra information (such as facts drawn from a knowledge base), and an extra attention layer for addressing this memory. We add this augmented memory to a <b>Generative</b> <b>Adversarial</b> <b>Network-inspired</b> <b>Transformer</b> architecture. This setup allows for implementing arbitrary felicity conditions on the generated language of the <b>Transformer.</b> We first demonstrate how this machinery can be deployed for handling factual questions in goal-oriented dialogues. Secondly, we demonstrate that our approach can be useful for applications like {\it style adaptation} as well: the adaptation of utterances according to certain stylistic (external) constraints, like social properties of human interlocutors in dialogues.</p></p class="citation"></blockquote><h3 id=1836--18254-survey-in-characterization-of-semantic-change-jader-martins-camboim-de-sá-et-al-2024>(18/36 | 18/254) Survey in Characterization of Semantic Change (Jader Martins Camboim de Sá et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jader Martins Camboim de Sá, Marcos Da Silveira, Cédric Pruski. (2024)<br><strong>Survey in Characterization of Semantic Change</strong><br><button class=copy-to-clipboard title="Survey in Characterization of Semantic Change" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Information Retrieval, Question Answering, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19088v1.pdf filename=2402.19088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Live languages continuously evolve to integrate the cultural change of human societies. This evolution manifests through neologisms (new words) or \textbf{semantic changes} of words (new meaning to existing words). Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods. In computer science, these words are relevant to computational linguistics algorithms such as translation, <b>information</b> <b>retrieval,</b> <b>question</b> <b>answering,</b> etc. Semantic changes can potentially impact the quality of the outcomes of these algorithms. Therefore, it is important to understand and characterize these changes formally. The study of this impact is a recent problem that has attracted the attention of the computational linguistics community. Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to characterize how the meaning of words changes and to reason about how to reduce the impact of semantic change. This survey provides an understandable overview of existing approaches to the \textit{characterization of semantic changes} and also formally defines three classes of characterizations: if the meaning of a word becomes more general or narrow (change in dimension) if the word is used in a more pejorative or positive/ameliorated sense (change in orientation), and if there is a trend to use the word in a, for instance, metaphoric or metonymic context (change in relation). We <b>summarized</b> the main aspects of the selected publications in a table and discussed the needs and trends in the research activities on semantic change characterization.</p></p class="citation"></blockquote><h3 id=1936--19254-pointing-out-the-shortcomings-of-relation-extraction-models-with-semantically-motivated-adversarials-gennaro-nolano-et-al-2024>(19/36 | 19/254) Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials (Gennaro Nolano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gennaro Nolano, Moritz Blum, Basil Ell, Philipp Cimiano. (2024)<br><strong>Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials</strong><br><button class=copy-to-clipboard title="Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Relation Extraction, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19076v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19076v1.pdf filename=2402.19076v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>large</b> <b>language</b> <b>models</b> have achieved state-of-the-art performance across various NLP tasks. However, investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to <b>out-of-distribution</b> (OOD) samples. For instance, in the context of <b>relation</b> <b>extraction</b> (RE), we would expect a model to identify the same <b>relation</b> <b>independently</b> of the entities involved in it. For example, consider the sentence &ldquo;Leonardo da Vinci painted the Mona Lisa&rdquo; expressing the created(Leonardo_da_Vinci, Mona_Lisa) <b>relation.</b> <b>If</b> we substiute &ldquo;Leonardo da Vinci&rdquo; with &ldquo;Barack Obama&rdquo;, then the sentence still expresses the created <b>relation.</b> <b>A</b> robust model is supposed to detect the same <b>relation</b> <b>in</b> both cases. In this work, we describe several semantically-motivated strategies to generate adversarial examples by replacing entity mentions and investigate how state-of-the-art RE models perform under pressure. Our analyses show that the performance of these models significantly deteriorates on the modified datasets (avg. of -48.5% in F1), which indicates that these models rely to a great extent on shortcuts, such as surface forms (or patterns therein) of entities, without making full use of the information present in the sentences.</p></p class="citation"></blockquote><h3 id=2036--20254-inappropriate-pause-detection-in-dysarthric-speech-using-large-scale-speech-recognition-jeehyun-lee-et-al-2024>(20/36 | 20/254) Inappropriate Pause Detection In Dysarthric Speech Using Large-Scale Speech Recognition (Jeehyun Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeehyun Lee, Yerin Choi, Tae-Jin Song, Myoung-Wan Koo. (2024)<br><strong>Inappropriate Pause Detection In Dysarthric Speech Using Large-Scale Speech Recognition</strong><br><button class=copy-to-clipboard title="Inappropriate Pause Detection In Dysarthric Speech Using Large-Scale Speech Recognition" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 30<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18923v1.pdf filename=2402.18923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dysarthria, a common issue among stroke patients, severely impacts <b>speech</b> <b>intelligibility.</b> Inappropriate pauses are crucial indicators in severity assessment and <b>speech-language</b> <b>therapy.</b> We propose to extend a large-scale <b>speech</b> <b>recognition</b> model for inappropriate pause detection in dysarthric <b>speech.</b> <b>To</b> this end, we propose task design, labeling strategy, and a <b>speech</b> <b>recognition</b> model with an inappropriate pause prediction layer. First, we treat pause detection as <b>speech</b> <b>recognition,</b> using an <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> model to convert <b>speech</b> <b>into</b> text with pause tags. According to the newly designed task, we label pause locations at the text level and their appropriateness. We collaborate with <b>speech-language</b> <b>pathologists</b> to establish labeling criteria, ensuring high-quality annotated data. Finally, we extend the <b>ASR</b> model with an inappropriate pause prediction layer for end-to-end inappropriate pause detection. Moreover, we propose a task-tailored metric for evaluating inappropriate pause detection independent of <b>ASR</b> performance. Our experiments show that the proposed method better detects inappropriate pauses in dysarthric <b>speech</b> <b>than</b> baselines. (Inappropriate Pause Error Rate: 14.47%)</p></p class="citation"></blockquote><h3 id=2136--21254-adamergex-cross-lingual-transfer-with-large-language-models-via-adaptive-adapter-merging-yiran-zhao-et-al-2024>(21/36 | 21/254) AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging (Yiran Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiran Zhao, Wenxuan Zhang, Huiming Wang, Kenji Kawaguchi, Lidong Bing. (2024)<br><strong>AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging</strong><br><button class=copy-to-clipboard title="AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18913v1.pdf filename=2402.18913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As an effective alternative to the direct <b>fine-tuning</b> on target tasks in specific languages, cross-lingual transfer addresses the challenges of limited training data by decoupling &lsquo;&rsquo;task ability&rsquo;&rsquo; and &lsquo;&rsquo;language ability&rsquo;&rsquo; by <b>fine-tuning</b> on the target task in the source language and another selected task in the target language, respectively. However, they fail to fully separate the task ability from the source language or the language ability from the chosen task. In this paper, we acknowledge the mutual reliance between task ability and language ability and direct our attention toward the gap between the target language and the source language on tasks. As the gap removes the impact of tasks, we assume that it remains consistent across tasks. Based on this assumption, we propose a new cross-lingual transfer method called $\texttt{AdaMergeX}$ that utilizes adaptive adapter merging. By introducing a reference task, we can determine that the divergence of adapters <b>fine-tuned</b> on the reference task in both languages follows the same distribution as the divergence of adapters <b>fine-tuned</b> on the target task in both languages. Hence, we can obtain target adapters by combining the other three adapters. Furthermore, we propose a structure-adaptive adapter merging method. Our empirical results demonstrate that our approach yields new and effective cross-lingual transfer, outperforming existing methods across all settings.</p></p class="citation"></blockquote><h3 id=2236--22254-reducing-hallucinations-in-entity-abstract-summarization-with-facts-template-decomposition-fangwei-zhu-et-al-2024>(22/36 | 22/254) Reducing Hallucinations in Entity Abstract Summarization with Facts-Template Decomposition (Fangwei Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangwei Zhu, Peiyi Wang, Zhifang Sui. (2024)<br><strong>Reducing Hallucinations in Entity Abstract Summarization with Facts-Template Decomposition</strong><br><button class=copy-to-clipboard title="Reducing Hallucinations in Entity Abstract Summarization with Facts-Template Decomposition" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Pre-trained Language Model, Pre-trained Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18873v1.pdf filename=2402.18873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Entity abstract <b>summarization</b> aims to generate a coherent description of a given entity based on a set of relevant Internet documents. <b>Pretrained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> have achieved significant success in this task, but they may suffer from hallucinations, i.e. generating non-factual information about the entity. To address this issue, we decompose the summary into two components: Facts that represent the factual information about the given entity, which <b>PLMs</b> are prone to fabricate; and Template that comprises generic content with designated slots for facts, which <b>PLMs</b> can generate competently. Based on the facts-template decomposition, we propose SlotSum, an explainable framework for entity abstract <b>summarization.</b> SlotSum first creates the template and then predicts the fact for each template slot based on the input documents. Benefiting from our facts-template decomposition, SlotSum can easily locate errors and further rectify hallucinated predictions with external knowledge. We construct a new dataset WikiFactSum to evaluate the performance of SlotSum. Experimental results demonstrate that SlotSum could generate summaries that are significantly more factual with credible external knowledge.</p></p class="citation"></blockquote><h3 id=2336--23254-how-do-large-language-models-handle-multilingualism-yiran-zhao-et-al-2024>(23/36 | 23/254) How do Large Language Models Handle Multilingualism? (Yiran Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, Lidong Bing. (2024)<br><strong>How do Large Language Models Handle Multilingualism?</strong><br><button class=copy-to-clipboard title="How do Large Language Models Handle Multilingualism?" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18815v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18815v1.pdf filename=2402.18815v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do <b>LLMs</b> handle multilingualism? We introduce a framework that depicts <b>LLMs&rsquo;</b> processing of multilingual inputs: In the first several layers, <b>LLMs</b> understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, <b>LLMs</b> engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the <b>self-attention</b> and feed-forward structures, respectively. In the last several layers, <b>LLMs</b> generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specific Neuron Detection ($\texttt{PLND}$) method that effectively measures the significance of neurons when handling multilingual inputs. By comprehensive ablation analysis through deactivating neurons of different layers and structures, we verify the framework that we propose. Additionally, we demonstrate that we can utilize such a framework to effectively enhance the multilingual ability with much less training effort.</p></p class="citation"></blockquote><h3 id=2436--24254-textttcosmic-mutual-information-for-task-agnostic-summarization-evaluation-maxime-darrin-et-al-2024>(24/36 | 24/254) $\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation (Maxime Darrin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maxime Darrin, Philippe Formont, Jackie Chi Kit Cheung, Pablo Piantanida. (2024)<br><strong>$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation</strong><br><button class=copy-to-clipboard title="$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Mutual Information, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19457v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19457v2.pdf filename=2402.19457v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks, while preserving task outcomes. We theoretically establish a direct relationship between the resulting error probability of these tasks and the <b>mutual</b> <b>information</b> between source texts and generated summaries. We introduce $\texttt{COSMIC}$ as a practical implementation of this metric, demonstrating its strong correlation with human judgment-based metrics and its effectiveness in predicting downstream task performance. Comparative analyses against established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$ highlight the competitive performance of $\texttt{COSMIC}$.</p></p class="citation"></blockquote><h3 id=2536--25254-plangpt-enhancing-urban-planning-with-tailored-language-model-and-efficient-retrieval-he-zhu-et-al-2024>(25/36 | 25/254) PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval (He Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>He Zhu, Wenjia Zhang, Nuoxian Huang, Boyang Li, Luyao Niu, Zipei Fan, Tianle Lun, Yicheng Tao, Junyou Su, Zhaoya Gong, Chenyu Fang, Xing Liu. (2024)<br><strong>PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval</strong><br><button class=copy-to-clipboard title="PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19273v1.pdf filename=2402.19273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of urban planning, general-purpose <b>large</b> <b>language</b> <b>models</b> often struggle to meet the specific needs of planners. Tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges. To enhance the efficiency of urban professionals and overcome these obstacles, we introduce PlanGPT, the first specialized <b>Large</b> <b>Language</b> <b>Model</b> tailored for urban and spatial planning. Developed through collaborative efforts with institutions like the Chinese Academy of Urban Planning, PlanGPT leverages a customized local database retrieval framework, domain-specific <b>fine-tuning</b> of base models, and advanced tooling capabilities. Empirical tests demonstrate that PlanGPT has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning.</p></p class="citation"></blockquote><h3 id=2636--26254-robust-guidance-for-unsupervised-data-selection-capturing-perplexing-named-entities-for-domain-specific-machine-translation-seunghyun-ji-et-al-2024>(26/36 | 26/254) Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation (Seunghyun Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seunghyun Ji, Hagai Raja Sinulingga, Darongsae Kwon. (2024)<br><strong>Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation</strong><br><button class=copy-to-clipboard title="Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19267v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19267v1.pdf filename=2402.19267v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Employing extensive datasets enables the training of multilingual <b>machine</b> <b>translation</b> models; however, these models often fail to accurately translate sentences within specialized domains. Although obtaining and translating domain-specific data incurs high costs, it is inevitable for high-quality translations. Hence, finding the most &rsquo;effective&rsquo; data with an <b>unsupervised</b> setting becomes a practical strategy for reducing labeling costs. Recent research indicates that this effective data could be found by selecting &lsquo;properly difficult data&rsquo; based on its volume. This means the data should not be excessively challenging or overly simplistic, especially if the amount of data is limited. However, we found that establishing a criterion for <b>unsupervised</b> data selection remains challenging, as the &lsquo;proper difficulty&rsquo; might vary based on the data domain being trained on. We introduce a novel <b>unsupervised</b> data selection method, &lsquo;Capturing Perplexing Named Entities&rsquo;, which adopts the maximum inference entropy in translated named entities as a selection measure. The motivation was that named entities in domain-specific data are considered the most complex portion of the data and should be predicted with high confidence. When verified with the &lsquo;Korean-English Parallel Corpus of Specialized Domains,&rsquo; our method served as a robust guidance for <b>unsupervised</b> data selection, in contrast to existing methods.</p></p class="citation"></blockquote><h3 id=2736--27254-evaluating-webcam-based-gaze-data-as-an-alternative-for-human-rationale-annotations-stephanie-brandl-et-al-2024>(27/36 | 27/254) Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale Annotations (Stephanie Brandl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephanie Brandl, Oliver Eberle, Tiago Ribeiro, Anders Søgaard, Nora Hollenstein. (2024)<br><strong>Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale Annotations</strong><br><button class=copy-to-clipboard title="Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale Annotations" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Transformer, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19133v1.pdf filename=2402.19133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rationales in the form of manually annotated input spans usually serve as ground truth when evaluating explainability methods in NLP. They are, however, time-consuming and often biased by the annotation process. In this paper, we debate whether human gaze, in the form of webcam-based eye-tracking recordings, poses a valid alternative when evaluating importance scores. We evaluate the additional information provided by gaze data, such as total reading times, gaze entropy, and decoding accuracy with respect to human rationale annotations. We compare WebQAmGaze, a multilingual dataset for information-seeking <b>QA,</b> with attention and explainability-based importance scores for 4 different multilingual <b>Transformer-based</b> language models (mBERT, distil-mBERT, XLMR, and XLMR-L) and 3 languages (English, Spanish, and German). Our pipeline can easily be applied to other tasks and languages. Our findings suggest that gaze data offers valuable linguistic insights that could be leveraged to infer task difficulty and further show a comparable ranking of explainability methods to that of human rationales.</p></p class="citation"></blockquote><h3 id=2836--28254-whispers-that-shake-foundations-analyzing-and-mitigating-false-premise-hallucinations-in-large-language-models-hongbang-yuan-et-al-2024>(28/36 | 28/254) Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models (Hongbang Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongbang Yuan, Pengfei Cao, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, Jun Zhao. (2024)<br><strong>Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models</strong><br><button class=copy-to-clipboard title="Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19103v1.pdf filename=2402.19103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when <b>LLMs</b> generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose \textbf{FAITH} (\textbf{F}alse premise \textbf{A}ttention head constra\textbf{I}ining for mi\textbf{T}igating \textbf{H}allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately $1%$ of the attention heads in the model yields a notable increase of nearly $20%$ of model performance.</p></p class="citation"></blockquote><h3 id=2936--29254-controllable-preference-optimization-toward-controllable-multi-objective-alignment-yiju-guo-et-al-2024>(29/36 | 29/254) Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment (Yiju Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment</strong><br><button class=copy-to-clipboard title="Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-SY, cs.CL, eess-SY<br>Keyword Score: 20<br>Keywords: Grounding, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19085v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19085v1.pdf filename=2402.19085v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the &ldquo;alignment tax&rdquo; -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of <b>grounding</b> <b>LLMs</b> with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the &ldquo;3H&rdquo; (helpfulness, honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving Pareto improvements in multi-objective alignment.</p></p class="citation"></blockquote><h3 id=3036--30254-popalm-popularity-aligned-language-models-for-social-media-trendy-response-prediction-erxin-yu-et-al-2024>(30/36 | 30/254) PopALM: Popularity-Aligned Language Models for Social Media Trendy Response Prediction (Erxin Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erxin Yu, Jing Li, Chunpu Xu. (2024)<br><strong>PopALM: Popularity-Aligned Language Models for Social Media Trendy Response Prediction</strong><br><button class=copy-to-clipboard title="PopALM: Popularity-Aligned Language Models for Social Media Trendy Response Prediction" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Curriculum Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18950v1.pdf filename=2402.18950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social media platforms are daily exhibiting millions of events. To preliminarily predict the mainstream public reaction to these events, we study trendy response prediction to automatically generate top-liked user replies to social media events. While previous works focus on generating responses without factoring in popularity, we propose Popularity-Aligned Language Models (PopALM) to distinguish responses liked by a larger audience through <b>reinforcement</b> <b>learning.</b> Recognizing the noisy labels from user &ldquo;likes&rdquo;, we tailor-make <b>curriculum</b> <b>learning</b> in proximal policy optimization (PPO) to help models capture the essential samples for easy-to-hard training. In experiments, we build a large-scale Weibo dataset for trendy response prediction, and its results show that PopALM can help boost the performance of advanced language models.</p></p class="citation"></blockquote><h3 id=3136--31254-semeval-2024----task-10-emotion-discovery-and-reasoning-its-flip-in-conversation-ediref-shivani-kumar-et-al-2024>(31/36 | 31/254) SemEval 2024 &ndash; Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF) (Shivani Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shivani Kumar, Md Shad Akhtar, Erik Cambria, Tanmoy Chakraborty. (2024)<br><strong>SemEval 2024 &ndash; Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF)</strong><br><button class=copy-to-clipboard title="SemEval 2024 -- Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF)" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Emotion Recognition, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18944v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18944v1.pdf filename=2402.18944v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present SemEval-2024 Task 10, a shared task centred on identifying <b>emotions</b> <b>and</b> finding the rationale behind their flips within monolingual English and Hindi-English code-mixed dialogues. This task comprises three distinct subtasks - <b>emotion</b> <b>recognition</b> in conversation for code-mixed dialogues, <b>emotion</b> <b>flip</b> <b>reasoning</b> for code-mixed dialogues, and <b>emotion</b> <b>flip</b> <b>reasoning</b> for English dialogues. Participating systems were tasked to automatically execute one or more of these subtasks. The datasets for these tasks comprise manually annotated conversations focusing on <b>emotions</b> <b>and</b> triggers for <b>emotion</b> <b>shifts</b> (The task data is available at <a href=https://github.com/LCS2-IIITD/EDiReF-SemEval2024.git)>https://github.com/LCS2-IIITD/EDiReF-SemEval2024.git)</a>. A total of 84 participants engaged in this task, with the most adept systems attaining F1-scores of 0.70, 0.79, and 0.76 for the respective subtasks. This paper summarises the results and findings from 24 teams alongside their system descriptions.</p></p class="citation"></blockquote><h3 id=3236--32254-when-does-word-order-matter-and-when-doesnt-it-xuanda-chen-et-al-2024>(32/36 | 32/254) When does word order matter and when doesn&rsquo;t it? (Xuanda Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuanda Chen, Timothy O&rsquo;Donnell, Siva Reddy. (2024)<br><strong>When does word order matter and when doesn&rsquo;t it?</strong><br><button class=copy-to-clipboard title="When does word order matter and when doesn't it?" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Mutual Information, Natural Language Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18838v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18838v2.pdf filename=2402.18838v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models (LMs) may appear insensitive to word order changes in <b>natural</b> <b>language</b> <b>understanding</b> (NLU) tasks. In this paper, we propose that linguistic redundancy can explain this phenomenon, whereby word order and other linguistic cues such as case markers provide overlapping and thus redundant information. Our hypothesis is that models exhibit insensitivity to word order when the order provides redundant information, and the degree of insensitivity varies across tasks. We quantify how informative word order is using <b>mutual</b> <b>information</b> (MI) between unscrambled and scrambled sentences. Our results show the effect that the less informative word order is, the more consistent the model&rsquo;s predictions are between unscrambled and scrambled sentences. We also find that the effect varies across tasks: for some tasks, like SST-2, LMs&rsquo; prediction is almost always consistent with the original one even if the Pointwise-MI (PMI) changes, while for others, like RTE, the consistency is near random when the PMI gets lower, i.e., word order is really important.</p></p class="citation"></blockquote><h3 id=3336--33254-utilizing-local-hierarchy-with-adversarial-training-for-hierarchical-text-classification-zihan-wang-et-al-2024>(33/36 | 33/254) Utilizing Local Hierarchy with Adversarial Training for Hierarchical Text Classification (Zihan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Wang, Peiyi Wang, Houfeng Wang. (2024)<br><strong>Utilizing Local Hierarchy with Adversarial Training for Hierarchical Text Classification</strong><br><button class=copy-to-clipboard title="Utilizing Local Hierarchy with Adversarial Training for Hierarchical Text Classification" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18825v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18825v1.pdf filename=2402.18825v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hierarchical <b>text</b> <b>classification</b> (HTC) is a challenging subtask of multi-label classification due to its complex taxonomic structure. Nearly all recent HTC works focus on how the labels are structured but ignore the sub-structure of ground-truth labels according to each input <b>text</b> <b>which</b> contains fruitful label co-occurrence information. In this work, we introduce this local hierarchy with an <b>adversarial</b> <b>framework.</b> We propose a HiAdv framework that can fit in nearly all HTC models and optimize them with the local hierarchy as auxiliary information. We test on two typical HTC models and find that HiAdv is effective in all scenarios and is adept at dealing with complex taxonomic hierarchies. Further experiments demonstrate that the promotion of our framework indeed comes from the local hierarchy and the local hierarchy is beneficial for rare classes which have insufficient training data.</p></p class="citation"></blockquote><h3 id=3436--34254-advancing-generative-ai-for-portuguese-with-open-decoder-gervásio-pt-rodrigo-santos-et-al-2024>(34/36 | 34/254) Advancing Generative AI for Portuguese with Open Decoder Gervásio PT* (Rodrigo Santos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rodrigo Santos, João Silva, Luís Gomes, João Rodrigues, António Branco. (2024)<br><strong>Advancing Generative AI for Portuguese with Open Decoder Gervásio PT</strong>*<br><button class=copy-to-clipboard title="Advancing Generative AI for Portuguese with Open Decoder Gervásio PT*" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Generative AI, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18766v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18766v1.pdf filename=2402.18766v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To advance the neural decoding of Portuguese, in this paper we present a fully open <b>Transformer-based,</b> instruction-tuned decoder model that sets a new state of the art in this respect. To develop this decoder, which we named Gerv'asio PT*, a strong LLaMA~2 7B model was used as a starting point, and its further improvement through additional training was done over language resources that include new instruction data sets of Portuguese prepared for this purpose, which are also contributed in this paper. All versions of Gerv'asio are open source and distributed for free under an open license, including for either research or commercial usage, and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in language technology for Portuguese.</p></p class="citation"></blockquote><h3 id=3536--35254-improving-legal-judgement-prediction-in-romanian-with-long-text-encoders-mihai-masala-et-al-2024>(35/36 | 35/254) Improving Legal Judgement Prediction in Romanian with Long Text Encoders (Mihai Masala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mihai Masala, Traian Rebedea, Horia Velicu. (2024)<br><strong>Improving Legal Judgement Prediction in Romanian with Long Text Encoders</strong><br><button class=copy-to-clipboard title="Improving Legal Judgement Prediction in Romanian with Long Text Encoders" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19170v1.pdf filename=2402.19170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years,the entire field of Natural Language Processing (NLP) has enjoyed amazing novel results achieving almost human-like performance on a variety of tasks. Legal NLP domain has also been part of this process, as it has seen an impressive growth. However, general-purpose models are not readily applicable for legal domain. Due to the nature of the domain (e.g. specialized vocabulary, long documents) specific models and methods are often needed for Legal NLP. In this work we investigate both specialized and general models for predicting the final ruling of a legal case, task known as Legal Judgment Prediction (LJP). We particularly focus on methods to extend to sequence length of <b>Transformer-based</b> models to better understand the long documents present in legal corpora. Extensive experiments on 4 LJP datasets in Romanian, originating from 2 sources with significantly different sizes and document lengths, show that specialized models and handling long texts are critical for a good performance.</p></p class="citation"></blockquote><h3 id=3636--36254-updating-language-models-with-unstructured-facts-towards-practical-knowledge-editing-xiaobao-wu-et-al-2024>(36/36 | 36/254) Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing (Xiaobao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaobao Wu, Liangming Pan, William Yang Wang, Anh Tuan Luu. (2024)<br><strong>Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing</strong><br><button class=copy-to-clipboard title="Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18909v1.pdf filename=2402.18909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date. However, its current evaluation strategies are notably impractical: they solely update with well-curated structured facts (triplets with subjects, relations, and objects), whereas real-world knowledge updates commonly emerge in unstructured texts like news articles. In this paper, we propose a new <b>benchmark,</b> Unstructured Knowledge Editing (UKE). It evaluates editing performance directly using unstructured texts as knowledge updates, termed unstructured facts. Hence UKE avoids the laborious construction of structured facts and enables efficient and responsive knowledge editing, becoming a more practical <b>benchmark.</b> We conduct extensive experiments on newly built datasets and demonstrate that UKE poses a significant challenge to state-of-the-art knowledge editing methods, resulting in their critical performance declines. We further show that this challenge persists even if we extract triplets as structured facts. Our analysis discloses key insights to motivate future research in UKE for more practical knowledge editing.</p></p class="citation"></blockquote><h2 id=cscv-65>cs.CV (65)</h2><h3 id=165--37254-typographic-attacks-in-large-multimodal-models-can-be-alleviated-by-more-informative-prompts-hao-cheng-et-al-2024>(1/65 | 37/254) Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts (Hao Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Cheng, Erjia Xiao, Renjing Xu. (2024)<br><strong>Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts</strong><br><button class=copy-to-clipboard title="Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 86<br>Keywords: Multi-modal, Multi-modal, Zero-shot, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model, Prompt, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19150v1.pdf filename=2402.19150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Multimodal</b> <b>Models</b> (LMMs) rely on pre-trained Vision Language Models (VLMs) and <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to perform amazing emergent abilities on various <b>multimodal</b> tasks in the joint space of vision and language. However, the Typographic Attack, which shows disruption to VLMs, has also been certified as a security vulnerability to LMMs. In this work, we first comprehensively investigate the distractibility of LMMs by typography. In particular, we introduce the Typographic Dataset designed to evaluate distractibility across various <b>multi-modal</b> subtasks, such as object recognition, visual attributes detection, enumeration, arithmetic computation, and <b>commonsense</b> <b>reasoning.</b> To further study the effect of typographic patterns on performance, we also scrutinize the effect of tuning various typographic factors, encompassing font size, color, opacity, and spatial positioning of typos. We discover that LMMs can partially distinguish visual contents and typos when confronting typographic attacks, which suggests that embeddings from vision encoders contain enough information to distinguish visual contents and typos in images. Inspired by such phenomena, we demonstrate that CLIP&rsquo;s performance of <b>zero-shot</b> classification on typo-ridden images can be significantly improved by providing more informative texts to match images. Furthermore, we also prove that LMMs can utilize more informative <b>prompts</b> to leverage information in embeddings to differentiate between visual content and typos. Finally, we propose a <b>prompt</b> information enhancement method that can effectively mitigate the effects of typography.</p></p class="citation"></blockquote><h3 id=265--38254-videomac-video-masked-autoencoders-meet-convnets-gensheng-pei-et-al-2024>(2/65 | 38/254) VideoMAC: Video Masked Autoencoders Meet ConvNets (Gensheng Pei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gensheng Pei, Tao Chen, Xiruo Jiang, Huafeng Liu, Zeren Sun, Yazhou Yao. (2024)<br><strong>VideoMAC: Video Masked Autoencoders Meet ConvNets</strong><br><button class=copy-to-clipboard title="VideoMAC: Video Masked Autoencoders Meet ConvNets" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 75<br>Keywords: Vision Transformer, Autoencoder, Convolution, Representation Learning, Self-supervised Learning, Self-supervised Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19082v1.pdf filename=2402.19082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the advancement of <b>self-supervised</b> <b>learning</b> techniques, like masked <b>autoencoders</b> (MAE), has greatly influenced visual <b>representation</b> <b>learning</b> for images and videos. Nevertheless, it is worth noting that the predominant approaches in existing masked image / video modeling rely excessively on resource-intensive <b>vision</b> <b>transformers</b> (ViTs) as the feature encoder. In this paper, we propose a new approach termed as \textbf{VideoMAC}, which combines video masked <b>autoencoders</b> with resource-friendly ConvNets. Specifically, VideoMAC employs symmetric masking on randomly sampled pairs of video frames. To prevent the issue of mask pattern dissipation, we utilize ConvNets which are implemented with sparse <b>convolutional</b> operators as encoders. Simultaneously, we present a simple yet effective masked video modeling (MVM) approach, a dual encoder architecture comprising an online encoder and an exponential moving average target encoder, aimed to facilitate inter-frame reconstruction consistency in videos. Additionally, we demonstrate that VideoMAC, empowering classical (ResNet) / modern (ConvNeXt) <b>convolutional</b> encoders to harness the benefits of MVM, outperforms ViT-based approaches on downstream tasks, including video object segmentation (+\textbf{5.2%} / \textbf{6.4%} $\mathcal{J}&\mathcal{F}$), body part propagation (+\textbf{6.3%} / \textbf{3.1%} mIoU), and human pose tracking (+\textbf{10.2%} / \textbf{11.1%} <a href=mailto:PCK@0.1>PCK@0.1</a>).</p></p class="citation"></blockquote><h3 id=365--39254-retrieval-augmented-generation-for-ai-generated-content-a-survey-penghao-zhao-et-al-2024>(3/65 | 39/254) Retrieval-Augmented Generation for AI-Generated Content: A Survey (Penghao Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Bin Cui. (2024)<br><strong>Retrieval-Augmented Generation for AI-Generated Content: A Survey</strong><br><button class=copy-to-clipboard title="Retrieval-Augmented Generation for AI-Generated Content: A Survey" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 73<br>Keywords: Benchmarking, Foundation Model, Knowledge Distillation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Information Retrieval, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19473v1.pdf filename=2402.19473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of Artificial Intelligence Generated Content (AIGC) has been facilitated by advancements in model algorithms, scalable <b>foundation</b> <b>model</b> architectures, and the availability of ample high-quality datasets. While AIGC has achieved remarkable performance, it still faces challenges, such as the difficulty of maintaining up-to-date and long-tail knowledge, the risk of data leakage, and the high costs associated with training and inference. <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> has recently emerged as a paradigm to address such challenges. In particular, <b>RAG</b> introduces the <b>information</b> <b>retrieval</b> <b>process,</b> <b>which</b> enhances AIGC results by retrieving relevant objects from available data stores, leading to greater accuracy and robustness. In this paper, we comprehensively review existing efforts that integrate <b>RAG</b> technique into AIGC scenarios. We first classify <b>RAG</b> <b>foundations</b> <b>according</b> to how the retriever augments the generator. We <b>distill</b> the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all <b>RAG</b> scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also <b>summarize</b> additional enhancements methods for <b>RAG,</b> facilitating effective engineering and implementation of <b>RAG</b> systems. Then from another view, we survey on practical applications of <b>RAG</b> across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the <b>benchmarks</b> for <b>RAG,</b> discuss the limitations of current <b>RAG</b> systems, and suggest potential directions for future research. Project: <a href=https://github.com/hymie122/RAG-Survey>https://github.com/hymie122/RAG-Survey</a></p></p class="citation"></blockquote><h3 id=465--40254-generalizable-whole-slide-image-classification-with-fine-grained-visual-semantic-interaction-hao-li-et-al-2024>(4/65 | 40/254) Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction (Hao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Li, Ying Chen, Yifei Chen, Wenxian Yang, Bowen Ding, Yuchen Han, Liansheng Wang, Rongshan Yu. (2024)<br><strong>Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction</strong><br><button class=copy-to-clipboard title="Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 55<br>Keywords: Few-shot, Multiple Instance Learning, Representation Learning, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19326v1.pdf filename=2402.19326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Whole Slide Image (WSI) classification is often formulated as a <b>Multiple</b> <b>Instance</b> <b>Learning</b> (MIL) problem. Recently, <b>Vision-Language</b> Models (VLMs) have demonstrated remarkable performance in WSI classification. However, existing methods leverage coarse-grained pathogenetic descriptions for visual <b>representation</b> <b>supervision,</b> which are insufficient to capture the complex visual appearance of pathogenetic images, hindering the generalizability of models on diverse downstream tasks. Additionally, processing high-resolution WSIs can be computationally expensive. In this paper, we propose a novel &ldquo;Fine-grained Visual-Semantic Interaction&rdquo; (FiVE) framework for WSI classification. It is designed to enhance the model&rsquo;s generalizability by leveraging the interplay between localized visual patterns and fine-grained pathological semantics. Specifically, with meticulously designed queries, we start by utilizing a <b>large</b> <b>language</b> <b>model</b> to extract fine-grained pathological descriptions from various non-standardized raw reports. The output descriptions are then reconstructed into fine-grained labels used for training. By introducing a Task-specific Fine-grained Semantics (TFS) module, we enable <b>prompts</b> to capture crucial visual information in WSIs, which enhances <b>representation</b> <b>learning</b> and augments generalization capabilities significantly. Furthermore, given that pathological visual patterns are redundantly distributed across tissue slices, we sample a subset of visual instances during training. Our method demonstrates robust generalizability and strong transferability, dominantly outperforming the counterparts on the TCGA Lung Cancer dataset with at least 9.19% higher accuracy in <b>few-shot</b> experiments.</p></p class="citation"></blockquote><h3 id=565--41254-assessing-visually-continuous-corruption-robustness-of-neural-networks-relative-to-human-performance-huakun-shen-et-al-2024>(5/65 | 41/254) Assessing Visually-Continuous Corruption Robustness of Neural Networks Relative to Human Performance (Huakun Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huakun Shen, Boyue Caroline Hu, Krzysztof Czarnecki, Lina Marsso, Marsha Chechik. (2024)<br><strong>Assessing Visually-Continuous Corruption Robustness of Neural Networks Relative to Human Performance</strong><br><button class=copy-to-clipboard title="Assessing Visually-Continuous Corruption Robustness of Neural Networks Relative to Human Performance" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Vision Transformer, Benchmarking, Convolution, Data Augmentation, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19401v1.pdf filename=2402.19401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While Neural Networks (NNs) have surpassed human accuracy in image classification on ImageNet, they often lack robustness against image corruption, i.e., corruption robustness. Yet such robustness is seemingly effortless for human perception. In this paper, we propose visually-continuous corruption robustness (VCR) &ndash; an extension of corruption robustness to allow assessing it over the wide and continuous range of changes that correspond to the human perceptive quality (i.e., from the original image to the full distortion of all perceived visual information), along with two novel human-aware metrics for NN evaluation. To compare VCR of NNs with human perception, we conducted extensive experiments on 14 commonly used image corruptions with 7,718 human participants and state-of-the-art robust NN models with different training objectives (e.g., standard, adversarial, corruption robustness), different architectures (e.g., <b>convolution</b> NNs, <b>vision</b> <b>transformers),</b> and different amounts of training <b>data</b> <b>augmentation.</b> Our study showed that: 1) assessing robustness against continuous corruption can reveal insufficient robustness undetected by existing <b>benchmarks;</b> as a result, 2) the gap between NN and human robustness is larger than previously known; and finally, 3) some image corruptions have a similar impact on human perception, offering opportunities for more cost-effective robustness assessments. Our validation set with 14 image corruptions, human robustness <b>data,</b> <b>and</b> the evaluation code is provided as a toolbox and a <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=665--42254-vixen-visual-text-comparison-network-for-image-difference-captioning-alexander-black-et-al-2024>(6/65 | 42/254) VIXEN: Visual Text Comparison Network for Image Difference Captioning (Alexander Black et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Black, Jing Shi, Yifei Fai, Tu Bui, John Collomosse. (2024)<br><strong>VIXEN: Visual Text Comparison Network for Image Difference Captioning</strong><br><button class=copy-to-clipboard title="VIXEN: Visual Text Comparison Network for Image Difference Captioning" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: GPT, GPT-3, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19119v1.pdf filename=2402.19119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present VIXEN - a technique that succinctly <b>summarizes</b> in text the visual differences between a pair of images in order to highlight any content manipulation present. Our proposed network linearly maps image features in a pairwise manner, constructing a soft <b>prompt</b> for a pretrained <b>large</b> <b>language</b> <b>model.</b> We address the challenge of low volume of training data and lack of manipulation variety in existing image difference captioning (IDC) datasets by training on synthetically manipulated images from the recent InstructPix2Pix dataset generated via <b>prompt-to-prompt</b> editing framework. We augment this dataset with change summaries produced via <b>GPT-3.</b> We show that VIXEN produces state-of-the-art, comprehensible difference captions for diverse image contents and edit types, offering a potential mitigation against misinformation disseminated via manipulated image content. Code and data are available at <a href=http://github.com/alexblck/vixen>http://github.com/alexblck/vixen</a></p></p class="citation"></blockquote><h3 id=765--43254-rsam-seg-a-sam-based-approach-with-prior-knowledge-integration-for-remote-sensing-image-semantic-segmentation-jie-zhang-et-al-2024>(7/65 | 43/254) RSAM-Seg: A SAM-based Approach with Prior Knowledge Integration for Remote Sensing Image Semantic Segmentation (Jie Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Zhang, Xubing Yang, Rui Jiang, Wei Shao, Li Zhang. (2024)<br><strong>RSAM-Seg: A SAM-based Approach with Prior Knowledge Integration for Remote Sensing Image Semantic Segmentation</strong><br><button class=copy-to-clipboard title="RSAM-Seg: A SAM-based Approach with Prior Knowledge Integration for Remote Sensing Image Semantic Segmentation" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Few-shot, Transformer, Prompt, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19004v1.pdf filename=2402.19004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of high-resolution remote sensing satellites has provided great convenience for research work related to remote sensing. Segmentation and extraction of specific targets are essential tasks when facing the vast and complex remote sensing images. Recently, the introduction of Segment Anything Model (SAM) provides a universal pre-training model for image segmentation tasks. While the direct application of SAM to remote sensing image segmentation tasks does not yield satisfactory results, we propose RSAM-Seg, which stands for Remote Sensing SAM with Semantic Segmentation, as a tailored modification of SAM for the remote sensing field and eliminates the need for manual intervention to provide <b>prompts.</b> Adapter-Scale, a set of supplementary scaling modules, are proposed in the multi-head attention blocks of the encoder part of SAM. Furthermore, Adapter-Feature are inserted between the <b>Vision</b> <b>Transformer</b> (ViT) blocks. These modules aim to incorporate high-frequency image information and image embedding features to generate image-informed <b>prompts.</b> Experiments are conducted on four distinct remote sensing scenarios, encompassing cloud detection, field monitoring, building detection and road mapping tasks . The experimental results not only showcase the improvement over the original SAM and U-Net across cloud, buildings, fields and roads scenarios, but also highlight the capacity of RSAM-Seg to discern absent areas within the ground truth of certain datasets, affirming its potential as an auxiliary annotation method. In addition, the performance in <b>few-shot</b> scenarios is commendable, underscores its potential in dealing with limited datasets.</p></p class="citation"></blockquote><h3 id=865--44254-edge-computing-enabled-real-time-video-analysis-via-adaptive-spatial-temporal-semantic-filtering-xiang-chen-et-al-2024>(8/65 | 44/254) Edge Computing Enabled Real-Time Video Analysis via Adaptive Spatial-Temporal Semantic Filtering (Xiang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Chen, Wenjie Zhu, Jiayuan Chen, Tong Zhang, Changyan Yi, Jun Cai. (2024)<br><strong>Edge Computing Enabled Real-Time Video Analysis via Adaptive Spatial-Temporal Semantic Filtering</strong><br><button class=copy-to-clipboard title="Edge Computing Enabled Real-Time Video Analysis via Adaptive Spatial-Temporal Semantic Filtering" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs-NI, cs.CV<br>Keyword Score: 50<br>Keywords: Object Detection, Bandit Algorithm, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18927v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18927v1.pdf filename=2402.18927v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a novel edge computing enabled real-time video analysis system for intelligent visual devices. The proposed system consists of a tracking-assisted <b>object</b> <b>detection</b> module (TAODM) and a region of interesting module (ROIM). TAODM adaptively determines the offloading decision to process each video frame locally with a tracking algorithm or to offload it to the edge server inferred by an <b>object</b> <b>detection</b> model. ROIM determines each offloading frame&rsquo;s resolution and detection model configuration to ensure that the analysis results can return in time. TAODM and ROIM interact jointly to filter the repetitive spatial-temporal semantic information to maximize the processing rate while ensuring high video analysis accuracy. Unlike most existing works, this paper investigates the real-time video analysis systems where the intelligent visual device connects to the edge server through a wireless network with fluctuating network conditions. We decompose the real-time video analysis problem into the offloading decision and configurations selection sub-problems. To solve these two sub-problems, we introduce a double deep Q network (DDQN) based offloading approach and a contextual multi-armed <b>bandit</b> (CMAB) based adaptive configurations selection approach, respectively. A DDQN-CMAB <b>reinforcement</b> <b>learning</b> (DCRL) training framework is further developed to integrate these two approaches to improve the overall video analyzing performance. Extensive <b>simulations</b> are conducted to evaluate the performance of the proposed solution, and demonstrate its superiority over counterparts.</p></p class="citation"></blockquote><h3 id=965--45254-stitching-gaps-fusing-situated-perceptual-knowledge-with-vision-transformers-for-high-level-image-classification-delfina-sol-martinez-pandiani-et-al-2024>(9/65 | 45/254) Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification (Delfina Sol Martinez Pandiani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Delfina Sol Martinez Pandiani, Nicolas Lazzari, Valentina Presutti. (2024)<br><strong>Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification</strong><br><button class=copy-to-clipboard title="Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Vision Transformer, Graph, Knowledge Graph, Knowledge Graph, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19339v1.pdf filename=2402.19339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing demand for automatic high-level image understanding, particularly in detecting abstract concepts (AC) within images, underscores the necessity for innovative and more interpretable approaches. These approaches need to harmonize traditional deep <b>vision</b> <b>methods</b> with the nuanced, context-dependent <b>knowledge</b> <b>humans</b> employ to interpret images at intricate semantic levels. In this work, we leverage situated perceptual <b>knowledge</b> <b>of</b> cultural images to enhance performance and interpretability in AC image classification. We automatically extract perceptual semantic units from images, which we then model and integrate into the ARTstract <b>Knowledge</b> <b>Graph</b> (AKG). This resource captures situated perceptual semantics gleaned from over 14,000 cultural images labeled with ACs. Additionally, we enhance the AKG with high-level linguistic frames. We compute <b>KG</b> embeddings and experiment with relative representations and hybrid approaches that fuse these embeddings with visual <b>transformer</b> embeddings. Finally, for interpretability, we conduct posthoc qualitative analyses by examining model similarities with training instances. Our results show that our hybrid KGE-ViT methods outperform existing techniques in AC image classification. The posthoc interpretability analyses reveal the visual <b>transformer&rsquo;s</b> proficiency in capturing pixel-level visual attributes, contrasting with our method&rsquo;s efficacy in representing more abstract and semantic scene elements. We demonstrate the synergy and complementarity between KGE embeddings&rsquo; situated perceptual <b>knowledge</b> <b>and</b> deep visual model&rsquo;s sensory-perceptual understanding for AC image classification. This work suggests a strong potential of neuro-symbolic methods for <b>knowledge</b> <b>integration</b> and robust image representation for use in downstream intricate visual comprehension tasks. All the materials and code are available online.</p></p class="citation"></blockquote><h3 id=1065--46254-a-simple-yet-effective-network-based-on-vision-transformer-for-camouflaged-object-and-salient-object-detection-chao-hao-et-al-2024>(10/65 | 46/254) A Simple yet Effective Network based on Vision Transformer for Camouflaged Object and Salient Object Detection (Chao Hao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Hao, Zitong Yu, Xin Liu, Jun Xu, Huanjing Yue, Jingyu Yang. (2024)<br><strong>A Simple yet Effective Network based on Vision Transformer for Camouflaged Object and Salient Object Detection</strong><br><button class=copy-to-clipboard title="A Simple yet Effective Network based on Vision Transformer for Camouflaged Object and Salient Object Detection" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Vision Transformer, Object Detection, Benchmarking, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18922v1.pdf filename=2402.18922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Camouflaged <b>object</b> <b>detection</b> (COD) and salient <b>object</b> <b>detection</b> (SOD) are two distinct yet closely-related computer <b>vision</b> <b>tasks</b> widely studied during the past decades. Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed <b>objects</b> <b>hidden</b> in the image, while SOD concentrates on the most prominent <b>objects</b> <b>in</b> the image. Previous works achieved good performance by stacking various hand-designed modules and multi-scale features. However, these carefully-designed complex networks often performed well on one task but not on another. In this work, we propose a simple yet effective network (SENet) based on <b>vision</b> <b>Transformer</b> (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones. Furthermore, to enhance the <b>Transformer&rsquo;s</b> ability to model local information, which is important for pixel-level binary segmentation tasks, we propose a local information capture module (LICM). We also propose a dynamic weighted loss (DW loss) based on Binary Cross-Entropy (BCE) and Intersection over Union (IoU) loss, which guides the network to pay more attention to those smaller and more difficult-to-find target <b>objects</b> <b>according</b> to their size. Moreover, we explore the issue of joint training of SOD and COD, and propose a preliminary solution to the conflict in joint training, further improving the performance of SOD. Extensive experiments on multiple <b>benchmark</b> datasets demonstrate the effectiveness of our method. The code is available at <a href=https://github.com/linuxsino/SENet>https://github.com/linuxsino/SENet</a>.</p></p class="citation"></blockquote><h3 id=1165--47254-maskfi-unsupervised-learning-of-wifi-and-vision-representations-for-multimodal-human-activity-recognition-jianfei-yang-et-al-2024>(11/65 | 47/254) MaskFi: Unsupervised Learning of WiFi and Vision Representations for Multimodal Human Activity Recognition (Jianfei Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianfei Yang, Shijie Tang, Yuecong Xu, Yunjiao Zhou, Lihua Xie. (2024)<br><strong>MaskFi: Unsupervised Learning of WiFi and Vision Representations for Multimodal Human Activity Recognition</strong><br><button class=copy-to-clipboard title="MaskFi: Unsupervised Learning of WiFi and Vision Representations for Multimodal Human Activity Recognition" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 41<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Representation Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19258v1.pdf filename=2402.19258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human activity recognition (HAR) has been playing an increasingly important role in various domains such as healthcare, security monitoring, and metaverse gaming. Though numerous HAR methods based on computer vision have been developed to show prominent performance, they still suffer from poor robustness in adverse visual conditions in particular low illumination, which motivates WiFi-based HAR to serve as a good complementary modality. Existing solutions using WiFi and vision modalities rely on massive labeled data that are very cumbersome to collect. In this paper, we propose a novel <b>unsupervised</b> <b>multimodal</b> HAR solution, MaskFi, that leverages only unlabeled video and WiFi activity data for model training. We propose a new algorithm, masked WiFi-vision modeling (MI2M), that enables the model to learn cross-modal and single-modal features by predicting the masked sections in <b>representation</b> <b>learning.</b> Benefiting from our <b>unsupervised</b> <b>learning</b> procedure, the network requires only a small amount of annotated data for <b>finetuning</b> and can adapt to the new environment with better performance. We conduct extensive experiments on two WiFi-vision datasets collected in-house, and our method achieves human activity recognition and human identification in terms of both robustness and accuracy.</p></p class="citation"></blockquote><h3 id=1265--48254-t3dnet-compressing-point-cloud-models-for-lightweight-3d-recognition-zhiyuan-yang-et-al-2024>(12/65 | 48/254) T3DNet: Compressing Point Cloud Models for Lightweight 3D Recognition (Zhiyuan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Yang, Yunjiao Zhou, Lihua Xie, Jianfei Yang. (2024)<br><strong>T3DNet: Compressing Point Cloud Models for Lightweight 3D Recognition</strong><br><button class=copy-to-clipboard title="T3DNet: Compressing Point Cloud Models for Lightweight 3D Recognition" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Knowledge Distillation, Pruning, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19264v1.pdf filename=2402.19264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D point cloud has been widely used in many mobile application scenarios, including autonomous driving and 3D sensing on mobile devices. However, existing 3D point cloud models tend to be large and cumbersome, making them hard to deploy on edged devices due to their high memory requirements and non-real-time latency. There has been a lack of research on how to compress 3D point cloud models into lightweight models. In this paper, we propose a method called T3DNet (Tiny 3D Network with augmEntation and <b>disTillation)</b> to address this issue. We find that the tiny model after network augmentation is much easier for a teacher to <b>distill.</b> Instead of gradually reducing the parameters through techniques such as <b>pruning</b> or <b>quantization,</b> we pre-define a tiny model and improve its performance through auxiliary supervision from augmented networks and the original model. We evaluate our method on several public datasets, including ModelNet40, ShapeNet, and ScanObjectNN. Our method can achieve high compression rates without significant accuracy sacrifice, achieving state-of-the-art performances on three datasets against existing methods. Amazingly, our T3DNet is 58 times smaller and 54 times faster than the original model yet with only 1.4% accuracy descent on the ModelNet40 dataset.</p></p class="citation"></blockquote><h3 id=1365--49254-weakly-supervised-monocular-3d-detection-with-a-single-view-image-xueying-jiang-et-al-2024>(13/65 | 49/254) Weakly Supervised Monocular 3D Detection with a Single-View Image (Xueying Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xueying Jiang, Sheng Jin, Lewei Lu, Xiaoqin Zhang, Shijian Lu. (2024)<br><strong>Weakly Supervised Monocular 3D Detection with a Single-View Image</strong><br><button class=copy-to-clipboard title="Weakly Supervised Monocular 3D Detection with a Single-View Image" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Knowledge Transfer, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19144v1.pdf filename=2402.19144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monocular 3D detection (M3D) aims for precise 3D object localization from a single-view image which usually involves labor-intensive annotation of 3D detection boxes. Weakly <b>supervised</b> M3D has recently been studied to obviate the 3D annotation process by leveraging many existing 2D annotations, but it often requires extra training data such as LiDAR point clouds or multi-view images which greatly degrades its applicability and usability in various applications. We propose SKD-WM3D, a weakly <b>supervised</b> monocular 3D detection framework that exploits depth information to achieve M3D with a single-view image exclusively without any 3D annotations or other training data. One key design in SKD-WM3D is a self-knowledge <b>distillation</b> framework, which transforms image features into 3D-like representations by fusing depth information and effectively mitigates the inherent depth ambiguity in monocular scenarios with little computational overhead in inference. In addition, we design an uncertainty-aware <b>distillation</b> loss and a gradient-targeted transfer modulation strategy which facilitate <b>knowledge</b> <b>acquisition</b> and <b>knowledge</b> <b>transfer,</b> respectively. Extensive experiments show that SKD-WM3D surpasses the state-of-the-art clearly and is even on par with many fully <b>supervised</b> methods.</p></p class="citation"></blockquote><h3 id=1465--50254-coft-ad-contrastive-fine-tuning-for-few-shot-anomaly-detection-jingyi-liao-et-al-2024>(14/65 | 50/254) COFT-AD: COntrastive Fine-Tuning for Few-Shot Anomaly Detection (Jingyi Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyi Liao, Xun Xu, Manh Cuong Nguyen, Adam Goodge, Chuan Sheng Foo. (2024)<br><strong>COFT-AD: COntrastive Fine-Tuning for Few-Shot Anomaly Detection</strong><br><button class=copy-to-clipboard title="COFT-AD: COntrastive Fine-Tuning for Few-Shot Anomaly Detection" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Anomaly Detection, Few-shot, Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18998v1.pdf filename=2402.18998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing approaches towards <b>anomaly</b> <b>detection~(AD)</b> often rely on a substantial amount of <b>anomaly-free</b> <b>data</b> to train representation and density models. However, large <b>anomaly-free</b> <b>datasets</b> may not always be available before the inference stage; in which case an <b>anomaly</b> <b>detection</b> model must be trained with only a handful of normal samples, a.k.a. <b>few-shot</b> <b>anomaly</b> <b>detection</b> (FSAD). In this paper, we propose a novel methodology to address the challenge of FSAD which incorporates two important techniques. Firstly, we employ a model pre-trained on a large source dataset to initialize model weights. Secondly, to ameliorate the covariate shift between source and target domains, we adopt contrastive training to <b>fine-tune</b> on the <b>few-shot</b> target domain data. To learn suitable representations for the downstream AD task, we additionally incorporate cross-instance positive pairs to encourage a tight cluster of the normal samples, and negative pairs for better separation between normal and synthesized negative samples. We evaluate <b>few-shot</b> <b>anomaly</b> <b>detection</b> on on 3 controlled AD tasks and 4 real-world AD tasks to demonstrate the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=1565--51254-dose-prediction-driven-radiotherapy-paramters-regression-via-intra--and-inter-relation-modeling-jiaqi-cui-et-al-2024>(15/65 | 51/254) Dose Prediction Driven Radiotherapy Paramters Regression via Intra- and Inter-Relation Modeling (Jiaqi Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqi Cui, Yuanyuan Xu, Jianghong Xiao, Yuchen Fei, Jiliu Zhou, Xingcheng Peng, Yan Wang. (2024)<br><strong>Dose Prediction Driven Radiotherapy Paramters Regression via Intra- and Inter-Relation Modeling</strong><br><button class=copy-to-clipboard title="Dose Prediction Driven Radiotherapy Paramters Regression via Intra- and Inter-Relation Modeling" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18879v1.pdf filename=2402.18879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning has facilitated the automation of radiotherapy by predicting accurate dose distribution maps. However, existing methods fail to derive the desirable radiotherapy parameters that can be directly input into the treatment planning system (TPS), impeding the full automation of radiotherapy. To enable more thorough automatic radiotherapy, in this paper, we propose a novel two-stage framework to directly regress the radiotherapy parameters, including a dose map prediction stage and a radiotherapy parameters regression stage. In stage one, we combine <b>transformer</b> and <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> to predict realistic dose maps with rich global and local information, providing accurate dosimetric knowledge for the subsequent parameters regression. In stage two, two elaborate modules, i.e., an intra-relation modeling (Intra-RM) module and an inter-relation modeling (Inter-RM) module, are designed to exploit the organ-specific and organ-shared features for precise parameters regression. Experimental results on a rectal cancer dataset demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=1665--52254-the-all-seeing-project-v2-towards-general-relation-comprehension-of-the-open-world-weiyun-wang-et-al-2024>(16/65 | 52/254) The All-Seeing Project V2: Towards General Relation Comprehension of the Open World (Weiyun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, Yu Qiao, Jifeng Dai. (2024)<br><strong>The All-Seeing Project V2: Towards General Relation Comprehension of the Open World</strong><br><button class=copy-to-clipboard title="The All-Seeing Project V2: Towards General Relation Comprehension of the Open World" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 39<br>Keywords: Graph, Benchmarking, Multi-modal, Text Generation, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19474v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19474v1.pdf filename=2402.19474v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the All-Seeing Project V2: a new model and dataset designed for understanding object relations in images. Specifically, we propose the All-Seeing Model V2 (ASMv2) that integrates the formulation of <b>text</b> <b>generation,</b> object localization, and relation comprehension into a relation conversation (ReC) task. Leveraging this unified task, our model excels not only in perceiving and recognizing all objects within the image but also in grasping the intricate relation <b>graph</b> between them, diminishing the relation hallucination often encountered by <b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs). To facilitate training and evaluation of MLLMs in relation understanding, we created the first high-quality ReC dataset ({AS-V2) which is aligned with the format of standard <b>instruction</b> <b>tuning</b> data. In addition, we design a new <b>benchmark,</b> termed Circular-based Relation Probing Evaluation (CRPE) for comprehensively evaluating the relation comprehension capabilities of MLLMs. Notably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware <b>benchmark,</b> surpassing the 43.14 of LLaVA-1.5 by a <b>large</b> <b>margin.</b> <b>We</b> hope that our work can inspire more future research and contribute to the evolution towards artificial general intelligence. Our project is released at <a href=https://github.com/OpenGVLab/all-seeing>https://github.com/OpenGVLab/all-seeing</a>.</p></p class="citation"></blockquote><h3 id=1765--53254-navigating-hallucinations-for-reasoning-of-unintentional-activities-shresth-grover-et-al-2024>(17/65 | 53/254) Navigating Hallucinations for Reasoning of Unintentional Activities (Shresth Grover et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shresth Grover, Vibhav Vineet, Yogesh S Rawat. (2024)<br><strong>Navigating Hallucinations for Reasoning of Unintentional Activities</strong><br><button class=copy-to-clipboard title="Navigating Hallucinations for Reasoning of Unintentional Activities" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Zero-shot, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19405v1.pdf filename=2402.19405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work we present a novel task of understanding unintentional human activities in videos. We formalize this problem as a <b>reasoning</b> task under <b>zero-shot</b> scenario, where given a video of an unintentional activity we want to know why it transitioned from intentional to unintentional. We first evaluate the effectiveness of current state-of-the-art Large <b>Multimodal</b> Models on this <b>reasoning</b> task and observe that they suffer from hallucination. We further propose a novel <b>prompting</b> technique,termed as Dream of Thoughts (DoT), which allows the model to navigate through hallucinated thoughts to achieve better <b>reasoning.</b> To evaluate the performance on this task, we also introduce three different specialized metrics designed to quantify the models <b>reasoning</b> capability. We perform our experiments on two different datasets, OOPs and UCF-Crimes, and our findings show that DOT <b>prompting</b> technique is able to outperform standard <b>prompting,</b> while minimizing hallucinations.</p></p class="citation"></blockquote><h3 id=1865--54254-entity-aware-multimodal-alignment-framework-for-news-image-captioning-junzhe-zhang-et-al-2024>(18/65 | 54/254) Entity-Aware Multimodal Alignment Framework for News Image Captioning (Junzhe Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junzhe Zhang, Huixuan Zhang, Xiaojun Wan. (2024)<br><strong>Entity-Aware Multimodal Alignment Framework for News Image Captioning</strong><br><button class=copy-to-clipboard title="Entity-Aware Multimodal Alignment Framework for News Image Captioning" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Zero-shot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19404v1.pdf filename=2402.19404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>News image captioning task is a variant of image captioning task which requires model to generate a more informative caption with news image and the associated news article. <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>models</b> have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in <b>zero-shot</b> setting. Their abilities to deal with the entities information are still limited after simply <b>fine-tuned</b> on news image captioning dataset. To obtain a more powerful model to handle the <b>multimodal</b> entity information, we design two <b>multimodal</b> entity-aware alignment tasks and an alignment framework to align the model and generate the news image captions. Our method achieves better results than previous state-of-the-art models in CIDEr score (72.33 -> 86.29) on GoodNews dataset and (70.83 -> 85.61) on NYTimes800k dataset.</p></p class="citation"></blockquote><h3 id=1965--55254-cricavpr-cross-image-correlation-aware-representation-learning-for-visual-place-recognition-feng-lu-et-al-2024>(19/65 | 55/254) CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition (Feng Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Lu, Xiangyuan Lan, Lijun Zhang, Dongmei Jiang, Yaowei Wang, Chun Yuan. (2024)<br><strong>CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition</strong><br><button class=copy-to-clipboard title="CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 35<br>Keywords: Convolution, Foundation Model, Representation Learning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19231v1.pdf filename=2402.19231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past decade, most methods in visual place recognition (VPR) have used neural networks to produce feature <b>representations.</b> <b>These</b> networks typically produce a global <b>representation</b> <b>of</b> a place image using only this image itself and neglect the cross-image variations (e.g. viewpoint and illumination), which limits their robustness in challenging scenes. In this paper, we propose a robust global <b>representation</b> <b>method</b> with cross-image correlation awareness for VPR, named CricaVPR. Our method uses the <b>self-attention</b> mechanism to correlate multiple images within a batch. These images can be taken in the same place with different conditions or viewpoints, or even captured from different places. Therefore, our method can utilize the cross-image variations as a cue to guide the <b>representation</b> <b>learning,</b> which ensures more robust features are produced. To further facilitate the robustness, we propose a multi-scale <b>convolution-enhanced</b> adaptation method to adapt pre-trained visual <b>foundation</b> <b>models</b> to the VPR task, which introduces the multi-scale local information to further enhance the cross-image correlation-aware <b>representation.</b> <b>Experimental</b> results show that our method outperforms state-of-the-art methods by a large margin with significantly less training time. Our method achieves 94.5% R@1 on Pitts30k using 512-dim global features. The code is released at <a href=https://github.com/Lu-Feng/CricaVPR>https://github.com/Lu-Feng/CricaVPR</a>.</p></p class="citation"></blockquote><h3 id=2065--56254-diffassemble-a-unified-graph-diffusion-model-for-2d-and-3d-reassembly-gianluca-scarpellini-et-al-2024>(20/65 | 56/254) DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly (Gianluca Scarpellini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gianluca Scarpellini, Stefano Fiorini, Francesco Giuliari, Pietro Morerio, Alessio Del Bue. (2024)<br><strong>DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly</strong><br><button class=copy-to-clipboard title="DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Diffusion Model, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19302v1.pdf filename=2402.19302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reassembly tasks play a fundamental role in many fields and multiple approaches exist to solve specific reassembly problems. In this context, we posit that a general unified model can effectively address them all, irrespective of the input data type (images, 3D, etc.). We introduce DiffAssemble, a <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)-based</b> architecture that learns to solve reassembly tasks using a <b>diffusion</b> <b>model</b> formulation. Our method treats the elements of a set, whether pieces of 2D patch or 3D object fragments, as nodes of a spatial <b>graph.</b> <b>Training</b> <b>is</b> performed by introducing noise into the position and rotation of the elements and iteratively denoising them to reconstruct the coherent initial pose. DiffAssemble achieves state-of-the-art (SOTA) results in most 2D and 3D reassembly tasks and is the first learning-based approach that solves 2D puzzles for both rotation and translation. Furthermore, we highlight its remarkable reduction in run-time, performing 11 times faster than the quickest optimization-based method for puzzle solving. Code available at <a href=https://github.com/IIT-PAVIS/DiffAssemble>https://github.com/IIT-PAVIS/DiffAssemble</a></p></p class="citation"></blockquote><h3 id=2165--57254-a-novel-approach-to-industrial-defect-generation-through-blended-latent-diffusion-model-with-online-adaptation-hanxi-li-et-al-2024>(21/65 | 57/254) A Novel Approach to Industrial Defect Generation through Blended Latent Diffusion Model with Online Adaptation (Hanxi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanxi Li, Zhengxun Zhang, Hao Chen, Lin Wu, Bo Li, Deyin Liu, Mingwen Wang. (2024)<br><strong>A Novel Approach to Industrial Defect Generation through Blended Latent Diffusion Model with Online Adaptation</strong><br><button class=copy-to-clipboard title="A Novel Approach to Industrial Defect Generation through Blended Latent Diffusion Model with Online Adaptation" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Anomaly Detection, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19330v1.pdf filename=2402.19330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effectively addressing the challenge of industrial <b>Anomaly</b> <b>Detection</b> (AD) necessitates an ample supply of defective samples, a constraint often hindered by their scarcity in industrial contexts. This paper introduces a novel algorithm designed to augment defective samples, thereby enhancing AD performance. The proposed method tailors the blended latent <b>diffusion</b> <b>model</b> for defect sample generation, employing a <b>diffusion</b> <b>model</b> to generate defective samples in the latent space. A feature editing process, controlled by a &ldquo;trimap&rdquo; mask and text <b>prompts,</b> refines the generated samples. The image generation inference process is structured into three stages: a free <b>diffusion</b> <b>stage,</b> an editing <b>diffusion</b> <b>stage,</b> and an online decoder adaptation stage. This sophisticated inference strategy yields high-quality synthetic defective samples with diverse pattern variations, leading to significantly improved AD accuracies based on the augmented training set. Specifically, on the widely recognized MVTec AD dataset, the proposed method elevates the state-of-the-art (SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD metrics AP, IAP, and IAP90, respectively. The implementation code of this work can be found at the GitHub repository <a href=https://github.com/GrandpaXun242/AdaBLDM.git>https://github.com/GrandpaXun242/AdaBLDM.git</a></p></p class="citation"></blockquote><h3 id=2265--58254-biggait-learning-gait-representation-you-want-by-large-vision-models-dingqiang-ye-et-al-2024>(22/65 | 58/254) BigGait: Learning Gait Representation You Want by Large Vision Models (Dingqiang Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dingqiang Ye, Chao Fan, Jingzhe Ma, Xiaoming Liu, Shiqi Yu. (2024)<br><strong>BigGait: Learning Gait Representation You Want by Large Vision Models</strong><br><button class=copy-to-clipboard title="BigGait: Learning Gait Representation You Want by Large Vision Models" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19122v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19122v1.pdf filename=2402.19122v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gait recognition stands as one of the most pivotal remote identification technologies and progressively expands across research and industrial communities. However, existing gait recognition methods heavily rely on task-specific upstream driven by <b>supervised</b> <b>learning</b> to provide explicit gait representations, which inevitably introduce expensive annotation costs and potentially cause cumulative errors. Escaping from this trend, this work explores effective gait representations based on the all-purpose knowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a simple yet efficient gait framework, termed BigGait. Specifically, the Gait Representation Extractor (GRE) in BigGait effectively transforms all-purpose knowledge into implicit gait features in an <b>unsupervised</b> manner, drawing from design principles of established gait representation construction approaches. Experimental results on CCPG, CAISA-B* and SUSTech1K indicate that BigGait significantly outperforms the previous methods in both self-domain and cross-domain tasks in most cases, and provides a more practical paradigm for learning the next-generation gait representation. Eventually, we delve into prospective challenges and promising directions in LVMs-based gait recognition, aiming to inspire future work in this emerging topic. The source code will be available at <a href=https://github.com/ShiqiYu/OpenGait>https://github.com/ShiqiYu/OpenGait</a>.</p></p class="citation"></blockquote><h3 id=2365--59254-analysis-of-the-two-step-heterogeneous-transfer-learning-for-laryngeal-blood-vessel-classification-issue-and-improvement-xinyi-fang-et-al-2024>(23/65 | 59/254) Analysis of the Two-Step Heterogeneous Transfer Learning for Laryngeal Blood Vessel Classification: Issue and Improvement (Xinyi Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyi Fang, Chak Fong Chong, Kei Long Wong, Yapeng Wang, Tiankui Zhang, Sio-Kei Im. (2024)<br><strong>Analysis of the Two-Step Heterogeneous Transfer Learning for Laryngeal Blood Vessel Classification: Issue and Improvement</strong><br><button class=copy-to-clipboard title="Analysis of the Two-Step Heterogeneous Transfer Learning for Laryngeal Blood Vessel Classification: Issue and Improvement" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19001v1.pdf filename=2402.19001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Transferring features learned from natural to medical images for classification is common. However, challenges arise due to the scarcity of certain medical image types and the feature disparities between natural and medical images. Two-step <b>transfer</b> <b>learning</b> has been recognized as a promising solution for this issue. However, choosing an appropriate intermediate domain would be critical in further improving the classification performance. In this work, we explore the effectiveness of using color fundus photographs of the diabetic retina dataset as an intermediate domain for two-step heterogeneous learning (THTL) to classify laryngeal vascular images with nine deep-learning models. Experiment results confirm that although the images in both the intermediate and target domains share vascularized characteristics, the accuracy is drastically reduced compared to one-step <b>transfer</b> <b>learning,</b> where only the last layer is <b>fine-tuned</b> (e.g., ResNet18 drops 14.7%, ResNet50 drops 14.8%). By analyzing the Layer Class Activation Maps (LayerCAM), we uncover a novel finding that the prevalent radial vascular pattern in the intermediate domain prevents learning the features of twisted and tangled vessels that distinguish the malignant class in the target domain. To address the performance drop, we propose the Step-Wise <b>Fine-Tuning</b> (SWFT) method on ResNet in the second step of THTL, resulting in substantial accuracy improvements. Compared to THTL&rsquo;s second step, where only the last layer is <b>fine-tuned,</b> accuracy increases by 26.1% for ResNet18 and 20.4% for ResNet50. Additionally, compared to training from scratch, using ImageNet as the source domain could slightly improve classification performance for laryngeal vascular, but the differences are insignificant.</p></p class="citation"></blockquote><h3 id=2465--60254-boosting-semi-supervised-object-detection-in-remote-sensing-images-with-active-teaching-boxuan-zhang-et-al-2024>(24/65 | 60/254) Boosting Semi-Supervised Object Detection in Remote Sensing Images With Active Teaching (Boxuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boxuan Zhang, Zengmao Wang, Bo Du. (2024)<br><strong>Boosting Semi-Supervised Object Detection in Remote Sensing Images With Active Teaching</strong><br><button class=copy-to-clipboard title="Boosting Semi-Supervised Object Detection in Remote Sensing Images With Active Teaching" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Active Learning, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18958v1.pdf filename=2402.18958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The lack of <b>object-level</b> <b>annotations</b> poses a significant challenge for <b>object</b> <b>detection</b> in remote sensing images (RSIs). To address this issue, <b>active</b> <b>learning</b> (AL) and <b>semi-supervised</b> <b>learning</b> (SSL) techniques have been proposed to enhance the quality and quantity of annotations. AL focuses on selecting the most informative samples for annotation, while SSL leverages the knowledge from unlabeled samples. In this letter, we propose a novel AL method to boost <b>semi-supervised</b> <b>object</b> <b>detection</b> (SSOD) for remote sensing images with a teacher student network, called SSOD-AT. The proposed method incorporates an RoI comparison module (RoICM) to generate high-confidence pseudo-labels for regions of interest (RoIs). Meanwhile, the RoICM is utilized to identify the top-K uncertain images. To reduce redundancy in the top-K uncertain images for human labeling, a diversity criterion is introduced based on <b>object-level</b> <b>prototypes</b> of different categories using both labeled and pseudo-labeled images. Extensive experiments on DOTA and DIOR, two popular datasets, demonstrate that our proposed method outperforms state-of-the-art methods for <b>object</b> <b>detection</b> in RSIs. Compared with the best performance in the SOTA methods, the proposed method achieves 1 percent improvement in most cases in the whole AL.</p></p class="citation"></blockquote><h3 id=2565--61254-decompose-and-compose-a-compositional-approach-to-mitigating-spurious-correlation-fahimeh-hosseini-noohdani-et-al-2024>(25/65 | 61/254) Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation (Fahimeh Hosseini Noohdani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fahimeh Hosseini Noohdani, Parsa Hosseini, Arian Yazdan Parast, Hamidreza Yaghoubi Araghi, Mahdieh Soleymani Baghshah. (2024)<br><strong>Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation</strong><br><button class=copy-to-clipboard title="Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Counter-factual, Distribution Shift, Distribution Shift, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18919v1.pdf filename=2402.18919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While standard Empirical Risk Minimization (ERM) training is proven effective for image classification on in-distribution data, it fails to perform well on <b>out-of-distribution</b> samples. One of the main sources of <b>distribution</b> <b>shift</b> for image classification is the compositional nature of images. Specifically, in addition to the main object or component(s) determining the label, some other image components usually exist, which may lead to the shift of input <b>distribution</b> <b>between</b> train and test environments. More importantly, these components may have spurious correlations with the label. To address this issue, we propose Decompose-and-Compose (DaC), which improves robustness to correlation shift by a compositional approach based on combining elements of images. Based on our observations, models trained with ERM usually highly attend to either the causal components or the components having a high spurious correlation with the label (especially in datapoints on which models have a high confidence). In fact, according to the amount of spurious correlation and the easiness of classification based on the causal or non-causal components, the model usually attends to one of these more (on samples with high confidence). Following this, we first try to identify the causal components of images using class activation maps of models trained with ERM. Afterward, we intervene on images by combining them and retraining the model on the augmented data, including the <b>counterfactual</b> ones. Along with its high interpretability, this work proposes a group-balancing method by intervening on images without requiring group labels or information regarding the spurious features during training. The method has an overall better worst group accuracy compared to previous methods with the same amount of supervision on the group labels in correlation shift.</p></p class="citation"></blockquote><h3 id=2665--62254-bfrformer-transformer-based-generator-for-real-world-blind-face-restoration-guojing-ge-et-al-2024>(26/65 | 62/254) BFRFormer: Transformer-based generator for Real-World Blind Face Restoration (Guojing Ge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guojing Ge, Qi Song, Guibo Zhu, Yuting Zhang, Jinglu Chen, Miao Xin, Ming Tang, Jinqiao Wang. (2024)<br><strong>BFRFormer: Transformer-based generator for Real-World Blind Face Restoration</strong><br><button class=copy-to-clipboard title="BFRFormer: Transformer-based generator for Real-World Blind Face Restoration" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18811v1.pdf filename=2402.18811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Blind face restoration is a challenging task due to the unknown and complex degradation. Although face prior-based methods and reference-based methods have recently demonstrated high-quality results, the restored images tend to contain over-smoothed results and lose identity-preserved details when the degradation is severe. It is observed that this is attributed to short-range dependencies, the intrinsic limitation of <b>convolutional</b> <b>neural</b> <b>networks.</b> To model long-range dependencies, we propose a <b>Transformer-based</b> blind face restoration method, named BFRFormer, to reconstruct images with more identity-preserved details in an end-to-end manner. In BFRFormer, to remove blocking artifacts, the wavelet discriminator and aggregated attention module are developed, and spectral normalization and balanced consistency regulation are adaptively applied to address the training instability and over-fitting problem, respectively. Extensive experiments show that our method outperforms state-of-the-art methods on a synthetic dataset and four real-world datasets. The source code, Casia-Test dataset, and pre-trained models are released at <a href=https://github.com/s8Znk/BFRFormer>https://github.com/s8Znk/BFRFormer</a>.</p></p class="citation"></blockquote><h3 id=2765--63254-a-quantitative-evaluation-of-score-distillation-sampling-based-text-to-3d-xiaohan-fei-et-al-2024>(27/65 | 63/254) A Quantitative Evaluation of Score Distillation Sampling Based Text-to-3D (Xiaohan Fei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohan Fei, Chethan Parameshwara, Jiawei Mo, Xiaolong Li, Ashwin Swaminathan, CJ Taylor, Paolo Favaro, Stefano Soatto. (2024)<br><strong>A Quantitative Evaluation of Score Distillation Sampling Based Text-to-3D</strong><br><button class=copy-to-clipboard title="A Quantitative Evaluation of Score Distillation Sampling Based Text-to-3D" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Knowledge Distillation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18780v1.pdf filename=2402.18780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of generative models that create 3D content from a text <b>prompt</b> has made considerable strides thanks to the use of the score <b>distillation</b> sampling (SDS) method on pre-trained <b>diffusion</b> <b>models</b> for image generation. However, the SDS method is also the source of several artifacts, such as the Janus problem, the misalignment between the text <b>prompt</b> and the generated 3D model, and 3D model inaccuracies. While existing methods heavily rely on the qualitative assessment of these artifacts through visual inspection of a limited set of samples, in this work we propose more objective quantitative evaluation metrics, which we cross-validate via human ratings, and show analysis of the failure cases of the SDS technique. We demonstrate the effectiveness of this analysis by designing a novel computationally efficient baseline model that achieves state-of-the-art performance on the proposed metrics while addressing all the above-mentioned artifacts.</p></p class="citation"></blockquote><h3 id=2865--64254-enhancing-visual-document-understanding-with-contrastive-learning-in-large-visual-language-models-xin-li-et-al-2024>(28/65 | 64/254) Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models (Xin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Li, Yunfei Wu, Xinghua Jiang, Zhihao Guo, Mingming Gong, Haoyu Cao, Yinsong Liu, Deqiang Jiang, Xing Sun. (2024)<br><strong>Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models</strong><br><button class=copy-to-clipboard title="Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Benchmarking, Contrastive Learning, Multi-modal, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19014v1.pdf filename=2402.19014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the advent of Large Visual-Language Models (LVLMs) has received increasing attention across various domains, particularly in the field of visual document understanding (VDU). Different from conventional <b>vision-language</b> tasks, VDU is specifically concerned with text-rich scenarios containing abundant document elements. Nevertheless, the importance of fine-grained features remains largely unexplored within the community of LVLMs, leading to suboptimal performance in text-rich scenarios. In this paper, we abbreviate it as the fine-grained feature collapse issue. With the aim of filling this gap, we propose a <b>contrastive</b> <b>learning</b> framework, termed Document Object <b>COntrastive</b> <b>learning</b> (DoCo), specifically tailored for the downstream tasks of VDU. DoCo leverages an auxiliary <b>multimodal</b> encoder to obtain the features of document objects and align them to the visual features generated by the vision encoder of LVLM, which enhances visual representation in text-rich scenarios. It can represent that the <b>contrastive</b> <b>learning</b> between the visual holistic representations and the <b>multimodal</b> fine-grained features of document objects can assist the vision encoder in acquiring more effective visual cues, thereby enhancing the comprehension of text-rich documents in LVLMs. We also demonstrate that the proposed DoCo serves as a plug-and-play pre-training method, which can be employed in the pre-training of various LVLMs without inducing any increase in computational complexity during the inference process. Extensive experimental results on multiple <b>benchmarks</b> of VDU reveal that LVLMs equipped with our proposed DoCo can achieve superior performance and mitigate the gap between VDU and generic <b>vision-language</b> tasks.</p></p class="citation"></blockquote><h3 id=2965--65254-percept-chat-and-then-adapt-multimodal-knowledge-transfer-of-foundation-models-for-open-world-video-recognition-boyu-chen-et-al-2024>(29/65 | 65/254) Percept, Chat, and then Adapt: Multimodal Knowledge Transfer of Foundation Models for Open-World Video Recognition (Boyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyu Chen, Siran Chen, Kunchang Li, Qinglin Xu, Yu Qiao, Yali Wang. (2024)<br><strong>Percept, Chat, and then Adapt: Multimodal Knowledge Transfer of Foundation Models for Open-World Video Recognition</strong><br><button class=copy-to-clipboard title="Percept, Chat, and then Adapt: Multimodal Knowledge Transfer of Foundation Models for Open-World Video Recognition" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Benchmarking, Foundation Model, Knowledge Transfer, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18951v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18951v1.pdf filename=2402.18951v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-world video recognition is challenging since traditional networks are not generalized well on complex environment variations. Alternatively, <b>foundation</b> <b>models</b> with rich <b>knowledge</b> <b>have</b> recently shown their generalization power. However, how to apply such <b>knowledge</b> <b>has</b> not been fully explored for open-world video recognition. To this end, we propose a generic <b>knowledge</b> <b>transfer</b> pipeline, which progressively exploits and integrates external <b>multimodal</b> <b>knowledge</b> <b>from</b> <b>foundation</b> <b>models</b> to boost open-world video recognition. We name it PCA, based on three stages of Percept, Chat, and Adapt. First, we perform Percept process to reduce the video domain gap and obtain external visual <b>knowledge.</b> <b>Second,</b> we generate rich linguistic semantics as external textual <b>knowledge</b> <b>in</b> Chat stage. Finally, we blend external <b>multimodal</b> <b>knowledge</b> <b>in</b> Adapt stage, by inserting <b>multimodal</b> <b>knowledge</b> <b>adaptation</b> modules into networks. We conduct extensive experiments on three challenging open-world video <b>benchmarks,</b> i.e., TinyVIRAT, ARID, and QV-Pipe. Our approach achieves state-of-the-art performance on all three datasets.</p></p class="citation"></blockquote><h3 id=3065--66254-panda-70m-captioning-70m-videos-with-multiple-cross-modality-teachers-tsai-shien-chen-et-al-2024>(30/65 | 66/254) Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers (Tsai-Shien Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, Sergey Tulyakov. (2024)<br><strong>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</strong><br><button class=copy-to-clipboard title="Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19479v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19479v1.pdf filename=2402.19479v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The quality of the data and annotation upper-bounds the quality of a downstream model. While there exist large text corpora and <b>image-text</b> pairs, high-quality video-text data is much harder to collect. First of all, manual labeling is more time-consuming, as it requires an annotator to watch an entire video. Second, videos have a temporal dimension, consisting of several scenes stacked together, and showing multiple actions. Accordingly, to establish a video dataset with high-quality captions, we propose an automatic approach leveraging <b>multimodal</b> inputs, such as textual video description, subtitles, and individual video frames. Specifically, we curate 3.8M high-resolution videos from the publicly available HD-VILA-100M dataset. We then split them into semantically consistent video clips, and apply multiple cross-modality teacher models to obtain captions for each video. Next, we <b>finetune</b> a retrieval model on a small subset where the best caption of each video is manually selected and then employ the model in the whole dataset to select the best caption as the annotation. In this way, we get 70M videos paired with high-quality text captions. We dub the dataset as Panda-70M. We show the value of the proposed dataset on three downstream tasks: video captioning, video and text retrieval, and text-driven video generation. The models trained on the proposed data score substantially better on the majority of metrics across all the tasks.</p></p class="citation"></blockquote><h3 id=3165--67254-debiased-novel-category-discovering-and-localization-juexiao-feng-et-al-2024>(31/65 | 67/254) Debiased Novel Category Discovering and Localization (Juexiao Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juexiao Feng, Yuhong Yang, Yanchun Xie, Yaqian Li, Yandong Guo, Yuchen Guo, Yuwei He, Liuyu Xiang, Guiguang Ding. (2024)<br><strong>Debiased Novel Category Discovering and Localization</strong><br><button class=copy-to-clipboard title="Debiased Novel Category Discovering and Localization" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Object Detection, Benchmarking, Clustering, Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18821v1.pdf filename=2402.18821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>object</b> <b>detection</b> in deep learning has experienced rapid development. However, most existing <b>object</b> <b>detection</b> models perform well only on closed-set datasets, ignoring a large number of potential <b>objects</b> <b>whose</b> categories are not defined in the training set. These <b>objects</b> <b>are</b> often identified as background or incorrectly classified as pre-defined categories by the detectors. In this paper, we focus on the challenging problem of Novel Class Discovery and Localization (NCDL), aiming to train detectors that can detect the categories present in the training data, while also actively discover, localize, and cluster new categories. We analyze existing NCDL methods and identify the core issue: <b>object</b> <b>detectors</b> tend to be biased towards seen <b>objects,</b> <b>and</b> this leads to the neglect of unseen targets. To address this issue, we first propose an Debiased Region Mining (DRM) approach that combines class-agnostic Region Proposal Network (RPN) and class-aware RPN in a complementary manner. Additionally, we suggest to improve the representation network through semi-supervised <b>contrastive</b> <b>learning</b> by leveraging unlabeled data. Finally, we adopt a simple and efficient mini-batch K-means <b>clustering</b> method for novel class discovery. We conduct extensive experiments on the NCDL <b>benchmark,</b> and the results demonstrate that the proposed DRM approach significantly outperforms previous methods, establishing a new state-of-the-art.</p></p class="citation"></blockquote><h3 id=3265--68254-learning-a-generalized-physical-face-model-from-data-lingchen-yang-et-al-2024>(32/65 | 68/254) Learning a Generalized Physical Face Model From Data (Lingchen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingchen Yang, Gaspard Zoss, Prashanth Chandran, Markus Gross, Barbara Solenthaler, Eftychios Sifakis, Derek Bradley. (2024)<br><strong>Learning a Generalized Physical Face Model From Data</strong><br><button class=copy-to-clipboard title="Learning a Generalized Physical Face Model From Data" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19477v1.pdf filename=2402.19477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Physically-based <b>simulation</b> is a powerful approach for 3D facial animation as the resulting deformations are governed by physical constraints, allowing to easily resolve self-collisions, respond to external forces and perform realistic anatomy edits. Today&rsquo;s methods are data-driven, where the actuations for finite elements are inferred from captured skin <b>geometry.</b> Unfortunately, these approaches have not been widely adopted due to the complexity of initializing the material space and learning the deformation model for each character separately, which often requires a skilled artist followed by lengthy network training. In this work, we aim to make physics-based facial animation more accessible by proposing a generalized physical face model that we learn from a large 3D face dataset in a <b>simulation-free</b> manner. Once trained, our model can be quickly fit to any unseen identity and produce a ready-to-animate physical face model automatically. Fitting is as easy as providing a single 3D face scan, or even a single face image. After fitting, we offer intuitive animation controls, as well as the ability to retarget animations across characters. All the while, the resulting animations allow for physical effects like collision avoidance, gravity, paralysis, bone reshaping and more.</p></p class="citation"></blockquote><h3 id=3365--69254-aligning-knowledge-graph-with-visual-perception-for-object-goal-navigation-nuo-xu-et-al-2024>(33/65 | 69/254) Aligning Knowledge Graph with Visual Perception for Object-goal Navigation (Nuo Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nuo Xu, Wen Wang, Rong Yang, Mengjie Qin, Zheyuan Lin, Wei Song, Chunlong Zhang, Jason Gu, Chao Li. (2024)<br><strong>Aligning Knowledge Graph with Visual Perception for Object-goal Navigation</strong><br><button class=copy-to-clipboard title="Aligning Knowledge Graph with Visual Perception for Object-goal Navigation" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 24<br>Keywords: Graph, Knowledge Graph, Multi-modal, Multi-modal, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18892v1.pdf filename=2402.18892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Object-goal navigation is a challenging task that requires guiding an agent to specific objects based on first-person visual observations. The ability of agent to comprehend its surroundings plays a crucial role in achieving successful object finding. However, existing <b>knowledge-graph-based</b> <b>navigators</b> often rely on discrete categorical one-hot vectors and vote counting strategy to construct <b>graph</b> representation of the scenes, which results in misalignment with visual images. To provide more accurate and coherent scene descriptions and address this misalignment issue, we propose the Aligning <b>Knowledge</b> <b>Graph</b> with Visual Perception (AKGVP) method for object-goal navigation. Technically, our approach introduces continuous modeling of the hierarchical scene architecture and leverages visual-language pre-training to align natural language description with visual perception. The integration of a continuous <b>knowledge</b> <b>graph</b> architecture and <b>multimodal</b> feature alignment empowers the navigator with a remarkable <b>zero-shot</b> navigation capability. We extensively evaluate our method using the AI2-THOR simulator and conduct a series of experiments to demonstrate the effectiveness and efficiency of our navigator. Code available: <a href=https://github.com/nuoxu/AKGVP>https://github.com/nuoxu/AKGVP</a>.</p></p class="citation"></blockquote><h3 id=3465--70254-semoli-what-moves-together-belongs-together-jenny-seidenschwarz-et-al-2024>(34/65 | 70/254) SeMoLi: What Moves Together Belongs Together (Jenny Seidenschwarz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jenny Seidenschwarz, Aljoša Ošep, Francesco Ferroni, Simon Lucey, Laura Leal-Taixé. (2024)<br><strong>SeMoLi: What Moves Together Belongs Together</strong><br><button class=copy-to-clipboard title="SeMoLi: What Moves Together Belongs Together" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Message-Passing, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19463v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19463v1.pdf filename=2402.19463v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We tackle semi-supervised <b>object</b> <b>detection</b> based on motion cues. Recent results suggest that heuristic-based <b>clustering</b> methods in conjunction with <b>object</b> <b>trackers</b> can be used to pseudo-label instances of moving <b>objects</b> <b>and</b> use these as supervisory signals to train 3D <b>object</b> <b>detectors</b> in Lidar data without manual supervision. We re-think this approach and suggest that both, <b>object</b> <b>detection,</b> as well as motion-inspired pseudo-labeling, can be tackled in a data-driven manner. We leverage recent advances in scene flow estimation to obtain point trajectories from which we extract long-term, class-agnostic motion patterns. Revisiting correlation <b>clustering</b> in the context of message passing networks, we learn to group those motion patterns to cluster points to <b>object</b> <b>instances.</b> By estimating the full extent of the <b>objects,</b> <b>we</b> obtain per-scan 3D bounding boxes that we use to supervise a Lidar <b>object</b> <b>detection</b> network. Our method not only outperforms prior heuristic-based approaches (57.5 AP, +14 improvement over prior work), more importantly, we show we can pseudo-label and train <b>object</b> <b>detectors</b> across datasets.</p></p class="citation"></blockquote><h3 id=3565--71254-pem-prototype-based-efficient-maskformer-for-image-segmentation-niccolò-cavagnero-et-al-2024>(35/65 | 71/254) PEM: Prototype-based Efficient MaskFormer for Image Segmentation (Niccolò Cavagnero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niccolò Cavagnero, Gabriele Rosi, Claudia Cuttano, Francesca Pistilli, Marco Ciccone, Giuseppe Averta, Fabio Cermelli. (2024)<br><strong>PEM: Prototype-based Efficient MaskFormer for Image Segmentation</strong><br><button class=copy-to-clipboard title="PEM: Prototype-based Efficient MaskFormer for Image Segmentation" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19422v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19422v2.pdf filename=2402.19422v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>transformer-based</b> architectures have shown impressive results in the field of image segmentation. Thanks to their flexibility, they obtain outstanding performance in multiple segmentation tasks, such as semantic and panoptic, under a single unified framework. To achieve such impressive performance, these architectures employ intensive operations and require substantial computational resources, which are often not available, especially on edge devices. To fill this gap, we propose Prototype-based Efficient MaskFormer (PEM), an efficient <b>transformer-based</b> architecture that can operate in multiple segmentation tasks. PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance. In addition, PEM introduces an efficient multi-scale feature pyramid network, capable of extracting features that have high semantic content in an efficient way, thanks to the combination of deformable <b>convolutions</b> and context-based self-modulation. We <b>benchmark</b> the proposed PEM architecture on two tasks, semantic and panoptic segmentation, evaluated on two different datasets, Cityscapes and ADE20K. PEM demonstrates outstanding performance on every task and dataset, outperforming task-specific architectures while being comparable and even better than computationally-expensive baselines.</p></p class="citation"></blockquote><h3 id=3665--72254-a-sam-guided-two-stream-lightweight-model-for-anomaly-detection-chenghao-li-et-al-2024>(36/65 | 72/254) A SAM-guided Two-stream Lightweight Model for Anomaly Detection (Chenghao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenghao Li, Lei Qi, Xin Geng. (2024)<br><strong>A SAM-guided Two-stream Lightweight Model for Anomaly Detection</strong><br><button class=copy-to-clipboard title="A SAM-guided Two-stream Lightweight Model for Anomaly Detection" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Anomaly Detection, Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19145v1.pdf filename=2402.19145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In industrial <b>anomaly</b> <b>detection,</b> model efficiency and mobile-friendliness become the primary concerns in real-world applications. Simultaneously, the impressive generalization capabilities of Segment Anything (SAM) have garnered broad academic attention, making it an ideal choice for localizing unseen anomalies and diverse real-world patterns. In this paper, considering these two critical factors, we propose a SAM-guided Two-stream Lightweight Model for <b>unsupervised</b> <b>anomaly</b> <b>detection</b> (STLM) that not only aligns with the two practical application requirements but also harnesses the robust generalization capabilities of SAM. We employ two lightweight image encoders, i.e., our two-stream lightweight module, guided by SAM&rsquo;s knowledge. To be specific, one stream is trained to generate discriminative and general feature representations in both normal and anomalous regions, while the other stream reconstructs the same images without anomalies, which effectively enhances the differentiation of two-stream representations when facing anomalous regions. Furthermore, we employ a shared mask decoder and a feature aggregation module to generate <b>anomaly</b> <b>maps.</b> Our experiments conducted on MVTec AD <b>benchmark</b> show that STLM, with about 16M parameters and achieving an inference time in 20ms, competes effectively with state-of-the-art methods in terms of performance, 98.26% on pixel-level AUC and 94.92% on PRO. We further experiment on more difficult datasets, e.g., VisA and DAGM, to demonstrate the effectiveness and generalizability of STLM.</p></p class="citation"></blockquote><h3 id=3765--73254-theoretically-achieving-continuous-representation-of-oriented-bounding-boxes-zikai-xiao-et-al-2024>(37/65 | 73/254) Theoretically Achieving Continuous Representation of Oriented Bounding Boxes (Zikai Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zikai Xiao, Guo-Ye Yang, Xue Yang, Tai-Jiang Mu, Junchi Yan, Shi-min Hu. (2024)<br><strong>Theoretically Achieving Continuous Representation of Oriented Bounding Boxes</strong><br><button class=copy-to-clipboard title="Theoretically Achieving Continuous Representation of Oriented Bounding Boxes" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18975v1.pdf filename=2402.18975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Considerable efforts have been devoted to Oriented <b>Object</b> <b>Detection</b> (OOD). However, one lasting issue regarding the discontinuity in Oriented Bounding Box (OBB) representation remains unresolved, which is an inherent bottleneck for extant OOD methods. This paper endeavors to completely solve this issue in a theoretically guaranteed manner and puts an end to the ad-hoc efforts in this direction. Prior studies typically can only address one of the two cases of discontinuity: rotation and aspect ratio, and often inadvertently introduce decoding discontinuity, e.g. Decoding Incompleteness (DI) and Decoding Ambiguity (DA) as discussed in literature. Specifically, we propose a novel representation method called Continuous OBB (COBB), which can be readily integrated into existing detectors e.g. Faster-RCNN as a plugin. It can theoretically ensure continuity in bounding box regression which to our best knowledge, has not been achieved in literature for rectangle-based <b>object</b> <b>representation.</b> For <b>fairness</b> and transparency of experiments, we have developed a modularized <b>benchmark</b> based on the open-source deep learning framework Jittor&rsquo;s detection toolbox JDet for OOD evaluation. On the popular DOTA dataset, by integrating Faster-RCNN as the same baseline model, our new method outperforms the peer method Gliding Vertex by 1.13% mAP50 (relative improvement 1.54%), and 2.46% mAP75 (relative improvement 5.91%), without any tricks.</p></p class="citation"></blockquote><h3 id=3865--74254-switchlight-co-design-of-physics-driven-architecture-and-pre-training-framework-for-human-portrait-relighting-hoon-kim-et-al-2024>(38/65 | 74/254) SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting (Hoon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hoon Kim, Minje Jang, Wonjun Yoon, Jisoo Lee, Donghyun Na, Sanghyun Woo. (2024)<br><strong>SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting</strong><br><button class=copy-to-clipboard title="SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18848v1.pdf filename=2402.18848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a co-designed approach for human portrait relighting that combines a physics-guided architecture with a pre-training framework. Drawing on the Cook-Torrance reflectance model, we have meticulously configured the architecture design to precisely simulate light-surface interactions. Furthermore, to overcome the limitation of scarce high-quality lightstage data, we have developed a <b>self-supervised</b> <b>pre-training</b> strategy. This novel combination of accurate physical modeling and expanded training dataset establishes a new <b>benchmark</b> in relighting realism.</p></p class="citation"></blockquote><h3 id=3965--75254-modality-agnostic-structural-image-representation-learning-for-deformable-multi-modality-medical-image-registration-tony-c-w-mok-et-al-2024>(39/65 | 75/254) Modality-Agnostic Structural Image Representation Learning for Deformable Multi-Modality Medical Image Registration (Tony C. W. Mok et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tony C. W. Mok, Zi Li, Yunhao Bai, Jianpeng Zhang, Wei Liu, Yan-Jie Zhou, Ke Yan, Dakai Jin, Yu Shi, Xiaoli Yin, Le Lu, Ling Zhang. (2024)<br><strong>Modality-Agnostic Structural Image Representation Learning for Deformable Multi-Modality Medical Image Registration</strong><br><button class=copy-to-clipboard title="Modality-Agnostic Structural Image Representation Learning for Deformable Multi-Modality Medical Image Registration" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 21<br>Keywords: Contrastive Learning, Multi-modal, Multi-modal, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18933v1.pdf filename=2402.18933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Establishing dense anatomical correspondence across distinct imaging modalities is a foundational yet challenging procedure for numerous medical image analysis studies and image-guided radiotherapy. Existing multi-modality image registration algorithms rely on statistical-based similarity measures or local structural image <b>representations.</b> <b>However,</b> the former is sensitive to locally varying noise, while the latter is not discriminative enough to cope with complex anatomical structures in <b>multimodal</b> scans, causing ambiguity in determining the anatomical correspondence across scans with different modalities. In this paper, we propose a modality-agnostic structural <b>representation</b> <b>learning</b> method, which leverages Deep Neighbourhood Self-similarity (DNS) and anatomy-aware <b>contrastive</b> <b>learning</b> to learn discriminative and contrast-invariance deep structural image <b>representations</b> <b>(DSIR)</b> without the need for anatomical delineations or pre-aligned training images. We evaluate our method on multiphase CT, abdomen MR-CT, and brain MR T1w-T2w registration. Comprehensive results demonstrate that our method is superior to the conventional local structural <b>representation</b> <b>and</b> statistical-based similarity measures in terms of discriminability and accuracy.</p></p class="citation"></blockquote><h3 id=4065--76254-effective-message-hiding-with-order-preserving-mechanisms-gao-yu-et-al-2024>(40/65 | 76/254) Effective Message Hiding with Order-Preserving Mechanisms (Gao Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gao Yu, Qiu Xuchong, Ye Zihan. (2024)<br><strong>Effective Message Hiding with Order-Preserving Mechanisms</strong><br><button class=copy-to-clipboard title="Effective Message Hiding with Order-Preserving Mechanisms" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19160v1.pdf filename=2402.19160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Message hiding, a technique that conceals secret message bits within a cover image, aims to achieve an optimal balance among message capacity, recovery accuracy, and imperceptibility. While <b>convolutional</b> <b>neural</b> <b>networks</b> have notably improved message capacity and imperceptibility, achieving high recovery accuracy remains challenging. This challenge arises because <b>convolutional</b> <b>operations</b> <b>struggle</b> to preserve the sequential order of message bits and effectively address the discrepancy between these two modalities. To address this, we propose StegaFormer, an innovative MLP-based framework designed to preserve bit order and enable global fusion between modalities. Specifically, StegaFormer incorporates three crucial components: Order-Preserving Message Encoder (OPME), Decoder (OPMD) and Global Message-Image Fusion (GMIF). OPME and OPMD aim to preserve the order of message bits by segmenting the entire sequence into equal-length segments and incorporating sequential information during encoding and decoding. Meanwhile, GMIF employs a cross-modality fusion mechanism to effectively fuse the features from the two uncorrelated modalities. Experimental results on the COCO and DIV2K datasets demonstrate that StegaFormer surpasses existing state-of-the-art methods in terms of recovery accuracy, message capacity, and imperceptibility. We will make our code publicly available.</p></p class="citation"></blockquote><h3 id=4165--77254-trajectory-consistency-distillation-jianbin-zheng-et-al-2024>(41/65 | 77/254) Trajectory Consistency Distillation (Jianbin Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, Tat-Jen Cham. (2024)<br><strong>Trajectory Consistency Distillation</strong><br><button class=copy-to-clipboard title="Trajectory Consistency Distillation" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19159v1.pdf filename=2402.19159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Latent Consistency Model (LCM) extends the Consistency Model to the latent space and leverages the guided consistency <b>distillation</b> technique to achieve impressive performance in accelerating <b>text-to-image</b> synthesis. However, we observed that LCM struggles to generate images with both clarity and detailed intricacy. To address this limitation, we initially delve into and elucidate the underlying causes. Our investigation identifies that the primary issue stems from errors in three distinct areas. Consequently, we introduce Trajectory Consistency <b>Distillation</b> (TCD), which encompasses trajectory consistency function and strategic stochastic sampling. The trajectory consistency function diminishes the <b>distillation</b> errors by broadening the scope of the self-consistency boundary condition and endowing the TCD with the ability to accurately trace the entire trajectory of the Probability Flow ODE. Additionally, strategic stochastic sampling is specifically designed to circumvent the accumulated errors inherent in multi-step consistency sampling, which is meticulously tailored to complement the TCD model. Experiments demonstrate that TCD not only significantly enhances image quality at low NFEs but also yields more detailed results compared to the teacher model at high NFEs.</p></p class="citation"></blockquote><h3 id=4265--78254-protop-od-explainable-object-detection-with-prototypical-parts-pavlos-rath-manakidis-et-al-2024>(42/65 | 78/254) ProtoP-OD: Explainable Object Detection with Prototypical Parts (Pavlos Rath-Manakidis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pavlos Rath-Manakidis, Frederik Strothmann, Tobias Glasmachers, Laurenz Wiskott. (2024)<br><strong>ProtoP-OD: Explainable Object Detection with Prototypical Parts</strong><br><button class=copy-to-clipboard title="ProtoP-OD: Explainable Object Detection with Prototypical Parts" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19142v1.pdf filename=2402.19142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interpretation and visualization of the behavior of detection <b>transformers</b> tends to highlight the locations in the image that the model attends to, but it provides limited insight into the \emph{semantics} that the model is focusing on. This paper introduces an extension to detection <b>transformers</b> that constructs prototypical local features and uses them in <b>object</b> <b>detection.</b> These custom features, which we call prototypical parts, are designed to be mutually exclusive and align with the classifications of the model. The proposed extension consists of a bottleneck module, the prototype neck, that computes a discretized representation of prototype activations and a new loss term that matches prototypes to <b>object</b> <b>classes.</b> This setup leads to interpretable representations in the prototype neck, allowing visual inspection of the image content perceived by the model and a better understanding of the model&rsquo;s reliability. We show experimentally that our method incurs only a limited performance penalty, and we provide examples that demonstrate the quality of the explanations provided by our method, which we argue outweighs the performance penalty.</p></p class="citation"></blockquote><h3 id=4365--79254-leveraging-representations-from-intermediate-encoder-blocks-for-synthetic-image-detection-christos-koutlis-et-al-2024>(43/65 | 79/254) Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection (Christos Koutlis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christos Koutlis, Symeon Papadopoulos. (2024)<br><strong>Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection</strong><br><button class=copy-to-clipboard title="Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Foundation Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19091v1.pdf filename=2402.19091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recently developed and publicly available synthetic image generation methods and services make it possible to create extremely realistic imagery on demand, raising great risks for the integrity and safety of online information. State-of-the-art Synthetic Image Detection (SID) research has led to strong evidence on the advantages of feature extraction from <b>foundation</b> <b>models.</b> However, such extracted features mostly encapsulate high-level visual semantics instead of fine-grained details, which are more important for the SID task. On the contrary, shallow layers encode low-level visual information. In this work, we leverage the image representations extracted by intermediate <b>Transformer</b> blocks of CLIP&rsquo;s image-encoder via a lightweight network that maps them to a learnable forgery-aware vector space capable of generalizing exceptionally well. We also employ a trainable module to incorporate the importance of each <b>Transformer</b> block to the final prediction. Our method is compared against the state-of-the-art by evaluating it on 20 test datasets and exhibits an average +10.6% absolute performance improvement. Notably, the best performing models require just a single epoch for training (~8 minutes). Code available at <a href=https://github.com/mever-team/rine>https://github.com/mever-team/rine</a>.</p></p class="citation"></blockquote><h3 id=4465--80254-atmospheric-turbulence-removal-with-video-sequence-deep-visual-priors-p-hill-et-al-2024>(44/65 | 80/254) Atmospheric Turbulence Removal with Video Sequence Deep Visual Priors (P. Hill et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>P. Hill, N. Anantrasirichai, A. Achim, D. R. Bull. (2024)<br><strong>Atmospheric Turbulence Removal with Video Sequence Deep Visual Priors</strong><br><button class=copy-to-clipboard title="Atmospheric Turbulence Removal with Video Sequence Deep Visual Priors" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19041v1.pdf filename=2402.19041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Atmospheric turbulence poses a challenge for the interpretation and visual perception of visual imagery due to its distortion effects. Model-based approaches have been used to address this, but such methods often suffer from artefacts associated with moving content. Conversely, deep learning based methods are dependent on large and diverse datasets that may not effectively represent any specific content. In this paper, we address these problems with a <b>self-supervised</b> <b>learning</b> method that does not require ground truth. The proposed method is not dependent on any dataset outside of the single data sequence being processed but is also able to improve the quality of any input raw sequences or pre-processed sequences. Specifically, our method is based on an accelerated Deep Image Prior (DIP), but integrates temporal information using pixel shuffling and a temporal sliding window. This efficiently learns spatio-temporal priors leading to a system that effectively mitigates atmospheric turbulence distortions. The experiments show that our method improves visual quality results qualitatively and quantitatively.</p></p class="citation"></blockquote><h3 id=4565--81254-progressive-contrastive-learning-with-multi-prototype-for-unsupervised-visible-infrared-person-re-identification-jiangming-shi-et-al-2024>(45/65 | 81/254) Progressive Contrastive Learning with Multi-Prototype for Unsupervised Visible-Infrared Person Re-identification (Jiangming Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiangming Shi, Xiangbo Yin, Yaoxing Wang, Xiaofeng Liu, Yuan Xie, Yanyun Qu. (2024)<br><strong>Progressive Contrastive Learning with Multi-Prototype for Unsupervised Visible-Infrared Person Re-identification</strong><br><button class=copy-to-clipboard title="Progressive Contrastive Learning with Multi-Prototype for Unsupervised Visible-Infrared Person Re-identification" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19026v1.pdf filename=2402.19026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> visible-infrared person re-identification (USVI-ReID) aims to match specified people in infrared images to visible images without annotation, and vice versa. USVI-ReID is a challenging yet under-explored task. Most existing methods address the USVI-ReID problem using cluster-based <b>contrastive</b> <b>learning,</b> which simply employs the cluster center as a representation of a person. However, the cluster center primarily focuses on shared information, overlooking disparity. To address the problem, we propose a Progressive <b>Contrastive</b> <b>Learning</b> with Multi-Prototype (PCLMP) method for USVI-ReID. In brief, we first generate the hard prototype by selecting the sample with the maximum distance from the cluster center. This hard prototype is used in the <b>contrastive</b> <b>loss</b> to emphasize disparity. Additionally, instead of rigidly aligning query images to a specific prototype, we generate the dynamic prototype by randomly picking samples within a cluster. This dynamic prototype is used to retain the natural variety of features while reducing instability in the simultaneous learning of both common and disparate information. Finally, we introduce a progressive learning strategy to gradually shift the model&rsquo;s attention towards hard samples, avoiding cluster deterioration. Extensive experiments conducted on the publicly available SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method. PCLMP outperforms the existing state-of-the-art method with an average mAP improvement of 3.9%. The source codes will be released.</p></p class="citation"></blockquote><h3 id=4665--82254-viewfusion-towards-multi-view-consistency-via-interpolated-denoising-xianghui-yang-et-al-2024>(46/65 | 82/254) ViewFusion: Towards Multi-View Consistency via Interpolated Denoising (Xianghui Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianghui Yang, Yan Zuo, Sameera Ramasinghe, Loris Bazzani, Gil Avraham, Anton van den Hengel. (2024)<br><strong>ViewFusion: Towards Multi-View Consistency via Interpolated Denoising</strong><br><button class=copy-to-clipboard title="ViewFusion: Towards Multi-View Consistency via Interpolated Denoising" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18842v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18842v1.pdf filename=2402.18842v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Novel-view synthesis through <b>diffusion</b> <b>models</b> has demonstrated remarkable potential for generating diverse and high-quality images. Yet, the independent process of image generation in these prevailing methods leads to challenges in maintaining multiple-view consistency. To address this, we introduce ViewFusion, a novel, training-free algorithm that can be seamlessly integrated into existing pre-trained <b>diffusion</b> <b>models.</b> Our approach adopts an auto-regressive method that implicitly leverages previously generated views as context for the next view generation, ensuring robust multi-view consistency during the novel-view generation process. Through a <b>diffusion</b> <b>process</b> that fuses known-view information via interpolated denoising, our framework successfully extends single-view conditioned models to work in multiple-view conditional settings without any additional <b>fine-tuning.</b> Extensive experimental results demonstrate the effectiveness of ViewFusion in generating consistent and detailed novel views.</p></p class="citation"></blockquote><h3 id=4765--83254-suppress-and-rebalance-towards-generalized-multi-modal-face-anti-spoofing-xun-lin-et-al-2024>(47/65 | 83/254) Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing (Xun Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xun Lin, Shuai Wang, Rizhao Cai, Yizhong Liu, Ying Fu, Zitong Yu, Wenzhong Tang, Alex Kot. (2024)<br><strong>Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing</strong><br><button class=copy-to-clipboard title="Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Face Recognition, Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19298v1.pdf filename=2402.19298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Face</b> <b>Anti-Spoofing</b> (FAS) is crucial for securing <b>face</b> <b>recognition</b> systems against presentation attacks. With advancements in sensor manufacture and <b>multi-modal</b> learning techniques, many <b>multi-modal</b> FAS approaches have emerged. However, they <b>face</b> <b>challenges</b> in generalizing to unseen attacks and deployment conditions. These challenges arise from (1) modality unreliability, where some modality sensors like depth and infrared undergo significant domain shifts in varying environments, leading to the spread of unreliable information during cross-modal feature fusion, and (2) modality imbalance, where training overly relies on a dominant modality hinders the convergence of others, reducing effectiveness against attack types that are indistinguishable sorely using the dominant modality. To address modality unreliability, we propose the Uncertainty-Guided Cross-Adapter (U-Adapter) to recognize unreliably detected regions within each modality and suppress the impact of unreliable regions on other modalities. For modality imbalance, we propose a Rebalanced Modality Gradient Modulation (ReGrad) strategy to rebalance the convergence speed of all modalities by adaptively adjusting their gradients. Besides, we provide the first large-scale <b>benchmark</b> for evaluating <b>multi-modal</b> FAS performance under domain generalization scenarios. Extensive experiments demonstrate that our method outperforms state-of-the-art methods. Source code and protocols will be released on <a href=https://github.com/OMGGGGG/mmdg>https://github.com/OMGGGGG/mmdg</a>.</p></p class="citation"></blockquote><h3 id=4865--84254-venvision3d-a-synthetic-perception-dataset-for-3d-multi-task-model-research-jiahao-zhou-et-al-2024>(48/65 | 84/254) VEnvision3D: A Synthetic Perception Dataset for 3D Multi-Task Model Research (Jiahao Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Zhou, Chen Long, Yue Xie, Jialiang Wang, Boheng Li, Haiping Wang, Zhe Chen, Zhen Dong. (2024)<br><strong>VEnvision3D: A Synthetic Perception Dataset for 3D Multi-Task Model Research</strong><br><button class=copy-to-clipboard title="VEnvision3D: A Synthetic Perception Dataset for 3D Multi-Task Model Research" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19059v1.pdf filename=2402.19059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing a unified multi-task <b>foundation</b> <b>model</b> has become a critical challenge in computer vision research. In the current field of 3D computer vision, most datasets solely focus on a relatively limited set of tasks, which complicates the concurrent training requirements of various downstream tasks. This makes the training of multi-objective networks difficult to proceed with, which further hinders the development of <b>foundation</b> <b>models</b> in the 3D vision field. In this paper, we introduce VEnvision3D, a large 3D synthetic perception dataset for multi-task learning, including depth completion, segmentation, upsampling, place recognition, and 3D reconstruction. Since the data for each task was collected in the same scenarios, tasks are inherently aligned in terms of the utilized data. Therefore, such a unique attribute can assist in exploring the potential for the multi-task model and even the <b>foundation</b> <b>model</b> without separate training methods. Several new <b>benchmarks</b> based on the characteristics of the proposed dataset were presented. Extensive studies were performed on end-to-end models, revealing new observations, challenges, and opportunities for future research. In addition, we designed a straightfoward multi-task network to uncover the ability that VEnvision3D can offer for the <b>foundation</b> <b>model.</b> Our dataset and code will be open-sourced upon acceptance.</p></p class="citation"></blockquote><h3 id=4965--85254-distrifusion-distributed-parallel-inference-for-high-resolution-diffusion-models-muyang-li-et-al-2024>(49/65 | 85/254) DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models (Muyang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, Song Han. (2024)<br><strong>DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</strong><br><button class=copy-to-clipboard title="DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19481v1.pdf filename=2402.19481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have achieved great success in synthesizing high-quality images. However, generating high-resolution images with <b>diffusion</b> <b>models</b> is still challenging due to the enormous computational costs, resulting in a prohibitive latency for interactive applications. In this paper, we propose DistriFusion to tackle this problem by leveraging parallelism across multiple GPUs. Our method splits the model input into multiple patches and assigns each patch to a GPU. However, na"{\i}vely implementing such an algorithm breaks the interaction between patches and loses fidelity, while incorporating such an interaction will incur tremendous communication overhead. To overcome this dilemma, we observe the high similarity between the input from adjacent <b>diffusion</b> <b>steps</b> and propose displaced patch parallelism, which takes advantage of the sequential nature of the <b>diffusion</b> <b>process</b> by reusing the pre-computed feature maps from the previous timestep to provide context for the current step. Therefore, our method supports asynchronous communication, which can be pipelined by computation. Extensive experiments show that our method can be applied to recent Stable <b>Diffusion</b> <b>XL</b> with no quality degradation and achieve up to a 6.1$\times$ speedup on eight NVIDIA A100s compared to one. Our code is publicly available at <a href=https://github.com/mit-han-lab/distrifuser>https://github.com/mit-han-lab/distrifuser</a>.</p></p class="citation"></blockquote><h3 id=5065--86254-hyenapixel-global-image-context-with-convolutions-julian-spravil-et-al-2024>(50/65 | 86/254) HyenaPixel: Global Image Context with Convolutions (Julian Spravil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julian Spravil, Sebastian Houben, Sven Behnke. (2024)<br><strong>HyenaPixel: Global Image Context with Convolutions</strong><br><button class=copy-to-clipboard title="HyenaPixel: Global Image Context with Convolutions" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19305v1.pdf filename=2402.19305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In vision tasks, a larger effective receptive field (ERF) is associated with better performance. While attention natively supports global context, <b>convolution</b> requires multiple stacked layers and a hierarchical structure for large context. In this work, we extend Hyena, a <b>convolution-based</b> attention replacement, from causal sequences to the non-causal two-dimensional image space. We scale the Hyena <b>convolution</b> kernels beyond the feature map size up to 191$\times$191 to maximize the ERF while maintaining sub-quadratic complexity in the number of pixels. We integrate our two-dimensional Hyena, HyenaPixel, and bidirectional Hyena into the MetaFormer framework. For image categorization, HyenaPixel and bidirectional Hyena achieve a competitive ImageNet-1k top-1 accuracy of 83.0% and 83.5%, respectively, while outperforming other large-kernel networks. Combining HyenaPixel with attention further increases accuracy to 83.6%. We attribute the success of attention to the lack of spatial bias in later stages and support this finding with bidirectional Hyena.</p></p class="citation"></blockquote><h3 id=5165--87254-continuous-sign-language-recognition-based-on-motor-attention-mechanism-and-frame-level-self-distillation-qidan-zhu-et-al-2024>(51/65 | 87/254) Continuous Sign Language Recognition Based on Motor attention mechanism and frame-level Self-distillation (Qidan Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qidan Zhu, Jing Li, Fei Yuan, Quan Gan. (2024)<br><strong>Continuous Sign Language Recognition Based on Motor attention mechanism and frame-level Self-distillation</strong><br><button class=copy-to-clipboard title="Continuous Sign Language Recognition Based on Motor attention mechanism and frame-level Self-distillation" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19118v1.pdf filename=2402.19118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Changes in facial expression, head movement, body movement and gesture movement are remarkable cues in sign language recognition, and most of the current continuous sign language recognition(CSLR) research methods mainly focus on static images in video sequences at the frame-level feature extraction stage, while ignoring the dynamic changes in the images. In this paper, we propose a novel motor attention mechanism to capture the distorted changes in local motion regions during sign language expression, and obtain a dynamic representation of image changes. And for the first time, we apply the <b>self-distillation</b> method to frame-level feature extraction for continuous sign language, which improves the feature expression without increasing the computational resources by self-distilling the features of adjacent stages and using the higher-order features as teachers to guide the lower-order features. The combination of the two constitutes our proposed holistic model of CSLR Based on motor attention mechanism and frame-level <b>Self-Distillation</b> (MAM-FSD), which improves the inference ability and robustness of the model. We conduct experiments on three publicly available datasets, and the experimental results show that our proposed method can effectively extract the sign language motion information in videos, improve the accuracy of CSLR and reach the state-of-the-art level.</p></p class="citation"></blockquote><h3 id=5265--88254-doze-a-dataset-for-open-vocabulary-zero-shot-object-navigation-in-dynamic-environments-ji-ma-et-al-2024>(52/65 | 88/254) DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments (Ji Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ji Ma, Hongming Dai, Yao Mu, Pengying Wu, Hao Wang, Xiaowei Chi, Yang Fei, Shanghang Zhang, Chang Liu. (2024)<br><strong>DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments</strong><br><button class=copy-to-clipboard title="DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19007v1.pdf filename=2402.19007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-Shot</b> Object Navigation (ZSON) requires agents to autonomously locate and approach unseen objects in unfamiliar environments and has emerged as a particularly challenging task within the domain of Embodied AI. Existing datasets for developing ZSON algorithms lack consideration of dynamic obstacles, object attribute diversity, and scene texts, thus exhibiting noticeable discrepancy from real-world situations. To address these issues, we propose a Dataset for Open-Vocabulary <b>Zero-Shot</b> Object Navigation in Dynamic Environments (DOZE) that comprises ten high-fidelity 3D scenes with over 18k tasks, aiming to mimic complex, dynamic real-world scenarios. Specifically, DOZE scenes feature multiple moving humanoid obstacles, a wide array of open-vocabulary objects, diverse distinct-attribute objects, and valuable textual hints. Besides, different from existing datasets that only provide collision checking between the agent and static obstacles, we enhance DOZE by integrating capabilities for detecting collisions between the agent and moving obstacles. This novel functionality enables evaluation of the agents&rsquo; collision avoidance abilities in dynamic environments. We test four representative ZSON methods on DOZE, revealing substantial room for improvement in existing approaches concerning navigation efficiency, safety, and object recognition accuracy. Our dataset could be found at <a href=https://DOZE-Dataset.github.io/>https://DOZE-Dataset.github.io/</a>.</p></p class="citation"></blockquote><h3 id=5365--89254-privateyes-appearance-based-gaze-estimation-using-federated-secure-multi-party-computation-mayar-elfares-et-al-2024>(53/65 | 89/254) PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation (Mayar Elfares et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mayar Elfares, Pascal Reisert, Zhiming Hu, Wenwu Tang, Ralf Küsters, Andreas Bulling. (2024)<br><strong>PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation</strong><br><button class=copy-to-clipboard title="PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-HC, cs.CV<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18970v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18970v1.pdf filename=2402.18970v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Latest gaze estimation methods require large-scale training data but their collection and exchange pose significant privacy risks. We propose PrivatEyes - the first privacy-enhancing training approach for appearance-based gaze estimation based on <b>federated</b> <b>learning</b> (FL) and secure multi-party computation (MPC). PrivatEyes enables training gaze estimators on multiple local datasets across different users and server-based secure aggregation of the individual estimators&rsquo; updates. PrivatEyes guarantees that individual gaze data remains private even if a majority of the aggregating servers is malicious. We also introduce a new data leakage attack DualView that shows that PrivatEyes limits the leakage of private training data more effectively than previous approaches. Evaluations on the MPIIGaze, MPIIFaceGaze, GazeCapture, and NVGaze datasets further show that the improved privacy does not lead to a lower gaze estimation accuracy or substantially higher computational costs - both of which are on par with its non-secure counterparts.</p></p class="citation"></blockquote><h3 id=5465--90254-towards-out-of-distribution-detection-for-breast-cancer-classification-in-point-of-care-ultrasound-imaging-jennie-karlsson-et-al-2024>(54/65 | 90/254) Towards Out-of-Distribution Detection for breast cancer classification in Point-of-Care Ultrasound Imaging (Jennie Karlsson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jennie Karlsson, Marisa Wodrich, Niels Christian Overgaard, Freja Sahlin, Kristina Lång, Anders Heyden, Ida Arvidsson. (2024)<br><strong>Towards Out-of-Distribution Detection for breast cancer classification in Point-of-Care Ultrasound Imaging</strong><br><button class=copy-to-clipboard title="Towards Out-of-Distribution Detection for breast cancer classification in Point-of-Care Ultrasound Imaging" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18960v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18960v1.pdf filename=2402.18960v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning has shown to have great potential in medical applications. In critical domains as such, it is of high interest to have trustworthy algorithms which are able to tell when reliable assessments cannot be guaranteed. Detecting <b>out-of-distribution</b> (OOD) samples is a crucial step towards building a safe classifier. Following a previous study, showing that it is possible to classify breast cancer in point-of-care ultrasound images, this study investigates OOD detection using three different methods: softmax, energy score and deep ensembles. All methods are tested on three different OOD data sets. The results show that the energy score method outperforms the softmax method, performing well on two of the data sets. The ensemble method is the most robust, performing the best at detecting OOD samples for all three OOD data sets.</p></p class="citation"></blockquote><h3 id=5565--91254-spectral-meets-spatial-harmonising-3d-shape-matching-and-interpolation-dongliang-cao-et-al-2024>(55/65 | 91/254) Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation (Dongliang Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers, Florian Bernard. (2024)<br><strong>Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation</strong><br><button class=copy-to-clipboard title="Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CG, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18920v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18920v1.pdf filename=2402.18920v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although 3D shape matching and interpolation are highly interrelated, they are often studied separately and applied sequentially to relate different 3D shapes, thus resulting in sub-optimal performance. In this work we present a unified framework to predict both point-wise correspondences and shape interpolation between 3D shapes. To this end, we combine the deep functional map framework with classical surface deformation models to map shapes in both spectral and spatial domains. On the one hand, by incorporating spatial maps, our method obtains more accurate and smooth point-wise correspondences compared to previous functional map methods for shape matching. On the other hand, by introducing spectral maps, our method gets rid of commonly used but computationally expensive geodesic distance constraints that are only valid for near-isometric shape deformations. Furthermore, we propose a novel test-time adaptation scheme to capture both pose-dominant and shape-dominant deformations. Using different challenging datasets, we demonstrate that our method outperforms previous state-of-the-art methods for both shape matching and interpolation, even compared to <b>supervised</b> approaches.</p></p class="citation"></blockquote><h3 id=5665--92254-enhancing-steganographic-text-extraction-evaluating-the-impact-of-nlp-models-on-accuracy-and-semantic-coherence-mingyang-li-et-al-2024>(56/65 | 92/254) Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence (Mingyang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyang Li, Maoqin Yuan, Luyao Li, Han Pengsihua. (2024)<br><strong>Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence</strong><br><button class=copy-to-clipboard title="Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18849v1.pdf filename=2402.18849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text. Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of <b>information</b> <b>extraction</b> when dealing with complex character encoding, such as Chinese characters. To address this issue, this study proposes an innovative LSB-NLP hybrid framework. This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as <b>information</b> <b>reconstruction</b> techniques, thereby significantly enhancing the robustness of steganographic text extraction. Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters. The findings of this study not only confirm the effectiveness of combining image steganography technology and NLP large models but also propose new ideas for research and application in the field of <b>information</b> <b>hiding.</b> The successful implementation of this interdisciplinary approach demonstrates the great potential of integrating image steganography technology with natural language processing technology in solving complex <b>information</b> <b>processing</b> problems.</p></p class="citation"></blockquote><h3 id=5765--93254-the-6th-affective-behavior-analysis-in-the-wild-abaw-competition-dimitrios-kollias-et-al-2024>(57/65 | 93/254) The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition (Dimitrios Kollias et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Stefanos Zafeiriou, Chunchang Shao, Guanyu Hu. (2024)<br><strong>The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition</strong><br><button class=copy-to-clipboard title="The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19344v1.pdf filename=2402.19344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes the 6th Affective Behavior Analysis in-the-wild (ABAW) Competition, which is part of the respective Workshop held in conjunction with IEEE CVPR 2024. The 6th ABAW Competition addresses contemporary challenges in understanding human emotions and behaviors, crucial for the development of human-centered technologies. In more detail, the Competition focuses on affect related <b>benchmarking</b> tasks and comprises of five sub-challenges: i) Valence-Arousal Estimation (the target is to estimate two continuous affect dimensions, valence and arousal), ii) Expression Recognition (the target is to recognise between the mutually exclusive classes of the 7 basic expressions and &lsquo;other&rsquo;), iii) Action Unit Detection (the target is to detect 12 action units), iv) Compound Expression Recognition (the target is to recognise between the 7 mutually exclusive compound expression classes), and v) Emotional Mimicry Intensity Estimation (the target is to estimate six continuous emotion dimensions). In the paper, we present these Challenges, describe their respective datasets and challenge protocols (we outline the evaluation metrics) and present the baseline systems as well as their obtained performance. More information for the Competition can be found in: \url{https://affective-behavior-analysis-in-the-wild.github.io/6th}.</p></p class="citation"></blockquote><h3 id=5865--94254-www-a-unified-framework-for-explaining-what-where-and-why-of-neural-networks-by-interpretation-of-neuron-concepts-yong-hyun-ahn-et-al-2024>(58/65 | 94/254) WWW: A Unified Framework for Explaining What, Where and Why of Neural Networks by Interpretation of Neuron Concepts (Yong Hyun Ahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yong Hyun Ahn, Hyeon Bae Kim, Seong Tae Kim. (2024)<br><strong>WWW: A Unified Framework for Explaining What, Where and Why of Neural Networks by Interpretation of Neuron Concepts</strong><br><button class=copy-to-clipboard title="WWW: A Unified Framework for Explaining What, Where and Why of Neural Networks by Interpretation of Neuron Concepts" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18956v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18956v1.pdf filename=2402.18956v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in neural networks have showcased their remarkable capabilities across various domains. Despite these successes, the <b>&ldquo;black</b> <b>box&rdquo;</b> problem still remains. Addressing this, we propose a novel framework, WWW, that offers the &lsquo;what&rsquo;, &lsquo;where&rsquo;, and &lsquo;why&rsquo; of the neural network decisions in human-understandable terms. Specifically, WWW utilizes adaptive selection for concept discovery, employing adaptive cosine similarity and thresholding techniques to effectively explain &lsquo;what&rsquo;. To address the &lsquo;where&rsquo; and &lsquo;why&rsquo;, we proposed a novel combination of neuron activation maps (NAMs) with Shapley values, generating localized concept maps and heatmaps for individual inputs. Furthermore, WWW introduces a method for predicting uncertainty, leveraging heatmap similarities to estimate &lsquo;how&rsquo; reliable the prediction is. Experimental evaluations of WWW demonstrate superior performance in both quantitative and qualitative metrics, outperforming existing methods in interpretability. WWW provides a unified solution for explaining &lsquo;what&rsquo;, &lsquo;where&rsquo;, and &lsquo;why&rsquo;, introducing a method for localized explanations from global interpretations and offering a plug-and-play solution adaptable to various architectures.</p></p class="citation"></blockquote><h3 id=5965--95254-pcdepth-pattern-based-complementary-learning-for-monocular-depth-estimation-by-best-of-both-worlds-haotian-liu-et-al-2024>(59/65 | 95/254) PCDepth: Pattern-based Complementary Learning for Monocular Depth Estimation by Best of Both Worlds (Haotian Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotian Liu, Sanqing Qu, Fan Lu, Zongtao Bu, Florian Roehrbein, Alois Knoll, Guang Chen. (2024)<br><strong>PCDepth: Pattern-based Complementary Learning for Monocular Depth Estimation by Best of Both Worlds</strong><br><button class=copy-to-clipboard title="PCDepth: Pattern-based Complementary Learning for Monocular Depth Estimation by Best of Both Worlds" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18925v1.pdf filename=2402.18925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event cameras can record scene dynamics with high temporal resolution, providing rich scene details for monocular depth estimation (MDE) even at low-level illumination. Therefore, existing complementary learning approaches for MDE fuse intensity information from images and scene details from event data for better scene understanding. However, most methods directly fuse two modalities at pixel level, ignoring that the attractive complementarity mainly impacts high-level patterns that only occupy a few pixels. For example, event data is likely to complement contours of scene objects. In this paper, we discretize the scene into a set of high-level patterns to explore the complementarity and propose a Pattern-based Complementary learning architecture for monocular Depth estimation (PCDepth). Concretely, PCDepth comprises two primary components: a complementary visual <b>representation</b> <b>learning</b> module for discretizing the scene into high-level patterns and integrating complementary patterns across modalities and a refined depth estimator aimed at scene reconstruction and depth prediction while maintaining an efficiency-accuracy balance. Through pattern-based complementary learning, PCDepth fully exploits two modalities and achieves more accurate predictions than existing methods, especially in challenging nighttime scenarios. Extensive experiments on MVSEC and DSEC datasets verify the effectiveness and superiority of our PCDepth. Remarkably, compared with state-of-the-art, PCDepth achieves a 37.9% improvement in accuracy in MVSEC nighttime scenarios.</p></p class="citation"></blockquote><h3 id=6065--96254-memonav-working-memory-model-for-visual-navigation-hongxin-li-et-al-2024>(60/65 | 96/254) MemoNav: Working Memory Model for Visual Navigation (Hongxin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongxin Li, Zeyu Wang, Xu Yang, Yuran Yang, Shuqi Mei, Zhaoxiang Zhang. (2024)<br><strong>MemoNav: Working Memory Model for Visual Navigation</strong><br><button class=copy-to-clipboard title="MemoNav: Working Memory Model for Visual Navigation" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19161v1.pdf filename=2402.19161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image-goal navigation is a challenging task that requires an agent to navigate to a goal indicated by an image in unfamiliar environments. Existing methods utilizing diverse scene memories suffer from inefficient exploration since they use all historical observations for decision-making without considering the goal-relevant fraction. To address this limitation, we present MemoNav, a novel memory model for image-goal navigation, which utilizes a working memory-inspired pipeline to improve navigation performance. Specifically, we employ three types of navigation memory. The node features on a map are stored in the short-term memory (STM), as these features are dynamically updated. A forgetting module then retains the informative STM fraction to increase efficiency. We also introduce long-term memory (LTM) to learn global scene representations by progressively aggregating STM features. Subsequently, a <b>graph</b> attention module encodes the retained STM and the LTM to generate working memory (WM) which contains the scene features essential for efficient navigation. The synergy among these three memory types boosts navigation performance by enabling the agent to learn and leverage goal-relevant scene features within a topological map. Our evaluation on multi-goal tasks demonstrates that MemoNav significantly outperforms previous methods across all difficulty levels in both Gibson and Matterport3D scenes. Qualitative results further illustrate that MemoNav plans more efficient routes.</p></p class="citation"></blockquote><h3 id=6165--97254-deeperaser-deep-iterative-context-mining-for-generic-text-eraser-hao-feng-et-al-2024>(61/65 | 97/254) DeepEraser: Deep Iterative Context Mining for Generic Text Eraser (Hao Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Feng, Wendi Wang, Shaokai Liu, Jiajun Deng, Wengang Zhou, Houqiang Li. (2024)<br><strong>DeepEraser: Deep Iterative Context Mining for Generic Text Eraser</strong><br><button class=copy-to-clipboard title="DeepEraser: Deep Iterative Context Mining for Generic Text Eraser" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19108v1.pdf filename=2402.19108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present DeepEraser, an effective deep network for generic text removal. DeepEraser utilizes a recurrent architecture that erases the text in an image via iterative operations. Our idea comes from the process of erasing pencil script, where the text area designated for removal is subject to continuous monitoring and the text is attenuated progressively, ensuring a thorough and clean erasure. Technically, at each iteration, an innovative erasing module is deployed, which not only explicitly aggregates the previous erasing progress but also mines additional semantic context to erase the target text. Through iterative refinements, the text regions are progressively replaced with more appropriate content and finally converge to a relatively accurate status. Furthermore, a custom mask generation strategy is introduced to improve the capability of DeepEraser for adaptive text removal, as opposed to indiscriminately removing all the text in an image. Our DeepEraser is notably compact with only 1.4M parameters and trained in an end-to-end manner. To verify its effectiveness, extensive experiments are conducted on several prevalent <b>benchmarks,</b> including SCUT-Syn, SCUT-EnsText, and Oxford Synthetic text dataset. The quantitative and qualitative results demonstrate the effectiveness of our DeepEraser over the state-of-the-art methods, as well as its strong generalization ability in custom mask text removal. The codes and pre-trained models are available at <a href=https://github.com/fh2019ustc/DeepEraser>https://github.com/fh2019ustc/DeepEraser</a></p></p class="citation"></blockquote><h3 id=6265--98254-goalnet-goal-areas-oriented-pedestrian-trajectory-prediction-ching-lin-lee-et-al-2024>(62/65 | 98/254) GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction (Ching-Lin Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ching-Lin Lee, Zhi-Xuan Wang, Kuan-Ting Lai, Amar Fadillah. (2024)<br><strong>GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction</strong><br><button class=copy-to-clipboard title="GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19002v1.pdf filename=2402.19002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting the future trajectories of pedestrians on the road is an important task for autonomous driving. The pedestrian trajectory prediction is affected by scene paths, pedestrian&rsquo;s intentions and decision-making, which is a <b>multi-modal</b> problem. Most recent studies use past trajectories to predict a variety of potential future trajectory distributions, which do not account for the scene context and pedestrian targets. Instead of predicting the future trajectory directly, we propose to use scene context and observed trajectory to predict the goal points first, and then reuse the goal points to predict the future trajectories. By leveraging the information from scene context and observed trajectory, the uncertainty can be limited to a few target areas, which represent the &ldquo;goals&rdquo; of the pedestrians. In this paper, we propose GoalNet, a new trajectory prediction neural network based on the goal areas of a pedestrian. Our network can predict both pedestrian&rsquo;s trajectories and bounding boxes. The overall model is efficient and modular, and its outputs can be changed according to the usage scenario. Experimental results show that GoalNet significantly improves the previous state-of-the-art performance by 48.7% on the JAAD and 40.8% on the PIE dataset.</p></p class="citation"></blockquote><h3 id=6365--99254-navigating-beyond-dropout-an-intriguing-solution-towards-generalizable-image-super-resolution-hongjun-wang-et-al-2024>(63/65 | 99/254) Navigating Beyond Dropout: An Intriguing Solution Towards Generalizable Image Super Resolution (Hongjun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongjun Wang, Jiyuan Chen, Yinqiang Zheng, Tieyong Zeng. (2024)<br><strong>Navigating Beyond Dropout: An Intriguing Solution Towards Generalizable Image Super Resolution</strong><br><button class=copy-to-clipboard title="Navigating Beyond Dropout: An Intriguing Solution Towards Generalizable Image Super Resolution" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18929v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18929v2.pdf filename=2402.18929v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning has led to a dramatic leap on Single Image Super-Resolution (SISR) performances in recent years. %Despite the substantial advancement% While most existing work assumes a simple and fixed degradation model (e.g., bicubic downsampling), the research of Blind SR seeks to improve model generalization ability with unknown degradation. Recently, Kong et al pioneer the investigation of a more suitable training strategy for Blind SR using Dropout. Although such method indeed brings substantial generalization improvements via mitigating overfitting, we argue that Dropout simultaneously introduces undesirable side-effect that compromises model&rsquo;s capacity to faithfully reconstruct fine details. We show both the theoretical and experimental analyses in our paper, and furthermore, we present another easy yet effective training strategy that enhances the generalization ability of the model by simply modulating its first and second-order features statistics. Experimental results have shown that our method could serve as a model-agnostic regularization and outperforms Dropout on seven <b>benchmark</b> datasets including both synthetic and real-world scenarios.</p></p class="citation"></blockquote><h3 id=6465--100254-sne-roadsegv2-advancing-heterogeneous-feature-fusion-and-fallibility-awareness-for-freespace-detection-yi-feng-et-al-2024>(64/65 | 100/254) SNE-RoadSegV2: Advancing Heterogeneous Feature Fusion and Fallibility Awareness for Freespace Detection (Yi Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Feng, Yu Ma, Qijun Chen, Ioannis Pitas, Rui Fan. (2024)<br><strong>SNE-RoadSegV2: Advancing Heterogeneous Feature Fusion and Fallibility Awareness for Freespace Detection</strong><br><button class=copy-to-clipboard title="SNE-RoadSegV2: Advancing Heterogeneous Feature Fusion and Fallibility Awareness for Freespace Detection" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18918v1.pdf filename=2402.18918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Feature-fusion networks with duplex encoders have proven to be an effective technique to solve the freespace detection problem. However, despite the compelling results achieved by previous research efforts, the exploration of adequate and discriminative heterogeneous feature fusion, as well as the development of fallibility-aware loss functions remains relatively scarce. This paper makes several significant contributions to address these limitations: (1) It presents a novel heterogeneous feature fusion block, comprising a holistic attention module, a heterogeneous feature contrast descriptor, and an affinity-weighted feature recalibrator, enabling a more in-depth exploitation of the inherent characteristics of the extracted features, (2) it incorporates both inter-scale and intra-scale skip connections into the decoder architecture while eliminating redundant ones, leading to both improved accuracy and computational efficiency, and (3) it introduces two fallibility-aware loss functions that separately focus on semantic-transition and depth-inconsistent regions, collectively contributing to greater supervision during model training. Our proposed heterogeneous feature fusion network (SNE-RoadSegV2), which incorporates all these innovative components, demonstrates superior performance in comparison to all other freespace detection algorithms across multiple public datasets. Notably, it ranks the 1st on the official KITTI Road <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=6565--101254-naruto-neural-active-reconstruction-from-uncertain-target-observations-ziyue-feng-et-al-2024>(65/65 | 101/254) NARUTO: Neural Active Reconstruction from Uncertain Target Observations (Ziyue Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyue Feng, Huangying Zhan, Zheng Chen, Qingan Yan, Xiangyu Xu, Changjiang Cai, Bing Li, Qilun Zhu, Yi Xu. (2024)<br><strong>NARUTO: Neural Active Reconstruction from Uncertain Target Observations</strong><br><button class=copy-to-clipboard title="NARUTO: Neural Active Reconstruction from Uncertain Target Observations" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18771v1.pdf filename=2402.18771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present NARUTO, a neural active reconstruction system that combines a hybrid neural representation with uncertainty learning, enabling high-fidelity surface reconstruction. Our approach leverages a multi-resolution hash-grid as the mapping backbone, chosen for its exceptional convergence speed and capacity to capture high-frequency local features.The centerpiece of our work is the incorporation of an uncertainty learning module that dynamically quantifies reconstruction uncertainty while actively reconstructing the environment. By harnessing learned uncertainty, we propose a novel uncertainty aggregation strategy for goal searching and efficient path planning. Our system autonomously explores by targeting uncertain observations and reconstructs environments with remarkable completeness and fidelity. We also demonstrate the utility of this uncertainty-aware approach by enhancing SOTA neural SLAM systems through an active ray sampling strategy. Extensive evaluations of NARUTO in various environments, using an indoor scene simulator, confirm its superior performance and state-of-the-art status in active reconstruction, as evidenced by its impressive results on <b>benchmark</b> datasets like Replica and MP3D.</p></p class="citation"></blockquote><h2 id=csir-5>cs.IR (5)</h2><h3 id=15--102254-crafting-knowledge-exploring-the-creative-mechanisms-of-chat-based-search-engines-lijia-ma-et-al-2024>(1/5 | 102/254) Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines (Lijia Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lijia Ma, Xingchen Xu, Yong Tan. (2024)<br><strong>Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines</strong><br><button class=copy-to-clipboard title="Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: J-4, cs-AI, cs-IR, cs.IR, econ-GN, q-fin-EC<br>Keyword Score: 80<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, GPT, GPT-4, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19421v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19421v1.pdf filename=2402.19421v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the domain of digital information dissemination, search engines act as pivotal conduits linking information seekers with providers. The advent of chat-based search engines utilizing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG),</b> exemplified by Bing Chat, marks an evolutionary leap in the search ecosystem. They demonstrate metacognitive abilities in interpreting web information and crafting responses with human-like understanding and creativity. Nonetheless, the intricate nature of <b>LLMs</b> renders their &ldquo;cognitive&rdquo; processes opaque, challenging even their designers&rsquo; understanding. This research aims to dissect the mechanisms through which an <b>LLM-powered</b> chat-based search engine, specifically Bing Chat, selects information sources for its responses. To this end, an extensive dataset has been compiled through engagements with New Bing, documenting the websites it cites alongside those listed by the conventional search engine. Employing natural language processing (NLP) techniques, the research reveals that Bing Chat exhibits a preference for content that is not only readable and formally structured, but also demonstrates lower <b>perplexity</b> levels, indicating a unique inclination towards text that is predictable by the underlying <b>LLM.</b> Further enriching our analysis, we procure an additional dataset through interactions with the <b>GPT-4</b> based knowledge <b>retrieval</b> <b>API,</b> <b>unveiling</b> a congruent text preference between the <b>RAG</b> API and Bing Chat. This consensus suggests that these text preferences intrinsically emerge from the underlying language models, rather than being explicitly crafted by Bing Chat&rsquo;s developers. Moreover, our investigation documents a greater similarity among websites cited by <b>RAG</b> technologies compared to those ranked highest by conventional search engines.</p></p class="citation"></blockquote><h3 id=25--103254-mentor-multi-level-self-supervised-learning-for-multimodal-recommendation-jinfeng-xu-et-al-2024>(2/5 | 103/254) MENTOR: Multi-level Self-supervised Learning for Multimodal Recommendation (Jinfeng Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinfeng Xu, Zheyu Chen, Shuo Yang, Jinze Li, Hewei Wang, Edith C. -H. Ngai. (2024)<br><strong>MENTOR: Multi-level Self-supervised Learning for Multimodal Recommendation</strong><br><button class=copy-to-clipboard title="MENTOR: Multi-level Self-supervised Learning for Multimodal Recommendation" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 79<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Multi-modal, Multi-modal, Recommendation, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19407v1.pdf filename=2402.19407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing multimedia information, <b>multimodal</b> <b>recommendation</b> has received extensive attention. It utilizes <b>multimodal</b> information to alleviate the data sparsity problem in <b>recommendation</b> systems, thus improving <b>recommendation</b> accuracy. However, the reliance on labeled data severely limits the performance of <b>multimodal</b> <b>recommendation</b> models. Recently, <b>self-supervised</b> <b>learning</b> has been used in <b>multimodal</b> <b>recommendations</b> to mitigate the label sparsity problem. Nevertheless, the state-of-the-art methods cannot avoid the modality noise when aligning <b>multimodal</b> information due to the large differences in the distributions of different modalities. To this end, we propose a Multi-level <b>sElf-supervised</b> <b>learNing</b> for <b>mulTimOdal</b> <b>Recommendation</b> (MENTOR) method to address the label sparsity problem and the modality alignment problem. Specifically, MENTOR first enhances the specific features of each modality using the <b>graph</b> <b>convolutional</b> <b>network</b> <b>(GCN)</b> and fuses the visual and textual modalities. It then enhances the item representation via the item semantic <b>graph</b> <b>for</b> <b>all</b> modalities, including the fused modality. Then, it introduces two multilevel <b>self-supervised</b> <b>tasks:</b> the multilevel cross-modal alignment task and the general feature enhancement task. The multilevel cross-modal alignment task aligns each modality under the guidance of the ID embedding from multiple levels while maintaining the historical interaction information. The general feature enhancement task enhances the general feature from both the <b>graph</b> <b>and</b> <b>feature</b> perspectives to improve the robustness of our model. Extensive experiments on three publicly available datasets demonstrate the effectiveness of our method. Our code is publicly available at <a href=https://github.com/Jinfeng-Xu/MENTOR>https://github.com/Jinfeng-Xu/MENTOR</a>.</p></p class="citation"></blockquote><h3 id=35--104254-paecter-patent-level-representation-learning-using-citation-informed-transformers-mainak-ghosh-et-al-2024>(3/5 | 104/254) PaECTER: Patent-level Representation Learning using Citation-informed Transformers (Mainak Ghosh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mainak Ghosh, Sebastian Erhardt, Michael E. Rose, Erik Buunk, Dietmar Harhoff. (2024)<br><strong>PaECTER: Patent-level Representation Learning using Citation-informed Transformers</strong><br><button class=copy-to-clipboard title="PaECTER: Patent-level Representation Learning using Citation-informed Transformers" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 45<br>Keywords: Fine-tuning, Representation Learning, BERT, Transformer, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19411v1.pdf filename=2402.19411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>PaECTER is a publicly available, open-source document-level encoder specific for patents. We <b>fine-tune</b> <b>BERT</b> for Patents with examiner-added citation information to generate numerical <b>representations</b> <b>for</b> patent documents. PaECTER performs better in similarity tasks than current state-of-the-art models used in the patent domain. More specifically, our model outperforms the next-best patent specific <b>pre-trained</b> <b>language</b> <b>model</b> <b>(BERT</b> for Patents) on our patent citation prediction test dataset on two different rank evaluation metrics. PaECTER predicts at least one most similar patent at a rank of 1.32 on average when compared against 25 irrelevant patents. Numerical <b>representations</b> <b>generated</b> by PaECTER from patent text can be used for downstream tasks such as classification, tracing knowledge flows, or semantic similarity search. Semantic similarity search is especially relevant in the context of prior art search for both inventors and patent examiners. PaECTER is available on Hugging Face.</p></p class="citation"></blockquote><h3 id=45--105254-aligning-language-models-for-versatile-text-based-item-retrieval-yuxuan-lei-et-al-2024>(4/5 | 105/254) Aligning Language Models for Versatile Text-based Item Retrieval (Yuxuan Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Lei, Jianxun Lian, Jing Yao, Mingqi Wu, Defu Lian, Xing Xie. (2024)<br><strong>Aligning Language Models for Versatile Text-based Item Retrieval</strong><br><button class=copy-to-clipboard title="Aligning Language Models for Versatile Text-based Item Retrieval" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Fine-tuning, Zero-shot, Large Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18899v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18899v1.pdf filename=2402.18899v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the gap between general-purpose <b>text</b> <b>embeddings</b> and the specific demands of item retrieval tasks. We demonstrate the shortcomings of existing models in capturing the nuances necessary for <b>zero-shot</b> performance on item retrieval tasks. To overcome these limitations, we propose generate in-domain dataset from ten tasks tailored to unlocking models&rsquo; representation ability for item retrieval. Our empirical studies demonstrate that <b>fine-tuning</b> embedding models on the dataset leads to remarkable improvements in a variety of retrieval tasks. We also illustrate the practical application of our refined model in a conversational setting, where it enhances the capabilities of <b>LLM-based</b> Recommender Agents like Chat-Rec. Our code is available at <a href=https://github.com/microsoft/RecAI>https://github.com/microsoft/RecAI</a>.</p></p class="citation"></blockquote><h3 id=55--106254-effective-two-stage-knowledge-transfer-for-multi-entity-cross-domain-recommendation-jianyu-guan-et-al-2024>(5/5 | 106/254) Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain Recommendation (Jianyu Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianyu Guan, Zongming Yin, Tianyi Zhang, Leihui Chen, Yin Zhang, Fei Huang, Jufeng Chen, Shuguang Han. (2024)<br><strong>Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain Recommendation</strong><br><button class=copy-to-clipboard title="Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain Recommendation" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 30<br>Keywords: Fine-tuning, Knowledge Transfer, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19101v1.pdf filename=2402.19101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the <b>recommendation</b> content on e-commerce platforms has become increasingly rich &ndash; a single user feed may contain multiple entities, such as selling products, short videos, and content posts. To deal with the multi-entity <b>recommendation</b> problem, an intuitive solution is to adopt the shared-network-based architecture for joint training. The idea is to transfer the extracted <b>knowledge</b> <b>from</b> one type of entity (source entity) to another (target entity). However, different from the conventional same-entity cross-domain <b>recommendation,</b> multi-entity <b>knowledge</b> <b>transfer</b> encounters several important issues: (1) data distributions of the source entity and target entity are naturally different, making the shared-network-based joint training susceptible to the negative transfer issue, (2) more importantly, the corresponding feature schema of each entity is not exactly aligned (e.g., price is an essential feature for selling product while missing for content posts), making the existing methods no longer appropriate. Recent researchers have also experimented with the pre-training and <b>fine-tuning</b> paradigm. Again, they only consider the scenarios with the same entity type and feature systems, which is inappropriate in our case. To this end, we design a pre-training & <b>fine-tuning</b> based Multi-entity <b>Knowledge</b> <b>Transfer</b> framework called MKT. MKT utilizes a multi-entity pre-training module to extract transferable <b>knowledge</b> <b>across</b> different entities. In particular, a feature alignment module is first applied to scale and align different feature schemas. Afterward, a couple of <b>knowledge</b> <b>extractors</b> are employed to extract the common and entity-specific <b>knowledge.</b> <b>In</b> the end, the extracted common <b>knowledge</b> <b>is</b> adopted for target entity model training. Through extensive offline and online experiments, we demonstrated the superiority of MKT over multiple State-Of-The-Art methods.</p></p class="citation"></blockquote><h2 id=cslg-52>cs.LG (52)</h2><h3 id=152--107254-dual-operating-modes-of-in-context-learning-ziqian-lin-et-al-2024>(1/52 | 107/254) Dual Operating Modes of In-Context Learning (Ziqian Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqian Lin, Kangwook Lee. (2024)<br><strong>Dual Operating Modes of In-Context Learning</strong><br><button class=copy-to-clipboard title="Dual Operating Modes of In-Context Learning" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Probabilistic Model, Zero-shot, Transformer, In-context Learning, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18819v1.pdf filename=2402.18819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-context</b> <b>learning</b> <b>(ICL)</b> exhibits dual operating modes: task learning, i.e., acquiring a new skill from <b>in-context</b> <b>samples,</b> and task retrieval, i.e., locating and activating a relevant pretrained skill. Recent theoretical work investigates various mathematical models to analyze <b>ICL,</b> but existing models explain only one operating mode at a time. We introduce a <b>probabilistic</b> <b>model,</b> with which one can explain the dual operating modes of <b>ICL</b> simultaneously. Focusing on <b>in-context</b> <b>learning</b> of linear functions, we extend existing models for pretraining data by introducing multiple task groups and task-dependent input distributions. We then analyze the behavior of the optimally pretrained model under the squared loss, i.e., the MMSE estimator of the label given <b>in-context</b> <b>examples.</b> Regarding pretraining task distribution as prior and <b>in-context</b> <b>examples</b> as the observation, we derive the closed-form expression of the task posterior distribution. With the closed-form expression, we obtain a quantitative understanding of the two operating modes of <b>ICL.</b> Furthermore, we shed light on an unexplained phenomenon observed in practice: under certain settings, the <b>ICL</b> risk initially increases and then decreases with more <b>in-context</b> <b>examples.</b> Our model offers a plausible explanation for this &ldquo;early ascent&rdquo; phenomenon: a limited number of <b>in-context</b> <b>samples</b> may lead to the retrieval of an incorrect skill, thereby increasing the risk, which will eventually diminish as task learning takes effect with more <b>in-context</b> <b>samples.</b> We also theoretically analyze <b>ICL</b> with biased labels, e.g., <b>zero-shot</b> <b>ICL,</b> where <b>in-context</b> <b>examples</b> are assigned random labels. Lastly, we validate our findings and predictions via experiments involving <b>Transformers</b> and <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=252--108254-generating-reconstructing-and-representing-discrete-and-continuous-data-generalized-diffusion-with-learnable-encoding-decoding-guangyi-liu-et-al-2024>(2/52 | 108/254) Generating, Reconstructing, and Representing Discrete and Continuous Data: Generalized Diffusion with Learnable Encoding-Decoding (Guangyi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyi Liu, Yu Wang, Zeyu Feng, Qiyu Wu, Liping Tang, Yuan Gao, Zhen Li, Shuguang Cui, Julian McAuley, Eric P. Xing, Zichao Yang, Zhiting Hu. (2024)<br><strong>Generating, Reconstructing, and Representing Discrete and Continuous Data: Generalized Diffusion with Learnable Encoding-Decoding</strong><br><button class=copy-to-clipboard title="Generating, Reconstructing, and Representing Discrete and Continuous Data: Generalized Diffusion with Learnable Encoding-Decoding" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Diffusion Model, Autoencoder, Generative Adversarial Network, Generative Adversarial Network, Variational Autoencoder, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19009v1.pdf filename=2402.19009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The vast applications of deep <b>generative</b> <b>models</b> <b>are</b> anchored in three core capabilities &ndash; generating new instances, reconstructing inputs, and learning compact representations &ndash; across various data types, such as discrete text/protein sequences and continuous images. Existing model families, like <b>Variational</b> <b>Autoencoders</b> (VAEs), <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs),</b> autoregressive models, and <b>diffusion</b> <b>models,</b> generally excel in specific capabilities and data types but fall short in others. We introduce generalized <b>diffusion</b> <b>with</b> learnable encoder-decoder (DiLED), that seamlessly integrates the core capabilities for broad applicability and enhanced performance. DiLED generalizes the Gaussian noising-denoising in standard <b>diffusion</b> <b>by</b> introducing parameterized encoding-decoding. Crucially, DiLED is compatible with the well-established <b>diffusion</b> <b>model</b> objective and training recipes, allowing effective learning of the encoder-decoder parameters jointly with <b>diffusion.</b> <b>By</b> choosing appropriate encoder/decoder (e.g., <b>large</b> <b>language</b> <b>models),</b> DiLED naturally applies to different data types. Extensive experiments on text, proteins, and images demonstrate DiLED&rsquo;s flexibility to handle diverse data and tasks and its strong improvement over various existing models.</p></p class="citation"></blockquote><h3 id=352--109254-analyzing-and-reducing-catastrophic-forgetting-in-parameter-efficient-tuning-weijieying-ren-et-al-2024>(3/52 | 109/254) Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning (Weijieying Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, Wei Qin. (2024)<br><strong>Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning</strong><br><button class=copy-to-clipboard title="Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Benchmarking, Continual Learning, Fine-tuning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18865v1.pdf filename=2402.18865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing research has shown that <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> exhibit remarkable performance in language understanding and generation. However, when <b>LLMs</b> are continuously <b>fine-tuned</b> on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the <b>continual</b> <b>LLMs</b> <b>fine-tuning</b> scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the <b>LLMs</b> <b>continual</b> <b>learning</b> scenario and find that it can strike a balance between plasticity and stability. Building upon these findings, we propose a simple yet effective method called Interpolation-based LoRA (I-LoRA), which constructs a dual-memory experience replay framework based on LoRA parameter interpolations. Extensive experiments and analysis on eight domain-specific CL <b>benchmarks</b> demonstrate that I-LoRA consistently show significant improvement over the previous state-of-the-art approaches with up to $11%$ performance gains, providing a strong baseline and insights for future research on the <b>large</b> <b>language</b> <b>model</b> <b>continual</b> <b>learning</b> problem. Our code is available at \url{https://github.com/which47/LLMCL}.</p></p class="citation"></blockquote><h3 id=452--110254-curiosity-driven-red-teaming-for-large-language-models-zhang-wei-hong-et-al-2024>(4/52 | 110/254) Curiosity-driven Red-teaming for Large Language Models (Zhang-Wei Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhang-Wei Hong, Idan Shenfeld, Tsun-Hsuan Wang, Yung-Sung Chuang, Aldo Pareja, James Glass, Akash Srivastava, Pulkit Agrawal. (2024)<br><strong>Curiosity-driven Red-teaming for Large Language Models</strong><br><button class=copy-to-clipboard title="Curiosity-driven Red-teaming for Large Language Models" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, Reinforcement Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19464v1.pdf filename=2402.19464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an <b>LLM</b> generates unwanted content, the current paradigm is to recruit a \textit{red team} of human testers to design input <b>prompts</b> (i.e., test cases) that elicit undesirable responses from <b>LLMs.</b> However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team <b>LLM</b> with <b>reinforcement</b> <b>learning</b> (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target <b>LLM.</b> However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of <b>prompts</b> that elicit undesirable responses from the target <b>LLM.</b> To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty. Our method of curiosity-driven red teaming (CRT) achieves greater coverage of test cases while mantaining or increasing their effectiveness compared to existing methods. Our method, CRT successfully provokes toxic responses from LLaMA2 model that has been heavily <b>fine-tuned</b> using human preferences to avoid toxic outputs. Code is available at \url{https://github.com/Improbable-AI/curiosity_redteam}</p></p class="citation"></blockquote><h3 id=552--111254-archer-training-language-model-agents-via-hierarchical-multi-turn-rl-yifei-zhou-et-al-2024>(5/52 | 111/254) ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL (Yifei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, Aviral Kumar. (2024)<br><strong>ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL</strong><br><button class=copy-to-clipboard title="ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, Reinforcement Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19446v1.pdf filename=2402.19446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A broad use case of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is in goal-directed decision-making tasks (or &ldquo;agent&rdquo; tasks), where an <b>LLM</b> needs to not just generate completions for a given <b>prompt,</b> but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). <b>Reinforcement</b> <b>learning</b> (RL) provides a general paradigm to address such agent tasks, but current RL methods for <b>LLMs</b> largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow <b>LLMs</b> with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions &ndash; all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for <b>fine-tuning</b> <b>LLMs,</b> that preserves the flexibility of existing single-turn RL methods for <b>LLMs</b> (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy value-based RL algorithm to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token policy within each utterance or turn. Our hierarchical framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to other RL methods. Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).</p></p class="citation"></blockquote><h3 id=652--112254-griffin-mixing-gated-linear-recurrences-with-local-attention-for-efficient-language-models-soham-de-et-al-2024>(6/52 | 112/254) Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models (Soham De et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar Gulcehre. (2024)<br><strong>Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models</strong><br><button class=copy-to-clipboard title="Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Graph Attention Networks, LLaMA, Recurrent Neural Network, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19427v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19427v1.pdf filename=2402.19427v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recurrent</b> <b>neural</b> <b>networks</b> <b>(RNNs)</b> have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an <b>RNN</b> with <b>gated</b> linear recurrences, and Griffin, a hybrid model that mixes <b>gated</b> linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of <b>Llama-2</b> despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of <b>Transformers</b> during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.</p></p class="citation"></blockquote><h3 id=752--113254-loss-free-machine-unlearning-jack-foster-et-al-2024>(7/52 | 113/254) Loss-Free Machine Unlearning (Jack Foster et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jack Foster, Stefan Schoepf, Alexandra Brintrup. (2024)<br><strong>Loss-Free Machine Unlearning</strong><br><button class=copy-to-clipboard title="Loss-Free Machine Unlearning" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Vision Transformer, Fine-tuning, Machine Unlearning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19308v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19308v1.pdf filename=2402.19308v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a <b>machine</b> <b>unlearning</b> approach that is both retraining- and label-free. Most existing <b>machine</b> <b>unlearning</b> approaches require a model to be <b>fine-tuned</b> to remove information while preserving performance. This is computationally expensive and necessitates the storage of the whole dataset for the lifetime of the model. Retraining-free approaches often utilise Fisher information, which is derived from the loss and requires labelled data which may not be available. Thus, we present an extension to the Selective Synaptic Dampening algorithm, substituting the diagonal of the Fisher information matrix for the gradient of the l2 norm of the model output to approximate sensitivity. We evaluate our method in a range of experiments using ResNet18 and <b>Vision</b> <b>Transformer.</b> Results show our label-free method is competitive with existing state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=852--114254-theoretical-foundations-of-deep-selective-state-space-models-nicola-muca-cirone-et-al-2024>(8/52 | 114/254) Theoretical Foundations of Deep Selective State-Space Models (Nicola Muca Cirone et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi, Terry Lyons. (2024)<br><strong>Theoretical Foundations of Deep Selective State-Space Models</strong><br><button class=copy-to-clipboard title="Theoretical Foundations of Deep Selective State-Space Models" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-DS<br>Keyword Score: 40<br>Keywords: Foundation Model, Transformer, Grounding, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19047v1.pdf filename=2402.19047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structured state-space models (SSMs) such as S4, <b>stemming</b> from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based <b>transformers.</b> Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered <b>foundation</b> <b>models</b> trained on text, at scales of billion parameters. In this paper, we give theoretical <b>grounding</b> to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input &ndash; capturing non-linear interactions between tokens at distinct timescales. Our theory not only motivates the success of modern selective state-space models such as Mamba but also provides a solid framework to understand the expressive power of future SSM variants.</p></p class="citation"></blockquote><h3 id=952--115254-loss-aware-curriculum-learning-for-heterogeneous-graph-neural-networks-zhen-hao-wong-et-al-2024>(9/52 | 115/254) Loss-aware Curriculum Learning for Heterogeneous Graph Neural Networks (Zhen Hao Wong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Hao Wong, Hansi Yang, Xiaoyi Fu, Quanming Yao. (2024)<br><strong>Loss-aware Curriculum Learning for Heterogeneous Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Loss-aware Curriculum Learning for Heterogeneous Graph Neural Networks" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Curriculum Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18875v1.pdf filename=2402.18875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Heterogeneous <b>Graph</b> <b>Neural</b> <b>Networks</b> (HGNNs) are a class of deep learning models designed specifically for heterogeneous <b>graphs,</b> <b>which</b> <b>are</b> <b>graphs</b> <b>that</b> <b>contain</b> different types of nodes and edges. This paper investigates the application of <b>curriculum</b> <b>learning</b> techniques to improve the performance and robustness of Heterogeneous <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs).</b> To better classify the quality of the data, we design a loss-aware training schedule, named LTS that measures the quality of every nodes of the data and incorporate the training dataset into the model in a progressive manner that increases difficulty step by step. LTS can be seamlessly integrated into various frameworks, effectively reducing bias and variance, mitigating the impact of noisy data, and enhancing overall accuracy. Our findings demonstrate the efficacy of <b>curriculum</b> <b>learning</b> in enhancing HGNNs capabilities for analyzing complex <b>graph-structured</b> <b>data.</b> <b>The</b> code is public at https: //github.com/LARS-research/CLGNN/.</p></p class="citation"></blockquote><h3 id=1052--116254-training-dynamics-of-multi-head-softmax-attention-for-in-context-learning-emergence-convergence-and-optimality-siyu-chen-et-al-2024>(10/52 | 116/254) Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality (Siyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyu Chen, Heejune Sheen, Tianhao Wang, Zhuoran Yang. (2024)<br><strong>Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality</strong><br><button class=copy-to-clipboard title="Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC, math-ST, stat-ML, stat-TH<br>Keyword Score: 30<br>Keywords: In-context Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19442v1.pdf filename=2402.19442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the dynamics of gradient flow for training a multi-head softmax attention model for <b>in-context</b> <b>learning</b> of multi-task linear regression. We establish the global convergence of gradient flow under suitable choices of initialization. In addition, we prove that an interesting &ldquo;task allocation&rdquo; phenomenon emerges during the gradient flow dynamics, where each attention head focuses on solving a single task of the multi-task model. Specifically, we prove that the gradient flow dynamics can be split into three phases &ndash; a warm-up phase where the loss decreases rather slowly and the attention heads gradually build up their inclination towards individual tasks, an emergence phase where each head selects a single task and the loss rapidly decreases, and a convergence phase where the attention parameters converge to a limit. Furthermore, we prove the optimality of gradient flow in the sense that the limiting model learned by gradient flow is on par with the best possible multi-head softmax attention model up to a constant factor. Our analysis also delineates a strict separation in terms of the prediction accuracy of <b>ICL</b> between single-head and multi-head attention models. The key technique for our convergence analysis is to map the gradient flow dynamics in the parameter space to a set of ordinary differential equations in the spectral domain, where the relative magnitudes of the semi-singular values of the attention weights determines task allocation. To our best knowledge, our work provides the first convergence result for the multi-head softmax attention model.</p></p class="citation"></blockquote><h3 id=1152--117254-verification-of-neural-networks-global-robustness-anan-kabaha-et-al-2024>(11/52 | 117/254) Verification of Neural Networks&rsquo; Global Robustness (Anan Kabaha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anan Kabaha, Dana Drachsler-Cohen. (2024)<br><strong>Verification of Neural Networks&rsquo; Global Robustness</strong><br><button class=copy-to-clipboard title="Verification of Neural Networks' Global Robustness" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs-PL, cs.LG<br>Keyword Score: 30<br>Keywords: Pruning, Stemming, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19322v1.pdf filename=2402.19322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural networks are successful in various applications but are also susceptible to <b>adversarial</b> <b>attacks.</b> To show the safety of network classifiers, many verifiers have been introduced to reason about the local robustness of a given input to a given perturbation. While successful, local robustness cannot generalize to unseen inputs. Several works analyze global robustness properties, however, neither can provide a precise guarantee about the cases where a network classifier does not change its classification. In this work, we propose a new global robustness property for classifiers aiming at finding the minimal globally robust bound, which naturally extends the popular local robustness property for classifiers. We introduce VHAGaR, an anytime verifier for computing this bound. VHAGaR relies on three main ideas: encoding the problem as a mixed-integer programming and <b>pruning</b> the search space by identifying dependencies <b>stemming</b> from the perturbation or network computation and generalizing <b>adversarial</b> <b>attacks</b> to unknown inputs. We evaluate VHAGaR on several datasets and classifiers and show that, given a three hour timeout, the average gap between the lower and upper bound on the minimal globally robust bound computed by VHAGaR is 1.9, while the gap of an existing global robustness verifier is 154.7. Moreover, VHAGaR is 130.6x faster than this verifier. Our results further indicate that leveraging dependencies and <b>adversarial</b> <b>attacks</b> makes VHAGaR 78.6x faster.</p></p class="citation"></blockquote><h3 id=1252--118254-investigating-gender-fairness-in-machine-learning-driven-personalized-care-for-chronic-pain-pratik-gajane-et-al-2024>(12/52 | 118/254) Investigating Gender Fairness in Machine Learning-driven Personalized Care for Chronic Pain (Pratik Gajane et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pratik Gajane, Sean Newman, John D. Piette. (2024)<br><strong>Investigating Gender Fairness in Machine Learning-driven Personalized Care for Chronic Pain</strong><br><button class=copy-to-clipboard title="Investigating Gender Fairness in Machine Learning-driven Personalized Care for Chronic Pain" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Fairness, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19226v1.pdf filename=2402.19226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates gender <b>fairness</b> in personalized pain care <b>recommendations</b> using machine learning algorithms. Leveraging a contextual <b>bandits</b> framework, personalized <b>recommendations</b> are formulated and evaluated using LinUCB algorithm on a dataset comprising interactions with $164$ patients across $10$ sessions each. Results indicate that while adjustments to algorithm parameters influence the quality of pain care <b>recommendations,</b> this impact remains consistent across genders. However, when certain patient information, such as self-reported pain measurements, is absent, the quality of pain care <b>recommendations</b> for women is notably inferior to that for men.</p></p class="citation"></blockquote><h3 id=1352--119254-collafuse-navigating-limited-resources-and-privacy-in-collaborative-generative-ai-domenique-zipperling-et-al-2024>(13/52 | 119/254) CollaFuse: Navigating Limited Resources and Privacy in Collaborative Generative AI (Domenique Zipperling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Domenique Zipperling, Simeon Allmendinger, Lukas Struppek, Niklas Kühl. (2024)<br><strong>CollaFuse: Navigating Limited Resources and Privacy in Collaborative Generative AI</strong><br><button class=copy-to-clipboard title="CollaFuse: Navigating Limited Resources and Privacy in Collaborative Generative AI" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Generative AI, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19105v1.pdf filename=2402.19105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the landscape of <b>generative</b> <b>artificial</b> intelligence, diffusion-based models present challenges for socio-technical systems in data requirements and privacy. Traditional approaches like <b>federated</b> <b>learning</b> distribute the learning process but strain individual clients, especially with constrained resources (e.g., edge devices). In response to these challenges, we introduce CollaFuse, a novel framework inspired by split learning. Tailored for efficient and collaborative use of denoising diffusion <b>probabilistic</b> <b>models,</b> CollaFuse enables shared server training and inference, alleviating client computational burdens. This is achieved by retaining data and computationally inexpensive GPU processes locally at each client while outsourcing the computationally expensive processes to the shared server. Demonstrated in a healthcare context, CollaFuse enhances privacy by highly reducing the need for sensitive information sharing. These capabilities hold the potential to impact various application areas, such as the design of edge computing solutions, healthcare research, or autonomous driving. In essence, our work advances distributed machine learning, shaping the future of collaborative GenAI networks.</p></p class="citation"></blockquote><h3 id=1452--120254-improving-group-connectivity-for-generalization-of-federated-deep-learning-zexi-li-et-al-2024>(14/52 | 120/254) Improving Group Connectivity for Generalization of Federated Deep Learning (Zexi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zexi Li, Jie Lin, Zhiqi Li, Didi Zhu, Chao Wu. (2024)<br><strong>Improving Group Connectivity for Generalization of Federated Deep Learning</strong><br><button class=copy-to-clipboard title="Improving Group Connectivity for Generalization of Federated Deep Learning" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Federated Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18949v1.pdf filename=2402.18949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) involves multiple heterogeneous clients collaboratively training a global model via iterative local updates and model fusion. The generalization of FL&rsquo;s global model has a large gap compared with centralized training, which is its bottleneck for broader applications. In this paper, we study and improve FL&rsquo;s generalization through a fundamental <code>connectivity'' perspective, which means how the local models are connected in the parameter region and fused into a generalized global model. The term </code>connectivity&rsquo;&rsquo; is derived from linear mode connectivity (LMC), studying the interpolated loss landscape of two different solutions (e.g., modes) of neural networks. Bridging the gap between LMC and FL, in this paper, we leverage fixed anchor models to empirically and theoretically study the transitivity property of connectivity from two models (LMC) to a group of models (model fusion in FL). Based on the findings, we propose FedGuCci and FedGuCci+, improving group connectivity for better generalization. It is shown that our methods can boost the generalization of FL under client heterogeneity across various tasks (4 CV datasets and 6 NLP datasets), models (both <b>convolutional</b> and <b>transformer-based),</b> and training paradigms (both from-scratch and pretrain-finetune).</p></p class="citation"></blockquote><h3 id=1552--121254-real-time-adaptive-safety-critical-control-with-gaussian-processes-in-high-order-uncertain-models-yu-zhang-et-al-2024>(15/52 | 121/254) Real-Time Adaptive Safety-Critical Control with Gaussian Processes in High-Order Uncertain Models (Yu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Zhang, Long Wen, Xiangtong Yao, Zhenshan Bing, Linghuan Kong, Wei He, Alois Knoll. (2024)<br><strong>Real-Time Adaptive Safety-Critical Control with Gaussian Processes in High-Order Uncertain Models</strong><br><button class=copy-to-clipboard title="Real-Time Adaptive Safety-Critical Control with Gaussian Processes in High-Order Uncertain Models" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18946v1.pdf filename=2402.18946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an adaptive online learning framework for systems with uncertain parameters to ensure safety-critical control in non-stationary environments. Our approach consists of two phases. The initial phase is centered on a novel sparse <b>Gaussian</b> <b>process</b> (GP) framework. We first integrate a forgetting factor to refine a variational sparse GP algorithm, thus enhancing its adaptability. Subsequently, the hyperparameters of the <b>Gaussian</b> <b>model</b> are trained with a specially compound kernel, and the <b>Gaussian</b> <b>model&rsquo;s</b> online inferential capability and computational efficiency are strengthened by updating a solitary inducing point derived from new samples, in conjunction with the learned hyperparameters. In the second phase, we propose a safety filter based on high-order control barrier functions (HOCBFs), synergized with the previously trained learning model. By leveraging the compound kernel from the first phase, we effectively address the inherent limitations of GPs in handling high-dimensional problems for real-time applications. The derived controller ensures a rigorous lower bound on the probability of satisfying the safety specification. Finally, the efficacy of our proposed algorithm is demonstrated through real-time obstacle avoidance experiments executed using both a <b>simulation</b> platform and a real-world 7-DOF robot.</p></p class="citation"></blockquote><h3 id=1652--122254-bp-deeponet-a-new-method-for-cuffless-blood-pressure-estimation-using-the-physcis-informed-deeponet-lingfeng-li-et-al-2024>(16/52 | 122/254) BP-DeepONet: A new method for cuffless blood pressure estimation using the physcis-informed DeepONet (Lingfeng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingfeng Li, Xue-Cheng Tai, Raymond Chan. (2024)<br><strong>BP-DeepONet: A new method for cuffless blood pressure estimation using the physcis-informed DeepONet</strong><br><button class=copy-to-clipboard title="BP-DeepONet: A new method for cuffless blood pressure estimation using the physcis-informed DeepONet" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-med-ph<br>Keyword Score: 30<br>Keywords: Meta Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18886v1.pdf filename=2402.18886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cardiovascular diseases (CVDs) are the leading cause of death worldwide, with blood pressure serving as a crucial indicator. Arterial blood pressure (ABP) waveforms provide continuous pressure measurements throughout the cardiac cycle and offer valuable diagnostic insights. Consequently, there is a significant demand for non-invasive and cuff-less methods to measure ABP waveforms continuously. Accurate prediction of ABP waveforms can also improve the estimation of mean blood pressure, an essential cardiovascular health characteristic. This study proposes a novel framework based on the physics-informed DeepONet approach to predict ABP waveforms. Unlike previous methods, our approach requires the predicted ABP waveforms to satisfy the Navier-Stokes equation with a time-periodic condition and a Windkessel boundary condition. Notably, our framework is the first to predict ABP waveforms continuously, both with location and time, within the part of the artery that is being simulated. Furthermore, our method only requires ground truth data at the outlet boundary and can handle periodic conditions with varying periods. Incorporating the Windkessel boundary condition in our solution allows for generating natural physical reflection waves, which closely resemble measurements observed in real-world cases. Moreover, accurately estimating the hyper-parameters in the Navier-Stokes equation for our <b>simulations</b> poses a significant challenge. To overcome this obstacle, we introduce the concept of <b>meta-learning,</b> <b>enabling</b> the neural networks to learn these parameters during the training process.</p></p class="citation"></blockquote><h3 id=1752--123254-stiefelgen-a-simple-model-agnostic-approach-for-time-series-data-augmentation-over-riemannian-manifolds-prasad-cheema-et-al-2024>(17/52 | 123/254) StiefelGen: A Simple, Model Agnostic Approach for Time Series Data Augmentation over Riemannian Manifolds (Prasad Cheema et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prasad Cheema, Mahito Sugiyama. (2024)<br><strong>StiefelGen: A Simple, Model Agnostic Approach for Time Series Data Augmentation over Riemannian Manifolds</strong><br><button class=copy-to-clipboard title="StiefelGen: A Simple, Model Agnostic Approach for Time Series Data Augmentation over Riemannian Manifolds" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Data Augmentation, Geometry, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19287v1.pdf filename=2402.19287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Data</b> <b>augmentation</b> is an area of research which has seen active development in many machine learning fields, such as in image-based learning models, <b>reinforcement</b> <b>learning</b> for self driving vehicles, and general noise injection for point cloud <b>data.</b> <b>However,</b> convincing methods for general time series <b>data</b> <b>augmentation</b> still leaves much to be desired, especially since the methods developed for these models do not readily cross-over. Three common approaches for time series <b>data</b> <b>augmentation</b> include: (i) Constructing a physics-based model and then imbuing uncertainty over the coefficient space (for example), (ii) Adding noise to the observed <b>data</b> <b>set(s),</b> and, (iii) Having access to ample amounts of time series <b>data</b> <b>sets</b> from which a robust generative neural network model can be trained. However, for many practical problems that work with time series <b>data</b> <b>in</b> the industry: (i) One usually does not have access to a robust physical model, (ii) The addition of noise can in of itself require large or difficult assumptions (for example, what probability distribution should be used? Or, how large should the noise variance be?), and, (iii) In practice, it can be difficult to source a large representative time series <b>data</b> <b>base</b> with which to train the neural network model for the underlying problem. In this paper, we propose a methodology which attempts to simultaneously tackle all three of these previous limitations to a large extent. The method relies upon the well-studied matrix differential <b>geometry</b> of the Stiefel manifold, as it proposes a simple way in which time series signals can placed on, and then smoothly perturbed over the manifold. We attempt to clarify how this method works by showcasing several potential use cases which in particular work to take advantage of the unique properties of this underlying manifold.</p></p class="citation"></blockquote><h3 id=1852--124254-deep-learning-for-cross-domain-data-fusion-in-urban-computing-taxonomy-advances-and-outlook-xingchen-zou-et-al-2024>(18/52 | 124/254) Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook (Xingchen Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingchen Zou, Yibo Yan, Xixuan Hao, Yuehong Hu, Haomin Wen, Erdong Liu, Junbo Zhang, Yong Li, Tianrui Li, Yu Zheng, Yuxuan Liang. (2024)<br><strong>Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook</strong><br><button class=copy-to-clipboard title="Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19348v1.pdf filename=2402.19348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods. Thirdly, we further categorize <b>multi-modal</b> urban applications into seven types: urban planning, transportation, economy, public safety, society, environment, and energy. Compared with previous surveys, we focus more on the synergy of deep learning methods with urban computing applications. Furthermore, we shed light on the interplay between <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and urban computing, postulating future research directions that could revolutionize the field. We firmly believe that the taxonomy, progress, and prospects delineated in our survey stand poised to significantly enrich the research community. The summary of the comprehensive and up-to-date paper list can be found at <a href=https://github.com/yoshall/Awesome-Multimodal-Urban-Computing>https://github.com/yoshall/Awesome-Multimodal-Urban-Computing</a>.</p></p class="citation"></blockquote><h3 id=1952--125254-fedstruct-federated-decoupled-learning-over-interconnected-graphs-javad-aliakbari-et-al-2024>(19/52 | 125/254) FedStruct: Federated Decoupled Learning over Interconnected Graphs (Javad Aliakbari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Javad Aliakbari, Johan Östman, Alexandre Graell i Amat. (2024)<br><strong>FedStruct: Federated Decoupled Learning over Interconnected Graphs</strong><br><button class=copy-to-clipboard title="FedStruct: Federated Decoupled Learning over Interconnected Graphs" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 23<br>Keywords: Node Classification, Graph, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19163v1.pdf filename=2402.19163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the challenge of <b>federated</b> <b>learning</b> on <b>graph-structured</b> data distributed across multiple clients. Specifically, we focus on the prevalent scenario of interconnected subgraphs, where inter-connections between different clients play a critical role. We present a novel framework for this scenario, named FedStruct, that harnesses deep structural dependencies. To uphold privacy, unlike existing methods, FedStruct eliminates the necessity of sharing or generating sensitive <b>node</b> <b>features</b> or embeddings among clients. Instead, it leverages explicit global <b>graph</b> structure information to capture inter-node dependencies. We validate the effectiveness of FedStruct through experimental results conducted on six datasets for semi-supervised <b>node</b> <b>classification,</b> showcasing performance close to the centralized approach across various scenarios, including different data partitioning methods, varying levels of label availability, and number of clients.</p></p class="citation"></blockquote><h3 id=2052--126254-timexer-empowering-transformers-for-time-series-forecasting-with-exogenous-variables-yuxuan-wang-et-al-2024>(20/52 | 126/254) TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables (Yuxuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Yunzhong Qiu, Haoran Zhang, Jianmin Wang, Mingsheng Long. (2024)<br><strong>TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables</strong><br><button class=copy-to-clipboard title="TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19072v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19072v1.pdf filename=2402.19072v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous series can provide valuable external information for endogenous variables. Thus, unlike prior well-established multivariate or univariate forecasting that either treats all the variables equally or overlooks exogenous information, this paper focuses on a practical setting, which is time series forecasting with exogenous variables. We propose a novel framework, TimeXer, to utilize external information to enhance the forecasting of endogenous variables. With a deftly designed embedding layer, TimeXer empowers the canonical <b>Transformer</b> architecture with the ability to reconcile endogenous and exogenous information, where patch-wise <b>self-attention</b> and variate-wise cross-attention are employed. Moreover, a global endogenous variate token is adopted to effectively bridge the exogenous series into endogenous temporal patches. Experimentally, TimeXer significantly improves time series forecasting with exogenous variables and achieves consistent state-of-the-art performance in twelve real-world forecasting <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=2152--127254-to-pool-or-not-to-pool-analyzing-the-regularizing-effects-of-group-fair-training-on-shared-models-cyrus-cousins-et-al-2024>(21/52 | 127/254) To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair Training on Shared Models (Cyrus Cousins et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cyrus Cousins, I. Elizabeth Kumar, Suresh Venkatasubramanian. (2024)<br><strong>To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair Training on Shared Models</strong><br><button class=copy-to-clipboard title="To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair Training on Shared Models" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Sample Size, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18803v1.pdf filename=2402.18803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In fair machine learning, one source of performance disparities between groups is over-fitting to groups with relatively few training <b>samples.</b> <b>We</b> derive group-specific bounds on the generalization error of welfare-centric fair machine learning that benefit from the larger <b>sample</b> <b>size</b> of the majority group. We do this by considering group-specific Rademacher averages over a restricted hypothesis class, which contains the family of models likely to perform well with respect to a fair learning objective (e.g., a power-mean). Our <b>simulations</b> demonstrate these bounds improve over a naive method, as expected by theory, with particularly significant improvement for smaller group sizes.</p></p class="citation"></blockquote><h3 id=2252--128254-mpat-building-robust-deep-neural-networks-against-textual-adversarial-attacks-fangyuan-zhang-et-al-2024>(22/52 | 128/254) MPAT: Building Robust Deep Neural Networks against Textual Adversarial Attacks (Fangyuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangyuan Zhang, Huichi Zhou, Shuangjiao Li, Hongtao Wang. (2024)<br><strong>MPAT: Building Robust Deep Neural Networks against Textual Adversarial Attacks</strong><br><button class=copy-to-clipboard title="MPAT: Building Robust Deep Neural Networks against Textual Adversarial Attacks" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Adversarial Learning, Benchmarking, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18792v1.pdf filename=2402.18792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks have been proven to be vulnerable to <b>adversarial</b> <b>examples</b> and various methods have been proposed to defend against <b>adversarial</b> <b>attacks</b> for natural language processing tasks. However, previous defense methods have limitations in maintaining effective defense while ensuring the performance of the original task. In this paper, we propose a malicious perturbation based <b>adversarial</b> <b>training</b> method (MPAT) for building robust deep neural networks against textual <b>adversarial</b> <b>attacks.</b> Specifically, we construct a multi-level malicious example generation strategy to generate <b>adversarial</b> <b>examples</b> with malicious perturbations, which are used instead of original inputs for model training. Additionally, we employ a novel training objective function to ensure achieving the defense goal without compromising the performance on the original task. We conduct comprehensive experiments to evaluate our defense method by attacking five victim models on three <b>benchmark</b> datasets. The result demonstrates that our method is more effective against malicious <b>adversarial</b> <b>attacks</b> compared with previous defense methods while maintaining or further improving the performance on the original task.</p></p class="citation"></blockquote><h3 id=2352--129254-heavy-tailed-class-imbalance-and-why-adam-outperforms-gradient-descent-on-language-models-frederik-kunstner-et-al-2024>(23/52 | 129/254) Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models (Frederik Kunstner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, Alberto Bietti. (2024)<br><strong>Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models</strong><br><button class=copy-to-clipboard title="Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19449v1.pdf filename=2402.19449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adam has been shown to outperform gradient descent in optimizing large language <b>transformers</b> empirically, and by a larger margin than on other tasks, but it is unclear why this happens. We show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics. When training with gradient descent, the loss associated with infrequent words decreases slower than the loss associated with frequent ones. As most samples come from relatively infrequent words, the average loss decreases slowly with gradient descent. On the other hand, Adam and sign-based methods do not suffer from this problem and improve predictions on all classes. To establish that this behavior is indeed caused by class imbalance, we show empirically that it persist through different architectures and data types, on language <b>transformers,</b> vision <b>CNNs,</b> and linear models. We further study this phenomenon on a linear classification with cross-entropy loss, showing that heavy-tailed class imbalance leads to ill-conditioning, and that the normalization used by Adam can counteract it.</p></p class="citation"></blockquote><h3 id=2452--130254-estimation-and-deconvolution-of-second-order-cyclostationary-signals-igor-makienko-et-al-2024>(24/52 | 130/254) Estimation and Deconvolution of Second Order Cyclostationary Signals (Igor Makienko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Igor Makienko, Michael Grebshtein, Eli Gildish. (2024)<br><strong>Estimation and Deconvolution of Second Order Cyclostationary Signals</strong><br><button class=copy-to-clipboard title="Estimation and Deconvolution of Second Order Cyclostationary Signals" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19290v1.pdf filename=2402.19290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This method solves the dual problem of blind deconvolution and estimation of the time waveform of noisy second-order cyclo-stationary (CS2) signals that traverse a Transfer Function (TF) en route to a sensor. We have proven that the deconvolution filter exists and eliminates the TF effect from signals whose statistics vary over time. This method is blind, meaning it does not require prior knowledge about the signals or TF. <b>Simulations</b> demonstrate the algorithm high precision across various signal types, TFs, and Signal-to-Noise Ratios (SNRs). In this study, the CS2 signals family is restricted to the product of a deterministic periodic function and white noise. Furthermore, this method has the potential to improve the training of Machine Learning models where the aggregation of signals from identical systems but with different TFs is required.</p></p class="citation"></blockquote><h3 id=2552--131254-sprifed-omp-a-differentially-private-federated-learning-algorithm-for-sparse-basis-recovery-ajinkya-kiran-mulay-et-al-2024>(25/52 | 131/254) SPriFed-OMP: A Differentially Private Federated Learning Algorithm for Sparse Basis Recovery (Ajinkya Kiran Mulay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ajinkya Kiran Mulay, Xiaojun Lin. (2024)<br><strong>SPriFed-OMP: A Differentially Private Federated Learning Algorithm for Sparse Basis Recovery</strong><br><button class=copy-to-clipboard title="SPriFed-OMP: A Differentially Private Federated Learning Algorithm for Sparse Basis Recovery" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Federated Learning, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19016v1.pdf filename=2402.19016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sparse basis recovery is a classical and important statistical learning problem when the number of model dimensions $p$ is much larger than the number of samples $n$. However, there has been little work that studies sparse basis recovery in the <b>Federated</b> <b>Learning</b> (FL) setting, where the client data&rsquo;s <b>differential</b> <b>privacy</b> (DP) must also be simultaneously protected. In particular, the performance guarantees of existing DP-FL algorithms (such as DP-SGD) will degrade significantly when $p \gg n$, and thus, they will fail to learn the true underlying sparse model accurately. In this work, we develop a new differentially private sparse basis recovery algorithm for the FL setting, called SPriFed-OMP. SPriFed-OMP converts OMP (Orthogonal Matching Pursuit) to the FL setting. Further, it combines SMPC (secure multi-party computation) and DP to ensure that only a small amount of noise needs to be added in order to achieve <b>differential</b> <b>privacy.</b> As a result, SPriFed-OMP can efficiently recover the true sparse basis for a linear model with only $n = O(\sqrt{p})$ samples. We further present an enhanced version of our approach, SPriFed-OMP-GRAD based on gradient privatization, that improves the performance of SPriFed-OMP. Our theoretical analysis and empirical results demonstrate that both SPriFed-OMP and SPriFed-OMP-GRAD terminate in a small number of steps, and they significantly outperform the previous state-of-the-art DP-FL solutions in terms of the accuracy-privacy trade-off.</p></p class="citation"></blockquote><h3 id=2652--132254-stop-relying-on-no-choice-and-do-not-repeat-the-moves-optimal-efficient-and-practical-algorithms-for-assortment-optimization-aadirupa-saha-et-al-2024>(26/52 | 132/254) Stop Relying on No-Choice and Do not Repeat the Moves: Optimal, Efficient and Practical Algorithms for Assortment Optimization (Aadirupa Saha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aadirupa Saha, Pierre Gaillard. (2024)<br><strong>Stop Relying on No-Choice and Do not Repeat the Moves: Optimal, Efficient and Practical Algorithms for Assortment Optimization</strong><br><button class=copy-to-clipboard title="Stop Relying on No-Choice and Do not Repeat the Moves: Optimal, Efficient and Practical Algorithms for Assortment Optimization" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18917v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18917v1.pdf filename=2402.18917v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the problem of active online assortment optimization problem with preference feedback, which is a framework for modeling user choices and subsetwise utility maximization. The framework is useful in various real-world applications including ad placement, online retail, <b>recommender</b> <b>systems,</b> <b>fine-tuning</b> language models, amongst many. The problem, although has been studied in the past, lacks an intuitive and practical solution approach with simultaneously efficient algorithm and optimal regret guarantee. E.g., popularly used assortment selection algorithms often require the presence of a <code>strong reference' which is always included in the choice sets, further they are also designed to offer the same assortments repeatedly until the reference item gets selected -- all such requirements are quite unrealistic for practical applications. In this paper, we designed efficient algorithms for the problem of regret minimization in assortment selection with \emph{Plackett Luce} (PL) based user choices. We designed a novel concentration guarantee for estimating the score parameters of the PL model using </code>\emph{Pairwise Rank-Breaking}&rsquo;, which builds the foundation of our proposed algorithms. Moreover, our methods are practical, provably optimal, and devoid of the aforementioned limitations of the existing methods. Empirical evaluations corroborate our findings and outperform the existing baselines.</p></p class="citation"></blockquote><h3 id=2752--133254-extended-flow-matching-a-method-of-conditional-generation-with-generalized-continuity-equation-noboru-isobe-et-al-2024>(27/52 | 133/254) Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation (Noboru Isobe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Noboru Isobe, Masanori Koyama, Kohei Hayashi, Kenji Fukumizu. (2024)<br><strong>Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation</strong><br><button class=copy-to-clipboard title="Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T07 (Primary), 49Q22 (Secondary), cs-LG, cs.LG, math-AP, math-FA, math-OC, math-PR<br>Keyword Score: 20<br>Keywords: Diffusion Model, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18839v1.pdf filename=2402.18839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of conditional generation is one of the most important applications of generative models, and numerous methods have been developed to date based on the celebrated <b>diffusion</b> <b>models,</b> with the guidance-based classifier-free method taking the lead. However, the theory of the guidance-based method not only requires the user to <b>fine-tune</b> the &ldquo;guidance strength,&rdquo; but its target vector field does not necessarily correspond to the conditional distribution used in training. In this paper, we develop the theory of conditional generation based on Flow Matching, a current strong contender of <b>diffusion</b> <b>methods.</b> Motivated by the interpretation of a probability path as a distribution on path space, we establish a novel theory of flow-based generation of conditional distribution by employing the mathematical framework of generalized continuity equation instead of the continuity equation in flow matching. This theory naturally derives a method that aims to match the matrix field as opposed to the vector field. Our framework ensures the continuity of the generated conditional distribution through the existence of flow between conditional distributions. We will present our theory through experiments and mathematical results.</p></p class="citation"></blockquote><h3 id=2852--134254-blockecho-retaining-long-range-dependencies-for-imputing-block-wise-missing-data-qiao-han-et-al-2024>(28/52 | 134/254) BlockEcho: Retaining Long-Range Dependencies for Imputing Block-Wise Missing Data (Qiao Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiao Han, Mingqian Li, Yao Yang, Yiteng Zhai. (2024)<br><strong>BlockEcho: Retaining Long-Range Dependencies for Imputing Block-Wise Missing Data</strong><br><button class=copy-to-clipboard title="BlockEcho: Retaining Long-Range Dependencies for Imputing Block-Wise Missing Data" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18800v1.pdf filename=2402.18800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Block-wise missing data poses significant challenges in real-world data imputation tasks. Compared to scattered missing data, block-wise gaps exacerbate adverse effects on subsequent analytic and machine learning tasks, as the lack of local neighboring elements significantly reduces the interpolation capability and predictive power. However, this issue has not received adequate attention. Most SOTA matrix completion methods appeared less effective, primarily due to overreliance on neighboring elements for predictions. We systematically analyze the issue and propose a novel matrix completion method ``BlockEcho" for a more comprehensive solution. This method creatively integrates Matrix Factorization (MF) within <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GAN)</b> to explicitly retain long-distance inter-element relationships in the original matrix. Besides, we incorporate an additional discriminator for <b>GAN,</b> comparing the generator&rsquo;s intermediate progress with pre-trained MF results to constrain high-order feature distributions. Subsequently, we evaluate BlockEcho on public datasets across three domains. Results demonstrate superior performance over both traditional and SOTA methods when imputing block-wise missing data, especially at higher missing rates. The advantage also holds for scattered missing data at high missing rates. We also contribute on the analyses in providing theoretical justification on the optimality and convergence of fusing MF and <b>GAN</b> for missing block data.</p></p class="citation"></blockquote><h3 id=2952--135254-benchmarking-uncertainty-disentanglement-specialized-uncertainties-for-specialized-tasks-bálint-mucsányi-et-al-2024>(29/52 | 135/254) Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks (Bálint Mucsányi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bálint Mucsányi, Michael Kirchhof, Seong Joon Oh. (2024)<br><strong>Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks</strong><br><button class=copy-to-clipboard title="Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19460v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19460v1.pdf filename=2402.19460v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uncertainty quantification, once a singular task, has evolved into a spectrum of tasks, including abstained prediction, <b>out-of-distribution</b> detection, and aleatoric uncertainty quantification. The latest goal is disentanglement: the construction of multiple estimators that are each tailored to one and only one task. Hence, there is a plethora of recent advances with different intentions - that often entirely deviate from practical behavior. This paper conducts a comprehensive evaluation of numerous uncertainty estimators across diverse tasks on ImageNet. We find that, despite promising theoretical endeavors, disentanglement is not yet achieved in practice. Additionally, we reveal which uncertainty estimators excel at which specific tasks, providing insights for practitioners and guiding future research toward task-centric and disentangled uncertainty estimation methods. Our code is available at <a href=https://github.com/bmucsanyi/bud>https://github.com/bmucsanyi/bud</a>.</p></p class="citation"></blockquote><h3 id=3052--136254-anomaly-detection-in-offshore-wind-turbine-structures-using-hierarchical-bayesian-modelling-s-m-smith-et-al-2024>(30/52 | 136/254) Anomaly Detection in Offshore Wind Turbine Structures using Hierarchical Bayesian Modelling (S. M. Smith et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S. M. Smith, A. J. Hughes, T. A. Dardeno, L. A. Bull, N. Dervilis, K. Worden. (2024)<br><strong>Anomaly Detection in Offshore Wind Turbine Structures using Hierarchical Bayesian Modelling</strong><br><button class=copy-to-clipboard title="Anomaly Detection in Offshore Wind Turbine Structures using Hierarchical Bayesian Modelling" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Anomaly Detection, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19295v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19295v1.pdf filename=2402.19295v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Population-based structural health monitoring (PBSHM), aims to share information between members of a population. An offshore wind (OW) farm could be considered as a population of nominally-identical wind-turbine structures. However, benign variations exist among members, such as <b>geometry,</b> sea-bed conditions and temperature differences. These factors could influence structural properties and therefore the dynamic response, making it more difficult to detect structural problems via traditional SHM techniques. This paper explores the use of a hierarchical Bayesian model to infer expected soil stiffness distributions at both population and local levels, as a basis to perform <b>anomaly</b> <b>detection,</b> in the form of scour, for new and existing turbines. To do this, observations of natural frequency will be generated as though they are from a small population of wind turbines. Differences between individual observations will be introduced by postulating distributions over the soil stiffness and measurement noise, as well as reducing soil depth (to represent scour), in the case of <b>anomaly</b> <b>detection.</b></p></p class="citation"></blockquote><h3 id=3152--137254-supervised-contrastive-representation-learning-landscape-analysis-with-unconstrained-features-tina-behnia-et-al-2024>(31/52 | 137/254) Supervised Contrastive Representation Learning: Landscape Analysis with Unconstrained Features (Tina Behnia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tina Behnia, Christos Thrampoulidis. (2024)<br><strong>Supervised Contrastive Representation Learning: Landscape Analysis with Unconstrained Features</strong><br><button class=copy-to-clipboard title="Supervised Contrastive Representation Learning: Landscape Analysis with Unconstrained Features" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 15<br>Keywords: Representation Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18884v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18884v1.pdf filename=2402.18884v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent findings reveal that over-parameterized deep neural networks, trained beyond zero training-error, exhibit a distinctive structural pattern at the final layer, termed as Neural-collapse (NC). These results indicate that the final hidden-layer outputs in such networks display minimal within-class variations over the training set. While existing research extensively investigates this phenomenon under cross-entropy loss, there are fewer studies focusing on its contrastive counterpart, <b>supervised</b> contrastive (SC) loss. Through the lens of NC, this paper employs an analytical approach to study the solutions derived from optimizing the SC loss. We adopt the unconstrained features model (UFM) as a representative proxy for unveiling NC-related phenomena in sufficiently over-parameterized deep networks. We show that, despite the non-convexity of SC loss minimization, all local minima are global minima. Furthermore, the minimizer is unique (up to a rotation). We prove our results by formalizing a tight convex relaxation of the UFM. Finally, through this convex formulation, we delve deeper into characterizing the properties of global solutions under label-imbalanced training data.</p></p class="citation"></blockquote><h3 id=3252--138254-flatnas-optimizing-flatness-in-neural-architecture-search-for-out-of-distribution-robustness-matteo-gambella-et-al-2024>(32/52 | 138/254) FlatNAS: optimizing Flatness in Neural Architecture Search for Out-of-Distribution Robustness (Matteo Gambella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Gambella, Fabrizio Pittorino, Manuel Roveri. (2024)<br><strong>FlatNAS: optimizing Flatness in Neural Architecture Search for Out-of-Distribution Robustness</strong><br><button class=copy-to-clipboard title="FlatNAS: optimizing Flatness in Neural Architecture Search for Out-of-Distribution Robustness" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19102v1.pdf filename=2402.19102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Architecture Search (NAS) paves the way for the automatic definition of Neural Network (NN) architectures, attracting increasing research attention and offering solutions in various scenarios. This study introduces a novel NAS solution, called Flat Neural Architecture Search (FlatNAS), which explores the interplay between a novel figure of merit based on robustness to weight perturbations and single NN optimization with Sharpness-Aware Minimization (SAM). FlatNAS is the first work in the literature to systematically explore flat regions in the loss landscape of NNs in a NAS procedure, while jointly optimizing their performance on in-distribution data, their <b>out-of-distribution</b> (OOD) robustness, and constraining the number of parameters in their architecture. Differently from current studies primarily concentrating on OOD algorithms, FlatNAS successfully evaluates the impact of NN architectures on OOD robustness, a crucial aspect in real-world applications of machine and deep learning. FlatNAS achieves a good trade-off between performance, OOD generalization, and the number of parameters, by using only in-distribution data in the NAS exploration. The OOD robustness of the NAS-designed models is evaluated by focusing on robustness to input data corruptions, using popular <b>benchmark</b> datasets in the literature.</p></p class="citation"></blockquote><h3 id=3352--139254-on-the-convergence-of-differentially-private-fine-tuning-to-linearly-probe-or-to-fully-fine-tune-shuqi-ke-et-al-2024>(33/52 | 139/254) On the Convergence of Differentially-Private Fine-tuning: To Linearly Probe or to Fully Fine-tune? (Shuqi Ke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuqi Ke, Charlie Hou, Giulia Fanti, Sewoong Oh. (2024)<br><strong>On the Convergence of Differentially-Private Fine-tuning: To Linearly Probe or to Fully Fine-tune?</strong><br><button class=copy-to-clipboard title="On the Convergence of Differentially-Private Fine-tuning: To Linearly Probe or to Fully Fine-tune?" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG, math-OC<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18905v1.pdf filename=2402.18905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Differentially private (DP) machine learning pipelines typically involve a two-phase process: non-private pre-training on a public dataset, followed by <b>fine-tuning</b> on private data using DP optimization techniques. In the DP setting, it has been observed that full <b>fine-tuning</b> may not always yield the best test accuracy, even for in-distribution data. This paper (1) analyzes the training dynamics of DP linear probing (LP) and full <b>fine-tuning</b> (FT), and (2) explores the phenomenon of sequential <b>fine-tuning,</b> starting with linear probing and transitioning to full <b>fine-tuning</b> (LP-FT), and its impact on test loss. We provide theoretical insights into the convergence of DP <b>fine-tuning</b> within an overparameterized neural network and establish a utility curve that determines the allocation of privacy budget between linear probing and full <b>fine-tuning.</b> The theoretical results are supported by empirical evaluations on various <b>benchmarks</b> and models. The findings reveal the complex nature of DP <b>fine-tuning</b> methods. These results contribute to a deeper understanding of DP machine learning and highlight the importance of considering the allocation of privacy budget in the <b>fine-tuning</b> process.</p></p class="citation"></blockquote><h3 id=3452--140254-a-model-based-approach-for-improving-reinforcement-learning-efficiency-leveraging-expert-observations-erhan-can-ozcan-et-al-2024>(34/52 | 140/254) A Model-Based Approach for Improving Reinforcement Learning Efficiency Leveraging Expert Observations (Erhan Can Ozcan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erhan Can Ozcan, Vittorio Giammarino, James Queeney, Ioannis Ch. Paschalidis. (2024)<br><strong>A Model-Based Approach for Improving Reinforcement Learning Efficiency Leveraging Expert Observations</strong><br><button class=copy-to-clipboard title="A Model-Based Approach for Improving Reinforcement Learning Efficiency Leveraging Expert Observations" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18836v1.pdf filename=2402.18836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates how to incorporate expert observations (without explicit information on expert actions) into a deep <b>reinforcement</b> <b>learning</b> setting to improve sample efficiency. First, we formulate an augmented policy loss combining a maximum entropy <b>reinforcement</b> <b>learning</b> objective with a behavioral cloning loss that leverages a forward dynamics model. Then, we propose an algorithm that automatically adjusts the weights of each component in the augmented loss function. Experiments on a variety of continuous control tasks demonstrate that the proposed algorithm outperforms various <b>benchmarks</b> by effectively utilizing available expert observations.</p></p class="citation"></blockquote><h3 id=3552--141254-a-scalable-and-transferable-time-series-prediction-framework-for-demand-forecasting-young-jin-park-et-al-2024>(35/52 | 141/254) A Scalable and Transferable Time Series Prediction Framework for Demand Forecasting (Young-Jin Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Young-Jin Park, Donghyun Kim, Frédéric Odermatt, Juho Lee, Kyung-Min Kim. (2024)<br><strong>A Scalable and Transferable Time Series Prediction Framework for Demand Forecasting</strong><br><button class=copy-to-clipboard title="A Scalable and Transferable Time Series Prediction Framework for Demand Forecasting" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19402v1.pdf filename=2402.19402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series forecasting is one of the most essential and ubiquitous tasks in many business problems, including demand forecasting and logistics optimization. Traditional time series forecasting methods, however, have resulted in small models with limited expressive power because they have difficulty in scaling their model size up while maintaining high accuracy. In this paper, we propose Forecasting orchestra (Forchestra), a simple but powerful framework capable of accurately predicting future demand for a diverse range of items. We empirically demonstrate that the model size is scalable to up to 0.8 billion parameters. The proposed method not only outperforms existing forecasting models with a significant margin, but it could generalize well to unseen data points when evaluated in a <b>zero-shot</b> fashion on downstream datasets. Last but not least, we present extensive qualitative and quantitative studies to analyze how the proposed model outperforms baseline models and differs from conventional approaches. The original paper was presented as a full paper at ICDM 2022 and is available at: <a href=https://ieeexplore.ieee.org/document/10027662>https://ieeexplore.ieee.org/document/10027662</a>.</p></p class="citation"></blockquote><h3 id=3652--142254-structure-preserving-diffusion-models-haoye-lu-et-al-2024>(36/52 | 142/254) Structure Preserving Diffusion Models (Haoye Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoye Lu, Spencer Szabados, Yaoliang Yu. (2024)<br><strong>Structure Preserving Diffusion Models</strong><br><button class=copy-to-clipboard title="Structure Preserving Diffusion Models" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19369v1.pdf filename=2402.19369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have become the leading distribution-learning method in recent years. Herein, we introduce structure-preserving <b>diffusion</b> <b>processes,</b> a family of <b>diffusion</b> <b>processes</b> for learning distributions that possess additional structure, such as group symmetries, by developing theoretical conditions under which the <b>diffusion</b> <b>transition</b> steps preserve said symmetry. While also enabling equivariant data sampling trajectories, we exemplify these results by developing a collection of different symmetry equivariant <b>diffusion</b> <b>models</b> capable of learning distributions that are inherently symmetric. Empirical studies, over both synthetic and real-world datasets, are used to validate the developed models adhere to the proposed theory and are capable of achieving improved performance over existing methods in terms of sample equality. We also show how the proposed models can be used to achieve theoretically guaranteed equivariant image noise reduction without prior knowledge of the image orientation.</p></p class="citation"></blockquote><h3 id=3752--143254-masks-signs-and-learning-rate-rewinding-advait-gadhikar-et-al-2024>(37/52 | 143/254) Masks, Signs, And Learning Rate Rewinding (Advait Gadhikar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Advait Gadhikar, Rebekka Burkholz. (2024)<br><strong>Masks, Signs, And Learning Rate Rewinding</strong><br><button class=copy-to-clipboard title="Masks, Signs, And Learning Rate Rewinding" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19262v1.pdf filename=2402.19262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning Rate Rewinding (LRR) has been established as a strong variant of Iterative Magnitude <b>Pruning</b> (IMP) to find lottery tickets in deep overparameterized neural networks. While both iterative <b>pruning</b> schemes couple structure and parameter learning, understanding how LRR excels in both aspects can bring us closer to the design of more flexible deep learning algorithms that can optimize diverse sets of sparse architectures. To this end, we conduct experiments that disentangle the effect of mask learning and parameter optimization and how both benefit from overparameterization. The ability of LRR to flip parameter signs early and stay robust to sign perturbations seems to make it not only more effective in mask identification but also in optimizing diverse sets of masks, including random ones. In support of this hypothesis, we prove in a simplified single hidden neuron setting that LRR succeeds in more cases than IMP, as it can escape initially problematic sign configurations.</p></p class="citation"></blockquote><h3 id=3852--144254-machine-learning-for-modular-multiplication-kristin-lauter-et-al-2024>(38/52 | 144/254) Machine learning for modular multiplication (Kristin Lauter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kristin Lauter, Cathy Yuanchen Li, Krystal Maughan, Rachel Newton, Megha Srivastava. (2024)<br><strong>Machine learning for modular multiplication</strong><br><button class=copy-to-clipboard title="Machine learning for modular multiplication" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19254v1.pdf filename=2402.19254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by cryptographic applications, we investigate two machine learning approaches to modular multiplication: namely circular regression and a sequence-to-sequence <b>transformer</b> model. The limited success of both methods demonstrated in our results gives evidence for the hardness of tasks involving modular multiplication upon which cryptosystems are based.</p></p class="citation"></blockquote><h3 id=3952--145254-uncertainty-based-extensible-codebook-for-discrete-federated-learning-in-heterogeneous-data-silos-tianyi-zhang-et-al-2024>(39/52 | 145/254) Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos (Tianyi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyi Zhang, Yu Cao, Dianbo Liu. (2024)<br><strong>Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos</strong><br><button class=copy-to-clipboard title="Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18888v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18888v2.pdf filename=2402.18888v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL), aimed at leveraging vast distributed datasets, confronts a crucial challenge: the heterogeneity of data across different silos. While previous studies have explored discrete representations to enhance model generalization across minor distributional shifts, these approaches often struggle to adapt to new data silos with significantly divergent distributions. In response, we have identified that models derived from FL exhibit markedly increased uncertainty when applied to data silos with unfamiliar distributions. Consequently, we propose an innovative yet straightforward iterative framework, termed Uncertainty-Based Extensible-Codebook <b>Federated</b> <b>Learning</b> (UEFL). This framework dynamically maps latent features to trainable discrete vectors, assesses the uncertainty, and specifically extends the discretization dictionary or codebook for silos exhibiting high uncertainty. Our approach aims to simultaneously enhance accuracy and reduce uncertainty by explicitly addressing the diversity of data distributions, all while maintaining minimal computational overhead in environments characterized by heterogeneous data silos. Through experiments conducted on five datasets, our method has demonstrated its superiority, achieving significant improvements in accuracy (by 3%&ndash;22.1%) and uncertainty reduction (by 38.83%&ndash;96.24%), thereby outperforming contemporary state-of-the-art methods. The source code is available at <a href=https://github.com/destiny301/uefl>https://github.com/destiny301/uefl</a>.</p></p class="citation"></blockquote><h3 id=4052--146254-dr-strategy-model-based-generalist-agents-with-strategic-dreaming-hany-hamed-et-al-2024>(40/52 | 146/254) Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming (Hany Hamed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hany Hamed, Subin Kim, Dongyeong Kim, Jaesik Yoon, Sungjin Ahn. (2024)<br><strong>Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming</strong><br><button class=copy-to-clipboard title="Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18866v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18866v1.pdf filename=2402.18866v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model-based <b>reinforcement</b> <b>learning</b> (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent. However, there has not been much effort toward enhancing the strategy of dreaming itself. Therefore, it is a question whether and how an agent can &ldquo;dream better&rdquo; in a more structured and strategic way. In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called Dr. Strategy, which is equipped with a novel Dreaming Strategy. The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming. This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy. With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achievement task in a more focused way. In experiments, we show that the proposed model outperforms prior pixel-based MBRL methods in various visually complex and partially observable navigation tasks. The source code will be available at <a href=https://github.com/ahn-ml/drstrategy>https://github.com/ahn-ml/drstrategy</a></p></p class="citation"></blockquote><h3 id=4152--147254-rethinking-multi-domain-generalization-with-a-general-learning-objective-zhaorui-tan-et-al-2024>(41/52 | 147/254) Rethinking Multi-domain Generalization with A General Learning Objective (Zhaorui Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaorui Tan, Xi Yang, Kaizhu Huang. (2024)<br><strong>Rethinking Multi-domain Generalization with A General Learning Objective</strong><br><button class=copy-to-clipboard title="Rethinking Multi-domain Generalization with A General Learning Objective" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18853v1.pdf filename=2402.18853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a $Y$-mapping to relax the constraint. We rethink the learning objective for mDG and design a new \textbf{general learning objective} to interpret and analyze most existing mDG wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of domain-independent conditional features, disclosing that many previous mDG endeavors actually \textbf{optimize partially the objective} and thus lead to limited performance. As such, our study <b>distills</b> a general learning objective into four practical components, providing a general, robust, and flexible mechanism to handle complex domain shifts. Extensive empirical results indicate that the proposed objective with $Y$-mapping leads to substantially better mDG performance in various downstream tasks, including regression, segmentation, and classification.</p></p class="citation"></blockquote><h3 id=4252--148254-applications-of-0-1-neural-networks-in-prescription-and-prediction-vrishabh-patil-et-al-2024>(42/52 | 148/254) Applications of 0-1 Neural Networks in Prescription and Prediction (Vrishabh Patil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vrishabh Patil, Kara Hoppe, Yonatan Mintz. (2024)<br><strong>Applications of 0-1 Neural Networks in Prescription and Prediction</strong><br><button class=copy-to-clipboard title="Applications of 0-1 Neural Networks in Prescription and Prediction" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18851v1.pdf filename=2402.18851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A key challenge in medical decision making is learning treatment policies for patients with limited observational data. This challenge is particularly evident in personalized healthcare decision-making, where models need to take into account the intricate relationships between patient characteristics, treatment options, and health outcomes. To address this, we introduce prescriptive networks (PNNs), shallow 0-1 neural networks trained with mixed integer programming that can be used with <b>counterfactual</b> estimation to optimize policies in medium data settings. These models offer greater interpretability than deep neural networks and can encode more complex policies than common models such as decision trees. We show that PNNs can outperform existing methods in both synthetic data experiments and in a case study of assigning treatments for postpartum hypertension. In particular, PNNs are shown to produce policies that could reduce peak blood pressure by 5.47 mm Hg (p=0.02) over existing clinical practice, and by 2 mm Hg (p=0.01) over the next best prescriptive modeling technique. Moreover PNNs were more likely than all other models to correctly identify clinically significant features while existing models relied on potentially dangerous features such as patient insurance information and race that could lead to bias in treatment.</p></p class="citation"></blockquote><h3 id=4352--149254-multi-fidelity-residual-neural-processes-for-scalable-surrogate-modeling-ruijia-niu-et-al-2024>(43/52 | 149/254) Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling (Ruijia Niu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruijia Niu, Dongxia Wu, Kai Kim, Yi-An Ma, Duncan Watson-Parris, Rose Yu. (2024)<br><strong>Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling</strong><br><button class=copy-to-clipboard title="Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18846v1.pdf filename=2402.18846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-fidelity surrogate modeling aims to learn an accurate surrogate at the highest fidelity level by combining data from multiple sources. Traditional methods relying on Gaussian processes can hardly scale to high-dimensional data. Deep learning approaches utilize neural network based encoders and decoders to improve scalability. These approaches share encoded representations across fidelities without including corresponding decoder parameters. At the highest fidelity, the representations are decoded with different parameters, making the shared information inherently inaccurate. This hinders inference performance, especially in <b>out-of-distribution</b> scenarios when the highest fidelity data has limited domain coverage. To address these limitations, we propose Multi-fidelity Residual Neural Processes (MFRNP), a novel multi-fidelity surrogate modeling framework. MFRNP optimizes lower fidelity decoders for accurate information sharing by aggregating lower fidelity surrogate outputs and models residual between the aggregation and ground truth on the highest fidelity. We show that MFRNP significantly outperforms current state-of-the-art in learning partial differential equations and a real-world climate modeling task.</p></p class="citation"></blockquote><h3 id=4452--150254-enhancing-the-immunity-of-mixture-of-experts-networks-for-adversarial-defense-qiao-han-et-al-2024>(44/52 | 150/254) Enhancing the &lsquo;Immunity&rsquo; of Mixture-of-Experts Networks for Adversarial Defense (Qiao Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiao Han, yong huang, xinling Guo, Yiteng Zhai, Yu Qin, Yao Yang. (2024)<br><strong>Enhancing the &lsquo;Immunity&rsquo; of Mixture-of-Experts Networks for Adversarial Defense</strong><br><button class=copy-to-clipboard title="Enhancing the 'Immunity' of Mixture-of-Experts Networks for Adversarial Defense" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18787v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18787v1.pdf filename=2402.18787v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have revealed the vulnerability of Deep Neural Networks (DNNs) to adversarial examples, which can easily fool DNNs into making incorrect predictions. To mitigate this deficiency, we propose a novel adversarial defense method called &ldquo;Immunity&rdquo; (Innovative MoE with <b>MUtual</b> <b>information</b> & positioN stabilITY) based on a modified Mixture-of-Experts (MoE) architecture in this work. The key enhancements to the standard MoE are two-fold: 1) integrating of Random Switch Gates (RSGs) to obtain diverse network structures via random permutation of RSG parameters at evaluation time, despite of RSGs being determined after one-time training; 2) devising innovative <b>Mutual</b> <b>Information</b> (MI)-based and Position Stability-based loss functions by capitalizing on Grad-CAM&rsquo;s explanatory power to increase the diversity and the causality of expert networks. Notably, our MI-based loss operates directly on the heatmaps, thereby inducing subtler negative impacts on the classification performance when compared to other losses of the same type, theoretically. Extensive evaluation validates the efficacy of the proposed approach in improving adversarial robustness against a wide range of attacks.</p></p class="citation"></blockquote><h3 id=4552--151254-disentangling-the-causes-of-plasticity-loss-in-neural-networks-clare-lyle-et-al-2024>(45/52 | 151/254) Disentangling the Causes of Plasticity Loss in Neural Networks (Clare Lyle et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Clare Lyle, Zeyu Zheng, Khimya Khetarpal, Hado van Hasselt, Razvan Pascanu, James Martens, Will Dabney. (2024)<br><strong>Disentangling the Causes of Plasticity Loss in Neural Networks</strong><br><button class=copy-to-clipboard title="Disentangling the Causes of Plasticity Loss in Neural Networks" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18762v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18762v1.pdf filename=2402.18762v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly innocuous assumption: that the network is trained on a \textit{stationary} data distribution. In settings where this assumption is violated, e.g.\ deep <b>reinforcement</b> <b>learning,</b> learning algorithms become unstable and brittle with respect to hyperparameters and even random seeds. One factor driving this instability is the loss of plasticity, meaning that updating the network&rsquo;s predictions in response to new information becomes more difficult as training progresses. While many recent works provide analyses and partial solutions to this phenomenon, a fundamental question remains unanswered: to what extent do known mechanisms of plasticity loss overlap, and how can mitigation strategies be combined to best maintain the trainability of a network? This paper addresses these questions, showing that loss of plasticity can be decomposed into multiple independent mechanisms and that, while intervening on any single mechanism is insufficient to avoid the loss of plasticity in all cases, intervening on multiple mechanisms in conjunction results in highly robust learning algorithms. We show that a combination of layer normalization and weight decay is highly effective at maintaining plasticity in a variety of synthetic nonstationary learning tasks, and further demonstrate its effectiveness on naturally arising nonstationarities, including <b>reinforcement</b> <b>learning</b> in the Arcade Learning Environment.</p></p class="citation"></blockquote><h3 id=4652--152254-lifelong-benchmarks-efficient-model-evaluation-in-an-era-of-rapid-progress-ameya-prabhu-et-al-2024>(46/52 | 152/254) Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress (Ameya Prabhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ameya Prabhu, Vishaal Udandarao, Philip Torr, Matthias Bethge, Adel Bibi, Samuel Albanie. (2024)<br><strong>Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress</strong><br><button class=copy-to-clipboard title="Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19472v1.pdf filename=2402.19472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Standardized <b>benchmarks</b> drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit <b>benchmark</b> idiosyncrasies. In our work, we seek to mitigate this challenge by compiling ever-expanding large-scale <b>benchmarks</b> called Lifelong <b>Benchmarks.</b> As exemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet, containing (for now) 1.69M and 1.98M test samples, respectively. While reducing overfitting, lifelong <b>benchmarks</b> introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set. To address this challenge, we also introduce an efficient evaluation framework: Sort & Search (S&amp;S), which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong <b>benchmarking.</b> Extensive empirical evaluations across 31,000 models demonstrate that S&amp;S achieves highly-efficient approximate accuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours (1000x reduction) on a single A100 GPU, with low approximation error. As such, lifelong <b>benchmarks</b> offer a robust, practical solution to the <b>&ldquo;benchmark</b> exhaustion&rdquo; problem.</p></p class="citation"></blockquote><h3 id=4752--153254-probabilistic-lipschitzness-and-the-stable-rank-for-comparing-explanation-models-lachlan-simpson-et-al-2024>(47/52 | 153/254) Probabilistic Lipschitzness and the Stable Rank for Comparing Explanation Models (Lachlan Simpson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lachlan Simpson, Kyle Millar, Adriel Cheng, Cheng-Chew Lim, Hong Gunn Chew. (2024)<br><strong>Probabilistic Lipschitzness and the Stable Rank for Comparing Explanation Models</strong><br><button class=copy-to-clipboard title="Probabilistic Lipschitzness and the Stable Rank for Comparing Explanation Models" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18863v1.pdf filename=2402.18863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Explainability models are now prevalent within machine learning to address the <b>black-box</b> <b>nature</b> of neural networks. The question now is which explainability model is most effective. Probabilistic Lipschitzness has demonstrated that the smoothness of a neural network is fundamentally linked to the quality of post hoc explanations. In this work, we prove theoretical lower bounds on the probabilistic Lipschitzness of Integrated Gradients, LIME and SmoothGrad. We propose a novel metric using probabilistic Lipschitzness, normalised astuteness, to compare the robustness of explainability models. Further, we prove a link between the local Lipschitz constant of a neural network and its stable rank. We then demonstrate that the stable rank of a neural network provides a heuristic for the robustness of explainability models.</p></p class="citation"></blockquote><h3 id=4852--154254-learnability-gaps-of-strategic-classification-lee-cohen-et-al-2024>(48/52 | 154/254) Learnability Gaps of Strategic Classification (Lee Cohen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lee Cohen, Yishay Mansour, Shay Moran, Han Shao. (2024)<br><strong>Learnability Gaps of Strategic Classification</strong><br><button class=copy-to-clipboard title="Learnability Gaps of Strategic Classification" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-GT, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19303v1.pdf filename=2402.19303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In contrast with standard classification tasks, strategic classification involves agents strategically modifying their features in an effort to receive favorable predictions. For instance, given a classifier determining loan approval based on credit scores, applicants may open or close their credit cards to fool the classifier. The learning goal is to find a classifier robust against strategic manipulations. Various settings, based on what and when information is known, have been explored in strategic classification. In this work, we focus on addressing a fundamental question: the learnability gaps between strategic classification and standard learning. We essentially show that any learnable class is also strategically learnable: we first consider a fully informative setting, where the manipulation structure (which is modeled by a manipulation <b>graph</b> $G^\star$) is known and during training time the learner has access to both the pre-manipulation data and post-manipulation data. We provide nearly tight sample complexity and regret bounds, offering significant improvements over prior results. Then, we relax the fully informative setting by introducing two natural types of uncertainty. First, following Ahmadi et al. (2023), we consider the setting in which the learner only has access to the post-manipulation data. We improve the results of Ahmadi et al. (2023) and close the gap between mistake upper bound and lower bound raised by them. Our second relaxation of the fully informative setting introduces uncertainty to the manipulation structure. That is, we assume that the manipulation <b>graph</b> is unknown but belongs to a known class of <b>graphs.</b> We provide nearly tight bounds on the learning complexity in various unknown manipulation <b>graph</b> settings. Notably, our algorithm in this setting is of independent interest and can be applied to other problems such as multi-label learning.</p></p class="citation"></blockquote><h3 id=4952--155254-degradation-modeling-and-prognostic-analysis-under-unknown-failure-modes-ying-fu-et-al-2024>(49/52 | 155/254) Degradation Modeling and Prognostic Analysis Under Unknown Failure Modes (Ying Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Fu, Ye Kwon Huh, Kaibo Liu. (2024)<br><strong>Degradation Modeling and Prognostic Analysis Under Unknown Failure Modes</strong><br><button class=copy-to-clipboard title="Degradation Modeling and Prognostic Analysis Under Unknown Failure Modes" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19294v1.pdf filename=2402.19294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Operating units often experience various failure modes in complex systems, leading to distinct degradation paths. Relying on a prognostic model trained on a single failure mode may lead to poor generalization performance across multiple failure modes. Therefore, accurately identifying the failure mode is of critical importance. Current prognostic approaches either ignore failure modes during degradation or assume known failure mode labels, which can be challenging to acquire in practice. Moreover, the high dimensionality and complex relations of sensor signals make it challenging to identify the failure modes accurately. To address these issues, we propose a novel failure mode diagnosis method that leverages a dimension reduction technique called UMAP (Uniform Manifold Approximation and Projection) to project and visualize each unit&rsquo;s degradation trajectory into a lower dimension. Then, using these degradation trajectories, we develop a time series-based <b>clustering</b> method to identify the training units&rsquo; failure modes. Finally, we introduce a monotonically constrained prognostic model to predict the failure mode labels and RUL of the test units simultaneously using the obtained failure modes of the training units. The proposed prognostic model provides failure mode-specific RUL predictions while preserving the monotonic property of the RUL predictions across consecutive time steps. We evaluate the proposed model using a case study with the aircraft gas turbine engine dataset.</p></p class="citation"></blockquote><h3 id=5052--156254-negative-binomial-randomized-gamma-markov-processes-for-heterogeneous-overdispersed-count-time-series-rui-huang-et-al-2024>(50/52 | 156/254) Negative-Binomial Randomized Gamma Markov Processes for Heterogeneous Overdispersed Count Time Series (Rui Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Huang, Sikun Yang, Heinz Koeppl. (2024)<br><strong>Negative-Binomial Randomized Gamma Markov Processes for Heterogeneous Overdispersed Count Time Series</strong><br><button class=copy-to-clipboard title="Negative-Binomial Randomized Gamma Markov Processes for Heterogeneous Overdispersed Count Time Series" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18995v1.pdf filename=2402.18995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modeling count-valued time series has been receiving increasing attention since count time series naturally arise in physical and social domains. Poisson gamma dynamical systems (PGDSs) are newly-developed methods, which can well capture the expressive latent transition structure and bursty dynamics behind count sequences. In particular, PGDSs demonstrate superior performance in terms of data imputation and prediction, compared with canonical linear dynamical system (LDS) based methods. Despite these advantages, PGDS cannot capture the heterogeneous overdispersed behaviours of the underlying dynamic processes. To mitigate this defect, we propose a negative-binomial-randomized gamma Markov process, which not only significantly improves the predictive performance of the proposed dynamical system, but also facilitates the fast convergence of the inference algorithm. Moreover, we develop methods to estimate both factor-structured and <b>graph-structured</b> transition dynamics, which enable us to infer more explainable latent structure, compared with PGDSs. Finally, we demonstrate the explainable latent structure learned by the proposed method, and show its superior performance in imputing missing data and forecasting future observations, compared with the related models.</p></p class="citation"></blockquote><h3 id=5152--157254-graph-generation-via-spectral-diffusion-giorgia-minello-et-al-2024>(51/52 | 157/254) Graph Generation via Spectral Diffusion (Giorgia Minello et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giorgia Minello, Alessandro Bicciato, Luca Rossi, Andrea Torsello, Luca Cosmo. (2024)<br><strong>Graph Generation via Spectral Diffusion</strong><br><button class=copy-to-clipboard title="Graph Generation via Spectral Diffusion" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18974v1.pdf filename=2402.18974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present GRASP, a novel <b>graph</b> generative model based on 1) the spectral decomposition of the <b>graph</b> Laplacian matrix and 2) a diffusion process. Specifically, we propose to use a denoising model to sample eigenvectors and eigenvalues from which we can reconstruct the <b>graph</b> Laplacian and adjacency matrix. Our permutation invariant model can also handle node features by concatenating them to the eigenvectors of each node. Using the Laplacian spectrum allows us to naturally capture the structural characteristics of the <b>graph</b> and work directly in the node space while avoiding the quadratic complexity bottleneck that limits the applicability of other methods. This is achieved by truncating the spectrum, which as we show in our experiments results in a faster yet accurate generative process. An extensive set of experiments on both synthetic and real world <b>graphs</b> demonstrates the strengths of our model against state-of-the-art alternatives.</p></p class="citation"></blockquote><h3 id=5252--158254-taking-second-life-batteries-from-exhausted-to-empowered-using-experiments-data-analysis-and-health-estimation-xiaofan-cui-et-al-2024>(52/52 | 158/254) Taking Second-life Batteries from Exhausted to Empowered using Experiments, Data Analysis, and Health Estimation (Xiaofan Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaofan Cui, Muhammad Aadil Khan, Gabriele Pozzato, Surinder Singh, Ratnesh Sharma, Simona Onori. (2024)<br><strong>Taking Second-life Batteries from Exhausted to Empowered using Experiments, Data Analysis, and Health Estimation</strong><br><button class=copy-to-clipboard title="Taking Second-life Batteries from Exhausted to Empowered using Experiments, Data Analysis, and Health Estimation" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18859v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18859v1.pdf filename=2402.18859v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The reuse of retired electric vehicle (EV) batteries in electric grid energy storage emerges as a promising strategy to address environmental concerns and boost economic value. This study concentrates on devising health monitoring algorithms for retired batteries (BMS$_2$) deployed in grid storage applications. Over 15 months of testing, we compile, analyze, and publicly share a dataset of second-life (SL) batteries, implementing a cycling protocol simulating grid energy storage load profiles within a 3 V-4 V voltage window. Four machine learning-based health estimation models, relying on BMS$_2$ features and initial capacity, are developed and compared, with the selected model achieving a Mean Absolute Percentage Error (MAPE) below 2.3% on test data. Additionally, an adaptive online health estimation algorithm is proposed by integrating a <b>clustering-based</b> method, limiting estimation errors during online deployment. These results constitute an initial proof of concept, showcasing the feasibility of repurposing retired batteries for second-life applications. Based on obtained data and representative power demand, these SL batteries exhibit the potential, under specific conditions, for over a decade of grid energy storage use.</p></p class="citation"></blockquote><h2 id=csse-5>cs.SE (5)</h2><h3 id=15--159254-compositional-api-recommendation-for-library-oriented-code-generation-zexiong-ma-et-al-2024>(1/5 | 159/254) Compositional API Recommendation for Library-Oriented Code Generation (Zexiong Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zexiong Ma, Shengnan An, Bing Xie, Zeqi Lin. (2024)<br><strong>Compositional API Recommendation for Library-Oriented Code Generation</strong><br><button class=copy-to-clipboard title="Compositional API Recommendation for Library-Oriented Code Generation" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-SE, cs.SE<br>Keyword Score: 53<br>Keywords: Benchmarking, Recommendation, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19431v1.pdf filename=2402.19431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved exceptional performance in <b>code</b> <b>generation.</b> However, the performance remains unsatisfactory in generating library-oriented <b>code,</b> <b>especially</b> for the libraries not present in the training data of <b>LLMs.</b> Previous work utilizes API <b>recommendation</b> technology to help <b>LLMs</b> use libraries: it retrieves APIs related to the user requirements, then leverages them as context to <b>prompt</b> <b>LLMs.</b> However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API <b>recommendation</b> a challenging task. To address this, we propose CAPIR (Compositional API <b>Recommendation),</b> which adopts a &ldquo;divide-and-conquer&rdquo; strategy to recommend APIs for coarse-grained requirements. Specifically, CAPIR employs an <b>LLM-based</b> Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based Retriever to identify relevant APIs corresponding to each subtask. Moreover, CAPIR leverages an <b>LLM-based</b> Reranker to filter out redundant APIs and provides the final <b>recommendation.</b> To facilitate the evaluation of API <b>recommendation</b> methods on coarse-grained requirements, we present two challenging <b>benchmarks,</b> RAPID (Recommend APIs based on Documentation) and LOCG (Library-Oriented <b>Code</b> <b>Generation).</b> Experimental results on these <b>benchmarks,</b> demonstrate the effectiveness of CAPIR in comparison to existing baselines. Specifically, on RAPID&rsquo;s Torchdata-AR dataset, compared to the state-of-the-art API <b>recommendation</b> approach, CAPIR improves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. On LOCG&rsquo;s Torchdata-Code dataset, compared to <b>code</b> <b>generation</b> without API <b>recommendation,</b> CAPIR improves pass@100 from 16.0% to 28.0%.</p></p class="citation"></blockquote><h3 id=25--160254-starcoder-2-and-the-stack-v2-the-next-generation-anton-lozhkov-et-al-2024>(2/5 | 160/254) StarCoder 2 and The Stack v2: The Next Generation (Anton Lozhkov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries. (2024)<br><strong>StarCoder 2 and The Stack v2: The Next Generation</strong><br><button class=copy-to-clipboard title="StarCoder 2 and The Stack v2: The Next Generation" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 53<br>Keywords: Benchmarking, High-Resource, Low-Resource, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19173v1.pdf filename=2402.19173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The BigCode project, an open-scientific collaboration focused on the responsible development of <b>Large</b> <b>Language</b> <b>Models</b> for Code (Code <b>LLMs),</b> introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code <b>LLM</b> <b>benchmarks.</b> We find that our small model, StarCoder2-3B, outperforms other Code <b>LLMs</b> of similar size on most <b>benchmarks,</b> and also outperforms StarCoderBase-15B. Our <b>large</b> <b>model,</b> <b>StarCoder2-</b> 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for <b>high-resource</b> languages, we find that StarCoder2-15B outperforms it on math and code <b>reasoning</b> <b>benchmarks,</b> as well as several <b>low-resource</b> languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.</p></p class="citation"></blockquote><h3 id=35--161254-the-counterfeit-conundrum-can-code-language-models-grasp-the-nuances-of-their-incorrect-generations-alex-gu-et-al-2024>(3/5 | 161/254) The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations? (Alex Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Gu, Wen-Ding Li, Naman Jain, Theo X. Olausson, Celine Lee, Koushik Sen, Armando Solar-Lezama. (2024)<br><strong>The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?</strong><br><button class=copy-to-clipboard title="The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-LG, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Code Generation, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19475v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19475v1.pdf filename=2402.19475v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While language models are increasingly more proficient at <b>code</b> <b>generation,</b> they still frequently generate incorrect programs. Many of these programs are obviously wrong, but others are more subtle and pass weaker correctness checks such as being able to compile. In this work, we focus on these counterfeit samples: programs sampled from a language model that 1) have a high enough log-probability to be generated at a moderate temperature and 2) pass weak correctness checks. Overall, we discover that most models have a very shallow understanding of counterfeits through three clear failure modes. First, models mistakenly classify them as correct. Second, models are worse at <b>reasoning</b> about the execution behaviour of counterfeits and often predict their execution results as if they were correct. Third, when asking models to fix counterfeits, the likelihood of a model successfully repairing a counterfeit is often even lower than that of sampling a correct program from scratch. Counterfeits also have very unexpected properties: first, counterfeit programs for problems that are easier for a model to solve are not necessarily easier to detect and only slightly easier to execute and repair. Second, counterfeits from a given model are just as confusing to the model itself as they are to other models. Finally, both strong and weak models are able to generate counterfeit samples that equally challenge all models. In light of our findings, we recommend that care and caution be taken when relying on models to understand their own samples, especially when no external feedback is incorporated.</p></p class="citation"></blockquote><h3 id=45--162254-understanding-fairness-in-software-engineering-insights-from-stack-exchange-emeralda-sesari-et-al-2024>(4/5 | 162/254) Understanding Fairness in Software Engineering: Insights from Stack Exchange (Emeralda Sesari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emeralda Sesari, Federica Sarro, Ayushi Rastogi. (2024)<br><strong>Understanding Fairness in Software Engineering: Insights from Stack Exchange</strong><br><button class=copy-to-clipboard title="Understanding Fairness in Software Engineering: Insights from Stack Exchange" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19038v1.pdf filename=2402.19038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Software practitioners discuss problems at work with peers, in-person and online. These discussions can be technical (e.g., how to fix a bug?) and social (e.g., how to assign work fairly?). While there is a growing body of knowledge exploring <b>fairness</b> problems and solutions in the human and social factors of software engineering, most focus has been on specific problems. This study provides <b>fairness</b> discussions by software practitioners on Stack Exchange sites. We present an exploratory study presenting the <b>fairness</b> experience of software practitioners and <b>fairness</b> expectations in software teams. We also want to identify the <b>fairness</b> aspects software practitioners talk about the most. For example, do they care more about <b>fairness</b> in income or how they are treated in the workplace? Our investigation of <b>fairness</b> discussions on eight Stack Exchange sites resulted in a list of 136 posts (28 questions and 108 answers) manually curated from 4,178 candidate posts. The study reveals that the majority of <b>fairness</b> discussions (24 posts) revolve around the topic of income suggesting that many software practitioners are highly interested in matters related to their pay and how it is fairly distributed. Further, we noted that while not discussed as often, discussions on <b>fairness</b> in recruitment tend to receive the highest number of views and scores. Interestingly, the study shows that unfairness experiences extend beyond the protected attributes. In this study, only 25 out of 136 posts mention protected attributes, with gender mainly being discussed.</p></p class="citation"></blockquote><h3 id=55--163254-cebin-a-cost-effective-framework-for-large-scale-binary-code-similarity-detection-hao-wang-et-al-2024>(5/5 | 163/254) CEBin: A Cost-Effective Framework for Large-Scale Binary Code Similarity Detection (Hao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wang, Zeyu Gao, Chao Zhang, Mingyang Sun, Yuchen Zhou, Han Qiu, Xi Xiao. (2024)<br><strong>CEBin: A Cost-Effective Framework for Large-Scale Binary Code Similarity Detection</strong><br><button class=copy-to-clipboard title="CEBin: A Cost-Effective Framework for Large-Scale Binary Code Similarity Detection" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CR, cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18818v1.pdf filename=2402.18818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Binary code similarity detection (BCSD) is a fundamental technique for various application. Many BCSD solutions have been proposed recently, which mostly are embedding-based, but have shown limited accuracy and efficiency especially when the volume of target binaries to search is large. To address this issue, we propose a cost-effective BCSD framework, CEBin, which fuses embedding-based and comparison-based approaches to significantly improve accuracy while minimizing overheads. Specifically, CEBin utilizes a refined embedding-based approach to extract features of target code, which efficiently narrows down the scope of candidate similar code and boosts performance. Then, it utilizes a comparison-based approach that performs a pairwise comparison on the candidates to capture more nuanced and complex relationships, which greatly improves the accuracy of similarity detection. By bridging the gap between embedding-based and comparison-based approaches, CEBin is able to provide an effective and efficient solution for detecting similar code (including vulnerable ones) in large-scale software ecosystems. Experimental results on three well-known datasets demonstrate the superiority of CEBin over existing state-of-the-art (SOTA) baselines. To further evaluate the usefulness of BCSD in real world, we construct a large-scale <b>benchmark</b> of vulnerability, offering the first precise evaluation scheme to assess BCSD methods for the 1-day vulnerability detection task. CEBin could identify the similar function from millions of candidate functions in just a few seconds and achieves an impressive recall rate of $85.46%$ on this more practical but challenging task, which are several order of magnitudes faster and $4.07\times$ better than the best SOTA baseline. Our code is available at <a href=https://github.com/Hustcw/CEBin>https://github.com/Hustcw/CEBin</a>.</p></p class="citation"></blockquote><h2 id=cscy-5>cs.CY (5)</h2><h3 id=15--164254-wisdom-of-the-silicon-crowd-llm-ensemble-prediction-capabilities-match-human-crowd-accuracy-philipp-schoenegger-et-al-2024>(1/5 | 164/254) Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy (Philipp Schoenegger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Schoenegger, Indre Tuminauskaite, Peter S. Park, Philip E. Tetlock. (2024)<br><strong>Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy</strong><br><button class=copy-to-clipboard title="Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.CY<br>Keyword Score: 53<br>Keywords: Benchmarking, Claude, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19379v1.pdf filename=2402.19379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human forecasting accuracy in practice relies on the &lsquo;wisdom of the crowd&rsquo; effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> suggests that frontier <b>LLMs,</b> as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an <b>LLM</b> ensemble approach consisting of a crowd of twelve <b>LLMs.</b> We compare the aggregated <b>LLM</b> predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the <b>LLM</b> crowd outperforms a simple no-information <b>benchmark</b> and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even split of positive and negative resolutions. Moreover, in Study 2, we test whether <b>LLM</b> predictions (of <b>GPT-4</b> and <b>Claude</b> 2) can be improved by drawing on human cognitive output. We find that both models&rsquo; forecasting accuracy benefits from exposure to the median human prediction as information, improving accuracy by between 17% and 28%: though this leads to less accurate predictions than simply averaging human and machine forecasts. Our results suggest that <b>LLMs</b> can achieve forecasting accuracy rivaling that of human crowd forecasting tournaments: via the simple, practically applicable method of forecast aggregation. This replicates the &lsquo;wisdom of the crowd&rsquo; effect for <b>LLMs,</b> and opens up their use for a variety applications throughout society.</p></p class="citation"></blockquote><h3 id=25--165254-mobile-health-text-misinformation-identification-using-mobile-data-mining-wen-chen-hu-et-al-2024>(2/5 | 165/254) Mobile Health Text Misinformation Identification Using Mobile Data Mining (Wen-Chen Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wen-Chen Hu, Sanjaikanth E Vadakkethil Somanathan Pillai, Abdelrahman Ahmed ElSaid. (2024)<br><strong>Mobile Health Text Misinformation Identification Using Mobile Data Mining</strong><br><button class=copy-to-clipboard title="Mobile Health Text Misinformation Identification Using Mobile Data Mining" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Information Retrieval, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19280v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19280v1.pdf filename=2402.19280v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>More than six million people died of the COVID-19 by April 2022. The heavy casualties have put people on great and urgent alert and people try to find all kinds of <b>information</b> <b>to</b> keep them from being inflected by the coronavirus. This research tries to find out whether the mobile health text <b>information</b> <b>sent</b> to peoples devices is correct as smartphones becoming the major <b>information</b> <b>source</b> for people. The proposed method uses various mobile <b>information</b> <b>retrieval</b> and data mining technologies including lexical analysis, stopword elimination, <b>stemming,</b> and decision trees to classify the mobile health text <b>information</b> <b>to</b> one of the following classes: (i) true, (ii) fake, (iii) misinformative, (iv) disinformative, and (v) neutral. Experiment results show the accuracy of the proposed method is above the threshold value 50 percentage, but is not optimal. It is because the problem, mobile text misinformation identification, is intrinsically difficult.</p></p class="citation"></blockquote><h3 id=35--166254-shared-lightweight-autonomous-vehicles-for-urban-food-deliveries-a-simulation-study-ainhoa-genua-cerviño-et-al-2024>(3/5 | 166/254) Shared lightweight autonomous vehicles for urban food deliveries: A simulation study (Ainhoa Genua Cerviño et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ainhoa Genua Cerviño, Naroa Coretti Sanchez, Elaine Liu Wang, Arnaud Grignard, Kent Larson. (2024)<br><strong>Shared lightweight autonomous vehicles for urban food deliveries: A simulation study</strong><br><button class=copy-to-clipboard title="Shared lightweight autonomous vehicles for urban food deliveries: A simulation study" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19233v1.pdf filename=2402.19233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the rapid growth of on-demand deliveries, especially in food deliveries, has spurred the exploration of innovative mobility solutions. In this context, lightweight autonomous vehicles have emerged as a potential alternative. However, their fleet-level behavior remains largely unexplored. To address this gap, we have developed an agent-based model and an environmental impact study assessing the fleet performance of lightweight autonomous food delivery vehicles. This model explores critical factors such as fleet sizing, service level, operational strategies, and environmental impacts. We have applied this model to a case study in Cambridge, MA, USA, where results indicate that there could be environmental benefits in replacing traditional car-based deliveries with shared lightweight autonomous vehicle fleets. Lastly, we introduce an interactive platform that offers a user-friendly means of comprehending the model&rsquo;s performance and potential trade-offs, which can help inform decision-makers in the evolving landscape of food delivery innovation.</p></p class="citation"></blockquote><h3 id=45--167254-fate-in-mmla-a-student-centred-exploration-of-fairness-accountability-transparency-and-ethics-in-multimodal-learning-analytics-yueqiao-jin-et-al-2024>(4/5 | 167/254) FATE in MMLA: A Student-Centred Exploration of Fairness, Accountability, Transparency, and Ethics in Multimodal Learning Analytics (Yueqiao Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yueqiao Jin, Vanessa Echeverria, Lixiang Yan, Linxuan Zhao, Riordan Alfredo, Yi-Shan Tsai, Dragan Gašević, Roberto Martinez-Maldonado. (2024)<br><strong>FATE in MMLA: A Student-Centred Exploration of Fairness, Accountability, Transparency, and Ethics in Multimodal Learning Analytics</strong><br><button class=copy-to-clipboard title="FATE in MMLA: A Student-Centred Exploration of Fairness, Accountability, Transparency, and Ethics in Multimodal Learning Analytics" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-HC, cs.CY<br>Keyword Score: 16<br>Keywords: Fairness, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19071v1.pdf filename=2402.19071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> Learning Analytics (MMLA) integrates novel sensing technologies and artificial intelligence algorithms, providing opportunities to enhance student reflection during complex, collaborative learning experiences. Although recent advancements in MMLA have shown its capability to generate insights into diverse learning behaviours across various learning settings, little research has been conducted to evaluate these systems in authentic learning contexts, particularly regarding students&rsquo; perceived <b>fairness,</b> accountability, transparency, and ethics (FATE). Understanding these perceptions is essential to using MMLA effectively without introducing ethical complications or negatively affecting how students learn. This study aimed to address this gap by assessing the FATE of MMLA in an authentic, collaborative learning context. We conducted semi-structured interviews with 14 undergraduate students who used MMLA visualisations for post-activity reflection. The findings highlighted the significance of accurate and comprehensive data representation to ensure visualisation <b>fairness,</b> the need for different levels of data access to foster accountability, the imperative of measuring and cultivating transparency with students, and the necessity of transforming informed consent from dichotomous to continuous and measurable scales. While students value the benefits of MMLA, they also emphasise the importance of ethical considerations, highlighting a pressing need for the LA and MMLA community to investigate and address FATE issues actively.</p></p class="citation"></blockquote><h3 id=55--168254-envisioning-the-applications-and-implications-of-generative-ai-for-news-media-sachita-nishal-et-al-2024>(5/5 | 168/254) Envisioning the Applications and Implications of Generative AI for News Media (Sachita Nishal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sachita Nishal, Nicholas Diakopoulos. (2024)<br><strong>Envisioning the Applications and Implications of Generative AI for News Media</strong><br><button class=copy-to-clipboard title="Envisioning the Applications and Implications of Generative AI for News Media" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: H-0; J-4, cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18835v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18835v1.pdf filename=2402.18835v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article considers the increasing use of algorithmic decision-support systems and synthetic media in the newsroom, and explores how <b>generative</b> <b>models</b> can help reporters and editors across a range of tasks from the conception of a news story to its distribution. Specifically, we draw from a taxonomy of tasks associated with news production, and discuss where <b>generative</b> <b>models</b> could appropriately support reporters, the journalistic and ethical values that must be preserved within these interactions, and the resulting implications for design contributions in this area in the future. Our essay is relevant to practitioners and researchers as they consider using <b>generative</b> <b>AI</b> systems to support different tasks and workflows.</p></p class="citation"></blockquote><h2 id=cscr-10>cs.CR (10)</h2><h3 id=110--169254-always-be-pre-training-representation-learning-for-network-intrusion-detection-with-gnns-zhengyao-gu-et-al-2024>(1/10 | 169/254) Always be Pre-Training: Representation Learning for Network Intrusion Detection with GNNs (Zhengyao Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyao Gu, Diego Troy Lopez, Lilas Alrahis, Ozgur Sinanoglu. (2024)<br><strong>Always be Pre-Training: Representation Learning for Network Intrusion Detection with GNNs</strong><br><button class=copy-to-clipboard title="Always be Pre-Training: Representation Learning for Network Intrusion Detection with GNNs" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 51<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Representation Learning, Supervised Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18986v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18986v1.pdf filename=2402.18986v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>network-based</b> network intrusion detection systems have recently demonstrated state-of-the-art performance on <b>benchmark</b> datasets. Nevertheless, these methods suffer from a reliance on target encoding for data pre-processing, limiting widespread adoption due to the associated need for annotated labels&ndash;a cost-prohibitive requirement. In this work, we propose a solution involving <b>in-context</b> pre-training and the utilization of dense <b>representations</b> <b>for</b> categorical features to jointly overcome the label-dependency limitation. Our approach exhibits remarkable data efficiency, achieving over 98% of the performance of the <b>supervised</b> state-of-the-art with less than 4% labeled data on the NF-UQ-NIDS-V2 dataset.</p></p class="citation"></blockquote><h3 id=210--170254-prsa-prompt-reverse-stealing-attacks-against-large-language-models-yong-yang-et-al-2024>(2/10 | 170/254) PRSA: Prompt Reverse Stealing Attacks against Large Language Models (Yong Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, Zonghui Wang. (2024)<br><strong>PRSA: Prompt Reverse Stealing Attacks against Large Language Models</strong><br><button class=copy-to-clipboard title="PRSA: Prompt Reverse Stealing Attacks against Large Language Models" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs.CR<br>Keyword Score: 50<br>Keywords: Fine-tuning, Pruning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19200v1.pdf filename=2402.19200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt,</b> recognized as crucial intellectual property, enables <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to perform specific tasks without the need of <b>fine-tuning,</b> underscoring their escalating importance. With the rise of <b>prompt-based</b> services, such as <b>prompt</b> marketplaces and <b>LLM</b> applications, providers often display <b>prompts&rsquo;</b> capabilities through input-output examples to attract users. However, this paradigm raises a pivotal security concern: does the exposure of input-output pairs pose the risk of potential <b>prompt</b> leakage, infringing on the intellectual property rights of the developers? To our knowledge, this problem still has not been comprehensively explored yet. To remedy this gap, in this paper, we perform the first in depth exploration and propose a novel attack framework for reverse-stealing <b>prompts</b> against commercial <b>LLMs,</b> namely PRSA. The main idea of PRSA is that by analyzing the critical features of the input-output pairs, we mimic and gradually infer (steal) the target <b>prompts.</b> In detail, PRSA mainly consists of two key phases: <b>prompt</b> mutation and <b>prompt</b> <b>pruning.</b> In the mutation phase, we propose a <b>prompt</b> attention algorithm based on differential feedback to capture these critical features for effectively inferring the target <b>prompts.</b> In the <b>prompt</b> <b>pruning</b> phase, we identify and mask the words dependent on specific inputs, enabling the <b>prompts</b> to accommodate diverse inputs for generalization. Through extensive evaluation, we verify that PRSA poses a severe threat in real world scenarios. We have reported these findings to <b>prompt</b> service providers and actively collaborate with them to take protective measures for <b>prompt</b> copyright.</p></p class="citation"></blockquote><h3 id=310--171254-syntactic-ghost-an-imperceptible-general-purpose-backdoor-attacks-on-pre-trained-language-models-pengzhou-cheng-et-al-2024>(3/10 | 171/254) Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models (Pengzhou Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengzhou Cheng, Wei Du, Zongru Wu, Fengwei Zhang, Libo Chen, Gongshen Liu. (2024)<br><strong>Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models</strong><br><button class=copy-to-clipboard title="Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs.CR<br>Keyword Score: 50<br>Keywords: Contrastive Learning, Natural Language Understanding, Perplexity, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18945v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18945v1.pdf filename=2402.18945v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> have been found susceptible to backdoor attacks, which can transfer vulnerabilities to various downstream tasks. However, existing <b>PLM</b> backdoors are conducted with explicit triggers under the manually aligned, thus failing to satisfy expectation goals simultaneously in terms of effectiveness, stealthiness, and universality. In this paper, we propose a novel approach to achieve invisible and general backdoor implantation, called \textbf{Syntactic Ghost} (synGhost for short). Specifically, the method hostilely manipulates poisoned samples with different predefined syntactic structures as stealth triggers and then implants the backdoor to <b>pre-trained</b> <b>representation</b> <b>space</b> without disturbing the primitive knowledge. The output representations of poisoned samples are distributed as uniformly as possible in the feature space via <b>contrastive</b> <b>learning,</b> forming a wide range of backdoors. Additionally, in light of the unique properties of syntactic triggers, we introduce an auxiliary module to drive the <b>PLMs</b> to learn this knowledge in priority, which can alleviate the interference between different syntactic structures. Experiments show that our method outperforms the previous methods and achieves the predefined objectives. Not only do severe threats to various <b>natural</b> <b>language</b> <b>understanding</b> (NLU) tasks on two tuning paradigms but also to multiple <b>PLMs.</b> Meanwhile, the synGhost is imperceptible against three countermeasures based on <b>perplexity,</b> fine-pruning, and the proposed maxEntropy.</p></p class="citation"></blockquote><h3 id=410--172254-sok-exploring-the-potential-of-large-language-models-for-improving-digital-forensic-investigation-efficiency-akila-wickramasekara-et-al-2024>(4/10 | 172/254) SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency (Akila Wickramasekara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akila Wickramasekara, Frank Breitinger, Mark Scanlon. (2024)<br><strong>SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency</strong><br><button class=copy-to-clipboard title="SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19366v1.pdf filename=2402.19366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing number of cases requiring digital forensic analysis raises concerns about law enforcement&rsquo;s ability to conduct investigations promptly. Consequently, this systemisation of knowledge paper delves into the potential and effectiveness of integrating <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> into digital forensic investigation to address these challenges. A thorough literature review is undertaken, encompassing existing digital forensic models, tools, <b>LLMs,</b> deep learning techniques, and the utilisation of <b>LLMs</b> in investigations. The review identifies current challenges within existing digital forensic processes and explores both the obstacles and possibilities of incorporating <b>LLMs.</b> In conclusion, the study asserts that the adoption of <b>LLMs</b> in digital forensics, with appropriate constraints, holds the potential to enhance investigation efficiency, improve traceability, and alleviate technical and judicial barriers faced by law enforcement entities.</p></p class="citation"></blockquote><h3 id=510--173254-watermark-stealing-in-large-language-models-nikola-jovanović-et-al-2024>(5/10 | 173/254) Watermark Stealing in Large Language Models (Nikola Jovanović et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikola Jovanović, Robin Staab, Martin Vechev. (2024)<br><strong>Watermark Stealing in Large Language Models</strong><br><button class=copy-to-clipboard title="Watermark Stealing in Large Language Models" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19361v1.pdf filename=2402.19361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLM</b> watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked <b>LLM</b> to approximately reverse-engineer a watermark enables practical spoofing attacks, as suggested in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about <b>LLM</b> watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at <a href=https://watermark-stealing.org>https://watermark-stealing.org</a>.</p></p class="citation"></blockquote><h3 id=610--174254-how-to-train-your-antivirus-rl-based-hardening-through-the-problem-space-jacopo-cortellazzi-et-al-2024>(6/10 | 174/254) How to Train your Antivirus: RL-based Hardening through the Problem-Space (Jacopo Cortellazzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacopo Cortellazzi, Ilias Tsingenopoulos, Branislav Bošanský, Simone Aonzo, Davy Preuveneers, Wouter Joosen, Fabio Pierazzi, Lorenzo Cavallaro. (2024)<br><strong>How to Train your Antivirus: RL-based Hardening through the Problem-Space</strong><br><button class=copy-to-clipboard title="How to Train your Antivirus: RL-based Hardening through the Problem-Space" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19027v1.pdf filename=2402.19027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>ML-based malware detection on dynamic analysis reports is vulnerable to both evasion and spurious correlations. In this work, we investigate a specific ML architecture employed in the pipeline of a widely-known commercial antivirus company, with the goal to harden it against <b>adversarial</b> <b>malware.</b> <b>Adversarial</b> <b>training,</b> the sole defensive technique that can confer empirical robustness, is not applicable out of the box in this domain, for the principal reason that gradient-based perturbations rarely map back to feasible problem-space programs. We introduce a novel <b>Reinforcement</b> <b>Learning</b> approach for constructing <b>adversarial</b> <b>examples,</b> a constituent part of adversarially training a model against evasion. Our approach comes with multiple advantages. It performs modifications that are feasible in the problem-space, and only those; thus it circumvents the inverse mapping problem. It also makes possible to provide theoretical guarantees on the robustness of the model against a particular set of <b>adversarial</b> <b>capabilities.</b> Our empirical exploration validates our theoretical insights, where we can consistently reach 0% Attack Success Rate after a few <b>adversarial</b> <b>retraining</b> iterations.</p></p class="citation"></blockquote><h3 id=710--175254-robwe-robust-watermark-embedding-for-personalized-federated-learning-model-ownership-protection-yang-xu-et-al-2024>(7/10 | 175/254) RobWE: Robust Watermark Embedding for Personalized Federated Learning Model Ownership Protection (Yang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Xu, Yunlin Tan, Cheng Zhang, Kai Chi, Peng Sun, Wenyuan Yang, Ju Ren, Hongbo Jiang, Yaoxue Zhang. (2024)<br><strong>RobWE: Robust Watermark Embedding for Personalized Federated Learning Model Ownership Protection</strong><br><button class=copy-to-clipboard title="RobWE: Robust Watermark Embedding for Personalized Federated Learning Model Ownership Protection" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19054v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19054v1.pdf filename=2402.19054v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Embedding watermarks into models has been widely used to protect model ownership in <b>federated</b> <b>learning</b> (FL). However, existing methods are inadequate for protecting the ownership of personalized models acquired by clients in personalized FL (PFL). This is due to the aggregation of the global model in PFL, resulting in conflicts over clients&rsquo; private watermarks. Moreover, malicious clients may tamper with embedded watermarks to facilitate model leakage and evade accountability. This paper presents a robust watermark embedding scheme, named RobWE, to protect the ownership of personalized models in PFL. We first decouple the watermark embedding of personalized models into two parts: head layer embedding and representation layer embedding. The head layer belongs to clients&rsquo; private part without participating in model aggregation, while the representation layer is the shared part for aggregation. For representation layer embedding, we employ a watermark slice embedding operation, which avoids watermark embedding conflicts. Furthermore, we design a malicious watermark detection scheme enabling the server to verify the correctness of watermarks before aggregating local models. We conduct an exhaustive experimental evaluation of RobWE. The results demonstrate that RobWE significantly outperforms the state-of-the-art watermark embedding schemes in FL in terms of fidelity, reliability, and robustness.</p></p class="citation"></blockquote><h3 id=810--176254-privacy-management-and-interface-design-for-a-smart-house-ana-maria-comeaga-et-al-2024>(8/10 | 176/254) Privacy Management and Interface Design for a Smart House (Ana-Maria Comeaga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ana-Maria Comeaga, Iuliana Marin. (2024)<br><strong>Privacy Management and Interface Design for a Smart House</strong><br><button class=copy-to-clipboard title="Privacy Management and Interface Design for a Smart House" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SE, cs.CR<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18973v1.pdf filename=2402.18973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s life, more and more people tend to opt for a smart house. In this way, the idea of including technology has become popular worldwide. Despite this concept&rsquo;s many benefits, managing security remains an essential problem due to the shared activities. The Internet of Things system behind a smart house is based on several sensors to measure temperature, humidity, air quality, and movement. Because of being <b>supervised</b> every day through sensors and controlling their house only with a simple click, many people can be afraid of this new approach in terms of their privacy, and this fact can constrain them from following their habits. The security aspects should be constantly analyzed to keep the data&rsquo;s confidentiality and make people feel safe in their own houses. In this context, the current paper puts light on an alternative design of a platform in which the safety of homeowners is the primary purpose, and they maintain complete control over the data generated by smart devices. The current research highlights the role of security and interface design in controlling a smart house. The study underscores the importance of providing an interface that can be used easily by any person to manage data and live activities in a modern residence in an era dominated by continuously developing technology.</p></p class="citation"></blockquote><h3 id=910--177254-attacks-against-mobility-prediction-in-5g-networks-syafiq-al-atiiq-et-al-2024>(9/10 | 177/254) Attacks Against Mobility Prediction in 5G Networks (Syafiq Al Atiiq et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Syafiq Al Atiiq, Yachao Yuan, Christian Gehrmann, Jakob Sternby, Luis Barriga. (2024)<br><strong>Attacks Against Mobility Prediction in 5G Networks</strong><br><button class=copy-to-clipboard title="Attacks Against Mobility Prediction in 5G Networks" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs-NI, cs.CR<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19319v1.pdf filename=2402.19319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The $5^{th}$ generation of mobile networks introduces a new Network Function (NF) that was not present in previous generations, namely the Network Data Analytics Function (NWDAF). Its primary objective is to provide advanced analytics services to various entities within the network and also towards external application services in the 5G ecosystem. One of the key use cases of NWDAF is mobility trajectory prediction, which aims to accurately support efficient mobility management of User Equipment (UE) in the network by allocating ``just in time&rsquo;&rsquo; necessary network resources. In this paper, we show that there are potential mobility attacks that can compromise the accuracy of these predictions. In a semi-realistic scenario with 10,000 subscribers, we demonstrate that an adversary equipped with the ability to hijack cellular mobile devices and clone them can significantly reduce the prediction accuracy from 75% to 40% using just 100 adversarial UEs. While a defense mechanism largely depends on the attack and the mobility types in a particular area, we prove that a basic KMeans <b>clustering</b> is effective in distinguishing legitimate and adversarial UEs.</p></p class="citation"></blockquote><h3 id=1010--178254-rahmani-sort-a-novel-variant-of-insertion-sort-algorithm-with-onlogn-complexity-mohammad-khalid-imam-rahmani-2024>(10/10 | 178/254) Rahmani Sort: A Novel Variant of Insertion Sort Algorithm with O(nlogn) Complexity (Mohammad Khalid Imam Rahmani, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Khalid Imam Rahmani. (2024)<br><strong>Rahmani Sort: A Novel Variant of Insertion Sort Algorithm with O(nlogn) Complexity</strong><br><button class=copy-to-clipboard title="Rahmani Sort: A Novel Variant of Insertion Sort Algorithm with O(nlogn) Complexity" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: 14J26 (Secondary), E-1, cs-CR, cs-DS, cs.CR<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19107v1.pdf filename=2402.19107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Various decision support systems are available that implement Data Mining and Data Warehousing techniques for diving into the sea of data for getting useful patterns of knowledge (pearls). Classification, regression, <b>clustering,</b> and many other algorithms are used to enhance the precision and accuracy of the decision process. So, there is scope for increasing the response time of the decision process, especially in mission-critical operations. If data are ordered with suitable and efficient sorting operation, the response time of the decision process can be minimized. Insertion sort is much more suitable for such applications due to its simple and straight logic along with its dynamic nature suitable for list implementation. But it is slower than merge sort and quick sort. The main reasons this is slow: firstly, a sequential search is used to find the actual position of the next key element into the sorted left subarray and secondly, shifting of elements is required by one position towards the right for accommodating the newly inserted element. Therefore, I propose a new algorithm by using a novel technique of binary search mechanism for finding the sorted location of the next key item into the previously sorted left subarray much quicker than the conventional insertion sort algorithm. Performance measurement in terms of the actual running time of the new algorithm has been compared with those of other conventional sorting algorithms apart from the insertion sort. The results obtained on various sample data show that the new algorithm is better in performance than the conventional insertion sort and merge sort algorithms.</p></p class="citation"></blockquote><h2 id=csce-2>cs.CE (2)</h2><h3 id=12--179254-generative-models-struggle-with-kirigami-metamaterials-gerrit-felsch-et-al-2024>(1/2 | 179/254) Generative models struggle with kirigami metamaterials (Gerrit Felsch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gerrit Felsch, Viacheslav Slesarenko. (2024)<br><strong>Generative models struggle with kirigami metamaterials</strong><br><button class=copy-to-clipboard title="Generative models struggle with kirigami metamaterials" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: J-2; I-6, cond-mat-mtrl-sci, cond-mat-soft, cs-CE, cs.CE<br>Keyword Score: 50<br>Keywords: Autoencoder, Generative Adversarial Network, Generative Adversarial Network, Probabilistic Model, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19196v1.pdf filename=2402.19196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>machine</b> <b>learning</b> models have shown notable success in identifying architectures for metamaterials - materials whose behavior is determined primarily by their internal organization - that match specific target properties. By examining kirigami metamaterials, in which dependencies between cuts yield complex design restrictions, we demonstrate that this perceived success in the employment of <b>generative</b> <b>models</b> <b>for</b> metamaterials might be akin to survivorship bias. We assess the performance of the four most popular <b>generative</b> <b>models</b> <b>-</b> the <b>Variational</b> <b>Autoencoder</b> (VAE), the <b>Generative</b> <b>Adversarial</b> <b>Network</b> <b>(GAN),</b> the Wasserstein <b>GAN</b> (WGAN), and the Denoising Diffusion <b>Probabilistic</b> <b>Model</b> (DDPM) - in generating kirigami structures. Prohibiting cut intersections can prevent the identification of an appropriate similarity measure for kirigami metamaterials, significantly impacting the effectiveness of VAE and WGAN, which rely on the Euclidean distance - a metric shown to be unsuitable for considered geometries. This imposes significant limitations on employing modern <b>generative</b> <b>models</b> <b>for</b> the creation of diverse metamaterials.</p></p class="citation"></blockquote><h3 id=22--180254-protein-multimer-structure-prediction-via-prompt-learning-ziqi-gao-et-al-2024>(2/2 | 180/254) Protein Multimer Structure Prediction via Prompt Learning (Ziqi Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqi Gao, Xiangguo Sun, Zijing Liu, Yu Li, Hong Cheng, Jia Li. (2024)<br><strong>Protein Multimer Structure Prediction via Prompt Learning</strong><br><button class=copy-to-clipboard title="Protein Multimer Structure Prediction via Prompt Learning" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 30<br>Keywords: Meta Learning, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18813v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18813v1.pdf filename=2402.18813v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the 3D structures of protein multimers is crucial, as they play a vital role in regulating various cellular processes. It has been empirically confirmed that the multimer structure prediction~(MSP) can be well handled in a step-wise assembly fashion using provided dimer structures and predicted protein-protein interactions~(PPIs). However, due to the biological gap in the formation of dimers and larger multimers, directly applying PPI prediction techniques can often cause a \textit{poor generalization} to the MSP task. To address this challenge, we aim to extend the PPI knowledge to multimers of different scales~(i.e., chain numbers). Specifically, we propose \textbf{\textsc{PromptMSP}}, a pre-training and \textbf{Prompt} tuning framework for \textbf{M}ultimer \textbf{S}tructure \textbf{P}rediction. First, we tailor the source and target tasks for effective PPI knowledge learning and efficient inference, respectively. We design PPI-inspired <b>prompt</b> <b>learning</b> to narrow the gaps of two task formats and generalize the PPI knowledge to multimers of different scales. We provide a <b>meta-learning</b> <b>strategy</b> to learn a reliable initialization of the <b>prompt</b> <b>model,</b> enabling our <b>prompting</b> <b>framework</b> to effectively adapt to limited data for large-scale multimers. Empirically, we achieve both significant accuracy (RMSD and TM-Score) and efficiency improvements compared to advanced MSP models. The code, data and checkpoints are released at \url{https://github.com/zqgao22/PromptMSP}.</p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=11--181254-a-protein-structure-prediction-approach-leveraging-transformer-and-cnn-integration-yanlin-zhou-et-al-2024>(1/1 | 181/254) A Protein Structure Prediction Approach Leveraging Transformer and CNN Integration (Yanlin Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanlin Zhou, Kai Tan, Xinyu Shen, Zheng He. (2024)<br><strong>A Protein Structure Prediction Approach Leveraging Transformer and CNN Integration</strong><br><button class=copy-to-clipboard title="A Protein Structure Prediction Approach Leveraging Transformer and CNN Integration" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19095v1.pdf filename=2402.19095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Proteins are essential for life, and their structure determines their function. The protein secondary structure is formed by the folding of the protein primary structure, and the protein tertiary structure is formed by the bending and folding of the secondary structure. Therefore, the study of protein secondary structure is very helpful to the overall understanding of protein structure. Although the accuracy of protein secondary structure prediction has continuously improved with the development of machine learning and deep learning, progress in the field of protein structure prediction, unfortunately, remains insufficient to meet the large demand for protein information. Therefore, based on the advantages of deep learning-based methods in feature extraction and learning ability, this paper adopts a two-dimensional fusion deep neural network model, DstruCCN, which uses <b>Convolutional</b> <b>Neural</b> <b>Networks</b> (CCN) and a <b>supervised</b> <b>Transformer</b> protein language model for single-sequence protein structure prediction. The training features of the two are combined to predict the protein <b>Transformer</b> binding site matrix, and then the three-dimensional structure is reconstructed using energy minimization.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-2>cond-mat.mtrl-sci (2)</h2><h3 id=12--182254-training-set-free-two-stage-deep-learning-for-spectroscopic-data-de-noising-dongchen-huang-junde-liu-et-al-2024>(1/2 | 182/254) Training-set-free two-stage deep learning for Spectroscopic data de-noising (Dongchen Huang. Junde Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongchen Huang. Junde Liu, Tian Qian, Hongming Weng. (2024)<br><strong>Training-set-free two-stage deep learning for Spectroscopic data de-noising</strong><br><button class=copy-to-clipboard title="Training-set-free two-stage deep learning for Spectroscopic data de-noising" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-LG, physics-data-an<br>Keyword Score: 45<br>Keywords: Geometry, Supervised Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18830v1.pdf filename=2402.18830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>De-noising is a prominent step in the spectra post-processing procedure. Previous machine learning-based methods are fast but mostly based on <b>supervised</b> <b>learning</b> and require a training set that may be typically expensive in real experimental measurements. <b>Unsupervised</b> <b>learning-based</b> algorithms are slow and require many iterations to achieve convergence. Here, we bridge this gap by proposing a training-set-free two-stage deep learning method. We show that the fuzzy fixed input in previous methods can be improved by introducing an adaptive prior. Combined with more advanced optimization techniques, our approach can achieve five times acceleration compared to previous work. Theoretically, we study the landscape of a corresponding non-convex linear problem, and our results indicates that this problem has benign <b>geometry</b> for first-order algorithms to converge.</p></p class="citation"></blockquote><h3 id=22--183254-accelerating-materials-discovery-for-polymer-solar-cells-data-driven-insights-enabled-by-natural-language-processing-pranav-shetty-et-al-2024>(2/2 | 183/254) Accelerating materials discovery for polymer solar cells: Data-driven insights enabled by natural language processing (Pranav Shetty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pranav Shetty, Aishat Adeboye, Sonakshi Gupta, Chao Zhang, Rampi Ramprasad. (2024)<br><strong>Accelerating materials discovery for polymer solar cells: Data-driven insights enabled by natural language processing</strong><br><button class=copy-to-clipboard title="Accelerating materials discovery for polymer solar cells: Data-driven insights enabled by natural language processing" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-CL, physics-app-ph<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19462v1.pdf filename=2402.19462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a natural language processing pipeline that was used to extract polymer solar cell property data from the literature and simulate various <b>active</b> <b>learning</b> strategies. While data-driven methods have been well established to discover novel materials faster than Edisonian trial-and-error approaches, their benefits have not been quantified. Our approach demonstrates a potential reduction in discovery time by approximately 75 %, equivalent to a 15 year acceleration in material innovation. Our pipeline enables us to extract data from more than 3300 papers which is ~5 times larger than similar data sets reported by others. We also trained machine learning models to predict the power conversion efficiency and used our model to identify promising donor-acceptor combinations that are as yet unreported. We thus demonstrate a workflow that goes from published literature to extracted material property data which in turn is used to obtain data-driven insights. Our insights include <b>active</b> <b>learning</b> strategies that can simultaneously optimize the material system and train strong predictive models of material properties. This work provides a valuable framework for research in material science.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=12--184254-a-sound-approach-using-large-language-models-to-generate-audio-descriptions-for-egocentric-text-audio-retrieval-andreea-maria-oncescu-et-al-2024>(1/2 | 184/254) A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval (Andreea-Maria Oncescu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreea-Maria Oncescu, João F. Henriques, Andrew Zisserman, Samuel Albanie, A. Sophia Koepke. (2024)<br><strong>A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval</strong><br><button class=copy-to-clipboard title="A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-IR, cs-SD, eess-AS, eess.AS<br>Keyword Score: 43<br>Keywords: Benchmarking, Zero-shot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19106v1.pdf filename=2402.19106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video databases from the internet are a valuable source of text-audio retrieval datasets. However, given that sound and vision streams represent different &ldquo;views&rdquo; of the data, treating visual descriptions as audio descriptions is far from optimal. Even if audio class labels are present, they commonly are not very detailed, making them unsuited for text-audio retrieval. To exploit relevant audio information from video-text datasets, we introduce a methodology for generating audio-centric descriptions using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> In this work, we consider the egocentric video setting and propose three new text-audio retrieval <b>benchmarks</b> based on the EpicMIR and EgoMCQ tasks, and on the EpicSounds dataset. Our approach for obtaining audio-centric descriptions gives significantly higher <b>zero-shot</b> performance than using the original visual-centric descriptions. Furthermore, we show that using the same <b>prompts,</b> we can successfully employ <b>LLMs</b> to improve the retrieval on EpicSounds, compared to using the original audio class labels of the dataset. Finally, we confirm that <b>LLMs</b> can be used to determine the difficulty of identifying the action associated with a sound.</p></p class="citation"></blockquote><h3 id=22--185254-extending-multilingual-speech-synthesis-to-100-languages-without-transcribed-data-takaaki-saeki-et-al-2024>(2/2 | 185/254) Extending Multilingual Speech Synthesis to 100+ Languages without Transcribed Data (Takaaki Saeki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takaaki Saeki, Gary Wang, Nobuyuki Morioka, Isaac Elias, Kyle Kastner, Andrew Rosenberg, Bhuvana Ramabhadran, Heiga Zen, Françoise Beaufays, Hadar Shemtov. (2024)<br><strong>Extending Multilingual Speech Synthesis to 100+ Languages without Transcribed Data</strong><br><button class=copy-to-clipboard title="Extending Multilingual Speech Synthesis to 100+ Languages without Transcribed Data" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 35<br>Keywords: Representation Learning, Unsupervised Learning, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18932v1.pdf filename=2402.18932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collecting high-quality studio recordings of audio is challenging, which limits the language coverage of <b>text-to-speech</b> <b>(TTS)</b> systems. This paper proposes a framework for scaling a multilingual <b>TTS</b> model to 100+ languages using found data without supervision. The proposed framework combines speech-text encoder pretraining with <b>unsupervised</b> training using untranscribed speech and unspoken text data sources, thereby leveraging massively multilingual joint speech and text <b>representation</b> <b>learning.</b> Without any transcribed speech in a new language, this <b>TTS</b> model can generate intelligible speech in >30 unseen languages (CER difference of &lt;10% to ground truth). With just 15 minutes of transcribed, found data, we can reduce the intelligibility difference to 1% or less from the ground-truth, and achieve naturalness scores that match the ground-truth in several languages.</p></p class="citation"></blockquote><h2 id=eesssy-4>eess.SY (4)</h2><h3 id=14--186254-closed-loop-training-of-static-output-feedback-neural-network-controllers-for-large-systems-a-distillation-case-study-e-m-turan-et-al-2024>(1/4 | 186/254) Closed-loop training of static output feedback neural network controllers for large systems: A distillation case study (E. M. Turan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>E. M. Turan, J. Jäschke. (2024)<br><strong>Closed-loop training of static output feedback neural network controllers for large systems: A distillation case study</strong><br><button class=copy-to-clipboard title="Closed-loop training of static output feedback neural network controllers for large systems: A distillation case study" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 40<br>Keywords: Heuristic Approach, Knowledge Distillation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19309v1.pdf filename=2402.19309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The online implementation of model predictive control for constrained multivariate systems has two main disadvantages: it requires an estimate of the entire model state and an optimisation problem must be solved online. These issues have typically been treated separately. This work proposes an integrated approach for the offline training of an output feedback neural network controller in closed loop. Online this neural network controller computers the plant inputs cheaply using noisy measurements. In addition, the controller can be trained to only make use of certain predefined measurements. Further, a <b>heuristic</b> <b>approach</b> is proposed to perform the automatic selection of important measurements. The proposed method is demonstrated by extensive <b>simulations</b> using a non-linear <b>distillation</b> column model of 50 states.</p></p class="citation"></blockquote><h3 id=24--187254-temporal-aware-deep-reinforcement-learning-for-energy-storage-bidding-in-energy-and-contingency-reserve-markets-jinhao-li-et-al-2024>(2/4 | 187/254) Temporal-Aware Deep Reinforcement Learning for Energy Storage Bidding in Energy and Contingency Reserve Markets (Jinhao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinhao Li, Changlong Wang, Yanru Zhang, Hao Wang. (2024)<br><strong>Temporal-Aware Deep Reinforcement Learning for Energy Storage Bidding in Energy and Contingency Reserve Markets</strong><br><button class=copy-to-clipboard title="Temporal-Aware Deep Reinforcement Learning for Energy Storage Bidding in Energy and Contingency Reserve Markets" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 28<br>Keywords: Benchmarking, Black Box, Reinforcement Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19110v1.pdf filename=2402.19110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The battery energy storage system (BESS) has immense potential for enhancing grid reliability and security through its participation in the electricity market. BESS often seeks various revenue streams by taking part in multiple markets to unlock its full potential, but effective algorithms for joint-market participation under price uncertainties are insufficiently explored in the existing research. To bridge this gap, we develop a novel BESS joint bidding strategy that utilizes deep <b>reinforcement</b> <b>learning</b> (DRL) to bid in the spot and contingency frequency control ancillary services (FCAS) markets. Our approach leverages a <b>transformer-based</b> temporal feature extractor to effectively respond to price fluctuations in seven markets simultaneously and helps DRL learn the best BESS bidding strategy in joint-market participation. Additionally, unlike conventional <b>&ldquo;black-box&rdquo;</b> <b>DRL</b> model, our approach is more interpretable and provides valuable insights into the temporal bidding behavior of BESS in the dynamic electricity market. We validate our method using realistic market prices from the Australian National Electricity Market. The results show that our strategy outperforms <b>benchmarks,</b> including both optimization-based and other DRL-based strategies, by substantial margins. Our findings further suggest that effective temporal-aware bidding can significantly increase profits in the spot and contingency FCAS markets compared to individual market participation.</p></p class="citation"></blockquote><h3 id=34--188254-ultraviolet-positioning-via-tdoa-error-analysis-and-system-prototype-shihui-yu-et-al-2024>(3/4 | 188/254) Ultraviolet Positioning via TDOA: Error Analysis and System Prototype (Shihui Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shihui Yu, Chubing Lv, Yueke Yang, Yuchen Pan, Lei Sun, Yubo Zhang, Juliang Cao, Ruihang Yu, Chen Gong, Wenqi Wu, Zhengyuan Xu. (2024)<br><strong>Ultraviolet Positioning via TDOA: Error Analysis and System Prototype</strong><br><button class=copy-to-clipboard title="Ultraviolet Positioning via TDOA: Error Analysis and System Prototype" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19013v1.pdf filename=2402.19013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work performs the design, real-time hardware realization, and experimental evaluation of a positioning system by ultra-violet (UV) communication under photon-level signal detection. The positioning is based on time-difference of arrival (TDOA) principle. Time division-based transmission of synchronization sequence from three transmitters with known positions is applied. We investigate the positioning error via decomposing it into two parts, the transmitter-side timing error and the receiver-side synchronization error. The theoretical average error matches well with the <b>simulation</b> results, which indicates that theoretical fitting can provide reliable guidance and prediction for hardware experiments. We also conduct real-time hardware realization of the TDOA-based positioning system using Field Programmable Gate Array (FPGA), which is experimentally evaluated via outdoor experiments. Experimental results match well with the theoretical and <b>simulation</b> results.</p></p class="citation"></blockquote><h3 id=44--189254-adaptive-testing-environment-generation-for-connected-and-automated-vehicles-with-dense-reinforcement-learning-jingxuan-yang-et-al-2024>(4/4 | 189/254) Adaptive Testing Environment Generation for Connected and Automated Vehicles with Dense Reinforcement Learning (Jingxuan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingxuan Yang, Ruoxuan Bai, Haoyuan Ji, Yi Zhang, Jianming Hu, Shuo Feng. (2024)<br><strong>Adaptive Testing Environment Generation for Connected and Automated Vehicles with Dense Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Adaptive Testing Environment Generation for Connected and Automated Vehicles with Dense Reinforcement Learning" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19275v1.pdf filename=2402.19275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The assessment of safety performance plays a pivotal role in the development and deployment of connected and automated vehicles (CAVs). A common approach involves designing testing scenarios based on prior knowledge of CAVs (e.g., surrogate models), conducting tests in these scenarios, and subsequently evaluating CAVs&rsquo; safety performances. However, substantial differences between CAVs and the prior knowledge can significantly diminish the evaluation efficiency. In response to this issue, existing studies predominantly concentrate on the adaptive design of testing scenarios during the CAV testing process. Yet, these methods have limitations in their applicability to high-dimensional scenarios. To overcome this challenge, we develop an adaptive testing environment that bolsters evaluation robustness by incorporating multiple surrogate models and optimizing the combination coefficients of these surrogate models to enhance evaluation efficiency. We formulate the optimization problem as a regression task utilizing quadratic programming. To efficiently obtain the regression target via <b>reinforcement</b> <b>learning,</b> we propose the dense <b>reinforcement</b> <b>learning</b> method and devise a new adaptive policy with high sample efficiency. Essentially, our approach centers on learning the values of critical scenes displaying substantial surrogate-to-real gaps. The effectiveness of our method is validated in high-dimensional overtaking scenarios, demonstrating that our approach achieves notable evaluation efficiency.</p></p class="citation"></blockquote><h2 id=csai-4>cs.AI (4)</h2><h3 id=14--190254-rl-gpt-integrating-reinforcement-learning-and-code-as-policy-shaoteng-liu-et-al-2024>(1/4 | 190/254) RL-GPT: Integrating Reinforcement Learning and Code-as-policy (Shaoteng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, Jiaya Jia. (2024)<br><strong>RL-GPT: Integrating Reinforcement Learning and Code-as-policy</strong><br><button class=copy-to-clipboard title="RL-GPT: Integrating Reinforcement Learning and Code-as-policy" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, GPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19299v1.pdf filename=2402.19299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control. In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate task-specific refinement, such as <b>Reinforcement</b> <b>Learning</b> (RL). To seamlessly integrate both modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising a slow agent and a fast agent. The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks. This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline. Our approach outperforms traditional RL methods and existing <b>GPT</b> agents, demonstrating superior efficiency. In the Minecraft game, it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it achieves SOTA performance across all designated MineDojo tasks.</p></p class="citation"></blockquote><h3 id=24--191254-functional-benchmarks-for-robust-evaluation-of-reasoning-performance-and-the-reasoning-gap-saurabh-srivastava-et-al-2024>(2/4 | 191/254) Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap (Saurabh Srivastava et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saurabh Srivastava, Annarose M B, Anto P V, Shashank Menon, Ajay Sukumar, Adwaith Samod T, Alan Philipose, Stevin Prince, Sooraj Thomas. (2024)<br><strong>Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap</strong><br><button class=copy-to-clipboard title="Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 23<br>Keywords: Benchmarking, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19450v1.pdf filename=2402.19450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a framework for robust evaluation of <b>reasoning</b> capabilities of language models, using functional variants of <b>benchmarks.</b> Models that solve a <b>reasoning</b> test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH <b>benchmark</b> into its functional variant MATH(), with functionalization of other <b>benchmarks</b> to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a <b>reasoning</b> gap &ndash; the percentage difference between the static and functional accuracies. We find <b>reasoning</b> gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static <b>benchmarks,</b> with the caveat that the gaps are likely to be smaller with more sophisticated <b>prompting</b> strategies. Here we show that models which anecdotally have good <b>reasoning</b> performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building &ldquo;gap 0&rdquo; models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at <a href=https://github.com/consequentai/fneval/>https://github.com/consequentai/fneval/</a>.</p></p class="citation"></blockquote><h3 id=34--192254-a-cognitive-based-trajectory-prediction-approach-for-autonomous-driving-haicheng-liao-et-al-2024>(3/4 | 192/254) A Cognitive-Based Trajectory Prediction Approach for Autonomous Driving (Haicheng Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haicheng Liao, Yongkang Li, Zhenning Li, Chengyue Wang, Zhiyong Cui, Shengbo Eben Li, Chengzhong Xu. (2024)<br><strong>A Cognitive-Based Trajectory Prediction Approach for Autonomous Driving</strong><br><button class=copy-to-clipboard title="A Cognitive-Based Trajectory Prediction Approach for Autonomous Driving" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-RO, cs.AI<br>Keyword Score: 23<br>Keywords: Benchmarking, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19251v1.pdf filename=2402.19251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In autonomous vehicle (AV) technology, the ability to accurately predict the movements of surrounding vehicles is paramount for ensuring safety and operational efficiency. Incorporating human decision-making insights enables AVs to more effectively anticipate the potential actions of other vehicles, significantly improving prediction accuracy and responsiveness in dynamic environments. This paper introduces the Human-Like Trajectory Prediction (HLTP) model, which adopts a teacher-student <b>knowledge</b> <b>distillation</b> framework inspired by human cognitive processes. The HLTP model incorporates a sophisticated teacher-student <b>knowledge</b> <b>distillation</b> framework. The &ldquo;teacher&rdquo; model, equipped with an adaptive visual sector, mimics the visual processing of the human brain, particularly the functions of the occipital and temporal lobes. The &ldquo;student&rdquo; model focuses on real-time interaction and decision-making, drawing parallels to prefrontal and parietal cortex functions. This approach allows for dynamic adaptation to changing driving scenarios, capturing essential perceptual cues for accurate prediction. Evaluated using the Macao Connected and Autonomous Driving (MoCAD) dataset, along with the NGSIM and HighD <b>benchmarks,</b> HLTP demonstrates superior performance compared to existing models, particularly in challenging environments with incomplete data. The project page is available at Github.</p></p class="citation"></blockquote><h3 id=44--193254-negative-sampling-in-knowledge-graph-representation-learning-a-review-tiroshan-madushanka-et-al-2024>(4/4 | 193/254) Negative Sampling in Knowledge Graph Representation Learning: A Review (Tiroshan Madushanka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiroshan Madushanka, Ryutaro Ichise. (2024)<br><strong>Negative Sampling in Knowledge Graph Representation Learning: A Review</strong><br><button class=copy-to-clipboard title="Negative Sampling in Knowledge Graph Representation Learning: A Review" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 23<br>Keywords: Graph, Graph Embedding, Knowledge Graph, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19195v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19195v1.pdf filename=2402.19195v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>graph</b> <b>representation</b> <b>learning</b> (KGRL) or <b>knowledge</b> <b>graph</b> <b>embedding</b> (KGE) plays a crucial role in AI applications for <b>knowledge</b> <b>construction</b> and information exploration. These models aim to encode entities and relations present in a <b>knowledge</b> <b>graph</b> <b>into</b> a lower-dimensional vector space. During the training process of KGE models, using positive and negative samples becomes essential for discrimination purposes. However, obtaining negative samples directly from existing <b>knowledge</b> <b>graphs</b> <b>poses</b> a challenge, emphasizing the need for effective generation techniques. The quality of these negative samples greatly impacts the accuracy of the learned embeddings, making their generation a critical aspect of KGRL. This comprehensive survey paper systematically reviews various negative sampling (NS) methods and their contributions to the success of KGRL. Their respective advantages and disadvantages are outlined by categorizing existing NS methods into five distinct categories. Moreover, this survey identifies open research questions that serve as potential directions for future investigations. By offering a generalization and alignment of fundamental NS concepts, this survey provides valuable insights for designing effective NS methods in the context of KGRL and serves as a motivating force for further advancements in the field.</p></p class="citation"></blockquote><h2 id=csro-9>cs.RO (9)</h2><h3 id=19--194254-mirage-cross-embodiment-zero-shot-policy-transfer-with-cross-painting-lawrence-yunliang-chen-et-al-2024>(1/9 | 194/254) Mirage: Cross-Embodiment Zero-Shot Policy Transfer with Cross-Painting (Lawrence Yunliang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lawrence Yunliang Chen, Kush Hari, Karthik Dharmarajan, Chenfeng Xu, Quan Vuong, Ken Goldberg. (2024)<br><strong>Mirage: Cross-Embodiment Zero-Shot Policy Transfer with Cross-Painting</strong><br><button class=copy-to-clipboard title="Mirage: Cross-Embodiment Zero-Shot Policy Transfer with Cross-Painting" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Fine-tuning, Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19249v1.pdf filename=2402.19249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to reuse collected data and transfer trained policies between robots could alleviate the burden of additional data collection and training. While existing approaches such as pretraining plus <b>finetuning</b> and co-training show promise, they do not generalize to robots unseen in training. Focusing on common robot arms with similar workspaces and 2-jaw grippers, we investigate the feasibility of <b>zero-shot</b> transfer. Through <b>simulation</b> studies on 8 manipulation tasks, we find that state-based Cartesian control policies can successfully <b>zero-shot</b> transfer to a target robot after accounting for forward dynamics. To address robot visual disparities for vision-based policies, we introduce Mirage, which uses &ldquo;cross-painting&rdquo;&ndash;masking out the unseen target robot and inpainting the seen source robot&ndash;during execution in real time so that it appears to the policy as if the trained source robot were performing the task. Despite its simplicity, our extensive <b>simulation</b> and physical experiments provide strong evidence that Mirage can successfully <b>zero-shot</b> transfer between different robot arms and grippers with only minimal performance degradation on a variety of manipulation tasks such as picking, stacking, and assembly, significantly outperforming a generalist policy. Project website: <a href=https://robot-mirage.github.io/>https://robot-mirage.github.io/</a></p></p class="citation"></blockquote><h3 id=29--195254-armchair-integrated-inverse-reinforcement-learning-and-model-predictive-control-for-human-robot-collaboration-angelo-caregnato-neto-et-al-2024>(2/9 | 195/254) ARMCHAIR: integrated inverse reinforcement learning and model predictive control for human-robot collaboration (Angelo Caregnato-Neto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angelo Caregnato-Neto, Luciano Cavalcante Siebert, Arkady Zgonnikov, Marcos Ricardo Omena de Albuquerque Maximo, Rubens Junqueira Magalhães Afonso. (2024)<br><strong>ARMCHAIR: integrated inverse reinforcement learning and model predictive control for human-robot collaboration</strong><br><button class=copy-to-clipboard title="ARMCHAIR: integrated inverse reinforcement learning and model predictive control for human-robot collaboration" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-HC, cs-MA, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 40<br>Keywords: Human Intervention, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19128v1.pdf filename=2402.19128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the key issues in <b>human-robot</b> <b>collaboration</b> is the development of computational models that allow robots to predict and adapt to <b>human</b> <b>behavior.</b> Much progress has been achieved in developing such models, as well as control techniques that address the autonomy problems of motion planning and decision-making in robotics. However, the integration of computational models of <b>human</b> <b>behavior</b> with such control techniques still poses a major challenge, resulting in a bottleneck for efficient collaborative <b>human-robot</b> <b>teams.</b> In this context, we present a novel architecture for <b>human-robot</b> <b>collaboration:</b> Adaptive Robot Motion for Collaboration with <b>Humans</b> <b>using</b> Adversarial Inverse <b>Reinforcement</b> <b>learning</b> (ARMCHAIR). Our solution leverages adversarial inverse <b>reinforcement</b> <b>learning</b> and model predictive control to compute optimal trajectories and decisions for a mobile multi-robot system that collaborates with a <b>human</b> <b>in</b> an exploration task. During the mission, ARMCHAIR operates without <b>human</b> <b>intervention,</b> autonomously identifying the necessity to support and acting accordingly. Our approach also explicitly addresses the network connectivity requirement of the <b>human-robot</b> <b>team.</b> Extensive <b>simulation-based</b> evaluations demonstrate that ARMCHAIR allows a group of robots to safely support a simulated <b>human</b> <b>in</b> an exploration scenario, preventing collisions and network disconnections, and improving the overall performance of the task.</p></p class="citation"></blockquote><h3 id=39--196254-conversational-language-models-for-human-in-the-loop-multi-robot-coordination-william-hunt-et-al-2024>(3/9 | 196/254) Conversational Language Models for Human-in-the-Loop Multi-Robot Coordination (William Hunt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William Hunt, Toby Godfrey, Mohammad D. Soorati. (2024)<br><strong>Conversational Language Models for Human-in-the-Loop Multi-Robot Coordination</strong><br><button class=copy-to-clipboard title="Conversational Language Models for Human-in-the-Loop Multi-Robot Coordination" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, human-in-the-loop, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19166v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19166v1.pdf filename=2402.19166v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing prevalence and diversity of robots interacting in the real world, there is need for flexible, on-the-fly planning and cooperation. <b>Large</b> <b>Language</b> <b>Models</b> are starting to be explored in a <b>multimodal</b> setup for communication, coordination, and planning in robotics. Existing approaches generally use a single agent building a plan, or have multiple homogeneous agents coordinating for a simple task. We present a decentralised, dialogical approach in which a team of agents with different abilities plans solutions through peer-to-peer and human-robot discussion. We suggest that argument-style dialogues are an effective way to facilitate adaptive use of each agent&rsquo;s abilities within a cooperative team. Two robots discuss how to solve a cleaning problem set by a human, define roles, and agree on paths they each take. Each step can be interrupted by a human advisor and agents check their plans with the human. Agents then execute this plan in the real world, collecting rubbish from people in each room. Our implementation uses text at every step, maintaining transparency and effective human-multi-robot interaction.</p></p class="citation"></blockquote><h3 id=49--197254-humanoid-locomotion-as-next-token-prediction-ilija-radosavovic-et-al-2024>(4/9 | 197/254) Humanoid Locomotion as Next Token Prediction (Ilija Radosavovic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, Jitendra Malik. (2024)<br><strong>Humanoid Locomotion as Next Token Prediction</strong><br><button class=copy-to-clipboard title="Humanoid Locomotion as Next Token Prediction" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Multi-modal, Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19469v1.pdf filename=2402.19469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal <b>transformer</b> trained via autoregressive prediction of sensorimotor trajectories. To account for the <b>multi-modal</b> nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco <b>zero-shot.</b> Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backward. These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor trajectories.</p></p class="citation"></blockquote><h3 id=59--198254-pushing-the-limits-of-cross-embodiment-learning-for-manipulation-and-navigation-jonathan-yang-et-al-2024>(5/9 | 198/254) Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation (Jonathan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Yang, Catherine Glossop, Arjun Bhorkar, Dhruv Shah, Quan Vuong, Chelsea Finn, Dorsa Sadigh, Sergey Levine. (2024)<br><strong>Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation</strong><br><button class=copy-to-clipboard title="Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: 68T40, I-2-9, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Foundation Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19432v1.pdf filename=2402.19432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years in robotics and imitation learning have shown remarkable progress in training large-scale <b>foundation</b> <b>models</b> by leveraging data across a multitude of embodiments. The success of such policies might lead us to wonder: just how diverse can the robots in the training set be while still facilitating positive transfer? In this work, we study this question in the context of heterogeneous embodiments, examining how even seemingly very different domains, such as robotic navigation and manipulation, can provide benefits when included in the training data for the same model. We train a single goal-conditioned policy that is capable of controlling robotic arms, quadcopters, quadrupeds, and mobile bases. We then investigate the extent to which transfer can occur across navigation and manipulation on these embodiments by framing them as a single goal-reaching task. We find that co-training with navigation data can enhance robustness and performance in goal-conditioned manipulation with a wrist-mounted camera. We then deploy our policy trained only from navigation-only and static manipulation-only data on a mobile manipulator, showing that it can control a novel embodiment in a <b>zero-shot</b> manner. These results provide evidence that large-scale robotic policies can benefit from data collected across various embodiments. Further information and robot videos can be found on our project website <a href=http://extreme-cross-embodiment.github.io>http://extreme-cross-embodiment.github.io</a>.</p></p class="citation"></blockquote><h3 id=69--199254-contact-implicit-model-predictive-control-for-dexterous-in-hand-manipulation-a-long-horizon-and-robust-approach-yongpeng-jiang-et-al-2024>(6/9 | 199/254) Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach (Yongpeng Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongpeng Jiang, Mingrui Yu, Xinghao Zhu, Masayoshi Tomizuka, Xiang Li. (2024)<br><strong>Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach</strong><br><button class=copy-to-clipboard title="Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18897v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18897v1.pdf filename=2402.18897v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dexterous in-hand manipulation is an essential skill of production and life. Nevertheless, the highly stiff and mutable features of contacts cause limitations to real-time contact discovery and inference, which degrades the performance of model-based methods. Inspired by recent advancements in contact-rich locomotion and manipulation, this paper proposes a novel model-based approach to control dexterous in-hand manipulation and overcome the current limitations. The proposed approach has the attractive feature, which allows the robot to robustly execute long-horizon in-hand manipulation without pre-defined contact sequences or separated planning procedures. Specifically, we design a contact-implicit model predictive controller at high-level to generate real-time contact plans, which are executed by the low-level tracking controller. Compared with other model-based methods, such a long-horizon feature enables replanning and robust execution of contact-rich motions to achieve large-displacement in-hand tasks more efficiently; Compared with existing learning-based methods, the proposed approach achieves the dexterity and also generalizes to different objects without any pre-training. Detailed <b>simulations</b> and ablation studies demonstrate the efficiency and effectiveness of our method. It runs at 20Hz on the 23-degree-of-freedom long-horizon in-hand object rotation task.</p></p class="citation"></blockquote><h3 id=79--200254-roadrunner----learning-traversability-estimation-for-autonomous-off-road-driving-jonas-frey-et-al-2024>(7/9 | 200/254) RoadRunner &ndash; Learning Traversability Estimation for Autonomous Off-road Driving (Jonas Frey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonas Frey, Shehryar Khattak, Manthan Patel, Deegan Atha, Julian Nubert, Curtis Padgett, Marco Hutter, Patrick Spieler. (2024)<br><strong>RoadRunner &ndash; Learning Traversability Estimation for Autonomous Off-road Driving</strong><br><button class=copy-to-clipboard title="RoadRunner -- Learning Traversability Estimation for Autonomous Off-road Driving" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 15<br>Keywords: Geometry, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19341v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19341v1.pdf filename=2402.19341v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous navigation at high speeds in off-road environments necessitates robots to comprehensively understand their surroundings using onboard sensing only. The extreme conditions posed by the off-road setting can cause degraded camera image quality due to poor lighting and motion blur, as well as limited sparse geometric information available from LiDAR sensing when driving at high speeds. In this work, we present RoadRunner, a novel framework capable of predicting terrain traversability and an elevation map directly from camera and LiDAR sensor inputs. RoadRunner enables reliable autonomous navigation, by fusing sensory information, handling of uncertainty, and generation of contextually informed predictions about the <b>geometry</b> and traversability of the terrain while operating at low latency. In contrast to existing methods relying on classifying handcrafted semantic classes and using heuristics to predict traversability costs, our method is trained end-to-end in a <b>self-supervised</b> fashion. The RoadRunner network architecture builds upon popular sensor fusion network architectures from the autonomous driving domain, which embed LiDAR and camera information into a common Bird&rsquo;s Eye View perspective. Training is enabled by utilizing an existing traversability estimation stack to generate training data in hindsight in a scalable manner from real-world off-road driving datasets. Furthermore, RoadRunner improves the system latency by a factor of roughly 4, from 500 ms to 140 ms, while improving the accuracy for traversability costs and elevation map predictions. We demonstrate the effectiveness of RoadRunner in enabling safe and reliable off-road navigation at high speeds in multiple real-world driving scenarios through unstructured desert environments.</p></p class="citation"></blockquote><h3 id=89--201254-genie-smart-ros-based-caching-for-connected-autonomous-robots-zexin-li-et-al-2024>(8/9 | 201/254) Genie: Smart ROS-based Caching for Connected Autonomous Robots (Zexin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zexin Li, Soroush Bateni, Cong Liu. (2024)<br><strong>Genie: Smart ROS-based Caching for Connected Autonomous Robots</strong><br><button class=copy-to-clipboard title="Genie: Smart ROS-based Caching for Connected Autonomous Robots" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19410v1.pdf filename=2402.19410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the promising future of autonomous robots, several key issues currently remain that can lead to compromised performance and safety. One such issue is latency, where we find that even the latest embedded platforms from NVIDIA fail to execute intelligence tasks (e.g., <b>object</b> <b>detection)</b> of autonomous vehicles in a real-time fashion. One remedy to this problem is the promising paradigm of edge computing. Through collaboration with our industry partner, we identify key prohibitive limitations of the current edge mindset: (1) servers are not distributed enough and thus, are not close enough to vehicles, (2) current proposed edge solutions do not provide substantially better performance and extra information specific to autonomous vehicles to warrant their cost to the user, and (3) the state-of-the-art solutions are not compatible with popular frameworks used in autonomous systems, particularly the Robot Operating System (ROS). To remedy these issues, we provide Genie, an encapsulation technique that can enable transparent caching in ROS in a non-intrusive way (i.e., without modifying the source code), can build the cache in a distributed manner (in contrast to traditional central caching methods), and can construct a collective three-dimensional <b>object</b> <b>map</b> to provide substantially better latency (even on low-power edge servers) and higher quality data to all vehicles in a certain locality. We fully implement our design on state-of-the-art industry-adopted embedded and edge platforms, using the prominent autonomous driving software Autoware, and find that Genie can enhance the latency of Autoware Vision Detector by 82% on average, enable <b>object</b> <b>reusability</b> 31% of the time on average and as much as 67% for the incoming requests, and boost the confidence in its <b>object</b> <b>map</b> considerably over time.</p></p class="citation"></blockquote><h3 id=99--202254-relead-resilient-localization-with-enhanced-lidar-odometry-in-adverse-environments-zhiqiang-chen-et-al-2024>(9/9 | 202/254) RELEAD: Resilient Localization with Enhanced LiDAR Odometry in Adverse Environments (Zhiqiang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiqiang Chen, Hongbo Chen, Yuhua Qi, Shipeng Zhong, Dapeng Feng, Wu Jin, Weisong Wen, Ming Liu. (2024)<br><strong>RELEAD: Resilient Localization with Enhanced LiDAR Odometry in Adverse Environments</strong><br><button class=copy-to-clipboard title="RELEAD: Resilient Localization with Enhanced LiDAR Odometry in Adverse Environments" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18934v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18934v1.pdf filename=2402.18934v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>LiDAR-based localization is valuable for applications like mining surveys and underground facility maintenance. However, existing methods can struggle when dealing with uninformative geometric structures in challenging scenarios. This paper presents RELEAD, a LiDAR-centric solution designed to address scan-matching degradation. Our method enables degeneracy-free point cloud registration by solving constrained ESIKF updates in the front end and incorporates multisensor constraints, even when dealing with outlier measurements, through <b>graph</b> optimization based on Graduated Non-Convexity (GNC). Additionally, we propose a robust Incremental Fixed Lag Smoother (rIFL) for efficient GNC-based optimization. RELEAD has undergone extensive evaluation in degenerate scenarios and has outperformed existing state-of-the-art LiDAR-Inertial odometry and LiDAR-Visual-Inertial odometry methods.</p></p class="citation"></blockquote><h2 id=eessiv-12>eess.IV (12)</h2><h3 id=112--203254-unsupervised-learning-of-high-resolution-light-field-imaging-via-beam-splitter-based-hybrid-lenses-jianxin-lei-et-al-2024>(1/12 | 203/254) Unsupervised Learning of High-resolution Light Field Imaging via Beam Splitter-based Hybrid Lenses (Jianxin Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianxin Lei, Chengcai Xu, Langqing Shi, Junhui Hou, Ping Zhou. (2024)<br><strong>Unsupervised Learning of High-resolution Light Field Imaging via Beam Splitter-based Hybrid Lenses</strong><br><button class=copy-to-clipboard title="Unsupervised Learning of High-resolution Light Field Imaging via Beam Splitter-based Hybrid Lenses" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Supervised Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19020v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19020v1.pdf filename=2402.19020v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we design a beam splitter-based hybrid light field imaging prototype to record 4D light field image and high-resolution 2D image simultaneously, and make a hybrid light field dataset. The 2D image could be considered as the high-resolution ground truth corresponding to the low-resolution central sub-aperture image of 4D light field image. Subsequently, we propose an <b>unsupervised</b> <b>learning-based</b> super-resolution framework with the hybrid light field dataset, which adaptively settles the light field spatial super-resolution problem with a complex degradation model. Specifically, we design two loss functions based on pre-trained models that enable the super-resolution network to learn the detailed features and light field parallax structure with only one ground truth. Extensive experiments demonstrate the same superiority of our approach with <b>supervised</b> <b>learning-based</b> state-of-the-art ones. To our knowledge, it is the first end-to-end <b>unsupervised</b> <b>learning-based</b> spatial super-resolution approach in light field imaging research, whose input is available from our beam splitter-based hybrid light field system. The hardware and software together may help promote the application of light field super-resolution to a great extent.</p></p class="citation"></blockquote><h3 id=212--204254-graph-convolutional-neural-networks-for-automated-echocardiography-view-recognition-a-holistic-approach-sarina-thomas-et-al-2024>(2/12 | 204/254) Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach (Sarina Thomas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarina Thomas, Cristiana Tiago, Børge Solli Andreassen, Svein Arne Aase, Jurica Šprem, Erik Steen, Anne Solberg, Guy Ben-Yosef. (2024)<br><strong>Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach</strong><br><button class=copy-to-clipboard title="Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 33<br>Keywords: Diffusion Model, Graph, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19062v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19062v2.pdf filename=2402.19062v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To facilitate diagnosis on cardiac ultrasound (US), clinical practice has established several standard views of the heart, which serve as reference points for diagnostic measurements and define viewports from which images are acquired. Automatic view recognition involves grouping those images into classes of standard views. Although deep learning techniques have been successful in achieving this, they still struggle with fully verifying the suitability of an image for specific measurements due to factors like the correct location, pose, and potential occlusions of cardiac structures. Our approach goes beyond view classification and incorporates a 3D mesh reconstruction of the heart that enables several more downstream tasks, like segmentation and pose estimation. In this work, we explore learning 3D heart meshes via <b>graph</b> <b>convolutions,</b> using similar techniques to learn 3D meshes in natural images, such as human pose estimation. As the availability of fully annotated 3D images is limited, we generate synthetic US images from 3D meshes by training an adversarial denoising <b>diffusion</b> <b>model.</b> Experiments were conducted on synthetic and clinical cases for view recognition and structure detection. The approach yielded good performance on synthetic images and, despite being exclusively trained on synthetic data, it already showed potential when applied to clinical images. With this proof-of-concept, we aim to demonstrate the benefits of <b>graphs</b> to improve cardiac view recognition that can ultimately lead to better efficiency in cardiac diagnosis.</p></p class="citation"></blockquote><h3 id=312--205254-sed-semantic-aware-discriminator-for-image-super-resolution-bingchen-li-et-al-2024>(3/12 | 205/254) SeD: Semantic-Aware Discriminator for Image Super-Resolution (Bingchen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingchen Li, Xin Li, Hanxin Zhu, Yeying Jin, Ruoyu Feng, Zhizheng Zhang, Zhibo Chen. (2024)<br><strong>SeD: Semantic-Aware Discriminator for Image Super-Resolution</strong><br><button class=copy-to-clipboard title="SeD: Semantic-Aware Discriminator for Image Super-Resolution" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Adversarial Learning, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19387v1.pdf filename=2402.19387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> have been widely used to recover vivid textures in image super-resolution (SR) tasks. In particular, one discriminator is utilized to enable the SR network to learn the distribution of real-world high-quality images in an <b>adversarial</b> <b>training</b> manner. However, the distribution learning is overly coarse-grained, which is susceptible to virtual textures and causes counter-intuitive generation results. To mitigate this, we propose the simple and effective Semantic-aware Discriminator (denoted as SeD), which encourages the SR network to learn the fine-grained distributions by introducing the semantics of images as a condition. Concretely, we aim to excavate the semantics of images from a well-trained semantic extractor. Under different semantics, the discriminator is able to distinguish the real-fake images individually and adaptively, which guides the SR network to learn the more fine-grained semantic-aware textures. To obtain accurate and abundant semantics, we take full advantage of recently popular pretrained vision models (PVMs) with extensive datasets, and then incorporate its semantic features into the discriminator through a well-designed spatial cross-attention module. In this way, our proposed semantic-aware discriminator empowered the SR network to produce more photo-realistic and pleasing images. Extensive experiments on two typical tasks, i.e., SR and Real SR have demonstrated the effectiveness of our proposed methods.</p></p class="citation"></blockquote><h3 id=412--206254-gdcnet-calibrationless-geometric-distortion-correction-of-echo-planar-imaging-data-using-deep-learning-marina-manso-jimeno-et-al-2024>(4/12 | 206/254) GDCNet: Calibrationless geometric distortion correction of echo planar imaging data using deep learning (Marina Manso Jimeno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marina Manso Jimeno, Keren Bachi, George Gardner, Yasmin L. Hurd, John Thomas Vaughan Jr., Sairam Geethanath. (2024)<br><strong>GDCNet: Calibrationless geometric distortion correction of echo planar imaging data using deep learning</strong><br><button class=copy-to-clipboard title="GDCNet: Calibrationless geometric distortion correction of echo planar imaging data using deep learning" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 23<br>Keywords: Benchmarking, Mutual Information, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18777v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18777v1.pdf filename=2402.18777v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Functional magnetic resonance imaging techniques benefit from echo-planar imaging&rsquo;s fast image acquisition but are susceptible to inhomogeneities in the main magnetic field, resulting in geometric distortion and signal loss artifacts in the images. Traditional methods leverage a field map or voxel displacement map for distortion correction. However, voxel displacement map estimation requires additional sequence acquisitions, and the accuracy of the estimation influences correction performance. This work implements a novel approach called GDCNet, which estimates a geometric distortion map by non-linear registration to T1-weighted anatomical images and applies it for distortion correction. GDCNet demonstrated fast distortion correction of functional images in retrospectively and prospectively acquired datasets. Among the compared models, the 2D <b>self-supervised</b> configuration resulted in a statistically significant improvement to normalized <b>mutual</b> <b>information</b> between distortion-corrected functional and T1-weighted images compared to the <b>benchmark</b> methods FUGUE and TOPUP. Furthermore, GDCNet models achieved processing speeds 14 times faster than TOPUP in the prospective dataset.</p></p class="citation"></blockquote><h3 id=512--207254-towards-generalizable-tumor-synthesis-qi-chen-et-al-2024>(5/12 | 207/254) Towards Generalizable Tumor Synthesis (Qi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Chen, Xiaoxi Chen, Haorui Song, Zhiwei Xiong, Alan Yuille, Chen Wei, Zongwei Zhou. (2024)<br><strong>Towards Generalizable Tumor Synthesis</strong><br><button class=copy-to-clipboard title="Towards Generalizable Tumor Synthesis" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19470v1.pdf filename=2402.19470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tumor synthesis enables the creation of artificial tumors in medical images, facilitating the training of AI models for tumor detection and segmentation. However, success in tumor synthesis hinges on creating visually realistic tumors that are generalizable across multiple organs and, furthermore, the resulting AI models being capable of detecting real tumors in images sourced from different domains (e.g., hospitals). This paper made a progressive stride toward generalizable tumor synthesis by leveraging a critical observation: early-stage tumors (&lt; 2cm) tend to have similar imaging characteristics in computed tomography (CT), whether they originate in the liver, pancreas, or kidneys. We have ascertained that <b>generative</b> <b>AI</b> models, e.g., <b>Diffusion</b> <b>Models,</b> can create realistic tumors generalized to a range of organs even when trained on a limited number of tumor examples from only one organ. Moreover, we have shown that AI models trained on these synthetic tumors can be generalized to detect and segment real tumors from CT volumes, encompassing a broad spectrum of patient demographics, imaging protocols, and healthcare facilities.</p></p class="citation"></blockquote><h3 id=612--208254-wdm-3d-wavelet-diffusion-models-for-high-resolution-medical-image-synthesis-paul-friedrich-et-al-2024>(6/12 | 208/254) WDM: 3D Wavelet Diffusion Models for High-Resolution Medical Image Synthesis (Paul Friedrich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Friedrich, Julia Wolleb, Florentin Bieder, Alicia Durrer, Philippe C. Cattin. (2024)<br><strong>WDM: 3D Wavelet Diffusion Models for High-Resolution Medical Image Synthesis</strong><br><button class=copy-to-clipboard title="WDM: 3D Wavelet Diffusion Models for High-Resolution Medical Image Synthesis" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19043v1.pdf filename=2402.19043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the three-dimensional nature of CT- or MR-scans, generative modeling of medical images is a particularly challenging task. Existing approaches mostly apply patch-wise, slice-wise, or cascaded generation techniques to fit the high-dimensional data into the limited GPU memory. However, these approaches may introduce artifacts and potentially restrict the model&rsquo;s applicability for certain downstream tasks. This work presents WDM, a wavelet-based medical image synthesis framework that applies a <b>diffusion</b> <b>model</b> on wavelet decomposed images. The presented approach is a simple yet effective way of scaling <b>diffusion</b> <b>models</b> to high resolutions and can be trained on a single 40 GB GPU. Experimental results on BraTS and LIDC-IDRI unconditional image generation at a resolution of $128 \times 128 \times 128$ show state-of-the-art image fidelity (FID) and sample diversity (MS-SSIM) scores compared to <b>GANs,</b> <b>Diffusion</b> <b>Models,</b> and Latent <b>Diffusion</b> <b>Models.</b> Our proposed method is the only one capable of generating high-quality images at a resolution of $256 \times 256 \times 256$.</p></p class="citation"></blockquote><h3 id=712--209254-camixersr-only-details-need-more-attention-yan-wang-et-al-2024>(7/12 | 209/254) CAMixerSR: Only Details Need More &lsquo;Attention&rsquo; (Yan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Wang, Shijie Zhao, Yi Liu, Junlin Li, Li Zhang. (2024)<br><strong>CAMixerSR: Only Details Need More &lsquo;Attention&rsquo;</strong><br><button class=copy-to-clipboard title="CAMixerSR: Only Details Need More 'Attention'" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19289v1.pdf filename=2402.19289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To satisfy the rapidly increasing demands on the large image (2K-8K) super-resolution (SR), prevailing methods follow two independent tracks: 1) accelerate existing networks by content-aware routing, and 2) design better super-resolution networks via token mixer refining. Despite directness, they encounter unavoidable defects (e.g., inflexible route or non-discriminative processing) limiting further improvements of quality-complexity trade-off. To erase the drawbacks, we integrate these schemes by proposing a content-aware mixer (CAMixer), which assigns <b>convolution</b> for simple contexts and additional deformable window-attention for sparse textures. Specifically, the CAMixer uses a learnable predictor to generate multiple bootstraps, including offsets for windows warping, a mask for classifying windows, and <b>convolutional</b> attentions for endowing <b>convolution</b> with the dynamic property, which modulates attention to include more useful textures self-adaptively and improves the representation capability of <b>convolution.</b> We further introduce a global classification loss to improve the accuracy of predictors. By simply stacking CAMixers, we obtain CAMixerSR which achieves superior performance on large-image SR, lightweight SR, and omnidirectional-image SR.</p></p class="citation"></blockquote><h3 id=812--210254-training-generative-image-super-resolution-models-by-wavelet-domain-losses-enables-better-control-of-artifacts-cansu-korkmaz-et-al-2024>(8/12 | 210/254) Training Generative Image Super-Resolution Models by Wavelet-Domain Losses Enables Better Control of Artifacts (Cansu Korkmaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cansu Korkmaz, A. Murat Tekalp, Zafer Dogan. (2024)<br><strong>Training Generative Image Super-Resolution Models by Wavelet-Domain Losses Enables Better Control of Artifacts</strong><br><button class=copy-to-clipboard title="Training Generative Image Super-Resolution Models by Wavelet-Domain Losses Enables Better Control of Artifacts" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19215v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19215v1.pdf filename=2402.19215v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Super-resolution (SR) is an ill-posed inverse problem, where the size of the set of feasible solutions that are consistent with a given low-resolution image is very large. Many algorithms have been proposed to find a &ldquo;good&rdquo; solution among the feasible solutions that strike a balance between fidelity and perceptual quality. Unfortunately, all known methods generate artifacts and hallucinations while trying to reconstruct high-frequency (HF) image details. A fundamental question is: Can a model learn to distinguish genuine image details from artifacts? Although some recent works focused on the differentiation of details and artifacts, this is a very challenging problem and a satisfactory solution is yet to be found. This paper shows that the characterization of genuine HF details versus artifacts can be better learned by training <b>GAN-based</b> SR models using wavelet-domain loss functions compared to RGB-domain or Fourier-space losses. Although wavelet-domain losses have been used in the literature before, they have not been used in the context of the SR task. More specifically, we train the discriminator only on the HF wavelet sub-bands instead of on RGB images and the generator is trained by a fidelity loss over wavelet subbands to make it sensitive to the scale and orientation of structures. Extensive experimental results demonstrate that our model achieves better perception-distortion trade-off according to multiple objective measures and visual evaluations.</p></p class="citation"></blockquote><h3 id=912--211254-deep-network-for-image-compressed-sensing-coding-using-local-structural-sampling-wenxue-cui-et-al-2024>(9/12 | 211/254) Deep Network for Image Compressed Sensing Coding Using Local Structural Sampling (Wenxue Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenxue Cui, Xingtao Wang, Xiaopeng Fan, Shaohui Liu, Xinwei Gao, Debin Zhao. (2024)<br><strong>Deep Network for Image Compressed Sensing Coding Using Local Structural Sampling</strong><br><button class=copy-to-clipboard title="Deep Network for Image Compressed Sensing Coding Using Local Structural Sampling" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19111v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19111v1.pdf filename=2402.19111v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing image compressed sensing (CS) coding frameworks usually solve an inverse problem based on measurement coding and optimization-based image reconstruction, which still exist the following two challenges: 1) The widely used random sampling matrix, such as the Gaussian Random Matrix (GRM), usually leads to low measurement coding efficiency. 2) The optimization-based reconstruction methods generally maintain a much higher computational complexity. In this paper, we propose a new <b>CNN</b> based image CS coding framework using local structural sampling (dubbed CSCNet) that includes three functional modules: local structural sampling, measurement coding and Laplacian pyramid reconstruction. In the proposed framework, instead of GRM, a new local structural sampling matrix is first developed, which is able to enhance the correlation between the measurements through a local perceptual sampling strategy. Besides, the designed local structural sampling matrix can be jointly optimized with the other functional modules during training process. After sampling, the measurements with high correlations are produced, which are then coded into final bitstreams by the third-party image codec. At last, a Laplacian pyramid reconstruction network is proposed to efficiently recover the target image from the measurement domain to the image domain. Extensive experimental results demonstrate that the proposed scheme outperforms the existing state-of-the-art CS coding methods, while maintaining fast computational speed.</p></p class="citation"></blockquote><h3 id=1012--212254-variable-rate-learned-image-compression-with-multi-objective-optimization-and-quantization-reconstruction-offsets-fatih-kamisli-et-al-2024>(10/12 | 212/254) Variable-Rate Learned Image Compression with Multi-Objective Optimization and Quantization-Reconstruction Offsets (Fatih Kamisli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatih Kamisli, Fabien Racape, Hyomin Choi. (2024)<br><strong>Variable-Rate Learned Image Compression with Multi-Objective Optimization and Quantization-Reconstruction Offsets</strong><br><button class=copy-to-clipboard title="Variable-Rate Learned Image Compression with Multi-Objective Optimization and Quantization-Reconstruction Offsets" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18930v1.pdf filename=2402.18930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Achieving successful variable bitrate compression with computationally simple algorithms from a single end-to-end learned image or video compression model remains a challenge. Many approaches have been proposed, including conditional auto-encoders, channel-adaptive gains for the latent tensor or uniformly quantizing all elements of the latent tensor. This paper follows the traditional approach to vary a single <b>quantization</b> step size to perform uniform <b>quantization</b> of all latent tensor elements. However, three modifications are proposed to improve the variable rate compression performance. First, multi objective optimization is used for (post) training. Second, a <b>quantization-reconstruction</b> offset is introduced into the <b>quantization</b> operation. Third, variable rate <b>quantization</b> is used also for the hyper latent. All these modifications can be made on a pre-trained single-rate compression model by performing post training. The algorithms are implemented into three well-known image compression models and the achieved variable rate compression results indicate negligible or minimal compression performance loss compared to training multiple models. (Codes will be shared at <a href=https://github.com/InterDigitalInc/CompressAI>https://github.com/InterDigitalInc/CompressAI</a>)</p></p class="citation"></blockquote><h3 id=1112--213254-lolisrflow-joint-single-image-low-light-enhancement-and-super-resolution-via-cross-scale-transformer-based-conditional-flow-ziyu-yue-et-al-2024>(11/12 | 213/254) LoLiSRFlow: Joint Single Image Low-light Enhancement and Super-resolution via Cross-scale Transformer-based Conditional Flow (Ziyu Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyu Yue, Jiaxin Gao, Sihan Xie, Yang Liu, Zhixun Su. (2024)<br><strong>LoLiSRFlow: Joint Single Image Low-light Enhancement and Super-resolution via Cross-scale Transformer-based Conditional Flow</strong><br><button class=copy-to-clipboard title="LoLiSRFlow: Joint Single Image Low-light Enhancement and Super-resolution via Cross-scale Transformer-based Conditional Flow" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18871v1.pdf filename=2402.18871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The visibility of real-world images is often limited by both low-light and low-resolution, however, these issues are only addressed in the literature through Low-Light Enhancement (LLE) and Super- Resolution (SR) methods. Admittedly, a simple cascade of these approaches cannot work harmoniously to cope well with the highly ill-posed problem for simultaneously enhancing visibility and resolution. In this paper, we propose a normalizing flow network, dubbed LoLiSRFLow, specifically designed to consider the degradation mechanism inherent in joint LLE and SR. To break the bonds of the one-to-many mapping for low-light low-resolution images to normal-light high-resolution images, LoLiSRFLow directly learns the conditional probability distribution over a variety of feasible solutions for high-resolution well-exposed images. Specifically, a multi-resolution parallel <b>transformer</b> acts as a conditional encoder that extracts the Retinex-induced resolution-and-illumination invariant map as the previous one. And the invertible network maps the distribution of usually exposed high-resolution images to a latent distribution. The backward inference is equivalent to introducing an additional constrained loss for the normal training route, thus enabling the manifold of the natural exposure of the high-resolution image to be immaculately depicted. We also propose a synthetic dataset modeling the realistic low-light low-resolution degradation, named DFSR-LLE, containing 7100 low-resolution dark-light/high-resolution normal sharp pairs. Quantitative and qualitative experimental results demonstrate the effectiveness of our method on both the proposed synthetic and real datasets.</p></p class="citation"></blockquote><h3 id=1212--214254-anatomy-guided-fiber-trajectory-distribution-estimation-for-cranial-nerves-tractography-lei-xie-et-al-2024>(12/12 | 214/254) Anatomy-guided fiber trajectory distribution estimation for cranial nerves tractography (Lei Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Xie, Qingrun Zeng, Huajun Zhou, Guoqiang Xie, Mingchu Li, Jiahao Huang, Jianan Cui, Hao Chen, Yuanjing Feng. (2024)<br><strong>Anatomy-guided fiber trajectory distribution estimation for cranial nerves tractography</strong><br><button class=copy-to-clipboard title="Anatomy-guided fiber trajectory distribution estimation for cranial nerves tractography" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18856v1.pdf filename=2402.18856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion MRI tractography is an important tool for identifying and analyzing the intracranial course of cranial nerves (CNs). However, the complex environment of the skull base leads to ambiguous spatial correspondence between diffusion directions and fiber <b>geometry,</b> and existing diffusion tractography methods of CNs identification are prone to producing erroneous trajectories and missing true positive connections. To overcome the above challenge, we propose a novel CNs identification framework with anatomy-guided fiber trajectory distribution, which incorporates anatomical shape prior knowledge during the process of CNs tracing to build diffusion tensor vector fields. We introduce higher-order streamline differential equations for continuous flow field representations to directly characterize the fiber trajectory distribution of CNs from the tract-based level. The experimental results on the vivo HCP dataset and the clinical MDM dataset demonstrate that the proposed method reduces false-positive fiber production compared to competing methods and produces reconstructed CNs (i.e. CN II, CN III, CN V, and CN VII/VIII) that are judged to better correspond to the known anatomy.</p></p class="citation"></blockquote><h2 id=cshc-4>cs.HC (4)</h2><h3 id=14--215254-artist-automated-text-simplification-for-task-guidance-in-augmented-reality-guande-wu-et-al-2024>(1/4 | 215/254) ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality (Guande Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guande Wu, Jing Qian, Sonia Castelo, Shaoyu Chen, Joao Rulff, Claudio Silva. (2024)<br><strong>ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality</strong><br><button class=copy-to-clipboard title="ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-1-2; I-2-7, cs-CL, cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: Few-shot, GPT, GPT-3, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18797v1.pdf filename=2402.18797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text presented in augmented reality provides in-situ, real-time information for users. However, this content can be challenging to apprehend quickly when engaging in cognitively demanding AR tasks, especially when it is presented on a head-mounted display. We propose ARTiST, an automatic text simplification system that uses a <b>few-shot</b> <b>prompt</b> and <b>GPT-3</b> models to specifically optimize the text length and semantic content for augmented reality. Developed out of a formative study that included seven users and three experts, our system combines a customized error calibration model with a <b>few-shot</b> <b>prompt</b> to integrate the syntactic, lexical, elaborative, and content simplification techniques, and generate simplified AR text for head-worn displays. Results from a 16-user empirical study showed that ARTiST lightens the cognitive load and improves performance significantly over both unmodified text and text modified via traditional methods. Our work constitutes a step towards automating the optimization of batch text data for readability and performance in augmented reality.</p></p class="citation"></blockquote><h3 id=24--216254-poetsofinstagram-navigating-the-practices-and-challenges-of-novice-poets-on-instagram-ankolika-de-et-al-2024>(2/4 | 216/254) #PoetsOfInstagram: Navigating The Practices And Challenges Of Novice Poets On Instagram (Ankolika De et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ankolika De, Zhicong Lu. (2024)<br><strong>#PoetsOfInstagram: Navigating The Practices And Challenges Of Novice Poets On Instagram</strong><br><button class=copy-to-clipboard title="#PoetsOfInstagram: Navigating The Practices And Challenges Of Novice Poets On Instagram" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-5-m; K-4-0, cs-CY, cs-HC, cs-SI, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19347v1.pdf filename=2402.19347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Commencing as a photo-sharing platform, Instagram has since become multifaceted, accommodating diverse art forms, with poetry emerging as a prominent one. However, the academic understanding of Instagram&rsquo;s poetry community is limited, yet its significance emerges from its distinctive utilization of a primarily visual social media platform guided by <b>recommendation</b> algorithms for disseminating poetry, further characterized by a predominantly novice creative population. We employ qualitative analysis to explore motivations, experiences, and algorithmic influence within Instagram&rsquo;s poetry community. We demonstrate that participants prioritize conforming to algorithmic constraints for visibility, yet maintain their community&rsquo;s values of integrity and originality, illustrating the tension between algorithmic growth and participant authenticity. We introduce the concept of Algorithmically Mediated Creative Labor, a phenomenon specific to non-monetizing creative users who are impacted by the prioritization of professional creators and continually adapt their creative endeavors to align with platform logic, thereby affecting their motivation and creative outputs.</p></p class="citation"></blockquote><h3 id=34--217254-discern-designing-decision-support-interfaces-to-investigate-the-complexities-of-workplace-social-decision-making-with-line-managers-pranav-khadpe-et-al-2024>(3/4 | 217/254) DISCERN: Designing Decision Support Interfaces to Investigate the Complexities of Workplace Social Decision-Making With Line Managers (Pranav Khadpe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pranav Khadpe, Lindy Le, Kate Nowak, Shamsi T. Iqbal, Jina Suh. (2024)<br><strong>DISCERN: Designing Decision Support Interfaces to Investigate the Complexities of Workplace Social Decision-Making With Line Managers</strong><br><button class=copy-to-clipboard title="DISCERN: Designing Decision Support Interfaces to Investigate the Complexities of Workplace Social Decision-Making With Line Managers" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19318v1.pdf filename=2402.19318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Line managers form the first level of management in organizations, and must make complex decisions, while maintaining relationships with those impacted by their decisions. Amidst growing interest in technology-supported decision-making at work, their needs remain understudied. Further, most existing design knowledge for supporting social decision-making comes from domains where decision-makers are more socially detached from those they decide for. We conducted iterative design research with line managers within a technology organization, investigating decision-making practices, and opportunities for technological support. Through formative research, development of a decision-representation tool &ndash; DISCERN &ndash; and user enactments, we identify their communication and analysis needs that lack adequate support. We found they preferred tools for externalizing <b>reasoning</b> rather than tools that replace interpersonal interactions, and they wanted tools to support a range of intuitive and calculative decision-making. We discuss how design of social decision-making supports, especially in the workplace, can more explicitly support highly interactional social decision-making.</p></p class="citation"></blockquote><h3 id=44--218254-think-fast-think-slow-think-critical-designing-an-automated-propaganda-detection-tool-liudmila-zavolokina-et-al-2024>(4/4 | 218/254) Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool (Liudmila Zavolokina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liudmila Zavolokina, Kilian Sprenkamp, Zoya Katashinskaya, Daniel Gordon Jones, Gerhard Schwabe. (2024)<br><strong>Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool</strong><br><button class=copy-to-clipboard title="Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19135v1.pdf filename=2402.19135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s digital age, characterized by rapid news consumption and increasing vulnerability to propaganda, fostering citizens&rsquo; critical thinking is crucial for stable democracies. This paper introduces the design of ClarifAI, a novel automated propaganda detection tool designed to nudge readers towards more critical news consumption by activating the analytical mode of thinking, following Kahneman&rsquo;s dual-system theory of cognition. Using <b>Large</b> <b>Language</b> <b>Models,</b> ClarifAI detects propaganda in news articles and provides context-rich explanations, enhancing users&rsquo; understanding and critical thinking. Our contribution is threefold: first, we propose the design of ClarifAI; second, in an online experiment, we demonstrate that this design effectively encourages news readers to engage in more critical reading; and third, we emphasize the value of explanations for fostering critical thinking. The study thus offers both a practical tool and useful design knowledge for mitigating propaganda in digital news.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--219254-probing-the-information-encoded-in-neural-based-acoustic-models-of-automatic-speech-recognition-systems-quentin-raymondaud-et-al-2024>(1/2 | 219/254) Probing the Information Encoded in Neural-based Acoustic Models of Automatic Speech Recognition Systems (Quentin Raymondaud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quentin Raymondaud, Mickael Rouvier, Richard Dufour. (2024)<br><strong>Probing the Information Encoded in Neural-based Acoustic Models of Automatic Speech Recognition Systems</strong><br><button class=copy-to-clipboard title="Probing the Information Encoded in Neural-based Acoustic Models of Automatic Speech Recognition Systems" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 35<br>Keywords: Black Box, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19443v1.pdf filename=2402.19443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning architectures have made significant progress in terms of performance in many research areas. The <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> field has thus benefited from these scientific and technological advances, particularly for acoustic modeling, now integrating deep neural network architectures. However, these performance gains have translated into increased complexity regarding the information learned and conveyed through these <b>black-box</b> <b>architectures.</b> Following many researches in neural networks interpretability, we propose in this article a protocol that aims to determine which and where information is located in an <b>ASR</b> acoustic model (AM). To do so, we propose to evaluate AM performance on a determined set of tasks using intermediate representations (here, at different layer levels). Regarding the performance variation and targeted tasks, we can emit hypothesis about which information is enhanced or perturbed at different architecture steps. Experiments are performed on both speaker verification, acoustic environment classification, gender classification, tempo-distortion detection systems and <b>speech</b> <b>sentiment/emotion</b> identification. Analysis showed that neural-based AMs hold heterogeneous information that seems surprisingly uncorrelated with phoneme recognition, such as emotion, sentiment or speaker identity. The low-level hidden layers globally appears useful for the structuring of information while the upper ones would tend to delete useless information for phoneme recognition.</p></p class="citation"></blockquote><h3 id=22--220254-unraveling-adversarial-examples-against-speaker-identification----techniques-for-attack-detection-and-victim-model-classification-sonal-joshi-et-al-2024>(2/2 | 220/254) Unraveling Adversarial Examples against Speaker Identification &ndash; Techniques for Attack Detection and Victim Model Classification (Sonal Joshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sonal Joshi, Thomas Thebaud, Jesús Villalba, Najim Dehak. (2024)<br><strong>Unraveling Adversarial Examples against Speaker Identification &ndash; Techniques for Attack Detection and Victim Model Classification</strong><br><button class=copy-to-clipboard title="Unraveling Adversarial Examples against Speaker Identification -- Techniques for Attack Detection and Victim Model Classification" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CR, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19355v1.pdf filename=2402.19355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>examples</b> have proven to threaten speaker identification systems, and several countermeasures against them have been proposed. In this paper, we propose a method to detect the presence of <b>adversarial</b> <b>examples,</b> i.e., a binary classifier distinguishing between benign and <b>adversarial</b> <b>examples.</b> We build upon and extend previous work on attack type classification by exploring new architectures. Additionally, we introduce a method for identifying the victim model on which the <b>adversarial</b> <b>attack</b> is carried out. To achieve this, we generate a new dataset containing multiple attacks performed against various victim models. We achieve an AUC of 0.982 for attack detection, with no more than a 0.03 drop in performance for unknown attacks. Our attack classification accuracy (excluding benign) reaches 86.48% across eight attack types using our LightResNet34 architecture, while our victim model classification accuracy reaches 72.28% across four victim models.</p></p class="citation"></blockquote><h2 id=csni-3>cs.NI (3)</h2><h3 id=13--221254-energy-efficient-uav-swarm-assisted-mec-with-dynamic-clustering-and-scheduling-jialiuyuan-li-et-al-2024>(1/3 | 221/254) Energy-Efficient UAV Swarm Assisted MEC with Dynamic Clustering and Scheduling (Jialiuyuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialiuyuan Li, Jiayuan Chen, Changyan Yi, Tong Zhang, Kun Zhu, Jun Cai. (2024)<br><strong>Energy-Efficient UAV Swarm Assisted MEC with Dynamic Clustering and Scheduling</strong><br><button class=copy-to-clipboard title="Energy-Efficient UAV Swarm Assisted MEC with Dynamic Clustering and Scheduling" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 33<br>Keywords: Clustering, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18936v1.pdf filename=2402.18936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, the energy-efficient unmanned aerial vehicle (UAV) swarm assisted mobile edge computing (MEC) with dynamic <b>clustering</b> and scheduling is studied. In the considered system model, UAVs are divided into multiple swarms, with each swarm consisting of a leader UAV and several follower UAVs to provide computing services to end-users. Unlike existing work, we allow UAVs to dynamically cluster into different swarms, i.e., each follower UAV can change its leader based on the time-varying spatial positions, updated application placement, etc. in a dynamic manner. Meanwhile, UAVs are required to dynamically schedule their energy replenishment, application placement, trajectory planning and task delegation. With the aim of maximizing the long-term energy efficiency of the UAV swarm assisted MEC system, a joint optimization problem of dynamic <b>clustering</b> and scheduling is formulated. Taking into account the underlying cooperation and competition among intelligent UAVs, we further reformulate this optimization problem as a combination of a series of strongly coupled multi-agent stochastic games, and then propose a novel <b>reinforcement</b> <b>learning-based</b> UAV swarm dynamic coordination (RLDC) algorithm for obtaining the equilibrium. <b>Simulations</b> are conducted to evaluate the performance of the RLDC algorithm and demonstrate its superiority over counterparts.</p></p class="citation"></blockquote><h3 id=23--222254-x-resq-reverse-annealing-for-quantum-mimo-detection-with-flexible-parallelism-minsung-kim-et-al-2024>(2/3 | 222/254) X-ResQ: Reverse Annealing for Quantum MIMO Detection with Flexible Parallelism (Minsung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minsung Kim, Abhishek Kumar Singh, Davide Venturelli, John Kaewell, Kyle Jamieson. (2024)<br><strong>X-ResQ: Reverse Annealing for Quantum MIMO Detection with Flexible Parallelism</strong><br><button class=copy-to-clipboard title="X-ResQ: Reverse Annealing for Quantum MIMO Detection with Flexible Parallelism" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, quant-ph<br>Keyword Score: 10<br>Keywords: Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18778v1.pdf filename=2402.18778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum Annealing <b>(QA)-accelerated</b> MIMO detection is an emerging research approach in the context of NextG wireless networks. The opportunity is to enable large MIMO systems and thus improve wireless performance. The approach aims to leverage <b>QA</b> to expedite the computation required for theoretically optimal but computationally-demanding Maximum Likelihood detection to overcome the limitations of the currently deployed linear detectors. This paper presents \textbf{X-ResQ}, a <b>QA-based</b> MIMO detector system featuring fine-grained quantum task parallelism that is uniquely enabled by the Reverse Annealing (RA) protocol. Unlike prior designs, X-ResQ has many desirable system properties for a parallel <b>QA</b> detector and has effectively improved detection performance as more qubits are assigned. In our evaluations on a state-of-the-art quantum annealer, fully parallel X-ResQ achieves near-optimal throughput (over 10 bits/s/Hz) for $4\times6$ MIMO with 16-QAM using six levels of parallelism with 240 qubits and $220~\mu$s <b>QA</b> compute time, achieving 2.5&ndash;5$\times$ gains compared against other tested detectors. For more comprehensive evaluations, we implement and evaluate X-ResQ in the non-quantum digital setting. This non-quantum X-ResQ demonstration showcases the potential to realize ultra-large $1024\times1024$ MIMO, significantly outperforming other MIMO detectors, including the state-of-the-art RA detector classically implemented in the same way.</p></p class="citation"></blockquote><h3 id=33--223254-vision-radio-experimental-infrastructure-architecture-towards-6g-filipe-b-teixeira-et-al-2024>(3/3 | 223/254) Vision-Radio Experimental Infrastructure Architecture Towards 6G (Filipe B. Teixeira et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filipe B. Teixeira, Manuel Ricardo, André Coelho, Hélder P. Oliveira, Paula Viana, Nuno Paulino, Helder Fontes, Paulo Marques, Rui Campos, Luis M. Pessoa. (2024)<br><strong>Vision-Radio Experimental Infrastructure Architecture Towards 6G</strong><br><button class=copy-to-clipboard title="Vision-Radio Experimental Infrastructure Architecture Towards 6G" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19416v1.pdf filename=2402.19416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Telecommunications and computer vision have evolved separately so far. Yet, with the shift to sub-terahertz (sub-THz) and terahertz (THz) radio communications, there is an opportunity to explore computer vision technologies together with radio communications, considering the dependency of both technologies on Line of Sight. The combination of radio sensing and computer vision can address challenges such as obstructions and poor lighting. Also, machine learning algorithms, capable of processing <b>multimodal</b> data, play a crucial role in deriving insights from raw and low-level sensing data, offering a new level of abstraction that can enhance various applications and use cases such as beamforming and terminal handovers. This paper introduces CONVERGE, a pioneering vision-radio paradigm that bridges this gap by leveraging Integrated Sensing and Communication (ISAC) to facilitate a dual &ldquo;View-to-Communicate, Communicate-to-View&rdquo; approach. CONVERGE offers tools that merge wireless communications and computer vision, establishing a novel Research Infrastructure (RI) that will be open to the scientific community and capable of providing open datasets. This new infrastructure will support future research in 6G and beyond concerning multiple verticals, such as telecommunications, automotive, manufacturing, media, and health.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=12--224254-flexllm-a-system-for-co-serving-large-language-model-inference-and-parameter-efficient-finetuning-xupeng-miao-et-al-2024>(1/2 | 224/254) FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning (Xupeng Miao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xupeng Miao, Gabriele Oliaro, Xinhao Cheng, Mengdi Wu, Colin Unger, Zhihao Jia. (2024)<br><strong>FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning</strong><br><button class=copy-to-clipboard title="FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-CL, cs-DC, cs-LG, cs.DC<br>Keyword Score: 33<br>Keywords: Graph, Fine-tuning, Pruning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18789v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18789v1.pdf filename=2402.18789v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient <b>finetuning</b> (PEFT) is a widely used technique to adapt <b>large</b> <b>language</b> <b>models</b> for different tasks. Service providers typically create separate systems for users to perform PEFT model <b>finetuning</b> and inference tasks. This is because existing systems cannot handle workloads that include a mix of inference and PEFT <b>finetuning</b> requests. As a result, shared GPU resources are underutilized, leading to inefficiencies. To address this problem, we present FlexLLM, the first system that can serve inference and parameter-efficient <b>finetuning</b> requests in the same iteration. Our system leverages the complementary nature of these two tasks and utilizes shared GPU resources to run them jointly, using a method called co-serving. To achieve this, FlexLLM introduces a novel token-level <b>finetuning</b> mechanism, which breaks down the <b>finetuning</b> computation of a sequence into smaller token-level computations and uses dependent parallelization and <b>graph</b> <b>pruning,</b> two static compilation optimizations, to minimize the memory overhead and latency for co-serving. Compared to existing systems, FlexLLM&rsquo;s co-serving approach reduces the activation GPU memory overhead by up to 8x, and the end-to-end GPU memory requirement of <b>finetuning</b> by up to 36% while maintaining a low inference latency and improving <b>finetuning</b> throughput. For example, under a heavy inference workload, FlexLLM can still preserve more than 80% of the peak <b>finetuning</b> throughput, whereas existing systems cannot make any progress with <b>finetuning.</b> The source code of FlexLLM is publicly available at <a href=https://github.com/flexflow/FlexFlow>https://github.com/flexflow/FlexFlow</a>.</p></p class="citation"></blockquote><h3 id=22--225254-arrow-matrix-decomposition-a-novel-approach-for-communication-efficient-sparse-matrix-multiplication-lukas-gianinazzi-et-al-2024>(2/2 | 225/254) Arrow Matrix Decomposition: A Novel Approach for Communication-Efficient Sparse Matrix Multiplication (Lukas Gianinazzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas Gianinazzi, Alexandros Nikolaos Ziogas, Langwen Huang, Piotr Luczynski, Saleh Ashkboos, Florian Scheidl, Armon Carigiet, Chio Ge, Nabil Abubaker, Maciej Besta, Tal Ben-Nun, Torsten Hoefler. (2024)<br><strong>Arrow Matrix Decomposition: A Novel Approach for Communication-Efficient Sparse Matrix Multiplication</strong><br><button class=copy-to-clipboard title="Arrow Matrix Decomposition: A Novel Approach for Communication-Efficient Sparse Matrix Multiplication" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: F-2-1, cs-DC, cs.DC<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19364v1.pdf filename=2402.19364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel approach to iterated sparse matrix dense matrix multiplication, a fundamental computational kernel in scientific computing and <b>graph</b> <b>neural</b> <b>network</b> training. In cases where matrix sizes exceed the memory of a single compute node, data transfer becomes a bottleneck. An approach based on dense matrix multiplication algorithms leads to suboptimal scalability and fails to exploit the sparsity in the problem. To address these challenges, we propose decomposing the sparse matrix into a small number of highly structured matrices called arrow matrices, which are connected by permutations. Our approach enables communication-avoiding multiplications, achieving a polynomial reduction in communication volume per iteration for matrices corresponding to planar <b>graphs</b> <b>and</b> <b>other</b> minor-excluded families of <b>graphs.</b> <b>Our</b> <b>evaluation</b> demonstrates that our approach outperforms a state-of-the-art method for sparse matrix multiplication on matrices with hundreds of millions of rows, offering near-linear strong and weak scaling.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--226254-statistical-estimation-in-the-spiked-tensor-model-via-the-quantum-approximate-optimization-algorithm-leo-zhou-et-al-2024>(1/1 | 226/254) Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm (Leo Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leo Zhou, Joao Basso, Song Mei. (2024)<br><strong>Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm</strong><br><button class=copy-to-clipboard title="Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-DS, math-PR, math-ST, quant-ph, quant-ph, stat-TH<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19456v1.pdf filename=2402.19456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The quantum approximate optimization algorithm (QAOA) is a general-purpose algorithm for combinatorial optimization. In this paper, we analyze the performance of the QAOA on a statistical estimation problem, namely, the spiked tensor model, which exhibits a statistical-computational gap classically. We prove that the weak recovery threshold of $1$-step QAOA matches that of $1$-step tensor power iteration. Additional heuristic calculations suggest that the weak recovery threshold of $p$-step QAOA matches that of $p$-step tensor power iteration when $p$ is a fixed constant. This further implies that multi-step QAOA with tensor unfolding could achieve, but not surpass, the classical computation threshold $\Theta(n^{(q-2)/4})$ for spiked $q$-tensors. Meanwhile, we characterize the asymptotic overlap distribution for $p$-step QAOA, finding an intriguing sine-Gaussian law verified through <b>simulations.</b> For some $p$ and $q$, the QAOA attains an overlap that is larger by a constant factor than the tensor power iteration overlap. Of independent interest, our proof techniques employ the Fourier transform to handle difficult combinatorial sums, a novel approach differing from prior QAOA analyses on spin-glass models without planted structure.</p></p class="citation"></blockquote><h2 id=mathap-1>math.AP (1)</h2><h3 id=11--227254-high-multiplicity-of-positive-solutions-in-a-superlinear-problem-of-moore-nehari-type-pablo-cubillos-et-al-2024>(1/1 | 227/254) High multiplicity of positive solutions in a superlinear problem of Moore-Nehari type (Pablo Cubillos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pablo Cubillos, Julián López-Gómez, Andrea Tellini. (2024)<br><strong>High multiplicity of positive solutions in a superlinear problem of Moore-Nehari type</strong><br><button class=copy-to-clipboard title="High multiplicity of positive solutions in a superlinear problem of Moore-Nehari type" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.AP<br>Categories: 35J25, 34B08, 35J60, 65N06, 65P30, cs-NA, math-AP, math-CA, math-NA, math.AP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19084v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19084v1.pdf filename=2402.19084v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we consider a superlinear one-dimensional elliptic boundary value problem that generalizes the one studied by Moore and Nehari in [43]. Specifically, we deal with piecewise-constant weight functions in front of the nonlinearity with an arbitrary number $\kappa\geq 1$ of vanishing regions. We study, from an analytic and numerical point of view, the number of positive solutions, depending on the value of a parameter $\lambda$ and on $\kappa$. Our main results are twofold. On the one hand, we study analytically the behavior of the solutions, as $\lambda\downarrow-\infty$, in the regions where the weight vanishes. Our result leads us to conjecture the existence of $2^{\kappa+1}-1$ solutions for sufficiently negative $\lambda$. On the other hand, we support such a conjecture with the results of numerical <b>simulations</b> which also shed light on the structure of the global bifurcation diagrams in $\lambda$ and the profiles of positive solutions. Finally, we give additional numerical results suggesting that the same high multiplicity result holds true for a much larger class of weights, also arbitrarily close to situations where there is uniqueness of positive solutions.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--228254-spyx-a-library-for-just-in-time-compiled-optimization-of-spiking-neural-networks-kade-m-heckel-et-al-2024>(1/2 | 228/254) Spyx: A Library for Just-In-Time Compiled Optimization of Spiking Neural Networks (Kade M. Heckel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kade M. Heckel, Thomas Nowotny. (2024)<br><strong>Spyx: A Library for Just-In-Time Compiled Optimization of Spiking Neural Networks</strong><br><button class=copy-to-clipboard title="Spyx: A Library for Just-In-Time Compiled Optimization of Spiking Neural Networks" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: I-2-5, cs-LG, cs-NE, cs.NE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18994v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18994v1.pdf filename=2402.18994v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the role of artificial intelligence becomes increasingly pivotal in modern society, the efficient training and deployment of deep neural networks have emerged as critical areas of focus. Recent advancements in attention-based large neural architectures have spurred the development of AI accelerators, facilitating the training of extensive, multi-billion parameter models. Despite their effectiveness, these powerful networks often incur high execution costs in production environments. Neuromorphic computing, inspired by biological neural processes, offers a promising alternative. By utilizing temporally-sparse computations, Spiking Neural Networks (SNNs) offer to enhance energy efficiency through a reduced and low-power hardware footprint. However, the training of SNNs can be challenging due to their recurrent nature which cannot as easily leverage the massive parallelism of modern AI accelerators. To facilitate the investigation of SNN architectures and dynamics researchers have sought to bridge Python-based deep learning frameworks such as PyTorch or TensorFlow with custom-implemented compute kernels. This paper introduces Spyx, a new and lightweight SNN <b>simulation</b> and optimization library designed in JAX. By pre-staging data in the expansive vRAM of contemporary accelerators and employing extensive JIT compilation, Spyx allows for SNN optimization to be executed as a unified, low-level program on NVIDIA GPUs or Google TPUs. This approach achieves optimal hardware utilization, surpassing the performance of many existing SNN training frameworks while maintaining considerable flexibility.</p></p class="citation"></blockquote><h3 id=22--229254-airport-take-off-and-landing-optimization-through-genetic-algorithms-fernando-guedan-pecker-et-al-2024>(2/2 | 229/254) Airport take-off and landing optimization through genetic algorithms (Fernando Guedan Pecker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fernando Guedan Pecker, Cristian Ramirez Atencia. (2024)<br><strong>Airport take-off and landing optimization through genetic algorithms</strong><br><button class=copy-to-clipboard title="Airport take-off and landing optimization through genetic algorithms" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19222v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19222v1.pdf filename=2402.19222v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research addresses the crucial issue of pollution from aircraft operations, focusing on optimizing both gate allocation and runway scheduling simultaneously, a novel approach not previously explored. The study presents an innovative genetic algorithm-based method for minimizing pollution from fuel combustion during aircraft take-off and landing at airports. This algorithm uniquely integrates the optimization of both landing gates and take-off/landing runways, considering the correlation between engine operation time and pollutant levels. The approach employs advanced constraint handling techniques to manage the intricate time and resource limitations inherent in airport operations. Additionally, the study conducts a thorough sensitivity analysis of the model, with a particular emphasis on the mutation factor and the type of penalty function, to <b>fine-tune</b> the optimization process. This dual-focus optimization strategy represents a significant advancement in reducing environmental impact in the aviation sector, establishing a new standard for comprehensive and efficient airport operation management.</p></p class="citation"></blockquote><h2 id=csit-3>cs.IT (3)</h2><h3 id=13--230254-helper-data-schemes-for-coded-modulation-and-shaping-in-physical-unclonable-functions-robert-f-h-fischer-2024>(1/3 | 230/254) Helper Data Schemes for Coded Modulation and Shaping in Physical Unclonable Functions (Robert F. H. Fischer, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert F. H. Fischer. (2024)<br><strong>Helper Data Schemes for Coded Modulation and Shaping in Physical Unclonable Functions</strong><br><button class=copy-to-clipboard title="Helper Data Schemes for Coded Modulation and Shaping in Physical Unclonable Functions" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18980v1.pdf filename=2402.18980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider the generation and utilization of helper data for physical unclonable functions (PUFs) that provide real-valued readout symbols. Compared to classical binary PUFs, more entropy can be extracted from each basic building block (PUF node), resulting in longer keys/fingerprints and/or a higher reliability. To this end, a coded modulation and signal shaping scheme that matches the (approximately) Gaussian distribution of the readout has to be employed. A new helper data scheme is proposed that works with any type of coded modulation/shaping scheme. Compared to the permutation scheme from the literature, less amount of helper data has to be generated and a higher reliability is achieved. Moreover, the recently proposed idea of a two-metric helper data scheme is generalized to coded modulation and a general S-metric scheme. It is shown how extra helper data can be generated to improve decodability. The proposed schemes are assessed by numerical <b>simulations</b> and by evaluation of measurement data. We compare multi-level codes using a new rate design strategy with bit-interleaved coded modulation and trellis shaping with a distribution matcher. By selecting a suitable design, the rate per PUF node that can be reliably extracted can be as high as 2~bit/node.</p></p class="citation"></blockquote><h3 id=23--231254-digital-twin-aided-massive-mimo-csi-compression-and-feedback-shuaifeng-jiang-et-al-2024>(2/3 | 231/254) Digital Twin Aided Massive MIMO: CSI Compression and Feedback (Shuaifeng Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuaifeng Jiang, Ahmed Alkhateeb. (2024)<br><strong>Digital Twin Aided Massive MIMO: CSI Compression and Feedback</strong><br><button class=copy-to-clipboard title="Digital Twin Aided Massive MIMO: CSI Compression and Feedback" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19434v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19434v2.pdf filename=2402.19434v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning (DL) approaches have demonstrated high performance in compressing and reconstructing the channel state information (CSI) and reducing the CSI feedback overhead in massive MIMO systems. One key challenge, however, with the DL approaches is the demand for extensive training data. Collecting this real-world CSI data incurs significant overhead that hinders the DL approaches from scaling to a large number of communication sites. To address this challenge, we propose a novel direction that utilizes site-specific \textit{digital twins} to aid the training of DL models. The proposed digital twin approach generates site-specific synthetic CSI data from the EM 3D model and ray tracing, which can then be used to train the DL model without real-world data collection. To further improve the performance, we adopt online data selection to refine the DL model training with a small real-world CSI dataset. Results show that a DL model trained solely on the digital twin data can achieve high performance when tested in a real-world deployment. Further, leveraging <b>domain</b> <b>adaptation</b> techniques, the proposed approach requires orders of magnitude less real-world data to approach the same performance of the model trained completely on a real-world CSI dataset.</p></p class="citation"></blockquote><h3 id=33--232254-evaluating-the-gilbert-varshamov-bound-for-constrained-systems-keshav-goyal-et-al-2024>(3/3 | 232/254) Evaluating the Gilbert-Varshamov Bound for Constrained Systems (Keshav Goyal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keshav Goyal, Han Mao Kiah. (2024)<br><strong>Evaluating the Gilbert-Varshamov Bound for Constrained Systems</strong><br><button class=copy-to-clipboard title="Evaluating the Gilbert-Varshamov Bound for Constrained Systems" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-DM, cs-IT, cs.IT, math-CO, math-IT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18869v1.pdf filename=2402.18869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We revisit the well-known Gilbert-Varshamov (GV) bound for constrained systems. In 1991, Kolesnik and Krachkovsky showed that GV bound can be determined via the solution of some optimization problem. Later, Marcus and Roth (1992) modified the optimization problem and improved the GV bound in many instances. In this work, we provide explicit numerical procedures to solve these two optimization problems and hence, compute the bounds. We then show the procedures can be further simplified when we plot the respective curves. In the case where the <b>graph</b> presentation comprise a single state, we provide explicit formulas for both bounds.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--233254-message-enhanced-degroot-model-huisheng-wang-et-al-2024>(1/1 | 233/254) Message-Enhanced DeGroot Model (Huisheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huisheng Wang, Zhanjiang Chen, H. Vicky Zhao. (2024)<br><strong>Message-Enhanced DeGroot Model</strong><br><button class=copy-to-clipboard title="Message-Enhanced DeGroot Model" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-SI, cs-SY, eess-SP, eess-SY, eess.SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18867v1.pdf filename=2402.18867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the impact of messages on agents&rsquo; opinions over social networks is important. However, to our best knowledge, there has been limited quantitative investigation into this phenomenon in the prior works. To address this gap, this paper proposes the Message-Enhanced DeGroot model. The Bounded Brownian Message model provides a quantitative description of the message evolution, jointly considering temporal continuity, randomness, and polarization from mass media theory. The Message-Enhanced DeGroot model, combining the Bounded Brownian Message model with the traditional DeGroot model, quantitatively describes the evolution of agents&rsquo; opinions under the influence of messages. We theoretically study the probability distribution and statistics of the messages and agents&rsquo; opinions and quantitatively analyze the impact of messages on opinions. We also conduct <b>simulations</b> to validate our analyses.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--234254-link-recommendation-to-augment-influence-diffusion-with-provable-guarantees-xiaolong-chen-et-al-2024>(1/2 | 234/254) Link Recommendation to Augment Influence Diffusion with Provable Guarantees (Xiaolong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaolong Chen, Yifan Song, Jing Tang. (2024)<br><strong>Link Recommendation to Augment Influence Diffusion with Provable Guarantees</strong><br><button class=copy-to-clipboard title="Link Recommendation to Augment Influence Diffusion with Provable Guarantees" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 13<br>Keywords: Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19189v1.pdf filename=2402.19189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Link <b>recommendation</b> systems in online social networks (OSNs), such as Facebook&rsquo;s <code>People You May Know'', Twitter's </code>Who to Follow&rsquo;&rsquo;, and Instagram&rsquo;s ``Suggested Accounts&rsquo;&rsquo;, facilitate the formation of new connections among users. This paper addresses the challenge of link <b>recommendation</b> for the purpose of social influence maximization. In particular, given a <b>graph</b> $G$ and the seed set $S$, our objective is to select $k$ edges that connect seed nodes and ordinary nodes to optimize the influence dissemination of the seed set. This problem, referred to as influence maximization with augmentation (IMA), has been proven to be NP-hard. In this paper, we propose an algorithm, namely \textsf{AIS}, consisting of an efficient estimator for augmented influence estimation and an accelerated sampling approach. \textsf{AIS} provides a $(1-1/\mathrm{e}-\varepsilon)$-approximate solution with a high probability of $1-\delta$, and runs in $O(k^2 (m+n) \log (n / \delta) / \varepsilon^2 + k \left|E_{\mathcal{C}}\right|)$ time assuming that the influence of any singleton node is smaller than that of the seed set. To the best of our knowledge, this is the first algorithm that can be implemented on large <b>graphs</b> containing millions of nodes while preserving strong theoretical guarantees. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed algorithm.</p></p class="citation"></blockquote><h3 id=22--235254-higher-order-networks-representation-and-learning-a-survey-hao-tian-et-al-2024>(2/2 | 235/254) Higher-Order Networks Representation and Learning: A Survey (Hao Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Tian, Reza Zafarani. (2024)<br><strong>Higher-Order Networks Representation and Learning: A Survey</strong><br><button class=copy-to-clipboard title="Higher-Order Networks Representation and Learning: A Survey" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: 68Q06, A-1; I-5-1, cs-DS, cs-SI, cs.SI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19414v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19414v1.pdf filename=2402.19414v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Network data has become widespread, larger, and more complex over the years. Traditional network data is dyadic, capturing the relations among pairs of entities. With the need to model interactions among more than two entities, significant research has focused on higher-order networks and ways to represent, analyze, and learn from them. There are two main directions to studying higher-order networks. One direction has focused on capturing higher-order patterns in traditional (dyadic) <b>graphs</b> by changing the basic unit of study from nodes to small frequently observed subgraphs, called motifs. As most existing network data comes in the form of pairwise dyadic relationships, studying higher-order structures within such <b>graphs</b> may uncover new insights. The second direction aims to directly model higher-order interactions using new and more complex representations such as simplicial complexes or hypergraphs. Some of these models have long been proposed, but improvements in computational power and the advent of new computational techniques have increased their popularity. Our goal in this paper is to provide a succinct yet comprehensive summary of the advanced higher-order network analysis techniques. We provide a systematic review of its foundations and algorithms, along with use cases and applications of higher-order networks in various scientific domains.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=11--236254-efficient-processing-of-subsequent-densest-subgraph-query-chia-yang-hung-et-al-2024>(1/1 | 236/254) Efficient Processing of Subsequent Densest Subgraph Query (Chia-Yang Hung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chia-Yang Hung, Chih-Ya Shen. (2024)<br><strong>Efficient Processing of Subsequent Densest Subgraph Query</strong><br><button class=copy-to-clipboard title="Efficient Processing of Subsequent Densest Subgraph Query" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: 68W27, cs-DS, cs.DS<br>Keyword Score: 13<br>Keywords: Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18883v1.pdf filename=2402.18883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dense subgraph extraction is a fundamental problem in <b>graph</b> analysis and data mining, aimed at identifying cohesive and densely connected substructures within a given <b>graph.</b> It plays a crucial role in various domains, including social network analysis, biological network analysis, <b>recommendation</b> systems, and community detection. However, extracting a subgraph with the highest node similarity is a lack of exploration. To address this problem, we studied the Member Selection Problem and extended it with a dynamic constraint variant. By incorporating dynamic constraints, our algorithm can adapt to changing conditions or requirements, allowing for more flexible and personalized subgraph extraction. This approach enables the algorithm to provide tailored solutions that meet specific needs, even in scenarios where constraints may vary over time. We also provide the theoretical analysis to show that our algorithm is 1/3-approximation. Eventually, the experiments show that our algorithm is effective and efficient in tackling the member selection problem with dynamic constraints.</p></p class="citation"></blockquote><h2 id=statml-1>stat.ML (1)</h2><h3 id=11--237254-listening-to-the-noise-blind-denoising-with-gibbs-diffusion-david-heurtel-depeiges-et-al-2024>(1/1 | 237/254) Listening to the Noise: Blind Denoising with Gibbs Diffusion (David Heurtel-Depeiges et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Heurtel-Depeiges, Charles C. Margossian, Ruben Ohana, Bruno Régaldo-Saint Blancard. (2024)<br><strong>Listening to the Noise: Blind Denoising with Gibbs Diffusion</strong><br><button class=copy-to-clipboard title="Listening to the Noise: Blind Denoising with Gibbs Diffusion" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: astro-ph-CO, cs-CV, cs-LG, eess-SP, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19455v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19455v1.pdf filename=2402.19455v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, denoising problems have become intertwined with the development of deep generative models. In particular, <b>diffusion</b> <b>models</b> are trained like denoisers, and the distribution they model coincide with denoising priors in the Bayesian picture. However, denoising through <b>diffusion-based</b> <b>posterior</b> sampling requires the noise level and covariance to be known, preventing blind denoising. We overcome this limitation by introducing Gibbs <b>Diffusion</b> <b>(GDiff),</b> a general methodology addressing posterior sampling of both the signal and the noise parameters. Assuming arbitrary parametric Gaussian noise, we develop a Gibbs algorithm that alternates sampling steps from a conditional <b>diffusion</b> <b>model</b> trained to map the signal prior to the family of noise distributions, and a Monte Carlo sampler to infer the noise parameters. Our theoretical analysis highlights potential pitfalls, guides diagnostic usage, and quantifies errors in the Gibbs stationary distribution caused by the <b>diffusion</b> <b>model.</b> We showcase our method for 1) blind denoising of natural images involving colored noises with unknown amplitude and spectral index, and 2) a cosmology problem, namely the analysis of cosmic microwave background data, where Bayesian inference of &ldquo;noise&rdquo; parameters means constraining models of the evolution of the Universe.</p></p class="citation"></blockquote><h2 id=csgt-2>cs.GT (2)</h2><h3 id=12--238254-understanding-iterative-combinatorial-auction-designs-via-multi-agent-reinforcement-learning-greg-deon-et-al-2024>(1/2 | 238/254) Understanding Iterative Combinatorial Auction Designs via Multi-Agent Reinforcement Learning (Greg d&rsquo;Eon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Greg d&rsquo;Eon, Neil Newman, Kevin Leyton-Brown. (2024)<br><strong>Understanding Iterative Combinatorial Auction Designs via Multi-Agent Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Understanding Iterative Combinatorial Auction Designs via Multi-Agent Reinforcement Learning" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-AI, cs-GT, cs-MA, cs.GT<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19420v1.pdf filename=2402.19420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Iterative combinatorial auctions are widely used in high stakes settings such as spectrum auctions. Such auctions can be hard to understand analytically, making it difficult for bidders to determine how to behave and for designers to optimize auction rules to ensure desirable outcomes such as high revenue or welfare. In this paper, we investigate whether multi-agent <b>reinforcement</b> <b>learning</b> (MARL) algorithms can be used to understand iterative combinatorial auctions, given that these algorithms have recently shown empirical success in several other domains. We find that MARL can indeed benefit auction analysis, but that deploying it effectively is nontrivial. We begin by describing modelling decisions that keep the resulting game tractable without sacrificing important features such as imperfect information or asymmetry between bidders. We also discuss how to navigate pitfalls of various MARL algorithms, how to overcome challenges in verifying convergence, and how to generate and interpret multiple equilibria. We illustrate the promise of our resulting approach by using it to evaluate a specific rule change to a clock auction, finding substantially different auction outcomes due to complex changes in bidders&rsquo; behavior.</p></p class="citation"></blockquote><h3 id=22--239254-conjectural-online-learning-with-first-order-beliefs-in-asymmetric-information-stochastic-games-tao-li-et-al-2024>(2/2 | 239/254) Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games (Tao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Li, Kim Hammar, Rolf Stadler, Quanyan Zhu. (2024)<br><strong>Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games</strong><br><button class=copy-to-clipboard title="Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-LG, cs-SY, cs.GT, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18781v1.pdf filename=2402.18781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stochastic games arise in many complex socio-technical systems, such as cyber-physical systems and IT infrastructures, where information asymmetry presents challenges for decision-making entities (players). Existing computational methods for asymmetric information stochastic games (AISG) are primarily offline, targeting special classes of AISGs to avoid belief hierarchies, and lack online adaptability to deviations from equilibrium. To address this limitation, we propose a conjectural online learning (COL), a learning scheme for generic AISGs. COL, structured as a forecaster-actor-critic (FAC) architecture, utilizes first-order beliefs over the hidden states and subjective forecasts of the opponent&rsquo;s strategies. Against the conjectured opponent, COL updates strategies in an actor-critic approach using online rollout and calibrates conjectures through Bayesian learning. We prove that conjecture in COL is asymptotically consistent with the information feedback in the sense of a relaxed Bayesian consistency. The resulting empirical strategy profile converges to the Berk-Nash equilibrium, a solution concept characterizing rationality under subjectivity. Experimental results from an intrusion response use case demonstrate COL&rsquo;s superiority over state-of-the-art <b>reinforcement</b> <b>learning</b> methods against nonstationary attacks.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--240254-deep-reinforcement-learning-a-convex-optimization-approach-ather-gattami-2024>(1/1 | 240/254) Deep Reinforcement Learning: A Convex Optimization Approach (Ather Gattami, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ather Gattami. (2024)<br><strong>Deep Reinforcement Learning: A Convex Optimization Approach</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning: A Convex Optimization Approach" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19212v1.pdf filename=2402.19212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider <b>reinforcement</b> <b>learning</b> of nonlinear systems with continuous state and action spaces. We present an episodic learning algorithm, where we for each episode use convex optimization to find a two-layer neural network approximation of the optimal $Q$-function. The convex optimization approach guarantees that the weights calculated at each episode are optimal, with respect to the given sampled states and actions of the current episode. For stable nonlinear systems, we show that the algorithm converges and that the converging parameters of the trained neural network can be made arbitrarily close to the optimal neural network parameters. In particular, if the regularization parameter is $\rho$ and the time horizon is $T$, then the parameters of the trained neural network converge to $w$, where the distance between $w$ from the optimal parameters $w^\star$ is bounded by $\mathcal{O}(\rho T^{-1})$. That is, when the number of episodes goes to infinity, there exists a constant $C$ such that [|w-w^\star| \le C\cdot\frac{\rho}{T}.] In particular, our algorithm converges arbitrarily close to the optimal neural network parameters as the time horizon increases or as the regularization parameter decreases.</p></p class="citation"></blockquote><h2 id=cslo-4>cs.LO (4)</h2><h3 id=14--241254-rewriting-and-inductive-reasoning-márton-hajdu-et-al-2024>(1/4 | 241/254) Rewriting and Inductive Reasoning (Márton Hajdu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Márton Hajdu, Laura Kovács, Michael Rawson. (2024)<br><strong>Rewriting and Inductive Reasoning</strong><br><button class=copy-to-clipboard title="Rewriting and Inductive Reasoning" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19199v1.pdf filename=2402.19199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rewriting techniques based on reduction orderings generate &ldquo;just enough&rdquo; consequences to retain first-order completeness. This is ideal for superposition-based first-order theorem proving, but for at least one approach to inductive <b>reasoning</b> we show that we are missing crucial consequences. We therefore extend the superposition calculus with rewriting-based techniques to generate sufficient consequences for automating induction in saturation. When applying our work within the unit-equational fragment, our experiments with the theorem prover Vampire show significant improvements for inductive <b>reasoning.</b></p></p class="citation"></blockquote><h3 id=24--242254-program-synthesis-in-saturation-petra-hozzová-et-al-2024>(2/4 | 242/254) Program Synthesis in Saturation (Petra Hozzová et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Petra Hozzová, Laura Kovács, Chase Norman, Andrei Voronkov. (2024)<br><strong>Program Synthesis in Saturation</strong><br><button class=copy-to-clipboard title="Program Synthesis in Saturation" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18962v1.pdf filename=2402.18962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an automated <b>reasoning</b> framework for synthesizing recursion-free programs using saturation-based theorem proving. Given a functional specification encoded as a first-order logical formula, we use a first-order theorem prover to both establish validity of this formula and discover program fragments satisfying the specification. As a result, when deriving a proof of program correctness, we also synthesize a program that is correct with respect to the given specification. We describe properties of the calculus that a saturation-based prover capable of synthesis should employ, and extend the superposition calculus in a corresponding way. We implemented our work in the first-order prover Vampire, extending the successful applicability of first-order proving to program synthesis. This is an extended version of an Automated Deduction &ndash; CADE 29 paper with the same title and the same authors.</p></p class="citation"></blockquote><h3 id=34--243254-getting-saturated-with-induction-márton-hajdu-et-al-2024>(3/4 | 243/254) Getting Saturated with Induction (Márton Hajdu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Márton Hajdu, Petra Hozzová, Laura Kovács, Giles Reger, Andrei Voronkov. (2024)<br><strong>Getting Saturated with Induction</strong><br><button class=copy-to-clipboard title="Getting Saturated with Induction" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18954v1.pdf filename=2402.18954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Induction in saturation-based first-order theorem proving is a new exciting direction in the automation of inductive <b>reasoning.</b> In this paper we survey our work on integrating induction directly into the saturation-based proof search framework of first-order theorem proving. We describe our induction inference rules proving properties with inductively defined datatypes and integers. We also present additional <b>reasoning</b> heuristics for strengthening inductive <b>reasoning,</b> as well as for using induction hypotheses and recursive function definitions for guiding induction. We present exhaustive experimental results demonstrating the practical impact of our approach as implemented within Vampire. This is an extended version of a Principles of Systems Design 2022 paper with the same title and the same authors.</p></p class="citation"></blockquote><h3 id=44--244254-invariant-checking-for-smt-based-systems-with-quantifiers-gianluca-redondi-et-al-2024>(4/4 | 244/254) Invariant Checking for SMT-based Systems with Quantifiers (Gianluca Redondi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gianluca Redondi, Alessandro Cimatti, Alberto Griggio, Kenneth McMillan. (2024)<br><strong>Invariant Checking for SMT-based Systems with Quantifiers</strong><br><button class=copy-to-clipboard title="Invariant Checking for SMT-based Systems with Quantifiers" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19028v1.pdf filename=2402.19028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the problem of checking invariant properties for a large class of symbolic transition systems, defined by a combination of SMT theories and quantifiers. State variables can be functions from an uninterpreted sort (finite, but unbounded) to an interpreted sort, such as the the integers under the theory of linear arithmetic. This formalism is very expressive and can be used for modeling parameterized systems, array-manipulating programs, and more. We propose two algorithms for finding universal inductive invariants for such systems. The first algorithm combines an IC3-style loop with a form of implicit predicate abstraction to construct an invariant in an incremental manner. The second algorithm constructs an under-approximation of the original problem, and searches for a formula which is an inductive invariant for this case; then, the invariant is generalized to the original case, and checked with a portfolio of techniques. We have implemented the two algorithms and conducted an extensive experimental evaluation, considering various <b>benchmarks</b> and different tools from the literature. As far as we know, our method is the first capable of handling in a large class of systems in a uniform way. The experiment shows that both algorithms are competitive with the state of the art.</p></p class="citation"></blockquote><h2 id=csar-2>cs.AR (2)</h2><h3 id=12--245254-mimdram-an-end-to-end-processing-using-dram-system-for-high-throughput-energy-efficient-and-programmer-transparent-multiple-instruction-multiple-data-processing-geraldo-f-oliveira-et-al-2024>(1/2 | 245/254) MIMDRAM: An End-to-End Processing-Using-DRAM System for High-Throughput, Energy-Efficient and Programmer-Transparent Multiple-Instruction Multiple-Data Processing (Geraldo F. Oliveira et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Geraldo F. Oliveira, Ataberk Olgun, Abdullah Giray Yağlıkçı, F. Nisa Bostancı, Juan Gómez-Luna, Saugata Ghose, Onur Mutlu. (2024)<br><strong>MIMDRAM: An End-to-End Processing-Using-DRAM System for High-Throughput, Energy-Efficient and Programmer-Transparent Multiple-Instruction Multiple-Data Processing</strong><br><button class=copy-to-clipboard title="MIMDRAM: An End-to-End Processing-Using-DRAM System for High-Throughput, Energy-Efficient and Programmer-Transparent Multiple-Instruction Multiple-Data Processing" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-DC, cs.AR<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19080v1.pdf filename=2402.19080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Processing-using-DRAM (PUD) is a processing-in-memory (PIM) approach that uses a DRAM array&rsquo;s massive internal parallelism to execute very-wide data-parallel operations, in a single-instruction multiple-data (SIMD) fashion. However, DRAM rows&rsquo; large and rigid granularity limit the effectiveness and applicability of PUD in three ways. First, since applications have varying degrees of SIMD parallelism, PUD execution often leads to underutilization, throughput loss, and energy waste. Second, most PUD architectures are limited to the execution of parallel map operations. Third, the need to feed the wide DRAM row with tens of thousands of data elements combined with the lack of adequate compiler support for PUD systems create a programmability barrier. Our goal is to design a flexible PUD system that overcomes the limitations caused by the large and rigid granularity of PUD. To this end, we propose MIMDRAM, a hardware/software co-designed PUD system that introduces new mechanisms to allocate and control only the necessary resources for a given PUD operation. The key idea of MIMDRAM is to leverage fine-grained DRAM (i.e., the ability to independently access smaller segments of a large DRAM row) for PUD computation. MIMDRAM exploits this key idea to enable a multiple-instruction multiple-data (MIMD) execution model in each DRAM subarray. We evaluate MIMDRAM using twelve real-world applications and 495 multi-programmed application mixes. Our evaluation shows that MIMDRAM provides 34x the performance, 14.3x the energy efficiency, 1.7x the throughput, and 1.3x the <b>fairness</b> of a state-of-the-art PUD framework, along with 30.6x and 6.8x the energy efficiency of a high-end CPU and GPU, respectively. MIMDRAM adds small area cost to a DRAM chip (1.11%) and CPU die (0.6%).</p></p class="citation"></blockquote><h3 id=22--246254-ozmac-an-energy-efficient-sparsity-exploiting-multiply-accumulate-unit-design-for-dl-inference-harideep-nair-et-al-2024>(2/2 | 246/254) OzMAC: An Energy-Efficient Sparsity-Exploiting Multiply-Accumulate-Unit Design for DL Inference (Harideep Nair et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harideep Nair, Prabhu Vellaisamy, Tsung-Han Lin, Perry Wang, Shawn Blanton, John Paul Shen. (2024)<br><strong>OzMAC: An Energy-Efficient Sparsity-Exploiting Multiply-Accumulate-Unit Design for DL Inference</strong><br><button class=copy-to-clipboard title="OzMAC: An Energy-Efficient Sparsity-Exploiting Multiply-Accumulate-Unit Design for DL Inference" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19376v1.pdf filename=2402.19376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>General Matrix Multiply (GEMM) hardware, employing large arrays of multiply-accumulate (MAC) units, perform bulk of the computation in deep learning (DL). Recent trends have established 8-bit integer (INT8) as the most widely used precision for DL inference. This paper proposes a novel MAC design capable of dynamically exploiting bit sparsity (i.e., number of `0&rsquo; bits within a binary value) in input data to achieve significant improvements on area, power and energy. The proposed architecture, called OzMAC (Omit-zero-MAC), skips over zeros within a binary input value and performs simple shift-and-add-based compute in place of expensive multipliers. We implement OzMAC in SystemVerilog and present post-synthesis performance-power-area (PPA) results using commercial TSMC N5 (5nm) process node. Using eight pretrained INT8 deep neural networks (DNNs) as <b>benchmarks,</b> we demonstrate the existence of high bit sparsity in real DNN workloads and show that 8-bit OzMAC improves all three metrics of area, power, and energy significantly by 21%, 70%, and 28%, respectively. Similar improvements are achieved when scaling data precisions (4, 8, 16 bits) and clock frequencies (0.5 GHz, 1 GHz, 1.5 GHz). For the 8-bit OzMAC, scaling its frequency to normalize the throughput relative to conventional MAC, it still achieves 30% improvement on both power and energy.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--247254-fractional-material-derivative-pointwise-representation-and-a-finite-volume-numerical-scheme-łukasz-płociniczak-et-al-2024>(1/2 | 247/254) Fractional material derivative: pointwise representation and a finite volume numerical scheme (Łukasz Płociniczak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Łukasz Płociniczak, Marek A. Teuerle. (2024)<br><strong>Fractional material derivative: pointwise representation and a finite volume numerical scheme</strong><br><button class=copy-to-clipboard title="Fractional material derivative: pointwise representation and a finite volume numerical scheme" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19015v1.pdf filename=2402.19015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The fractional material derivative appears as the fractional operator that governs the dynamics of the scaling limits of L'evy walks - a stochastic process that originates from the famous <b>continuous-time</b> <b>random</b> walks. It is usually defined as the Fourier-Laplace multiplier, therefore, it can be thought of as a pseudo-differential operator. In this paper, we show that there exists a local representation in time and space, pointwise, of the fractional material derivative. This allows us to define it on a space of locally integrable functions which is larger than the original one in which Fourier and Laplace transform exist as functions. We consider several typical differential equations involving the fractional material derivative and provide conditions for their solutions to exist. In some cases, the analytical solution can be found. For the general initial value problem, we devise a finite volume method and prove its stability, convergence, and conservation of probability. Numerical illustrations verify our analytical findings. Moreover, our numerical experiments show superiority in the computation time of the proposed numerical scheme over a Monte Carlo method applied to the problem of probability density function&rsquo;s derivation.</p></p class="citation"></blockquote><h3 id=22--248254-an-asymptotic-preserving-method-for-the-three-temperature-radiative-transfer-model-ruo-li-et-al-2024>(2/2 | 248/254) An asymptotic-preserving method for the three-temperature radiative transfer model (Ruo Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruo Li, Weiming Li, Shengtong Liang, Yuehan Shao, Min Tang, Yanli Wang. (2024)<br><strong>An asymptotic-preserving method for the three-temperature radiative transfer model</strong><br><button class=copy-to-clipboard title="An asymptotic-preserving method for the three-temperature radiative transfer model" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-MP, math-NA, math-ph, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19191v1.pdf filename=2402.19191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an asymptotic-preserving (AP) numerical method for solving the three-temperature radiative transfer model, which holds significant importance in inertial confinement fusion. A carefully designedsplitting method is developed that can provide a general framework of extending AP schemes for the gray radiative transport equation to the more complex three-temperature radiative transfer model. The proposed scheme captures two important limiting models: the three-temperature radiation diffusion equation (3TRDE) when opacity approaches infinity and the two-temperature limit when the ion-electron coupling coefficient goes to infinity. We have rigorously demonstrated the AP property and energy conservation characteristics of the proposed scheme and its efficiency has been validated through a series of <b>benchmark</b> tests in the numerical part.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--249254-3d-gaussian-model-for-animation-and-texturing-xiangzhi-eric-wang-et-al-2024>(1/1 | 249/254) 3D Gaussian Model for Animation and Texturing (Xiangzhi Eric Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangzhi Eric Wang, Zackary P. T. Sin. (2024)<br><strong>3D Gaussian Model for Animation and Texturing</strong><br><button class=copy-to-clipboard title="3D Gaussian Model for Animation and Texturing" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19441v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19441v1.pdf filename=2402.19441v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D Gaussian Splatting has made a marked impact on neural rendering by achieving impressive fidelity and performance. Despite this achievement, however, it is not readily applicable to developing interactive applications. Real-time applications like XR apps and games require functions such as animation, UV-mapping, and model editing simultaneously manipulated through the usage of a 3D model. We propose a modeling that is analogous to typical 3D models, which we call 3D Gaussian Model (3DGM); it provides a manipulatable proxy for novel animation and texture transfer. By binding the 3D Gaussians in texture space and re-projecting them back to world space through implicit shell mapping, we show how our 3D modeling can serve as a valid rendering methodology for interactive applications. It is further noted that recently, 3D mesh reconstruction works have been able to produce high-quality mesh for rendering. Our work, on the other hand, only requires an approximated <b>geometry</b> for rendering an object in high fidelity. Applicationwise, we will show that our proxy-based 3DGM is capable of driving novel animation without animated training data and texture transferring via UV mapping of the 3D Gaussians. We believe the result indicates the potential of our work for enabling interactive applications for 3D Gaussian Splatting.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--250254-on-efficient-computation-of-dire-committees-kunal-relia-2024>(1/1 | 250/254) On Efficient Computation of DiRe Committees (Kunal Relia, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kunal Relia. (2024)<br><strong>On Efficient Computation of DiRe Committees</strong><br><button class=copy-to-clipboard title="On Efficient Computation of DiRe Committees" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-CY, cs-GT, cs.CC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19365v1.pdf filename=2402.19365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Consider a committee election consisting of (i) a set of candidates who are divided into arbitrary groups each of size \emph{at most} two and a diversity constraint that stipulates the selection of \emph{at least} one candidate from each group and (ii) a set of voters who are divided into arbitrary populations each approving \emph{at most} two candidates and a representation constraint that stipulates the selection of \emph{at least} one candidate from each population who has a non-null set of approved candidates. The DiRe (Diverse + Representative) committee feasibility problem (a.k.a. the minimum vertex cover problem on unweighted undirected <b>graphs)</b> concerns the determination of the smallest size committee that satisfies the given constraints. Here, for this problem, we discover an unconditional deterministic polynomial-time algorithm that is an amalgamation of maximum matching, breadth-first search, maximal matching, and local minimization.</p></p class="citation"></blockquote><h2 id=mathco-3>math.CO (3)</h2><h3 id=13--251254-oriented-trees-in-ok-sqrtk-chromatic-digraphs-a-subquadratic-bound-for-burrs-conjecture-stéphane-bessy-et-al-2024>(1/3 | 251/254) Oriented trees in $O(k \sqrt{k})$-chromatic digraphs, a subquadratic bound for Burr&rsquo;s conjecture (Stéphane Bessy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stéphane Bessy, Daniel Gonçalves, Amadeus Reinald. (2024)<br><strong>Oriented trees in $O(k \sqrt{k})$-chromatic digraphs, a subquadratic bound for Burr&rsquo;s conjecture</strong><br><button class=copy-to-clipboard title="Oriented trees in $O(k \sqrt{k})$-chromatic digraphs, a subquadratic bound for Burr's conjecture" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C20, cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19351v1.pdf filename=2402.19351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In 1980, Burr conjectured that every directed <b>graph</b> with chromatic number $2k-2$ contains any oriented tree of order $k$ as a subdigraph. Burr showed that chromatic number $(k-1)^2$ suffices, which was improved in 2013 to $\frac{k^2}{2} - \frac{k}{2} + 1$ by Addario-Berry et al. We give the first subquadratic bound for Burr&rsquo;s conjecture, by showing that every directed <b>graph</b> with chromatic number $8\sqrt{\frac{2}{15}} k \sqrt{k} + O(k)$ contains any oriented tree of order $k$. Moreover, we provide improved bounds of $\sqrt{\frac{4}{3}} k \sqrt{k}+O(k)$ for arborescences, and $(b-1)(k-3)+3$ for paths on $b$ blocks, with $b\ge 2$.</p></p class="citation"></blockquote><h3 id=23--252254-broadcast-independence-number-of-oriented-circulant-graphs-abdelamin-laouar-et-al-2024>(2/3 | 252/254) Broadcast independence number of oriented circulant graphs (Abdelamin Laouar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdelamin Laouar, Isma Bouchemakh, Eric Sopena. (2024)<br><strong>Broadcast independence number of oriented circulant graphs</strong><br><button class=copy-to-clipboard title="Broadcast independence number of oriented circulant graphs" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C12, 05C69, cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19234v1.pdf filename=2402.19234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In 2001, D. Erwin \cite{Erw01} introduced in his Ph.D. dissertation the notion of broadcast independence in unoriented <b>graphs.</b> Since then, some results but not many, are published on this notion, including research work on the broadcast independence number of unoriented circulant <b>graphs</b> \cite{LBS23}. In this paper, we are focused in the same parameter but of the class of oriented circulant <b>graphs.</b> An independent broadcast on an oriented <b>graph</b> $\overrightarrow{G}$ is a function $f: V\longrightarrow {0,\ldots,\diam(\overrightarrow{G})}$ such that $(i)$ $f(v)\leq e(v)$ for every vertex $v\in V(\overrightarrow{G})$, where $\diam(\overrightarrow{G})$ denotes the diameter of $\overrightarrow{G}$ and $e(v)$ the eccentricity of vertex $v$, and $(ii)$ $d_{\overrightarrow{G}}(u,v) > f(u)$ for every distinct vertices $u$, $v$ with $f(u)$, $f(v)>0$, where $d_{\overrightarrow{G}}(u,v)$ denotes the length of a shortest oriented path from $u$ to $v$. The broadcast independence number $\beta_b(\overrightarrow{G})$ of $\overrightarrow{G}$ is then the maximum value of $\sum_{v \in V} f(v)$, taken over all independent broadcasts on $\overrightarrow{G}$. The goal of this paper is to study the properties of independent broadcasts of oriented circulant <b>graphs</b> $\overrightarrow{C}(n;1,a)$, for any integers $n$ and $a$ with $n>|a|\geq 1$ and $a \notin {1,n-1}$. Then, we give some bounds and some exact values for the number $\beta_b(\overrightarrow{C}(n;1,a))$.</p></p class="citation"></blockquote><h3 id=33--253254-graph-burning-bounds-and-hardness-dhanyamol-antony-et-al-2024>(3/3 | 253/254) Graph Burning: Bounds and Hardness (Dhanyamol Antony et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dhanyamol Antony, Anita Das, Shirish Gosavi, Dalu Jacob, Shashanka Kulamarva. (2024)<br><strong>Graph Burning: Bounds and Hardness</strong><br><button class=copy-to-clipboard title="Graph Burning: Bounds and Hardness" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C12, 68Q17, 05C85, 05C38, 05C05, cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18984v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18984v1.pdf filename=2402.18984v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The burning number of a <b>graph</b> $G$, denoted by $b(G)$, is the minimum number of steps required to burn all the vertices of a <b>graph</b> where in each step the existing fire spreads to all the adjacent vertices and one additional vertex can be burned as a new fire source. In this paper, we study the burning number problem both from an algorithmic and a structural point of view. The decision problem of computing the burning number of an input <b>graph</b> is known to be NP-Complete for trees with maximum degree at most three and interval <b>graphs.</b> Here, we prove that this problem is NP-Complete even when restricted to connected proper interval <b>graphs</b> and connected cubic <b>graphs.</b> The well-known burning number conjecture asserts that all the vertices of any <b>graph</b> of order $n$ can be burned in $\lceil \sqrt{n}~\rceil$ steps. In line with this conjecture, upper and lower bounds of $b(G)$ are well-studied for various special <b>graph</b> classes. Here, we provide an improved upper bound for the burning number of connected $P_k$-free <b>graphs</b> and show that the bound is tight up to an additive constant $1$. Finally, we study two variants of the problem, namely edge burning (only edges are burned) and total burning (both vertices and edges are burned). In particular, we establish their relationship with the burning number problem and evaluate the complexity of these variants.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--254254-more-algorithmic-results-for-problems-of-spread-of-influence-in-edge-weighted-graphs-with-and-without-incentives-siavash-askari-et-al-2024>(1/1 | 254/254) More algorithmic results for problems of spread of influence in edge-weighted graphs with and without incentives (Siavash Askari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siavash Askari, Manouchehr Zaker. (2024)<br><strong>More algorithmic results for problems of spread of influence in edge-weighted graphs with and without incentives</strong><br><button class=copy-to-clipboard title="More algorithmic results for problems of spread of influence in edge-weighted graphs with and without incentives" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: 05C69, 05C85, 68Q25, 91D30, cs-DM, cs.DM, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.19257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.19257v1.pdf filename=2402.19257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many phenomena in real world social networks are interpreted as spread of influence between activated and non-activated network elements. These phenomena are formulated by combinatorial <b>graphs,</b> where vertices represent the elements and edges represent social ties between elements. A main problem is to study important subsets of elements (target sets or dynamic monopolies) such that their activation spreads to the entire network. In edge-weighted networks the influence between two adjacent vertices depends on the weight of their edge. In models with incentives, the main problem is to minimize total amount of incentives (called optimal target vectors) which can be offered to vertices such that some vertices are activated and their activation spreads to the whole network. Algorithmic study of target sets and vectors is a hot research field. We prove an inapproximability result for optimal target sets in edge weighted networks even for complete <b>graphs.</b> Some other hardness and polynomial time results are presented for optimal target vectors and degenerate threshold assignments in edge-weighted networks.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.01</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Bandit Algorithm Basic</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-36>cs.CL (36)</a><ul><li><a href=#136--1254-compact-speech-translation-models-via-discrete-speech-units-pretraining-tsz-kin-lam-et-al-2024>(1/36 | 1/254) Compact Speech Translation Models via Discrete Speech Units Pretraining (Tsz Kin Lam et al., 2024)</a></li><li><a href=#236--2254-teaching-large-language-models-an-unseen-language-on-the-fly-chen-zhang-et-al-2024>(2/36 | 2/254) Teaching Large Language Models an Unseen Language on the Fly (Chen Zhang et al., 2024)</a></li><li><a href=#336--3254-openmedlm-prompt-engineering-can-out-perform-fine-tuning-in-medical-question-answering-with-open-source-large-language-models-jenish-maharjan-et-al-2024>(3/36 | 3/254) OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models (Jenish Maharjan et al., 2024)</a></li><li><a href=#436--4254-exploring-the-efficacy-of-large-language-models-in-summarizing-mental-health-counseling-sessions-a-benchmark-study-prottay-kumar-adhikary-et-al-2024>(4/36 | 4/254) Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study (Prottay Kumar Adhikary et al., 2024)</a></li><li><a href=#536--5254-prompting-explicit-and-implicit-knowledge-for-multi-hop-question-answering-based-on-human-reading-process-guangming-huang-et-al-2024>(5/36 | 5/254) Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process (Guangming Huang et al., 2024)</a></li><li><a href=#636--6254-on-the-decision-making-abilities-in-role-playing-using-large-language-models-chenglei-shen-et-al-2024>(6/36 | 6/254) On the Decision-Making Abilities in Role-Playing using Large Language Models (Chenglei Shen et al., 2024)</a></li><li><a href=#736--7254-gsm-plus-a-comprehensive-benchmark-for-evaluating-the-robustness-of-llms-as-mathematical-problem-solvers-qintong-li-et-al-2024>(7/36 | 7/254) GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers (Qintong Li et al., 2024)</a></li><li><a href=#836--8254-how-to-understand-support-an-implicit-enhanced-causal-inference-approach-for-weakly-supervised-phrase-grounding-jiamin-luo-et-al-2024>(8/36 | 8/254) How to Understand &lsquo;Support&rsquo;? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding (Jiamin Luo et al., 2024)</a></li><li><a href=#936--9254-let-llms-take-on-the-latest-challenges-a-chinese-dynamic-question-answering-benchmark-zhikun-xu-et-al-2024>(9/36 | 9/254) Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark (Zhikun Xu et al., 2024)</a></li><li><a href=#1036--10254-tv-trees-multimodal-entailment-trees-for-neuro-symbolic-video-reasoning-kate-sanders-et-al-2024>(10/36 | 10/254) TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning (Kate Sanders et al., 2024)</a></li><li><a href=#1136--11254-loose-lips-sink-ships-asking-questions-in-battleship-with-language-informed-program-sampling-gabriel-grand-et-al-2024>(11/36 | 11/254) Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling (Gabriel Grand et al., 2024)</a></li><li><a href=#1236--12254-towards-tracing-trustworthiness-dynamics-revisiting-pre-training-period-of-large-language-models-chen-qian-et-al-2024>(12/36 | 12/254) Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models (Chen Qian et al., 2024)</a></li><li><a href=#1336--13254-heres-a-free-lunch-sanitizing-backdoored-models-with-model-merge-ansh-arora-et-al-2024>(13/36 | 13/254) Here&rsquo;s a Free Lunch: Sanitizing Backdoored Models with Model Merge (Ansh Arora et al., 2024)</a></li><li><a href=#1436--14254-pelle-encoder-based-language-models-for-brazilian-portuguese-based-on-open-data-guilherme-lamartine-de-mello-et-al-2024>(14/36 | 14/254) PeLLE: Encoder-based language models for Brazilian Portuguese based on open data (Guilherme Lamartine de Mello et al., 2024)</a></li><li><a href=#1536--15254-tencdm-understanding-the-properties-of-diffusion-model-in-the-space-of-language-model-encodings-alexander-shabalin-et-al-2024>(15/36 | 15/254) TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings (Alexander Shabalin et al., 2024)</a></li><li><a href=#1636--16254-on-the-scaling-laws-of-geographical-representation-in-language-models-nathan-godey-et-al-2024>(16/36 | 16/254) On the Scaling Laws of Geographical Representation in Language Models (Nathan Godey et al., 2024)</a></li><li><a href=#1736--17254-memory-augmented-generative-adversarial-transformers-stephan-raaijmakers-et-al-2024>(17/36 | 17/254) Memory-Augmented Generative Adversarial Transformers (Stephan Raaijmakers et al., 2024)</a></li><li><a href=#1836--18254-survey-in-characterization-of-semantic-change-jader-martins-camboim-de-sá-et-al-2024>(18/36 | 18/254) Survey in Characterization of Semantic Change (Jader Martins Camboim de Sá et al., 2024)</a></li><li><a href=#1936--19254-pointing-out-the-shortcomings-of-relation-extraction-models-with-semantically-motivated-adversarials-gennaro-nolano-et-al-2024>(19/36 | 19/254) Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials (Gennaro Nolano et al., 2024)</a></li><li><a href=#2036--20254-inappropriate-pause-detection-in-dysarthric-speech-using-large-scale-speech-recognition-jeehyun-lee-et-al-2024>(20/36 | 20/254) Inappropriate Pause Detection In Dysarthric Speech Using Large-Scale Speech Recognition (Jeehyun Lee et al., 2024)</a></li><li><a href=#2136--21254-adamergex-cross-lingual-transfer-with-large-language-models-via-adaptive-adapter-merging-yiran-zhao-et-al-2024>(21/36 | 21/254) AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging (Yiran Zhao et al., 2024)</a></li><li><a href=#2236--22254-reducing-hallucinations-in-entity-abstract-summarization-with-facts-template-decomposition-fangwei-zhu-et-al-2024>(22/36 | 22/254) Reducing Hallucinations in Entity Abstract Summarization with Facts-Template Decomposition (Fangwei Zhu et al., 2024)</a></li><li><a href=#2336--23254-how-do-large-language-models-handle-multilingualism-yiran-zhao-et-al-2024>(23/36 | 23/254) How do Large Language Models Handle Multilingualism? (Yiran Zhao et al., 2024)</a></li><li><a href=#2436--24254-textttcosmic-mutual-information-for-task-agnostic-summarization-evaluation-maxime-darrin-et-al-2024>(24/36 | 24/254) $\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation (Maxime Darrin et al., 2024)</a></li><li><a href=#2536--25254-plangpt-enhancing-urban-planning-with-tailored-language-model-and-efficient-retrieval-he-zhu-et-al-2024>(25/36 | 25/254) PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval (He Zhu et al., 2024)</a></li><li><a href=#2636--26254-robust-guidance-for-unsupervised-data-selection-capturing-perplexing-named-entities-for-domain-specific-machine-translation-seunghyun-ji-et-al-2024>(26/36 | 26/254) Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation (Seunghyun Ji et al., 2024)</a></li><li><a href=#2736--27254-evaluating-webcam-based-gaze-data-as-an-alternative-for-human-rationale-annotations-stephanie-brandl-et-al-2024>(27/36 | 27/254) Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale Annotations (Stephanie Brandl et al., 2024)</a></li><li><a href=#2836--28254-whispers-that-shake-foundations-analyzing-and-mitigating-false-premise-hallucinations-in-large-language-models-hongbang-yuan-et-al-2024>(28/36 | 28/254) Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models (Hongbang Yuan et al., 2024)</a></li><li><a href=#2936--29254-controllable-preference-optimization-toward-controllable-multi-objective-alignment-yiju-guo-et-al-2024>(29/36 | 29/254) Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment (Yiju Guo et al., 2024)</a></li><li><a href=#3036--30254-popalm-popularity-aligned-language-models-for-social-media-trendy-response-prediction-erxin-yu-et-al-2024>(30/36 | 30/254) PopALM: Popularity-Aligned Language Models for Social Media Trendy Response Prediction (Erxin Yu et al., 2024)</a></li><li><a href=#3136--31254-semeval-2024----task-10-emotion-discovery-and-reasoning-its-flip-in-conversation-ediref-shivani-kumar-et-al-2024>(31/36 | 31/254) SemEval 2024 &ndash; Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF) (Shivani Kumar et al., 2024)</a></li><li><a href=#3236--32254-when-does-word-order-matter-and-when-doesnt-it-xuanda-chen-et-al-2024>(32/36 | 32/254) When does word order matter and when doesn&rsquo;t it? (Xuanda Chen et al., 2024)</a></li><li><a href=#3336--33254-utilizing-local-hierarchy-with-adversarial-training-for-hierarchical-text-classification-zihan-wang-et-al-2024>(33/36 | 33/254) Utilizing Local Hierarchy with Adversarial Training for Hierarchical Text Classification (Zihan Wang et al., 2024)</a></li><li><a href=#3436--34254-advancing-generative-ai-for-portuguese-with-open-decoder-gervásio-pt-rodrigo-santos-et-al-2024>(34/36 | 34/254) Advancing Generative AI for Portuguese with Open Decoder Gervásio PT* (Rodrigo Santos et al., 2024)</a></li><li><a href=#3536--35254-improving-legal-judgement-prediction-in-romanian-with-long-text-encoders-mihai-masala-et-al-2024>(35/36 | 35/254) Improving Legal Judgement Prediction in Romanian with Long Text Encoders (Mihai Masala et al., 2024)</a></li><li><a href=#3636--36254-updating-language-models-with-unstructured-facts-towards-practical-knowledge-editing-xiaobao-wu-et-al-2024>(36/36 | 36/254) Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing (Xiaobao Wu et al., 2024)</a></li></ul></li><li><a href=#cscv-65>cs.CV (65)</a><ul><li><a href=#165--37254-typographic-attacks-in-large-multimodal-models-can-be-alleviated-by-more-informative-prompts-hao-cheng-et-al-2024>(1/65 | 37/254) Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts (Hao Cheng et al., 2024)</a></li><li><a href=#265--38254-videomac-video-masked-autoencoders-meet-convnets-gensheng-pei-et-al-2024>(2/65 | 38/254) VideoMAC: Video Masked Autoencoders Meet ConvNets (Gensheng Pei et al., 2024)</a></li><li><a href=#365--39254-retrieval-augmented-generation-for-ai-generated-content-a-survey-penghao-zhao-et-al-2024>(3/65 | 39/254) Retrieval-Augmented Generation for AI-Generated Content: A Survey (Penghao Zhao et al., 2024)</a></li><li><a href=#465--40254-generalizable-whole-slide-image-classification-with-fine-grained-visual-semantic-interaction-hao-li-et-al-2024>(4/65 | 40/254) Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction (Hao Li et al., 2024)</a></li><li><a href=#565--41254-assessing-visually-continuous-corruption-robustness-of-neural-networks-relative-to-human-performance-huakun-shen-et-al-2024>(5/65 | 41/254) Assessing Visually-Continuous Corruption Robustness of Neural Networks Relative to Human Performance (Huakun Shen et al., 2024)</a></li><li><a href=#665--42254-vixen-visual-text-comparison-network-for-image-difference-captioning-alexander-black-et-al-2024>(6/65 | 42/254) VIXEN: Visual Text Comparison Network for Image Difference Captioning (Alexander Black et al., 2024)</a></li><li><a href=#765--43254-rsam-seg-a-sam-based-approach-with-prior-knowledge-integration-for-remote-sensing-image-semantic-segmentation-jie-zhang-et-al-2024>(7/65 | 43/254) RSAM-Seg: A SAM-based Approach with Prior Knowledge Integration for Remote Sensing Image Semantic Segmentation (Jie Zhang et al., 2024)</a></li><li><a href=#865--44254-edge-computing-enabled-real-time-video-analysis-via-adaptive-spatial-temporal-semantic-filtering-xiang-chen-et-al-2024>(8/65 | 44/254) Edge Computing Enabled Real-Time Video Analysis via Adaptive Spatial-Temporal Semantic Filtering (Xiang Chen et al., 2024)</a></li><li><a href=#965--45254-stitching-gaps-fusing-situated-perceptual-knowledge-with-vision-transformers-for-high-level-image-classification-delfina-sol-martinez-pandiani-et-al-2024>(9/65 | 45/254) Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification (Delfina Sol Martinez Pandiani et al., 2024)</a></li><li><a href=#1065--46254-a-simple-yet-effective-network-based-on-vision-transformer-for-camouflaged-object-and-salient-object-detection-chao-hao-et-al-2024>(10/65 | 46/254) A Simple yet Effective Network based on Vision Transformer for Camouflaged Object and Salient Object Detection (Chao Hao et al., 2024)</a></li><li><a href=#1165--47254-maskfi-unsupervised-learning-of-wifi-and-vision-representations-for-multimodal-human-activity-recognition-jianfei-yang-et-al-2024>(11/65 | 47/254) MaskFi: Unsupervised Learning of WiFi and Vision Representations for Multimodal Human Activity Recognition (Jianfei Yang et al., 2024)</a></li><li><a href=#1265--48254-t3dnet-compressing-point-cloud-models-for-lightweight-3d-recognition-zhiyuan-yang-et-al-2024>(12/65 | 48/254) T3DNet: Compressing Point Cloud Models for Lightweight 3D Recognition (Zhiyuan Yang et al., 2024)</a></li><li><a href=#1365--49254-weakly-supervised-monocular-3d-detection-with-a-single-view-image-xueying-jiang-et-al-2024>(13/65 | 49/254) Weakly Supervised Monocular 3D Detection with a Single-View Image (Xueying Jiang et al., 2024)</a></li><li><a href=#1465--50254-coft-ad-contrastive-fine-tuning-for-few-shot-anomaly-detection-jingyi-liao-et-al-2024>(14/65 | 50/254) COFT-AD: COntrastive Fine-Tuning for Few-Shot Anomaly Detection (Jingyi Liao et al., 2024)</a></li><li><a href=#1565--51254-dose-prediction-driven-radiotherapy-paramters-regression-via-intra--and-inter-relation-modeling-jiaqi-cui-et-al-2024>(15/65 | 51/254) Dose Prediction Driven Radiotherapy Paramters Regression via Intra- and Inter-Relation Modeling (Jiaqi Cui et al., 2024)</a></li><li><a href=#1665--52254-the-all-seeing-project-v2-towards-general-relation-comprehension-of-the-open-world-weiyun-wang-et-al-2024>(16/65 | 52/254) The All-Seeing Project V2: Towards General Relation Comprehension of the Open World (Weiyun Wang et al., 2024)</a></li><li><a href=#1765--53254-navigating-hallucinations-for-reasoning-of-unintentional-activities-shresth-grover-et-al-2024>(17/65 | 53/254) Navigating Hallucinations for Reasoning of Unintentional Activities (Shresth Grover et al., 2024)</a></li><li><a href=#1865--54254-entity-aware-multimodal-alignment-framework-for-news-image-captioning-junzhe-zhang-et-al-2024>(18/65 | 54/254) Entity-Aware Multimodal Alignment Framework for News Image Captioning (Junzhe Zhang et al., 2024)</a></li><li><a href=#1965--55254-cricavpr-cross-image-correlation-aware-representation-learning-for-visual-place-recognition-feng-lu-et-al-2024>(19/65 | 55/254) CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition (Feng Lu et al., 2024)</a></li><li><a href=#2065--56254-diffassemble-a-unified-graph-diffusion-model-for-2d-and-3d-reassembly-gianluca-scarpellini-et-al-2024>(20/65 | 56/254) DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly (Gianluca Scarpellini et al., 2024)</a></li><li><a href=#2165--57254-a-novel-approach-to-industrial-defect-generation-through-blended-latent-diffusion-model-with-online-adaptation-hanxi-li-et-al-2024>(21/65 | 57/254) A Novel Approach to Industrial Defect Generation through Blended Latent Diffusion Model with Online Adaptation (Hanxi Li et al., 2024)</a></li><li><a href=#2265--58254-biggait-learning-gait-representation-you-want-by-large-vision-models-dingqiang-ye-et-al-2024>(22/65 | 58/254) BigGait: Learning Gait Representation You Want by Large Vision Models (Dingqiang Ye et al., 2024)</a></li><li><a href=#2365--59254-analysis-of-the-two-step-heterogeneous-transfer-learning-for-laryngeal-blood-vessel-classification-issue-and-improvement-xinyi-fang-et-al-2024>(23/65 | 59/254) Analysis of the Two-Step Heterogeneous Transfer Learning for Laryngeal Blood Vessel Classification: Issue and Improvement (Xinyi Fang et al., 2024)</a></li><li><a href=#2465--60254-boosting-semi-supervised-object-detection-in-remote-sensing-images-with-active-teaching-boxuan-zhang-et-al-2024>(24/65 | 60/254) Boosting Semi-Supervised Object Detection in Remote Sensing Images With Active Teaching (Boxuan Zhang et al., 2024)</a></li><li><a href=#2565--61254-decompose-and-compose-a-compositional-approach-to-mitigating-spurious-correlation-fahimeh-hosseini-noohdani-et-al-2024>(25/65 | 61/254) Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation (Fahimeh Hosseini Noohdani et al., 2024)</a></li><li><a href=#2665--62254-bfrformer-transformer-based-generator-for-real-world-blind-face-restoration-guojing-ge-et-al-2024>(26/65 | 62/254) BFRFormer: Transformer-based generator for Real-World Blind Face Restoration (Guojing Ge et al., 2024)</a></li><li><a href=#2765--63254-a-quantitative-evaluation-of-score-distillation-sampling-based-text-to-3d-xiaohan-fei-et-al-2024>(27/65 | 63/254) A Quantitative Evaluation of Score Distillation Sampling Based Text-to-3D (Xiaohan Fei et al., 2024)</a></li><li><a href=#2865--64254-enhancing-visual-document-understanding-with-contrastive-learning-in-large-visual-language-models-xin-li-et-al-2024>(28/65 | 64/254) Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models (Xin Li et al., 2024)</a></li><li><a href=#2965--65254-percept-chat-and-then-adapt-multimodal-knowledge-transfer-of-foundation-models-for-open-world-video-recognition-boyu-chen-et-al-2024>(29/65 | 65/254) Percept, Chat, and then Adapt: Multimodal Knowledge Transfer of Foundation Models for Open-World Video Recognition (Boyu Chen et al., 2024)</a></li><li><a href=#3065--66254-panda-70m-captioning-70m-videos-with-multiple-cross-modality-teachers-tsai-shien-chen-et-al-2024>(30/65 | 66/254) Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers (Tsai-Shien Chen et al., 2024)</a></li><li><a href=#3165--67254-debiased-novel-category-discovering-and-localization-juexiao-feng-et-al-2024>(31/65 | 67/254) Debiased Novel Category Discovering and Localization (Juexiao Feng et al., 2024)</a></li><li><a href=#3265--68254-learning-a-generalized-physical-face-model-from-data-lingchen-yang-et-al-2024>(32/65 | 68/254) Learning a Generalized Physical Face Model From Data (Lingchen Yang et al., 2024)</a></li><li><a href=#3365--69254-aligning-knowledge-graph-with-visual-perception-for-object-goal-navigation-nuo-xu-et-al-2024>(33/65 | 69/254) Aligning Knowledge Graph with Visual Perception for Object-goal Navigation (Nuo Xu et al., 2024)</a></li><li><a href=#3465--70254-semoli-what-moves-together-belongs-together-jenny-seidenschwarz-et-al-2024>(34/65 | 70/254) SeMoLi: What Moves Together Belongs Together (Jenny Seidenschwarz et al., 2024)</a></li><li><a href=#3565--71254-pem-prototype-based-efficient-maskformer-for-image-segmentation-niccolò-cavagnero-et-al-2024>(35/65 | 71/254) PEM: Prototype-based Efficient MaskFormer for Image Segmentation (Niccolò Cavagnero et al., 2024)</a></li><li><a href=#3665--72254-a-sam-guided-two-stream-lightweight-model-for-anomaly-detection-chenghao-li-et-al-2024>(36/65 | 72/254) A SAM-guided Two-stream Lightweight Model for Anomaly Detection (Chenghao Li et al., 2024)</a></li><li><a href=#3765--73254-theoretically-achieving-continuous-representation-of-oriented-bounding-boxes-zikai-xiao-et-al-2024>(37/65 | 73/254) Theoretically Achieving Continuous Representation of Oriented Bounding Boxes (Zikai Xiao et al., 2024)</a></li><li><a href=#3865--74254-switchlight-co-design-of-physics-driven-architecture-and-pre-training-framework-for-human-portrait-relighting-hoon-kim-et-al-2024>(38/65 | 74/254) SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting (Hoon Kim et al., 2024)</a></li><li><a href=#3965--75254-modality-agnostic-structural-image-representation-learning-for-deformable-multi-modality-medical-image-registration-tony-c-w-mok-et-al-2024>(39/65 | 75/254) Modality-Agnostic Structural Image Representation Learning for Deformable Multi-Modality Medical Image Registration (Tony C. W. Mok et al., 2024)</a></li><li><a href=#4065--76254-effective-message-hiding-with-order-preserving-mechanisms-gao-yu-et-al-2024>(40/65 | 76/254) Effective Message Hiding with Order-Preserving Mechanisms (Gao Yu et al., 2024)</a></li><li><a href=#4165--77254-trajectory-consistency-distillation-jianbin-zheng-et-al-2024>(41/65 | 77/254) Trajectory Consistency Distillation (Jianbin Zheng et al., 2024)</a></li><li><a href=#4265--78254-protop-od-explainable-object-detection-with-prototypical-parts-pavlos-rath-manakidis-et-al-2024>(42/65 | 78/254) ProtoP-OD: Explainable Object Detection with Prototypical Parts (Pavlos Rath-Manakidis et al., 2024)</a></li><li><a href=#4365--79254-leveraging-representations-from-intermediate-encoder-blocks-for-synthetic-image-detection-christos-koutlis-et-al-2024>(43/65 | 79/254) Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection (Christos Koutlis et al., 2024)</a></li><li><a href=#4465--80254-atmospheric-turbulence-removal-with-video-sequence-deep-visual-priors-p-hill-et-al-2024>(44/65 | 80/254) Atmospheric Turbulence Removal with Video Sequence Deep Visual Priors (P. Hill et al., 2024)</a></li><li><a href=#4565--81254-progressive-contrastive-learning-with-multi-prototype-for-unsupervised-visible-infrared-person-re-identification-jiangming-shi-et-al-2024>(45/65 | 81/254) Progressive Contrastive Learning with Multi-Prototype for Unsupervised Visible-Infrared Person Re-identification (Jiangming Shi et al., 2024)</a></li><li><a href=#4665--82254-viewfusion-towards-multi-view-consistency-via-interpolated-denoising-xianghui-yang-et-al-2024>(46/65 | 82/254) ViewFusion: Towards Multi-View Consistency via Interpolated Denoising (Xianghui Yang et al., 2024)</a></li><li><a href=#4765--83254-suppress-and-rebalance-towards-generalized-multi-modal-face-anti-spoofing-xun-lin-et-al-2024>(47/65 | 83/254) Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing (Xun Lin et al., 2024)</a></li><li><a href=#4865--84254-venvision3d-a-synthetic-perception-dataset-for-3d-multi-task-model-research-jiahao-zhou-et-al-2024>(48/65 | 84/254) VEnvision3D: A Synthetic Perception Dataset for 3D Multi-Task Model Research (Jiahao Zhou et al., 2024)</a></li><li><a href=#4965--85254-distrifusion-distributed-parallel-inference-for-high-resolution-diffusion-models-muyang-li-et-al-2024>(49/65 | 85/254) DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models (Muyang Li et al., 2024)</a></li><li><a href=#5065--86254-hyenapixel-global-image-context-with-convolutions-julian-spravil-et-al-2024>(50/65 | 86/254) HyenaPixel: Global Image Context with Convolutions (Julian Spravil et al., 2024)</a></li><li><a href=#5165--87254-continuous-sign-language-recognition-based-on-motor-attention-mechanism-and-frame-level-self-distillation-qidan-zhu-et-al-2024>(51/65 | 87/254) Continuous Sign Language Recognition Based on Motor attention mechanism and frame-level Self-distillation (Qidan Zhu et al., 2024)</a></li><li><a href=#5265--88254-doze-a-dataset-for-open-vocabulary-zero-shot-object-navigation-in-dynamic-environments-ji-ma-et-al-2024>(52/65 | 88/254) DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments (Ji Ma et al., 2024)</a></li><li><a href=#5365--89254-privateyes-appearance-based-gaze-estimation-using-federated-secure-multi-party-computation-mayar-elfares-et-al-2024>(53/65 | 89/254) PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation (Mayar Elfares et al., 2024)</a></li><li><a href=#5465--90254-towards-out-of-distribution-detection-for-breast-cancer-classification-in-point-of-care-ultrasound-imaging-jennie-karlsson-et-al-2024>(54/65 | 90/254) Towards Out-of-Distribution Detection for breast cancer classification in Point-of-Care Ultrasound Imaging (Jennie Karlsson et al., 2024)</a></li><li><a href=#5565--91254-spectral-meets-spatial-harmonising-3d-shape-matching-and-interpolation-dongliang-cao-et-al-2024>(55/65 | 91/254) Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation (Dongliang Cao et al., 2024)</a></li><li><a href=#5665--92254-enhancing-steganographic-text-extraction-evaluating-the-impact-of-nlp-models-on-accuracy-and-semantic-coherence-mingyang-li-et-al-2024>(56/65 | 92/254) Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence (Mingyang Li et al., 2024)</a></li><li><a href=#5765--93254-the-6th-affective-behavior-analysis-in-the-wild-abaw-competition-dimitrios-kollias-et-al-2024>(57/65 | 93/254) The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition (Dimitrios Kollias et al., 2024)</a></li><li><a href=#5865--94254-www-a-unified-framework-for-explaining-what-where-and-why-of-neural-networks-by-interpretation-of-neuron-concepts-yong-hyun-ahn-et-al-2024>(58/65 | 94/254) WWW: A Unified Framework for Explaining What, Where and Why of Neural Networks by Interpretation of Neuron Concepts (Yong Hyun Ahn et al., 2024)</a></li><li><a href=#5965--95254-pcdepth-pattern-based-complementary-learning-for-monocular-depth-estimation-by-best-of-both-worlds-haotian-liu-et-al-2024>(59/65 | 95/254) PCDepth: Pattern-based Complementary Learning for Monocular Depth Estimation by Best of Both Worlds (Haotian Liu et al., 2024)</a></li><li><a href=#6065--96254-memonav-working-memory-model-for-visual-navigation-hongxin-li-et-al-2024>(60/65 | 96/254) MemoNav: Working Memory Model for Visual Navigation (Hongxin Li et al., 2024)</a></li><li><a href=#6165--97254-deeperaser-deep-iterative-context-mining-for-generic-text-eraser-hao-feng-et-al-2024>(61/65 | 97/254) DeepEraser: Deep Iterative Context Mining for Generic Text Eraser (Hao Feng et al., 2024)</a></li><li><a href=#6265--98254-goalnet-goal-areas-oriented-pedestrian-trajectory-prediction-ching-lin-lee-et-al-2024>(62/65 | 98/254) GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction (Ching-Lin Lee et al., 2024)</a></li><li><a href=#6365--99254-navigating-beyond-dropout-an-intriguing-solution-towards-generalizable-image-super-resolution-hongjun-wang-et-al-2024>(63/65 | 99/254) Navigating Beyond Dropout: An Intriguing Solution Towards Generalizable Image Super Resolution (Hongjun Wang et al., 2024)</a></li><li><a href=#6465--100254-sne-roadsegv2-advancing-heterogeneous-feature-fusion-and-fallibility-awareness-for-freespace-detection-yi-feng-et-al-2024>(64/65 | 100/254) SNE-RoadSegV2: Advancing Heterogeneous Feature Fusion and Fallibility Awareness for Freespace Detection (Yi Feng et al., 2024)</a></li><li><a href=#6565--101254-naruto-neural-active-reconstruction-from-uncertain-target-observations-ziyue-feng-et-al-2024>(65/65 | 101/254) NARUTO: Neural Active Reconstruction from Uncertain Target Observations (Ziyue Feng et al., 2024)</a></li></ul></li><li><a href=#csir-5>cs.IR (5)</a><ul><li><a href=#15--102254-crafting-knowledge-exploring-the-creative-mechanisms-of-chat-based-search-engines-lijia-ma-et-al-2024>(1/5 | 102/254) Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines (Lijia Ma et al., 2024)</a></li><li><a href=#25--103254-mentor-multi-level-self-supervised-learning-for-multimodal-recommendation-jinfeng-xu-et-al-2024>(2/5 | 103/254) MENTOR: Multi-level Self-supervised Learning for Multimodal Recommendation (Jinfeng Xu et al., 2024)</a></li><li><a href=#35--104254-paecter-patent-level-representation-learning-using-citation-informed-transformers-mainak-ghosh-et-al-2024>(3/5 | 104/254) PaECTER: Patent-level Representation Learning using Citation-informed Transformers (Mainak Ghosh et al., 2024)</a></li><li><a href=#45--105254-aligning-language-models-for-versatile-text-based-item-retrieval-yuxuan-lei-et-al-2024>(4/5 | 105/254) Aligning Language Models for Versatile Text-based Item Retrieval (Yuxuan Lei et al., 2024)</a></li><li><a href=#55--106254-effective-two-stage-knowledge-transfer-for-multi-entity-cross-domain-recommendation-jianyu-guan-et-al-2024>(5/5 | 106/254) Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain Recommendation (Jianyu Guan et al., 2024)</a></li></ul></li><li><a href=#cslg-52>cs.LG (52)</a><ul><li><a href=#152--107254-dual-operating-modes-of-in-context-learning-ziqian-lin-et-al-2024>(1/52 | 107/254) Dual Operating Modes of In-Context Learning (Ziqian Lin et al., 2024)</a></li><li><a href=#252--108254-generating-reconstructing-and-representing-discrete-and-continuous-data-generalized-diffusion-with-learnable-encoding-decoding-guangyi-liu-et-al-2024>(2/52 | 108/254) Generating, Reconstructing, and Representing Discrete and Continuous Data: Generalized Diffusion with Learnable Encoding-Decoding (Guangyi Liu et al., 2024)</a></li><li><a href=#352--109254-analyzing-and-reducing-catastrophic-forgetting-in-parameter-efficient-tuning-weijieying-ren-et-al-2024>(3/52 | 109/254) Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning (Weijieying Ren et al., 2024)</a></li><li><a href=#452--110254-curiosity-driven-red-teaming-for-large-language-models-zhang-wei-hong-et-al-2024>(4/52 | 110/254) Curiosity-driven Red-teaming for Large Language Models (Zhang-Wei Hong et al., 2024)</a></li><li><a href=#552--111254-archer-training-language-model-agents-via-hierarchical-multi-turn-rl-yifei-zhou-et-al-2024>(5/52 | 111/254) ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL (Yifei Zhou et al., 2024)</a></li><li><a href=#652--112254-griffin-mixing-gated-linear-recurrences-with-local-attention-for-efficient-language-models-soham-de-et-al-2024>(6/52 | 112/254) Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models (Soham De et al., 2024)</a></li><li><a href=#752--113254-loss-free-machine-unlearning-jack-foster-et-al-2024>(7/52 | 113/254) Loss-Free Machine Unlearning (Jack Foster et al., 2024)</a></li><li><a href=#852--114254-theoretical-foundations-of-deep-selective-state-space-models-nicola-muca-cirone-et-al-2024>(8/52 | 114/254) Theoretical Foundations of Deep Selective State-Space Models (Nicola Muca Cirone et al., 2024)</a></li><li><a href=#952--115254-loss-aware-curriculum-learning-for-heterogeneous-graph-neural-networks-zhen-hao-wong-et-al-2024>(9/52 | 115/254) Loss-aware Curriculum Learning for Heterogeneous Graph Neural Networks (Zhen Hao Wong et al., 2024)</a></li><li><a href=#1052--116254-training-dynamics-of-multi-head-softmax-attention-for-in-context-learning-emergence-convergence-and-optimality-siyu-chen-et-al-2024>(10/52 | 116/254) Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality (Siyu Chen et al., 2024)</a></li><li><a href=#1152--117254-verification-of-neural-networks-global-robustness-anan-kabaha-et-al-2024>(11/52 | 117/254) Verification of Neural Networks&rsquo; Global Robustness (Anan Kabaha et al., 2024)</a></li><li><a href=#1252--118254-investigating-gender-fairness-in-machine-learning-driven-personalized-care-for-chronic-pain-pratik-gajane-et-al-2024>(12/52 | 118/254) Investigating Gender Fairness in Machine Learning-driven Personalized Care for Chronic Pain (Pratik Gajane et al., 2024)</a></li><li><a href=#1352--119254-collafuse-navigating-limited-resources-and-privacy-in-collaborative-generative-ai-domenique-zipperling-et-al-2024>(13/52 | 119/254) CollaFuse: Navigating Limited Resources and Privacy in Collaborative Generative AI (Domenique Zipperling et al., 2024)</a></li><li><a href=#1452--120254-improving-group-connectivity-for-generalization-of-federated-deep-learning-zexi-li-et-al-2024>(14/52 | 120/254) Improving Group Connectivity for Generalization of Federated Deep Learning (Zexi Li et al., 2024)</a></li><li><a href=#1552--121254-real-time-adaptive-safety-critical-control-with-gaussian-processes-in-high-order-uncertain-models-yu-zhang-et-al-2024>(15/52 | 121/254) Real-Time Adaptive Safety-Critical Control with Gaussian Processes in High-Order Uncertain Models (Yu Zhang et al., 2024)</a></li><li><a href=#1652--122254-bp-deeponet-a-new-method-for-cuffless-blood-pressure-estimation-using-the-physcis-informed-deeponet-lingfeng-li-et-al-2024>(16/52 | 122/254) BP-DeepONet: A new method for cuffless blood pressure estimation using the physcis-informed DeepONet (Lingfeng Li et al., 2024)</a></li><li><a href=#1752--123254-stiefelgen-a-simple-model-agnostic-approach-for-time-series-data-augmentation-over-riemannian-manifolds-prasad-cheema-et-al-2024>(17/52 | 123/254) StiefelGen: A Simple, Model Agnostic Approach for Time Series Data Augmentation over Riemannian Manifolds (Prasad Cheema et al., 2024)</a></li><li><a href=#1852--124254-deep-learning-for-cross-domain-data-fusion-in-urban-computing-taxonomy-advances-and-outlook-xingchen-zou-et-al-2024>(18/52 | 124/254) Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook (Xingchen Zou et al., 2024)</a></li><li><a href=#1952--125254-fedstruct-federated-decoupled-learning-over-interconnected-graphs-javad-aliakbari-et-al-2024>(19/52 | 125/254) FedStruct: Federated Decoupled Learning over Interconnected Graphs (Javad Aliakbari et al., 2024)</a></li><li><a href=#2052--126254-timexer-empowering-transformers-for-time-series-forecasting-with-exogenous-variables-yuxuan-wang-et-al-2024>(20/52 | 126/254) TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables (Yuxuan Wang et al., 2024)</a></li><li><a href=#2152--127254-to-pool-or-not-to-pool-analyzing-the-regularizing-effects-of-group-fair-training-on-shared-models-cyrus-cousins-et-al-2024>(21/52 | 127/254) To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair Training on Shared Models (Cyrus Cousins et al., 2024)</a></li><li><a href=#2252--128254-mpat-building-robust-deep-neural-networks-against-textual-adversarial-attacks-fangyuan-zhang-et-al-2024>(22/52 | 128/254) MPAT: Building Robust Deep Neural Networks against Textual Adversarial Attacks (Fangyuan Zhang et al., 2024)</a></li><li><a href=#2352--129254-heavy-tailed-class-imbalance-and-why-adam-outperforms-gradient-descent-on-language-models-frederik-kunstner-et-al-2024>(23/52 | 129/254) Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models (Frederik Kunstner et al., 2024)</a></li><li><a href=#2452--130254-estimation-and-deconvolution-of-second-order-cyclostationary-signals-igor-makienko-et-al-2024>(24/52 | 130/254) Estimation and Deconvolution of Second Order Cyclostationary Signals (Igor Makienko et al., 2024)</a></li><li><a href=#2552--131254-sprifed-omp-a-differentially-private-federated-learning-algorithm-for-sparse-basis-recovery-ajinkya-kiran-mulay-et-al-2024>(25/52 | 131/254) SPriFed-OMP: A Differentially Private Federated Learning Algorithm for Sparse Basis Recovery (Ajinkya Kiran Mulay et al., 2024)</a></li><li><a href=#2652--132254-stop-relying-on-no-choice-and-do-not-repeat-the-moves-optimal-efficient-and-practical-algorithms-for-assortment-optimization-aadirupa-saha-et-al-2024>(26/52 | 132/254) Stop Relying on No-Choice and Do not Repeat the Moves: Optimal, Efficient and Practical Algorithms for Assortment Optimization (Aadirupa Saha et al., 2024)</a></li><li><a href=#2752--133254-extended-flow-matching-a-method-of-conditional-generation-with-generalized-continuity-equation-noboru-isobe-et-al-2024>(27/52 | 133/254) Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation (Noboru Isobe et al., 2024)</a></li><li><a href=#2852--134254-blockecho-retaining-long-range-dependencies-for-imputing-block-wise-missing-data-qiao-han-et-al-2024>(28/52 | 134/254) BlockEcho: Retaining Long-Range Dependencies for Imputing Block-Wise Missing Data (Qiao Han et al., 2024)</a></li><li><a href=#2952--135254-benchmarking-uncertainty-disentanglement-specialized-uncertainties-for-specialized-tasks-bálint-mucsányi-et-al-2024>(29/52 | 135/254) Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks (Bálint Mucsányi et al., 2024)</a></li><li><a href=#3052--136254-anomaly-detection-in-offshore-wind-turbine-structures-using-hierarchical-bayesian-modelling-s-m-smith-et-al-2024>(30/52 | 136/254) Anomaly Detection in Offshore Wind Turbine Structures using Hierarchical Bayesian Modelling (S. M. Smith et al., 2024)</a></li><li><a href=#3152--137254-supervised-contrastive-representation-learning-landscape-analysis-with-unconstrained-features-tina-behnia-et-al-2024>(31/52 | 137/254) Supervised Contrastive Representation Learning: Landscape Analysis with Unconstrained Features (Tina Behnia et al., 2024)</a></li><li><a href=#3252--138254-flatnas-optimizing-flatness-in-neural-architecture-search-for-out-of-distribution-robustness-matteo-gambella-et-al-2024>(32/52 | 138/254) FlatNAS: optimizing Flatness in Neural Architecture Search for Out-of-Distribution Robustness (Matteo Gambella et al., 2024)</a></li><li><a href=#3352--139254-on-the-convergence-of-differentially-private-fine-tuning-to-linearly-probe-or-to-fully-fine-tune-shuqi-ke-et-al-2024>(33/52 | 139/254) On the Convergence of Differentially-Private Fine-tuning: To Linearly Probe or to Fully Fine-tune? (Shuqi Ke et al., 2024)</a></li><li><a href=#3452--140254-a-model-based-approach-for-improving-reinforcement-learning-efficiency-leveraging-expert-observations-erhan-can-ozcan-et-al-2024>(34/52 | 140/254) A Model-Based Approach for Improving Reinforcement Learning Efficiency Leveraging Expert Observations (Erhan Can Ozcan et al., 2024)</a></li><li><a href=#3552--141254-a-scalable-and-transferable-time-series-prediction-framework-for-demand-forecasting-young-jin-park-et-al-2024>(35/52 | 141/254) A Scalable and Transferable Time Series Prediction Framework for Demand Forecasting (Young-Jin Park et al., 2024)</a></li><li><a href=#3652--142254-structure-preserving-diffusion-models-haoye-lu-et-al-2024>(36/52 | 142/254) Structure Preserving Diffusion Models (Haoye Lu et al., 2024)</a></li><li><a href=#3752--143254-masks-signs-and-learning-rate-rewinding-advait-gadhikar-et-al-2024>(37/52 | 143/254) Masks, Signs, And Learning Rate Rewinding (Advait Gadhikar et al., 2024)</a></li><li><a href=#3852--144254-machine-learning-for-modular-multiplication-kristin-lauter-et-al-2024>(38/52 | 144/254) Machine learning for modular multiplication (Kristin Lauter et al., 2024)</a></li><li><a href=#3952--145254-uncertainty-based-extensible-codebook-for-discrete-federated-learning-in-heterogeneous-data-silos-tianyi-zhang-et-al-2024>(39/52 | 145/254) Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos (Tianyi Zhang et al., 2024)</a></li><li><a href=#4052--146254-dr-strategy-model-based-generalist-agents-with-strategic-dreaming-hany-hamed-et-al-2024>(40/52 | 146/254) Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming (Hany Hamed et al., 2024)</a></li><li><a href=#4152--147254-rethinking-multi-domain-generalization-with-a-general-learning-objective-zhaorui-tan-et-al-2024>(41/52 | 147/254) Rethinking Multi-domain Generalization with A General Learning Objective (Zhaorui Tan et al., 2024)</a></li><li><a href=#4252--148254-applications-of-0-1-neural-networks-in-prescription-and-prediction-vrishabh-patil-et-al-2024>(42/52 | 148/254) Applications of 0-1 Neural Networks in Prescription and Prediction (Vrishabh Patil et al., 2024)</a></li><li><a href=#4352--149254-multi-fidelity-residual-neural-processes-for-scalable-surrogate-modeling-ruijia-niu-et-al-2024>(43/52 | 149/254) Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling (Ruijia Niu et al., 2024)</a></li><li><a href=#4452--150254-enhancing-the-immunity-of-mixture-of-experts-networks-for-adversarial-defense-qiao-han-et-al-2024>(44/52 | 150/254) Enhancing the &lsquo;Immunity&rsquo; of Mixture-of-Experts Networks for Adversarial Defense (Qiao Han et al., 2024)</a></li><li><a href=#4552--151254-disentangling-the-causes-of-plasticity-loss-in-neural-networks-clare-lyle-et-al-2024>(45/52 | 151/254) Disentangling the Causes of Plasticity Loss in Neural Networks (Clare Lyle et al., 2024)</a></li><li><a href=#4652--152254-lifelong-benchmarks-efficient-model-evaluation-in-an-era-of-rapid-progress-ameya-prabhu-et-al-2024>(46/52 | 152/254) Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress (Ameya Prabhu et al., 2024)</a></li><li><a href=#4752--153254-probabilistic-lipschitzness-and-the-stable-rank-for-comparing-explanation-models-lachlan-simpson-et-al-2024>(47/52 | 153/254) Probabilistic Lipschitzness and the Stable Rank for Comparing Explanation Models (Lachlan Simpson et al., 2024)</a></li><li><a href=#4852--154254-learnability-gaps-of-strategic-classification-lee-cohen-et-al-2024>(48/52 | 154/254) Learnability Gaps of Strategic Classification (Lee Cohen et al., 2024)</a></li><li><a href=#4952--155254-degradation-modeling-and-prognostic-analysis-under-unknown-failure-modes-ying-fu-et-al-2024>(49/52 | 155/254) Degradation Modeling and Prognostic Analysis Under Unknown Failure Modes (Ying Fu et al., 2024)</a></li><li><a href=#5052--156254-negative-binomial-randomized-gamma-markov-processes-for-heterogeneous-overdispersed-count-time-series-rui-huang-et-al-2024>(50/52 | 156/254) Negative-Binomial Randomized Gamma Markov Processes for Heterogeneous Overdispersed Count Time Series (Rui Huang et al., 2024)</a></li><li><a href=#5152--157254-graph-generation-via-spectral-diffusion-giorgia-minello-et-al-2024>(51/52 | 157/254) Graph Generation via Spectral Diffusion (Giorgia Minello et al., 2024)</a></li><li><a href=#5252--158254-taking-second-life-batteries-from-exhausted-to-empowered-using-experiments-data-analysis-and-health-estimation-xiaofan-cui-et-al-2024>(52/52 | 158/254) Taking Second-life Batteries from Exhausted to Empowered using Experiments, Data Analysis, and Health Estimation (Xiaofan Cui et al., 2024)</a></li></ul></li><li><a href=#csse-5>cs.SE (5)</a><ul><li><a href=#15--159254-compositional-api-recommendation-for-library-oriented-code-generation-zexiong-ma-et-al-2024>(1/5 | 159/254) Compositional API Recommendation for Library-Oriented Code Generation (Zexiong Ma et al., 2024)</a></li><li><a href=#25--160254-starcoder-2-and-the-stack-v2-the-next-generation-anton-lozhkov-et-al-2024>(2/5 | 160/254) StarCoder 2 and The Stack v2: The Next Generation (Anton Lozhkov et al., 2024)</a></li><li><a href=#35--161254-the-counterfeit-conundrum-can-code-language-models-grasp-the-nuances-of-their-incorrect-generations-alex-gu-et-al-2024>(3/5 | 161/254) The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations? (Alex Gu et al., 2024)</a></li><li><a href=#45--162254-understanding-fairness-in-software-engineering-insights-from-stack-exchange-emeralda-sesari-et-al-2024>(4/5 | 162/254) Understanding Fairness in Software Engineering: Insights from Stack Exchange (Emeralda Sesari et al., 2024)</a></li><li><a href=#55--163254-cebin-a-cost-effective-framework-for-large-scale-binary-code-similarity-detection-hao-wang-et-al-2024>(5/5 | 163/254) CEBin: A Cost-Effective Framework for Large-Scale Binary Code Similarity Detection (Hao Wang et al., 2024)</a></li></ul></li><li><a href=#cscy-5>cs.CY (5)</a><ul><li><a href=#15--164254-wisdom-of-the-silicon-crowd-llm-ensemble-prediction-capabilities-match-human-crowd-accuracy-philipp-schoenegger-et-al-2024>(1/5 | 164/254) Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy (Philipp Schoenegger et al., 2024)</a></li><li><a href=#25--165254-mobile-health-text-misinformation-identification-using-mobile-data-mining-wen-chen-hu-et-al-2024>(2/5 | 165/254) Mobile Health Text Misinformation Identification Using Mobile Data Mining (Wen-Chen Hu et al., 2024)</a></li><li><a href=#35--166254-shared-lightweight-autonomous-vehicles-for-urban-food-deliveries-a-simulation-study-ainhoa-genua-cerviño-et-al-2024>(3/5 | 166/254) Shared lightweight autonomous vehicles for urban food deliveries: A simulation study (Ainhoa Genua Cerviño et al., 2024)</a></li><li><a href=#45--167254-fate-in-mmla-a-student-centred-exploration-of-fairness-accountability-transparency-and-ethics-in-multimodal-learning-analytics-yueqiao-jin-et-al-2024>(4/5 | 167/254) FATE in MMLA: A Student-Centred Exploration of Fairness, Accountability, Transparency, and Ethics in Multimodal Learning Analytics (Yueqiao Jin et al., 2024)</a></li><li><a href=#55--168254-envisioning-the-applications-and-implications-of-generative-ai-for-news-media-sachita-nishal-et-al-2024>(5/5 | 168/254) Envisioning the Applications and Implications of Generative AI for News Media (Sachita Nishal et al., 2024)</a></li></ul></li><li><a href=#cscr-10>cs.CR (10)</a><ul><li><a href=#110--169254-always-be-pre-training-representation-learning-for-network-intrusion-detection-with-gnns-zhengyao-gu-et-al-2024>(1/10 | 169/254) Always be Pre-Training: Representation Learning for Network Intrusion Detection with GNNs (Zhengyao Gu et al., 2024)</a></li><li><a href=#210--170254-prsa-prompt-reverse-stealing-attacks-against-large-language-models-yong-yang-et-al-2024>(2/10 | 170/254) PRSA: Prompt Reverse Stealing Attacks against Large Language Models (Yong Yang et al., 2024)</a></li><li><a href=#310--171254-syntactic-ghost-an-imperceptible-general-purpose-backdoor-attacks-on-pre-trained-language-models-pengzhou-cheng-et-al-2024>(3/10 | 171/254) Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models (Pengzhou Cheng et al., 2024)</a></li><li><a href=#410--172254-sok-exploring-the-potential-of-large-language-models-for-improving-digital-forensic-investigation-efficiency-akila-wickramasekara-et-al-2024>(4/10 | 172/254) SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency (Akila Wickramasekara et al., 2024)</a></li><li><a href=#510--173254-watermark-stealing-in-large-language-models-nikola-jovanović-et-al-2024>(5/10 | 173/254) Watermark Stealing in Large Language Models (Nikola Jovanović et al., 2024)</a></li><li><a href=#610--174254-how-to-train-your-antivirus-rl-based-hardening-through-the-problem-space-jacopo-cortellazzi-et-al-2024>(6/10 | 174/254) How to Train your Antivirus: RL-based Hardening through the Problem-Space (Jacopo Cortellazzi et al., 2024)</a></li><li><a href=#710--175254-robwe-robust-watermark-embedding-for-personalized-federated-learning-model-ownership-protection-yang-xu-et-al-2024>(7/10 | 175/254) RobWE: Robust Watermark Embedding for Personalized Federated Learning Model Ownership Protection (Yang Xu et al., 2024)</a></li><li><a href=#810--176254-privacy-management-and-interface-design-for-a-smart-house-ana-maria-comeaga-et-al-2024>(8/10 | 176/254) Privacy Management and Interface Design for a Smart House (Ana-Maria Comeaga et al., 2024)</a></li><li><a href=#910--177254-attacks-against-mobility-prediction-in-5g-networks-syafiq-al-atiiq-et-al-2024>(9/10 | 177/254) Attacks Against Mobility Prediction in 5G Networks (Syafiq Al Atiiq et al., 2024)</a></li><li><a href=#1010--178254-rahmani-sort-a-novel-variant-of-insertion-sort-algorithm-with-onlogn-complexity-mohammad-khalid-imam-rahmani-2024>(10/10 | 178/254) Rahmani Sort: A Novel Variant of Insertion Sort Algorithm with O(nlogn) Complexity (Mohammad Khalid Imam Rahmani, 2024)</a></li></ul></li><li><a href=#csce-2>cs.CE (2)</a><ul><li><a href=#12--179254-generative-models-struggle-with-kirigami-metamaterials-gerrit-felsch-et-al-2024>(1/2 | 179/254) Generative models struggle with kirigami metamaterials (Gerrit Felsch et al., 2024)</a></li><li><a href=#22--180254-protein-multimer-structure-prediction-via-prompt-learning-ziqi-gao-et-al-2024>(2/2 | 180/254) Protein Multimer Structure Prediction via Prompt Learning (Ziqi Gao et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#11--181254-a-protein-structure-prediction-approach-leveraging-transformer-and-cnn-integration-yanlin-zhou-et-al-2024>(1/1 | 181/254) A Protein Structure Prediction Approach Leveraging Transformer and CNN Integration (Yanlin Zhou et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-2>cond-mat.mtrl-sci (2)</a><ul><li><a href=#12--182254-training-set-free-two-stage-deep-learning-for-spectroscopic-data-de-noising-dongchen-huang-junde-liu-et-al-2024>(1/2 | 182/254) Training-set-free two-stage deep learning for Spectroscopic data de-noising (Dongchen Huang. Junde Liu et al., 2024)</a></li><li><a href=#22--183254-accelerating-materials-discovery-for-polymer-solar-cells-data-driven-insights-enabled-by-natural-language-processing-pranav-shetty-et-al-2024>(2/2 | 183/254) Accelerating materials discovery for polymer solar cells: Data-driven insights enabled by natural language processing (Pranav Shetty et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#12--184254-a-sound-approach-using-large-language-models-to-generate-audio-descriptions-for-egocentric-text-audio-retrieval-andreea-maria-oncescu-et-al-2024>(1/2 | 184/254) A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval (Andreea-Maria Oncescu et al., 2024)</a></li><li><a href=#22--185254-extending-multilingual-speech-synthesis-to-100-languages-without-transcribed-data-takaaki-saeki-et-al-2024>(2/2 | 185/254) Extending Multilingual Speech Synthesis to 100+ Languages without Transcribed Data (Takaaki Saeki et al., 2024)</a></li></ul></li><li><a href=#eesssy-4>eess.SY (4)</a><ul><li><a href=#14--186254-closed-loop-training-of-static-output-feedback-neural-network-controllers-for-large-systems-a-distillation-case-study-e-m-turan-et-al-2024>(1/4 | 186/254) Closed-loop training of static output feedback neural network controllers for large systems: A distillation case study (E. M. Turan et al., 2024)</a></li><li><a href=#24--187254-temporal-aware-deep-reinforcement-learning-for-energy-storage-bidding-in-energy-and-contingency-reserve-markets-jinhao-li-et-al-2024>(2/4 | 187/254) Temporal-Aware Deep Reinforcement Learning for Energy Storage Bidding in Energy and Contingency Reserve Markets (Jinhao Li et al., 2024)</a></li><li><a href=#34--188254-ultraviolet-positioning-via-tdoa-error-analysis-and-system-prototype-shihui-yu-et-al-2024>(3/4 | 188/254) Ultraviolet Positioning via TDOA: Error Analysis and System Prototype (Shihui Yu et al., 2024)</a></li><li><a href=#44--189254-adaptive-testing-environment-generation-for-connected-and-automated-vehicles-with-dense-reinforcement-learning-jingxuan-yang-et-al-2024>(4/4 | 189/254) Adaptive Testing Environment Generation for Connected and Automated Vehicles with Dense Reinforcement Learning (Jingxuan Yang et al., 2024)</a></li></ul></li><li><a href=#csai-4>cs.AI (4)</a><ul><li><a href=#14--190254-rl-gpt-integrating-reinforcement-learning-and-code-as-policy-shaoteng-liu-et-al-2024>(1/4 | 190/254) RL-GPT: Integrating Reinforcement Learning and Code-as-policy (Shaoteng Liu et al., 2024)</a></li><li><a href=#24--191254-functional-benchmarks-for-robust-evaluation-of-reasoning-performance-and-the-reasoning-gap-saurabh-srivastava-et-al-2024>(2/4 | 191/254) Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap (Saurabh Srivastava et al., 2024)</a></li><li><a href=#34--192254-a-cognitive-based-trajectory-prediction-approach-for-autonomous-driving-haicheng-liao-et-al-2024>(3/4 | 192/254) A Cognitive-Based Trajectory Prediction Approach for Autonomous Driving (Haicheng Liao et al., 2024)</a></li><li><a href=#44--193254-negative-sampling-in-knowledge-graph-representation-learning-a-review-tiroshan-madushanka-et-al-2024>(4/4 | 193/254) Negative Sampling in Knowledge Graph Representation Learning: A Review (Tiroshan Madushanka et al., 2024)</a></li></ul></li><li><a href=#csro-9>cs.RO (9)</a><ul><li><a href=#19--194254-mirage-cross-embodiment-zero-shot-policy-transfer-with-cross-painting-lawrence-yunliang-chen-et-al-2024>(1/9 | 194/254) Mirage: Cross-Embodiment Zero-Shot Policy Transfer with Cross-Painting (Lawrence Yunliang Chen et al., 2024)</a></li><li><a href=#29--195254-armchair-integrated-inverse-reinforcement-learning-and-model-predictive-control-for-human-robot-collaboration-angelo-caregnato-neto-et-al-2024>(2/9 | 195/254) ARMCHAIR: integrated inverse reinforcement learning and model predictive control for human-robot collaboration (Angelo Caregnato-Neto et al., 2024)</a></li><li><a href=#39--196254-conversational-language-models-for-human-in-the-loop-multi-robot-coordination-william-hunt-et-al-2024>(3/9 | 196/254) Conversational Language Models for Human-in-the-Loop Multi-Robot Coordination (William Hunt et al., 2024)</a></li><li><a href=#49--197254-humanoid-locomotion-as-next-token-prediction-ilija-radosavovic-et-al-2024>(4/9 | 197/254) Humanoid Locomotion as Next Token Prediction (Ilija Radosavovic et al., 2024)</a></li><li><a href=#59--198254-pushing-the-limits-of-cross-embodiment-learning-for-manipulation-and-navigation-jonathan-yang-et-al-2024>(5/9 | 198/254) Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation (Jonathan Yang et al., 2024)</a></li><li><a href=#69--199254-contact-implicit-model-predictive-control-for-dexterous-in-hand-manipulation-a-long-horizon-and-robust-approach-yongpeng-jiang-et-al-2024>(6/9 | 199/254) Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach (Yongpeng Jiang et al., 2024)</a></li><li><a href=#79--200254-roadrunner----learning-traversability-estimation-for-autonomous-off-road-driving-jonas-frey-et-al-2024>(7/9 | 200/254) RoadRunner &ndash; Learning Traversability Estimation for Autonomous Off-road Driving (Jonas Frey et al., 2024)</a></li><li><a href=#89--201254-genie-smart-ros-based-caching-for-connected-autonomous-robots-zexin-li-et-al-2024>(8/9 | 201/254) Genie: Smart ROS-based Caching for Connected Autonomous Robots (Zexin Li et al., 2024)</a></li><li><a href=#99--202254-relead-resilient-localization-with-enhanced-lidar-odometry-in-adverse-environments-zhiqiang-chen-et-al-2024>(9/9 | 202/254) RELEAD: Resilient Localization with Enhanced LiDAR Odometry in Adverse Environments (Zhiqiang Chen et al., 2024)</a></li></ul></li><li><a href=#eessiv-12>eess.IV (12)</a><ul><li><a href=#112--203254-unsupervised-learning-of-high-resolution-light-field-imaging-via-beam-splitter-based-hybrid-lenses-jianxin-lei-et-al-2024>(1/12 | 203/254) Unsupervised Learning of High-resolution Light Field Imaging via Beam Splitter-based Hybrid Lenses (Jianxin Lei et al., 2024)</a></li><li><a href=#212--204254-graph-convolutional-neural-networks-for-automated-echocardiography-view-recognition-a-holistic-approach-sarina-thomas-et-al-2024>(2/12 | 204/254) Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach (Sarina Thomas et al., 2024)</a></li><li><a href=#312--205254-sed-semantic-aware-discriminator-for-image-super-resolution-bingchen-li-et-al-2024>(3/12 | 205/254) SeD: Semantic-Aware Discriminator for Image Super-Resolution (Bingchen Li et al., 2024)</a></li><li><a href=#412--206254-gdcnet-calibrationless-geometric-distortion-correction-of-echo-planar-imaging-data-using-deep-learning-marina-manso-jimeno-et-al-2024>(4/12 | 206/254) GDCNet: Calibrationless geometric distortion correction of echo planar imaging data using deep learning (Marina Manso Jimeno et al., 2024)</a></li><li><a href=#512--207254-towards-generalizable-tumor-synthesis-qi-chen-et-al-2024>(5/12 | 207/254) Towards Generalizable Tumor Synthesis (Qi Chen et al., 2024)</a></li><li><a href=#612--208254-wdm-3d-wavelet-diffusion-models-for-high-resolution-medical-image-synthesis-paul-friedrich-et-al-2024>(6/12 | 208/254) WDM: 3D Wavelet Diffusion Models for High-Resolution Medical Image Synthesis (Paul Friedrich et al., 2024)</a></li><li><a href=#712--209254-camixersr-only-details-need-more-attention-yan-wang-et-al-2024>(7/12 | 209/254) CAMixerSR: Only Details Need More &lsquo;Attention&rsquo; (Yan Wang et al., 2024)</a></li><li><a href=#812--210254-training-generative-image-super-resolution-models-by-wavelet-domain-losses-enables-better-control-of-artifacts-cansu-korkmaz-et-al-2024>(8/12 | 210/254) Training Generative Image Super-Resolution Models by Wavelet-Domain Losses Enables Better Control of Artifacts (Cansu Korkmaz et al., 2024)</a></li><li><a href=#912--211254-deep-network-for-image-compressed-sensing-coding-using-local-structural-sampling-wenxue-cui-et-al-2024>(9/12 | 211/254) Deep Network for Image Compressed Sensing Coding Using Local Structural Sampling (Wenxue Cui et al., 2024)</a></li><li><a href=#1012--212254-variable-rate-learned-image-compression-with-multi-objective-optimization-and-quantization-reconstruction-offsets-fatih-kamisli-et-al-2024>(10/12 | 212/254) Variable-Rate Learned Image Compression with Multi-Objective Optimization and Quantization-Reconstruction Offsets (Fatih Kamisli et al., 2024)</a></li><li><a href=#1112--213254-lolisrflow-joint-single-image-low-light-enhancement-and-super-resolution-via-cross-scale-transformer-based-conditional-flow-ziyu-yue-et-al-2024>(11/12 | 213/254) LoLiSRFlow: Joint Single Image Low-light Enhancement and Super-resolution via Cross-scale Transformer-based Conditional Flow (Ziyu Yue et al., 2024)</a></li><li><a href=#1212--214254-anatomy-guided-fiber-trajectory-distribution-estimation-for-cranial-nerves-tractography-lei-xie-et-al-2024>(12/12 | 214/254) Anatomy-guided fiber trajectory distribution estimation for cranial nerves tractography (Lei Xie et al., 2024)</a></li></ul></li><li><a href=#cshc-4>cs.HC (4)</a><ul><li><a href=#14--215254-artist-automated-text-simplification-for-task-guidance-in-augmented-reality-guande-wu-et-al-2024>(1/4 | 215/254) ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality (Guande Wu et al., 2024)</a></li><li><a href=#24--216254-poetsofinstagram-navigating-the-practices-and-challenges-of-novice-poets-on-instagram-ankolika-de-et-al-2024>(2/4 | 216/254) #PoetsOfInstagram: Navigating The Practices And Challenges Of Novice Poets On Instagram (Ankolika De et al., 2024)</a></li><li><a href=#34--217254-discern-designing-decision-support-interfaces-to-investigate-the-complexities-of-workplace-social-decision-making-with-line-managers-pranav-khadpe-et-al-2024>(3/4 | 217/254) DISCERN: Designing Decision Support Interfaces to Investigate the Complexities of Workplace Social Decision-Making With Line Managers (Pranav Khadpe et al., 2024)</a></li><li><a href=#44--218254-think-fast-think-slow-think-critical-designing-an-automated-propaganda-detection-tool-liudmila-zavolokina-et-al-2024>(4/4 | 218/254) Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool (Liudmila Zavolokina et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--219254-probing-the-information-encoded-in-neural-based-acoustic-models-of-automatic-speech-recognition-systems-quentin-raymondaud-et-al-2024>(1/2 | 219/254) Probing the Information Encoded in Neural-based Acoustic Models of Automatic Speech Recognition Systems (Quentin Raymondaud et al., 2024)</a></li><li><a href=#22--220254-unraveling-adversarial-examples-against-speaker-identification----techniques-for-attack-detection-and-victim-model-classification-sonal-joshi-et-al-2024>(2/2 | 220/254) Unraveling Adversarial Examples against Speaker Identification &ndash; Techniques for Attack Detection and Victim Model Classification (Sonal Joshi et al., 2024)</a></li></ul></li><li><a href=#csni-3>cs.NI (3)</a><ul><li><a href=#13--221254-energy-efficient-uav-swarm-assisted-mec-with-dynamic-clustering-and-scheduling-jialiuyuan-li-et-al-2024>(1/3 | 221/254) Energy-Efficient UAV Swarm Assisted MEC with Dynamic Clustering and Scheduling (Jialiuyuan Li et al., 2024)</a></li><li><a href=#23--222254-x-resq-reverse-annealing-for-quantum-mimo-detection-with-flexible-parallelism-minsung-kim-et-al-2024>(2/3 | 222/254) X-ResQ: Reverse Annealing for Quantum MIMO Detection with Flexible Parallelism (Minsung Kim et al., 2024)</a></li><li><a href=#33--223254-vision-radio-experimental-infrastructure-architecture-towards-6g-filipe-b-teixeira-et-al-2024>(3/3 | 223/254) Vision-Radio Experimental Infrastructure Architecture Towards 6G (Filipe B. Teixeira et al., 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#12--224254-flexllm-a-system-for-co-serving-large-language-model-inference-and-parameter-efficient-finetuning-xupeng-miao-et-al-2024>(1/2 | 224/254) FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning (Xupeng Miao et al., 2024)</a></li><li><a href=#22--225254-arrow-matrix-decomposition-a-novel-approach-for-communication-efficient-sparse-matrix-multiplication-lukas-gianinazzi-et-al-2024>(2/2 | 225/254) Arrow Matrix Decomposition: A Novel Approach for Communication-Efficient Sparse Matrix Multiplication (Lukas Gianinazzi et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--226254-statistical-estimation-in-the-spiked-tensor-model-via-the-quantum-approximate-optimization-algorithm-leo-zhou-et-al-2024>(1/1 | 226/254) Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm (Leo Zhou et al., 2024)</a></li></ul></li><li><a href=#mathap-1>math.AP (1)</a><ul><li><a href=#11--227254-high-multiplicity-of-positive-solutions-in-a-superlinear-problem-of-moore-nehari-type-pablo-cubillos-et-al-2024>(1/1 | 227/254) High multiplicity of positive solutions in a superlinear problem of Moore-Nehari type (Pablo Cubillos et al., 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--228254-spyx-a-library-for-just-in-time-compiled-optimization-of-spiking-neural-networks-kade-m-heckel-et-al-2024>(1/2 | 228/254) Spyx: A Library for Just-In-Time Compiled Optimization of Spiking Neural Networks (Kade M. Heckel et al., 2024)</a></li><li><a href=#22--229254-airport-take-off-and-landing-optimization-through-genetic-algorithms-fernando-guedan-pecker-et-al-2024>(2/2 | 229/254) Airport take-off and landing optimization through genetic algorithms (Fernando Guedan Pecker et al., 2024)</a></li></ul></li><li><a href=#csit-3>cs.IT (3)</a><ul><li><a href=#13--230254-helper-data-schemes-for-coded-modulation-and-shaping-in-physical-unclonable-functions-robert-f-h-fischer-2024>(1/3 | 230/254) Helper Data Schemes for Coded Modulation and Shaping in Physical Unclonable Functions (Robert F. H. Fischer, 2024)</a></li><li><a href=#23--231254-digital-twin-aided-massive-mimo-csi-compression-and-feedback-shuaifeng-jiang-et-al-2024>(2/3 | 231/254) Digital Twin Aided Massive MIMO: CSI Compression and Feedback (Shuaifeng Jiang et al., 2024)</a></li><li><a href=#33--232254-evaluating-the-gilbert-varshamov-bound-for-constrained-systems-keshav-goyal-et-al-2024>(3/3 | 232/254) Evaluating the Gilbert-Varshamov Bound for Constrained Systems (Keshav Goyal et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--233254-message-enhanced-degroot-model-huisheng-wang-et-al-2024>(1/1 | 233/254) Message-Enhanced DeGroot Model (Huisheng Wang et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--234254-link-recommendation-to-augment-influence-diffusion-with-provable-guarantees-xiaolong-chen-et-al-2024>(1/2 | 234/254) Link Recommendation to Augment Influence Diffusion with Provable Guarantees (Xiaolong Chen et al., 2024)</a></li><li><a href=#22--235254-higher-order-networks-representation-and-learning-a-survey-hao-tian-et-al-2024>(2/2 | 235/254) Higher-Order Networks Representation and Learning: A Survey (Hao Tian et al., 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#11--236254-efficient-processing-of-subsequent-densest-subgraph-query-chia-yang-hung-et-al-2024>(1/1 | 236/254) Efficient Processing of Subsequent Densest Subgraph Query (Chia-Yang Hung et al., 2024)</a></li></ul></li><li><a href=#statml-1>stat.ML (1)</a><ul><li><a href=#11--237254-listening-to-the-noise-blind-denoising-with-gibbs-diffusion-david-heurtel-depeiges-et-al-2024>(1/1 | 237/254) Listening to the Noise: Blind Denoising with Gibbs Diffusion (David Heurtel-Depeiges et al., 2024)</a></li></ul></li><li><a href=#csgt-2>cs.GT (2)</a><ul><li><a href=#12--238254-understanding-iterative-combinatorial-auction-designs-via-multi-agent-reinforcement-learning-greg-deon-et-al-2024>(1/2 | 238/254) Understanding Iterative Combinatorial Auction Designs via Multi-Agent Reinforcement Learning (Greg d&rsquo;Eon et al., 2024)</a></li><li><a href=#22--239254-conjectural-online-learning-with-first-order-beliefs-in-asymmetric-information-stochastic-games-tao-li-et-al-2024>(2/2 | 239/254) Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games (Tao Li et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--240254-deep-reinforcement-learning-a-convex-optimization-approach-ather-gattami-2024>(1/1 | 240/254) Deep Reinforcement Learning: A Convex Optimization Approach (Ather Gattami, 2024)</a></li></ul></li><li><a href=#cslo-4>cs.LO (4)</a><ul><li><a href=#14--241254-rewriting-and-inductive-reasoning-márton-hajdu-et-al-2024>(1/4 | 241/254) Rewriting and Inductive Reasoning (Márton Hajdu et al., 2024)</a></li><li><a href=#24--242254-program-synthesis-in-saturation-petra-hozzová-et-al-2024>(2/4 | 242/254) Program Synthesis in Saturation (Petra Hozzová et al., 2024)</a></li><li><a href=#34--243254-getting-saturated-with-induction-márton-hajdu-et-al-2024>(3/4 | 243/254) Getting Saturated with Induction (Márton Hajdu et al., 2024)</a></li><li><a href=#44--244254-invariant-checking-for-smt-based-systems-with-quantifiers-gianluca-redondi-et-al-2024>(4/4 | 244/254) Invariant Checking for SMT-based Systems with Quantifiers (Gianluca Redondi et al., 2024)</a></li></ul></li><li><a href=#csar-2>cs.AR (2)</a><ul><li><a href=#12--245254-mimdram-an-end-to-end-processing-using-dram-system-for-high-throughput-energy-efficient-and-programmer-transparent-multiple-instruction-multiple-data-processing-geraldo-f-oliveira-et-al-2024>(1/2 | 245/254) MIMDRAM: An End-to-End Processing-Using-DRAM System for High-Throughput, Energy-Efficient and Programmer-Transparent Multiple-Instruction Multiple-Data Processing (Geraldo F. Oliveira et al., 2024)</a></li><li><a href=#22--246254-ozmac-an-energy-efficient-sparsity-exploiting-multiply-accumulate-unit-design-for-dl-inference-harideep-nair-et-al-2024>(2/2 | 246/254) OzMAC: An Energy-Efficient Sparsity-Exploiting Multiply-Accumulate-Unit Design for DL Inference (Harideep Nair et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--247254-fractional-material-derivative-pointwise-representation-and-a-finite-volume-numerical-scheme-łukasz-płociniczak-et-al-2024>(1/2 | 247/254) Fractional material derivative: pointwise representation and a finite volume numerical scheme (Łukasz Płociniczak et al., 2024)</a></li><li><a href=#22--248254-an-asymptotic-preserving-method-for-the-three-temperature-radiative-transfer-model-ruo-li-et-al-2024>(2/2 | 248/254) An asymptotic-preserving method for the three-temperature radiative transfer model (Ruo Li et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--249254-3d-gaussian-model-for-animation-and-texturing-xiangzhi-eric-wang-et-al-2024>(1/1 | 249/254) 3D Gaussian Model for Animation and Texturing (Xiangzhi Eric Wang et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--250254-on-efficient-computation-of-dire-committees-kunal-relia-2024>(1/1 | 250/254) On Efficient Computation of DiRe Committees (Kunal Relia, 2024)</a></li></ul></li><li><a href=#mathco-3>math.CO (3)</a><ul><li><a href=#13--251254-oriented-trees-in-ok-sqrtk-chromatic-digraphs-a-subquadratic-bound-for-burrs-conjecture-stéphane-bessy-et-al-2024>(1/3 | 251/254) Oriented trees in $O(k \sqrt{k})$-chromatic digraphs, a subquadratic bound for Burr&rsquo;s conjecture (Stéphane Bessy et al., 2024)</a></li><li><a href=#23--252254-broadcast-independence-number-of-oriented-circulant-graphs-abdelamin-laouar-et-al-2024>(2/3 | 252/254) Broadcast independence number of oriented circulant graphs (Abdelamin Laouar et al., 2024)</a></li><li><a href=#33--253254-graph-burning-bounds-and-hardness-dhanyamol-antony-et-al-2024>(3/3 | 253/254) Graph Burning: Bounds and Hardness (Dhanyamol Antony et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--254254-more-algorithmic-results-for-problems-of-spread-of-influence-in-edge-weighted-graphs-with-and-without-incentives-siavash-askari-et-al-2024>(1/1 | 254/254) More algorithmic results for problems of spread of influence in edge-weighted graphs with and without incentives (Siavash Askari et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>