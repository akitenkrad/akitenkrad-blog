<!doctype html><html><head><title>arXiv @ 2024.03.25</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.25"><meta property="og:description" content="Primary Categories cond-mat.mtrl-sci (1) cs.AI (10) cs.CL (18) cs.CR (1) cs.CV (28) cs.CY (2) cs.DC (1) cs.DS (1) cs.GR (1) cs.HC (1) cs.IR (1) cs.IT (4) cs.LG (13) cs.MA (1) cs.NI (3) cs.RO (7) cs.SE (3) cs.SI (2) cs.SY (1) eess.IV (3) eess.SY (10) math.NA (1) stat.ME (2) Keywords keyword cs.AI cs.CL cs.CV cs.LG eess.SY AI-generated Text Detection 1 Active Learning 1 1 Adversarial Attack 1 Adversarial Learning 1 Autoencoder 1 2 Automatic Speech Recognition 1 BERT 2 Benchmarking 2 3 6 1 Black Box 1 Chain-of-thought Prompt 1 ChatGPT 1 Chatbot 1 Claude 1 Clustering 1 1 Code Generation 1 Common-sense Reasoning 1 Contrastive Learning 1 ControlNet 2 Convolution 2 3 1 Convolutional Neural Network 1 1 3 2 Data Augmentation 1 Dialogue System 1 Differential Privacy 1 Diffusion Model 4 1 Discrete Time 2 Distribution Shift 2 Domain Adaptation 1 Fairness 1 Federated Learning 1 2 Few-shot 1 2 3 Few-shot Learning 1 1 Fine-tuning 3 1 2 GLUE 1 GPT 1 2 2 GPT-3 1 GPT-3."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240325000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-25T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-25T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.25"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240325000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Monday, Mar 25, 2024</p></div><div class=title><h1>arXiv @ 2024.03.25</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#csai-10>cs.AI (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#cscl-18>cs.CL (18)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#cscr-1>cs.CR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#cscv-28>cs.CV (28)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#cscy-2>cs.CY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#cshc-1>cs.HC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#csir-1>cs.IR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#csit-4>cs.IT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#cslg-13>cs.LG (13)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#csni-3>cs.NI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#csro-7>cs.RO (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#csse-3>cs.SE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#cssy-1>cs.SY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#eessiv-3>eess.IV (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#eesssy-10>eess.SY (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#mathna-1>math.NA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/#statme-2>stat.ME (2)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>eess.SY</th></tr></thead><tbody><tr><td>AI-generated Text Detection</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Active Learning</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>BERT</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>2</td><td>3</td><td>6</td><td>1</td><td></td></tr><tr><td>Black Box</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Chain-of-thought Prompt</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Claude</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Code Generation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>ControlNet</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>2</td><td>3</td><td>1</td><td></td></tr><tr><td>Convolutional Neural Network</td><td>1</td><td>1</td><td>3</td><td>2</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Dialogue System</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>4</td><td>1</td><td></td></tr><tr><td>Discrete Time</td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Distribution Shift</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Federated Learning</td><td>1</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Few-shot</td><td>1</td><td>2</td><td>3</td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td></td><td>3</td><td>1</td><td>2</td><td></td></tr><tr><td>GLUE</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT</td><td>1</td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>GPT-3</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>1</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph</td><td>2</td><td>4</td><td>2</td><td></td><td>1</td></tr><tr><td>Graph Convolutional Network</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>In-context Learning</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>2</td><td>2</td><td>1</td><td></td></tr><tr><td>Knowledge Graph</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>8</td><td>18</td><td>2</td><td></td><td></td></tr><tr><td>Low-Resource</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Model Distillation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Online Reinforcement Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Prompt</td><td>2</td><td>3</td><td>2</td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Question Answering</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td></td><td>3</td><td>2</td><td></td><td></td></tr><tr><td>Recommendation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>2</td><td></td><td>2</td></tr><tr><td>Reinforcement Learning</td><td>1</td><td></td><td></td><td>3</td><td>1</td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td></td><td>6</td><td></td><td></td><td></td></tr><tr><td>RoBERTa</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>1</td><td>4</td><td>2</td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td>1</td><td>1</td><td>8</td></tr><tr><td>Simulator</td><td></td><td></td><td>1</td><td>1</td><td>8</td></tr><tr><td>Stance Detection</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td>2</td><td>1</td></tr><tr><td>Summarization</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>2</td><td>1</td><td>4</td><td></td><td></td></tr><tr><td>Text Augmentation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Text Generation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2SQL</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transformer</td><td>1</td><td>3</td><td>3</td><td></td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>2</td><td>1</td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-18>cs.CL (18)</h2><h3 id=118--1115-leveraging-zero-shot-prompting-for-efficient-language-model-distillation-lukas-vöge-et-al-2024>(1/18 | 1/115) Leveraging Zero-Shot Prompting for Efficient Language Model Distillation (Lukas Vöge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas Vöge, Vincent Gurgul, Stefan Lessmann. (2024)<br><strong>Leveraging Zero-Shot Prompting for Efficient Language Model Distillation</strong><br><button class=copy-to-clipboard title="Leveraging Zero-Shot Prompting for Efficient Language Model Distillation" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 100<br>Keywords: Few-shot, Fine-tuning, Human Intervention, Knowledge Distillation, Knowledge Distillation, Model Distillation, Zero-shot, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15886v1.pdf filename=2403.15886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel approach for efficiently <b>distilling</b> <b>LLMs</b> into smaller, application-specific <b>models,</b> <b>significantly</b> reducing operational costs and manual labor. Addressing the challenge of deploying computationally intensive <b>LLMs</b> in specific applications or edge devices, this technique utilizes <b>LLMs&rsquo;</b> <b>reasoning</b> capabilities to generate labels and natural language rationales for unlabeled data. Our approach enhances both <b>finetuning</b> and <b>distillation</b> by employing a multi-task training framework where student <b>models</b> <b>mimic</b> these rationales alongside teacher predictions. Key contributions include the employment of <b>zero-shot</b> <b>prompting</b> to elicit teacher <b>model</b> <b>rationales,</b> reducing the necessity for handcrafted <b>few-shot</b> examples and lowering the overall token count required, which directly translates to cost savings given the pay-per-token billing <b>model</b> <b>of</b> major tech companies&rsquo; <b>LLM</b> APIs. Additionally, the paper investigates the impact of explanation properties on <b>distillation</b> efficiency, demonstrating that minimal performance loss occurs even when rationale augmentation is not applied across the entire dataset, facilitating further reductions of tokens. This research marks a step toward the efficient training of task-specific <b>models</b> <b>with</b> minimal <b>human</b> <b>intervention,</b> offering substantial cost-savings while maintaining, or even enhancing, performance.</p></p class="citation"></blockquote><h3 id=218--2115-eagle-a-domain-generalization-framework-for-ai-generated-text-detection-amrita-bhattacharjee-et-al-2024>(2/18 | 2/115) EAGLE: A Domain Generalization Framework for AI-generated Text Detection (Amrita Bhattacharjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu. (2024)<br><strong>EAGLE: A Domain Generalization Framework for AI-generated Text Detection</strong><br><button class=copy-to-clipboard title="EAGLE: A Domain Generalization Framework for AI-generated Text Detection" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 100<br>Keywords: Adversarial Learning, Contrastive Learning, Self-supervised Learning, Supervised Learning, Claude, GPT, GPT-4, AI-generated Text Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15690v1.pdf filename=2403.15690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advancement in capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> one major step in the responsible and safe use of such <b>LLMs</b> is to be able to detect text generated by these models. While <b>supervised</b> <b>AI-generated</b> <b>text</b> <b>detectors</b> perform well on text generated by older <b>LLMs,</b> with the frequent release of new <b>LLMs,</b> building <b>supervised</b> detectors for identifying text from such new models would require new labeled training data, which is infeasible in practice. In this work, we tackle this problem and propose a domain generalization framework for the detection of <b>AI-generated</b> <b>text</b> <b>from</b> unseen target generators. Our proposed framework, EAGLE, leverages the labeled data that is available so far from older language models and learns features invariant across these generators, in order to detect text generated by an unknown target generator. EAGLE learns such domain-invariant features by combining the representational power of <b>self-supervised</b> <b>contrastive</b> <b>learning</b> with domain <b>adversarial</b> <b>training.</b> Through our experiments we demonstrate how EAGLE effectively achieves impressive performance in detecting text generated by unseen target generators, including recent state-of-the-art ones such as <b>GPT-4</b> and <b>Claude,</b> reaching detection scores of within 4.7% of a fully <b>supervised</b> detector.</p></p class="citation"></blockquote><h3 id=318--3115-llambert-large-scale-low-cost-data-annotation-in-nlp-bálint-csanády-et-al-2024>(3/18 | 3/115) LlamBERT: Large-scale low-cost data annotation in NLP (Bálint Csanády et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bálint Csanády, Lajos Muzsai, Péter Vedres, Zoltán Nádasdy, András Lukács. (2024)<br><strong>LlamBERT: Large-scale low-cost data annotation in NLP</strong><br><button class=copy-to-clipboard title="LlamBERT: Large-scale low-cost data annotation in NLP" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; F-1-1, cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, BERT, GPT, GPT-4, LLaMA, RoBERTa, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15938v1.pdf filename=2403.15938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> such as <b>GPT-4</b> and <b>Llama</b> 2, show remarkable proficiency in a wide range of natural language processing (NLP) tasks. Despite their effectiveness, the high costs associated with their use pose a challenge. We present LlamBERT, a hybrid approach that leverages <b>LLMs</b> to annotate a small subset of <b>large,</b> <b>unlabeled</b> <b>databases</b> and uses the results for <b>fine-tuning</b> <b>transformer</b> encoders like <b>BERT</b> and <b>RoBERTa.</b> This strategy is evaluated on two diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our results indicate that the LlamBERT approach slightly compromises on accuracy while offering much greater cost-effectiveness.</p></p class="citation"></blockquote><h3 id=418--4115-llms-instruct-llmsan-extraction-and-editing-method-xin-zhang-et-al-2024>(4/18 | 4/115) LLMs Instruct LLMs:An Extraction and Editing Method (Xin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Zhang, Tianjie Ju, Huijia Liang, Ying Fu, Qin Zhang. (2024)<br><strong>LLMs Instruct LLMs:An Extraction and Editing Method</strong><br><button class=copy-to-clipboard title="LLMs Instruct LLMs:An Extraction and Editing Method" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15736v1.pdf filename=2403.15736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The interest in updating <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> without retraining from scratch is substantial, yet it comes with some challenges.This is especially true for situations demanding complex <b>reasoning</b> with limited samples, a scenario we refer to as the Paucity-Constrained Complex <b>Reasoning</b> Adaptation for <b>LLMs</b> (PCRA-LLM).Traditional methods like Low-Rank Adaptation (LoRA) and <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> are inadequate for this critical issue, particularly evident in our exploration of a specific medical context that epitomize the PCRA-LLM&rsquo;s distinct needs.To address the issue, we propose a Sequential Fusion method to incorporate <b>knowledge</b> <b>from</b> complex context into <b>LLMs.</b> This method employs a two-stage framework: initially, it leverages general <b>LLMs</b> to construct <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> for extracting <b>knowledge</b> <b>from</b> complex texts; subsequently, it updates the domain <b>LLMs</b> through <b>knowledge</b> <b>edit.</b> According to our method, the domain <b>LLM</b> achieved a 71.69% accuracy in <b>question</b> <b>answering</b> tasks. Subsequently, we broadened our assessment to a novel dataset we developed in the economics and management field, where our method realized a 75% accuracy. These outcomes underline the efficacy and adaptability of our approach for PCRA-LLM across various domains.</p></p class="citation"></blockquote><h3 id=518--5115-edda-a-encoder-decoder-data-augmentation-framework-for-zero-shot-stance-detection-daijun-ding-et-al-2024>(5/18 | 5/115) EDDA: A Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance Detection (Daijun Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daijun Ding, Li Dong, Zhichao Huang, Guangning Xu, Xu Huang, Bo Liu, Liwen Jing, Bowen Zhang. (2024)<br><strong>EDDA: A Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance Detection</strong><br><button class=copy-to-clipboard title="EDDA: A Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance Detection" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Data Augmentation, Zero-shot, Stance Detection, Text Augmentation, Chain-of-thought Prompt, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15715v1.pdf filename=2403.15715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Stance</b> <b>detection</b> aims to determine the attitude expressed in <b>text</b> <b>towards</b> a given target. <b>Zero-shot</b> <b>stance</b> <b>detection</b> (ZSSD) has emerged to classify <b>stances</b> <b>towards</b> unseen targets during inference. Recent <b>data</b> <b>augmentation</b> techniques for ZSSD increase transferable knowledge between targets through <b>text</b> <b>or</b> target augmentation. However, these methods exhibit limitations. Target augmentation lacks logical connections between generated targets and source <b>text,</b> <b>while</b> <b>text</b> <b>augmentation</b> relies solely on training <b>data,</b> <b>resulting</b> in insufficient generalization. To address these issues, we propose an encoder-decoder <b>data</b> <b>augmentation</b> (EDDA) framework. The encoder leverages <b>large</b> <b>language</b> <b>models</b> and <b>chain-of-thought</b> <b>prompting</b> to <b>summarize</b> <b>texts</b> <b>into</b> target-specific if-then rationales, establishing logical relationships. The decoder generates new samples based on these expressions using a semantic correlation word replacement strategy to increase syntactic diversity. We also analyze the generated expressions to develop a rationale-enhanced network that fully utilizes the augmented <b>data.</b> <b>Experiments</b> on <b>benchmark</b> datasets demonstrate our approach substantially improves over state-of-the-art ZSSD techniques. The proposed EDDA framework increases semantic relevance and syntactic variety in augmented <b>texts</b> <b>while</b> enabling interpretable rationale-based learning.</p></p class="citation"></blockquote><h3 id=618--6115-towards-a-rag-based-summarization-agent-for-the-electron-ion-collider-karthik-suresh-et-al-2024>(6/18 | 6/115) Towards a RAG-based Summarization Agent for the Electron-Ion Collider (Karthik Suresh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karthik Suresh, Neeltje Kackar, Luke Schleck, Cristiano Fanelli. (2024)<br><strong>Towards a RAG-based Summarization Agent for the Electron-Ion Collider</strong><br><button class=copy-to-clipboard title="Towards a RAG-based Summarization Agent for the Electron-Ion Collider" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL, hep-ex, physics-ins-det<br>Keyword Score: 80<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Instruction Tuning, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15729v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15729v2.pdf filename=2403.15729v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The complexity and sheer volume of information encompassing documents, papers, data, and other resources from <b>large-scale</b> <b>experiments</b> <b>demand</b> significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists. To tackle this issue, a <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)&ndash;based</b> <b>Summarization</b> AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use <b>RAG</b> assessments (RAGAs) scoring mechanisms to assess the effectiveness of responses. Furthermore, we describe the concept of <b>prompt</b> template-based <b>instruction-tuning</b> <b>which</b> provides flexibility and accuracy in <b>summarization.</b> Importantly, the implementation relies on LangChain, which serves as the foundation of our entire workflow. This integration ensures efficiency and scalability, facilitating smooth deployment and accessibility for various user groups within the Electron Ion Collider (EIC) community. This innovative AI-driven framework not only simplifies the understanding of vast datasets but also encourages collaborative participation, thereby empowering researchers. As a demonstration, a web application has been developed to explain each stage of the <b>RAG</b> Agent development in detail.</p></p class="citation"></blockquote><h3 id=718--7115-mrc-based-nested-medical-ner-with-co-prediction-and-adaptive-pre-training-xiaojing-du-et-al-2024>(7/18 | 7/115) MRC-based Nested Medical NER with Co-prediction and Adaptive Pre-training (Xiaojing Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaojing Du, Hanjie Zhao, Danyan Xing, Yuxiang Jia, Hongying Zan. (2024)<br><strong>MRC-based Nested Medical NER with Co-prediction and Adaptive Pre-training</strong><br><button class=copy-to-clipboard title="MRC-based Nested Medical NER with Co-prediction and Adaptive Pre-training" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 61<br>Keywords: Graph, Benchmarking, Convolution, Knowledge Graph, Information Retrieval, Named Entity Recognition, Named Entity Recognition, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15800v1.pdf filename=2403.15800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In medical <b>information</b> <b>extraction,</b> medical <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> is indispensable, playing a crucial role in developing medical <b>knowledge</b> <b>graphs,</b> enhancing medical <b>question-answering</b> <b>systems,</b> and analyzing electronic medical records. The challenge in medical <b>NER</b> arises from the complex nested structures and sophisticated medical terminologies, distinguishing it from its counterparts in traditional domains. In response to these complexities, we propose a medical <b>NER</b> model based on Machine Reading Comprehension (MRC), which uses a task-adaptive pre-training strategy to improve the model&rsquo;s capability in the medical field. Meanwhile, our model introduces multiple word-pair embeddings and multi-granularity dilated <b>convolution</b> to enhance the model&rsquo;s representation ability and uses a combined predictor of Biaffine and MLP to improve the model&rsquo;s recognition performance. Experimental evaluations conducted on the CMeEE, a <b>benchmark</b> for Chinese nested medical <b>NER,</b> demonstrate that our proposed model outperforms the compared state-of-the-art (SOTA) models.</p></p class="citation"></blockquote><h3 id=818--8115-stentconv-predicting-disagreement-with-stance-detection-and-a-signed-graph-convolutional-network-isabelle-lorge-et-al-2024>(8/18 | 8/115) STEntConv: Predicting Disagreement with Stance Detection and a Signed Graph Convolutional Network (Isabelle Lorge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Isabelle Lorge, Li Zhang, Xiaowen Dong, Janet B. Pierrehumbert. (2024)<br><strong>STEntConv: Predicting Disagreement with Stance Detection and a Signed Graph Convolutional Network</strong><br><button class=copy-to-clipboard title="STEntConv: Predicting Disagreement with Stance Detection and a Signed Graph Convolutional Network" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Unsupervised Learning, Stance Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15885v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15885v2.pdf filename=2403.15885v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of social media platforms has led to an increase in polarised online discussions, especially on political and socio-cultural topics such as elections and climate change. We propose a simple and novel <b>unsupervised</b> method to predict whether the authors of two posts agree or disagree, leveraging user <b>stances</b> <b>about</b> named entities obtained from their posts. We present STEntConv, a model which builds a <b>graph</b> <b>of</b> <b>users</b> and named entities weighted by <b>stance</b> <b>and</b> trains a Signed <b>Graph</b> <b>Convolutional</b> <b>Network</b> (SGCN) to detect disagreement between comment and reply posts. We run experiments and ablation studies and show that including this information improves disagreement detection performance on a dataset of Reddit posts for a range of controversial subreddit topics, without the need for platform-specific features or user history.</p></p class="citation"></blockquote><h3 id=918--9115-vlue-a-new-benchmark-and-multi-task-knowledge-transfer-learning-for-vietnamese-natural-language-understanding-phong-nguyen-thuan-do-et-al-2024>(9/18 | 9/115) VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for Vietnamese Natural Language Understanding (Phong Nguyen-Thuan Do et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Phong Nguyen-Thuan Do, Son Quoc Tran, Phu Gia Hoang, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen. (2024)<br><strong>VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for Vietnamese Natural Language Understanding</strong><br><button class=copy-to-clipboard title="VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for Vietnamese Natural Language Understanding" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Knowledge Transfer, Transfer Learning, Natural Language Understanding, Text Classification, GLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15882v1.pdf filename=2403.15882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The success of <b>Natural</b> <b>Language</b> <b>Understanding</b> (NLU) <b>benchmarks</b> in various languages, such as <b>GLUE</b> for English, CLUE for Chinese, KLUE for Korean, and IndoNLU for Indonesian, has facilitated the evaluation of new NLU models across a wide range of tasks. To establish a standardized set of <b>benchmarks</b> for Vietnamese NLU, we introduce the first Vietnamese Language Understanding Evaluation (VLUE) <b>benchmark.</b> The VLUE <b>benchmark</b> encompasses five datasets covering different NLU tasks, including <b>text</b> <b>classification,</b> span extraction, and <b>natural</b> <b>language</b> <b>understanding.</b> To provide an insightful overview of the current state of Vietnamese NLU, we then evaluate seven state-of-the-art pre-trained models, including both multilingual and Vietnamese monolingual models, on our proposed VLUE <b>benchmark.</b> Furthermore, we present CafeBERT, a new state-of-the-art pre-trained model that achieves superior results across all tasks in the VLUE <b>benchmark.</b> Our model combines the proficiency of a multilingual pre-trained model with Vietnamese linguistic <b>knowledge.</b> <b>CafeBERT</b> is developed based on the XLM-RoBERTa model, with an additional pretraining step utilizing a significant amount of Vietnamese textual data to enhance its adaptation to the Vietnamese language. For the purpose of future research, CafeBERT is made publicly available for research purposes.</p></p class="citation"></blockquote><h3 id=1018--10115-ghost-sentence-a-tool-for-everyday-users-to-copyright-data-from-large-language-models-shuai-zhao-et-al-2024>(10/18 | 10/115) Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models (Shuai Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Zhao, Linchao Zhu, Ruijie Quan, Yi Yang. (2024)<br><strong>Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models</strong><br><button class=copy-to-clipboard title="Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs-IR, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, LLaMA, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15740v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15740v1.pdf filename=2403.15740v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Web user data plays a central role in the ecosystem of pre-trained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and their <b>fine-tuned</b> variants. Billions of data are crawled from the web and fed to <b>LLMs.</b> How can \textit{\textbf{everyday web users}} confirm if <b>LLMs</b> misuse their data without permission? In this work, we suggest that users repeatedly insert personal passphrases into their documents, enabling <b>LLMs</b> to memorize them. These concealed passphrases in user documents, referred to as \textit{ghost sentences}, once they are identified in the generated content of <b>LLMs,</b> users can be sure that their data is used for training. To explore the effectiveness and usage of this copyrighting tool, we define the \textit{user training data identification} task with ghost sentences. Multiple datasets from various sources at different scales are created and tested with <b>LLMs</b> of different sizes. For evaluation, we introduce a last $k$ words verification manner along with two metrics: document and user identification accuracy. In the specific case of <b>instruction</b> <b>tuning</b> of a 3B <b>LLaMA</b> model, 11 out of 16 users with ghost sentences identify their data within the generation content. These 16 users contribute 383 examples to $\sim$1.8M training documents. For continuing pre-training of a 1.1B TinyLlama model, 61 out of 64 users with ghost sentences identify their data within the <b>LLM</b> output. These 64 users contribute 1156 examples to $\sim$10M training documents.</p></p class="citation"></blockquote><h3 id=1118--11115-few-shot-dialogue-strategy-learning-for-motivational-interviewing-via-inductive-reasoning-zhouhang-xie-et-al-2024>(11/18 | 11/115) Few-shot Dialogue Strategy Learning for Motivational Interviewing via Inductive Reasoning (Zhouhang Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhouhang Xie, Bodhisattwa Prasad Majumder, Mengjie Zhao, Yoshinori Maeda, Keiichi Yamada, Hiromi Wakaki, Julian McAuley. (2024)<br><strong>Few-shot Dialogue Strategy Learning for Motivational Interviewing via Inductive Reasoning</strong><br><button class=copy-to-clipboard title="Few-shot Dialogue Strategy Learning for Motivational Interviewing via Inductive Reasoning" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Few-shot, Dialogue System, Instruction Following, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15737v1.pdf filename=2403.15737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the task of building a <b>dialogue</b> <b>system</b> that can motivate users to adopt positive lifestyle changes: Motivational Interviewing. Addressing such a task requires a system that can infer \textit{how} to motivate a user effectively. We propose DIIT, a framework that is capable of learning and applying conversation strategies in the form of natural language inductive rules from expert demonstrations. Automatic and human evaluation on <b>instruction-following</b> <b>large</b> <b>language</b> <b>models</b> show natural language strategy descriptions discovered by DIIR can improve active listening skills, reduce unsolicited advice, and promote more collaborative and less authoritative responses, outperforming various demonstration utilization methods.</p></p class="citation"></blockquote><h3 id=1218--12115-modeling-unified-semantic-discourse-structure-for-high-quality-headline-generation-minghui-xu-et-al-2024>(12/18 | 12/115) Modeling Unified Semantic Discourse Structure for High-quality Headline Generation (Minghui Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minghui Xu, Hao Fei, Fei Li, Shengqiong Wu, Rui Sun, Chong Teng, Donghong Ji. (2024)<br><strong>Modeling Unified Semantic Discourse Structure for High-quality Headline Generation</strong><br><button class=copy-to-clipboard title="Modeling Unified Semantic Discourse Structure for High-quality Headline Generation" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Graph, Pruning, Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15776v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15776v1.pdf filename=2403.15776v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Headline generation aims to <b>summarize</b> a long document with a short, catchy title that reflects the main idea. This requires accurately capturing the core document semantics, which is challenging due to the lengthy and background information-rich na ture of the texts. In this work, We propose using a unified semantic discourse structure (S3) to represent document semantics, achieved by combining document-level rhetorical structure theory (RST) trees with sentence-level abstract meaning representation (AMR) <b>graphs</b> to construct S3 <b>graphs.</b> The hierarchical composition of sentence, clause, and word intrinsically characterizes the semantic meaning of the overall document. We then develop a headline generation framework, in which the S3 <b>graphs</b> are encoded as contextual features. To consolidate the efficacy of S3 <b>graphs,</b> we further devise a hierarchical structure <b>pruning</b> mechanism to dynamically screen the redundant and nonessential nodes within the <b>graph.</b> Experimental results on two headline generation datasets demonstrate that our method outperforms existing state-of-art methods consistently. Our work can be instructive for a broad range of document modeling tasks, more than headline or <b>summarization</b> generation.</p></p class="citation"></blockquote><h3 id=1318--13115-ai-for-biomedicine-in-the-era-of-large-language-models-zhenyu-bi-et-al-2024>(13/18 | 13/115) AI for Biomedicine in the Era of Large Language Models (Zhenyu Bi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyu Bi, Sajib Acharjee Dip, Daniel Hajialigol, Sindhura Kommu, Hanwen Liu, Meng Lu, Xuan Wang. (2024)<br><strong>AI for Biomedicine in the Era of Large Language Models</strong><br><button class=copy-to-clipboard title="AI for Biomedicine in the Era of Large Language Models" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Multi-modal, ChatGPT, Chatbot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15673v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15673v1.pdf filename=2403.15673v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The capabilities of AI for biomedicine span a wide spectrum, from the atomic level, where it solves partial differential equations for quantum systems, to the molecular level, predicting chemical or protein structures, and further extending to societal predictions like infectious disease outbreaks. Recent advancements in <b>large</b> <b>language</b> <b>models,</b> exemplified by models like <b>ChatGPT,</b> have showcased significant prowess in natural language tasks, such as translating languages, constructing <b>chatbots,</b> and answering questions. When we consider biomedical data, we observe a resemblance to natural language in terms of sequences: biomedical literature and health records presented as text, biological sequences or sequencing data arranged in sequences, or sensor data like brain signals as time series. The question arises: Can we harness the potential of recent <b>large</b> <b>language</b> <b>models</b> to drive biomedical knowledge discoveries? In this survey, we will explore the application of <b>large</b> <b>language</b> <b>models</b> to three crucial categories of biomedical data: 1) textual data, 2) biological sequences, and 3) brain signals. Furthermore, we will delve into <b>large</b> <b>language</b> <b>model</b> challenges in biomedical research, including ensuring trustworthiness, achieving personalization, and adapting to <b>multi-modal</b> data representation</p></p class="citation"></blockquote><h3 id=1418--14115-peace-a-chemistry-oriented-dataset-for-optical-character-recognition-on-scientific-documents-nan-zhang-et-al-2024>(14/18 | 14/115) PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on Scientific Documents (Nan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nan Zhang, Connor Heaton, Sean Timothy Okonsky, Prasenjit Mitra, Hilal Ezgi Toraman. (2024)<br><strong>PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on Scientific Documents</strong><br><button class=copy-to-clipboard title="PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on Scientific Documents" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Optical Character Recognition, Optical Character Recognition, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15724v1.pdf filename=2403.15724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Optical</b> <b>Character</b> <b>Recognition</b> <b>(OCR)</b> is an established task with the objective of identifying the text present in an image. While many off-the-shelf <b>OCR</b> models exist, they are often trained for either scientific (e.g., formulae) or generic printed English text. Extracting text from chemistry publications requires an <b>OCR</b> model that is capable in both realms. Nougat, a recent tool, exhibits strong ability to parse academic documents, but is unable to parse tables in PubMed articles, which comprises a significant part of the academic community and is the focus of this work. To mitigate this gap, we present the Printed English and Chemical Equations (PEaCE) dataset, containing both synthetic and real-world records, and evaluate the efficacy of <b>transformer-based</b> <b>OCR</b> models when trained on this resource. Given that real-world records contain artifacts not present in synthetic records, we propose transformations that mimic such qualities. We perform a suite of experiments to explore the impact of patch size, multi-domain training, and our proposed transformations, ultimately finding that models with a small patch size trained on multiple domains using the proposed transformations yield the best performance. Our dataset and code is available at <a href=https://github.com/ZN1010/PEaCE>https://github.com/ZN1010/PEaCE</a>.</p></p class="citation"></blockquote><h3 id=1518--15115-computational-sentence-level-metrics-predicting-human-sentence-comprehension-kun-sun-et-al-2024>(15/18 | 15/115) Computational Sentence-level Metrics Predicting Human Sentence Comprehension (Kun Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Sun, Rong Wang. (2024)<br><strong>Computational Sentence-level Metrics Predicting Human Sentence Comprehension</strong><br><button class=copy-to-clipboard title="Computational Sentence-level Metrics Predicting Human Sentence Comprehension" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, stat-ML<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15822v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15822v1.pdf filename=2403.15822v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The majority of research in computational psycholinguistics has concentrated on the processing of words. This study introduces innovative methods for computing sentence-level metrics using multilingual <b>large</b> <b>language</b> <b>models.</b> The metrics developed sentence surprisal and sentence relevance and then are tested and compared to validate whether they can predict how humans comprehend sentences as a whole across languages. These metrics offer significant interpretability and achieve high accuracy in predicting human sentence reading speeds. Our results indicate that these computational sentence-level metrics are exceptionally effective at predicting and elucidating the processing difficulties encountered by readers in comprehending sentences as a whole across a variety of languages. Their impressive performance and generalization capabilities provide a promising avenue for future research in integrating <b>LLMs</b> and cognitive science.</p></p class="citation"></blockquote><h3 id=1618--16115-feel-a-framework-for-evaluating-emotional-support-capability-with-large-language-models-huaiwen-zhang-et-al-2024>(16/18 | 16/115) FEEL: A Framework for Evaluating Emotional Support Capability with Large Language Models (Huaiwen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huaiwen Zhang, Yu Chen, Ming Wang, Shi Feng. (2024)<br><strong>FEEL: A Framework for Evaluating Emotional Support Capability with Large Language Models</strong><br><button class=copy-to-clipboard title="FEEL: A Framework for Evaluating Emotional Support Capability with Large Language Models" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15699v1.pdf filename=2403.15699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emotional Support Conversation (ESC) is a typical dialogue that can effec-tively assist the user in mitigating emotional pressures. However, owing to the inherent subjectivity involved in analyzing emotions, current non-artificial methodologies face challenges in effectively appraising the emo-tional support capability. These metrics exhibit a low correlation with human judgments. Concurrently, manual evaluation methods extremely will cause high costs. To solve these problems, we propose a novel model FEEL (Framework for Evaluating Emotional Support Capability with <b>Large</b> <b>Lan-guage</b> <b>Models),</b> employing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as evaluators to assess emotional support capabilities. The model meticulously considers var-ious evaluative aspects of ESC to apply a more comprehensive and accurate evaluation method for ESC. Additionally, it employs a probability distribu-tion approach for a more stable result and integrates an ensemble learning strategy, leveraging multiple <b>LLMs</b> with assigned weights to enhance evalua-tion accuracy. To appraise the performance of FEEL, we conduct extensive experiments on existing ESC model dialogues. Experimental results demon-strate our model exhibits a substantial enhancement in alignment with human evaluations compared to the baselines. Our source code is available at <a href=https://github.com/Ansisy/FEEL>https://github.com/Ansisy/FEEL</a>.</p></p class="citation"></blockquote><h3 id=1718--17115-geotokens-and-geotransformers-eren-unlu-2024>(17/18 | 17/115) Geotokens and Geotransformers (Eren Unlu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eren Unlu. (2024)<br><strong>Geotokens and Geotransformers</strong><br><button class=copy-to-clipboard title="Geotokens and Geotransformers" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15940v1.pdf filename=2403.15940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>transformer</b> architectures, position encoding primarily provides a sense of sequence for input tokens. While the original <b>transformer</b> paper&rsquo;s method has shown satisfactory results in general language processing tasks, there have been new proposals, such as Rotary Position Embedding (RoPE), for further improvement. This paper presents geotokens, input components for <b>transformers,</b> each linked to a specific geological location. Unlike typical language sequences, for these tokens, the order is not as vital as the geographical coordinates themselves. To represent the relative position in this context and to keep a balance between the real world distance and the distance in the embedding space, we design a position encoding approach drawing from the RoPE structure but tailored for spherical coordinates.</p></p class="citation"></blockquote><h3 id=1818--18115-raamove-a-corpus-for-analyzing-moves-in-research-article-abstracts-hongzheng-li-et-al-2024>(18/18 | 18/115) RAAMove: A Corpus for Analyzing Moves in Research Article Abstracts (Hongzheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongzheng Li, Ruojin Wang, Ge Shi, Xing Lv, Lei Lei, Chong Feng, Fang Liu, Jinkun Lin, Yangguang Mei, Lingnan Xu. (2024)<br><strong>RAAMove: A Corpus for Analyzing Moves in Research Article Abstracts</strong><br><button class=copy-to-clipboard title="RAAMove: A Corpus for Analyzing Moves in Research Article Abstracts" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15872v1.pdf filename=2403.15872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Move structures have been studied in English for Specific Purposes (ESP) and English for Academic Purposes (EAP) for decades. However, there are few move annotation corpora for Research Article (RA) abstracts. In this paper, we introduce RAAMove, a comprehensive multi-domain corpus dedicated to the annotation of move structures in RA abstracts. The primary objective of RAAMove is to facilitate move analysis and automatic move identification. This paper provides a thorough discussion of the corpus construction process, including the scheme, data collection, annotation guidelines, and annotation procedures. The corpus is constructed through two stages: initially, expert annotators manually annotate high-quality data; subsequently, based on the human-annotated data, a <b>BERT-based</b> model is employed for automatic annotation with the help of experts&rsquo; modification. The result is a large-scale and high-quality corpus comprising 33,988 annotated instances. We also conduct preliminary move identification experiments using the <b>BERT-based</b> model to verify the effectiveness of the proposed corpus and model. The annotated corpus is available for academic research purposes and can serve as essential resources for move analysis, English language teaching and writing, as well as move/discourse-related tasks in Natural Language Processing (NLP).</p></p class="citation"></blockquote><h2 id=cscv-28>cs.CV (28)</h2><h3 id=128--19115-illusionvqa-a-challenging-optical-illusion-dataset-for-vision-language-models-haz-sameen-shahgir-et-al-2024>(1/28 | 19/115) IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models (Haz Sameen Shahgir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haz Sameen Shahgir, Khondker Salman Sayeed, Abhik Bhattacharjee, Wasi Uddin Ahmad, Yue Dong, Rifat Shahriyar. (2024)<br><strong>IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models</strong><br><button class=copy-to-clipboard title="IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 80<br>Keywords: Few-shot, Common-sense Reasoning, Reasoning, Visual Question Answering, In-context Learning, In-context Learning, In-context Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15952v1.pdf filename=2403.15952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of Vision Language Models (VLM) has allowed researchers to investigate the visual understanding of a neural network using natural language. Beyond object classification and detection, VLMs are capable of visual comprehension and <b>common-sense</b> <b>reasoning.</b> This naturally led to the question: How do VLMs respond when the image itself is inherently unreasonable? To this end, we present IllusionVQA: a diverse dataset of challenging optical illusions and hard-to-interpret scenes to test the capability of VLMs in two distinct multiple-choice <b>VQA</b> tasks - comprehension and soft localization. GPT4V, the best-performing VLM, achieves 62.99% accuracy (4-shot) on the comprehension task and 49.7% on the localization task (4-shot and Chain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100% accuracy in comprehension and localization. We discover that <b>In-Context</b> <b>Learning</b> <b>(ICL)</b> and Chain-of-Thought <b>reasoning</b> substantially degrade the performance of GeminiPro on the localization task. Tangentially, we discover a potential weakness in the <b>ICL</b> capabilities of VLMs: they fail to locate optical illusions even when the correct answer is in the context window as a <b>few-shot</b> example.</p></p class="citation"></blockquote><h3 id=228--20115-technical-report-masked-skeleton-sequence-modeling-for-learning-larval-zebrafish-behavior-latent-embeddings-lanxin-xu-et-al-2024>(2/28 | 20/115) Technical Report: Masked Skeleton Sequence Modeling for Learning Larval Zebrafish Behavior Latent Embeddings (Lanxin Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lanxin Xu, Shuo Wang. (2024)<br><strong>Technical Report: Masked Skeleton Sequence Modeling for Learning Larval Zebrafish Behavior Latent Embeddings</strong><br><button class=copy-to-clipboard title="Technical Report: Masked Skeleton Sequence Modeling for Learning Larval Zebrafish Behavior Latent Embeddings" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Autoencoder, Convolutional Neural Network, Self-supervised Learning, Self-supervised Learning, GPT, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15693v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15693v1.pdf filename=2403.15693v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this report, we introduce a novel <b>self-supervised</b> <b>learning</b> method for extracting latent embeddings from behaviors of larval zebrafish. Drawing inspiration from Masked Modeling techniquesutilized in image processing with Masked <b>Autoencoders</b> (MAE) \cite{he2022masked} and in natural language processing with Generative Pre-trained <b>Transformer</b> <b>(GPT)</b> \cite{radford2018improving}, we treat behavior sequences as a blend of images and language. For the skeletal sequences of swimming zebrafish, we propose a pioneering <b>Transformer-CNN</b> architecture, the Sequence Spatial-Temporal <b>Transformer</b> (SSTFormer), designed to capture the inter-frame correlation of different joints. This correlation is particularly valuable, as it reflects the coordinated movement of various parts of the fish body across adjacent frames. To handle the high frame rate, we segment the skeleton sequence into distinct time slices, analogous to &ldquo;words&rdquo; in a sentence, and employ <b>self-attention</b> <b>transformer</b> layers to encode the consecutive frames within each slice, capturing the spatial correlation among different joints. Furthermore, we incorporate a <b>CNN-based</b> attention module to enhance the representations outputted by the <b>transformer</b> layers. Lastly, we introduce a temporal feature aggregation operation between time slices to improve the discrimination of similar behaviors.</p></p class="citation"></blockquote><h3 id=328--21115-temporal-spatial-object-relations-modeling-for-vision-and-language-navigation-bowen-huang-et-al-2024>(3/28 | 21/115) Temporal-Spatial Object Relations Modeling for Vision-and-Language Navigation (Bowen Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Huang, Yanwei Zheng, Chuanlin Lan, Xinpeng Zhao, Dongxiao yu, Yifei Zou. (2024)<br><strong>Temporal-Spatial Object Relations Modeling for Vision-and-Language Navigation</strong><br><button class=copy-to-clipboard title="Temporal-Spatial Object Relations Modeling for Vision-and-Language Navigation" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15691v1.pdf filename=2403.15691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-and-Language</b> Navigation (VLN) is a challenging task where an agent is required to navigate to a natural language described location via vision observations. The navigation abilities of the agent can be enhanced by the relations between objects, which are usually learned using internal objects or external datasets. The relationships between internal objects are modeled employing <b>graph</b> <b>convolutional</b> <b>network</b> <b>(GCN)</b> in traditional studies. However, <b>GCN</b> tends to be shallow, limiting its modeling ability. To address this issue, we utilize a cross attention mechanism to learn the connections between objects over a trajectory, which takes temporal continuity into account, termed as Temporal Object Relations (TOR). The external datasets have a gap with the navigation environment, leading to inaccurate modeling of relations. To avoid this problem, we construct object connections based on observations from all viewpoints in the navigational environment, which ensures complete spatial coverage and eliminates the gap, called Spatial Object Relations (SOR). Additionally, we observe that agents may repeatedly visit the same location during navigation, significantly hindering their performance. For resolving this matter, we introduce the Turning Back Penalty (TBP) loss function, which penalizes the agent&rsquo;s repetitive visiting behavior, substantially reducing the navigational distance. Experimental results on the REVERIE, SOON, and R2R datasets demonstrate the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=428--22115-an-embarrassingly-simple-defense-against-backdoor-attacks-on-ssl-aryan-satpathy-et-al-2024>(4/28 | 22/115) An Embarrassingly Simple Defense Against Backdoor Attacks On SSL (Aryan Satpathy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aryan Satpathy, Nilaksh, Dhruva Rajwade. (2024)<br><strong>An Embarrassingly Simple Defense Against Backdoor Attacks On SSL</strong><br><button class=copy-to-clipboard title="An Embarrassingly Simple Defense Against Backdoor Attacks On SSL" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Self-supervised Learning, Self-supervised Learning, Supervised Learning, Supervised Learning, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15918v1.pdf filename=2403.15918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Self <b>Supervised</b> <b>Learning</b> (SSL) has emerged as a powerful paradigm to tackle data landscapes with absence of human supervision. The ability to learn meaningful tasks without the use of labeled data makes SSL a popular method to manage large chunks of data in the absence of labels. However, recent work indicates SSL to be vulnerable to backdoor attacks, wherein models can be controlled, possibly maliciously, to suit an adversary&rsquo;s motives. Li et.al (2022) introduce a novel frequency-based backdoor attack: CTRL. They show that CTRL can be used to efficiently and stealthily gain control over a victim&rsquo;s model trained using SSL. In this work, we devise two defense strategies against frequency-based attacks in SSL: One applicable before model training and the second to be applied during model inference. Our first contribution utilizes the invariance property of the downstream task to defend against backdoor attacks in a generalizable fashion. We observe the <b>ASR</b> (Attack Success Rate) to reduce by over 60% across experiments. Our Inference-time defense relies on evasiveness of the attack and uses the luminance channel to defend against attacks. Using object classification as the downstream task for SSL, we demonstrate successful defense strategies that do not require re-training of the model. Code is available at <a href=https://github.com/Aryan-Satpathy/Backdoor>https://github.com/Aryan-Satpathy/Backdoor</a>.</p></p class="citation"></blockquote><h3 id=528--23115-vlm-cpl-consensus-pseudo-labels-from-vision-language-models-for-human-annotation-free-pathological-image-classification-lanfeng-zhong-et-al-2024>(5/28 | 23/115) VLM-CPL: Consensus Pseudo Labels from Vision-Language Models for Human Annotation-Free Pathological Image Classification (Lanfeng Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lanfeng Zhong, Xin Liao, Shaoting Zhang, Xiaofan Zhang, Guotai Wang. (2024)<br><strong>VLM-CPL: Consensus Pseudo Labels from Vision-Language Models for Human Annotation-Free Pathological Image Classification</strong><br><button class=copy-to-clipboard title="VLM-CPL: Consensus Pseudo Labels from Vision-Language Models for Human Annotation-Free Pathological Image Classification" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Clustering, Semi-Supervised Learning, Zero-shot, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15836v1.pdf filename=2403.15836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite that deep learning methods have achieved remarkable performance in pathology image classification, they heavily rely on labeled data, demanding extensive human annotation efforts. In this study, we present a novel human annotation-free method for pathology image classification by leveraging pre-trained <b>Vision-Language</b> Models (VLMs). Without human annotation, pseudo labels of the training set are obtained by utilizing the <b>zero-shot</b> inference capabilities of VLM, which may contain a lot of noise due to the domain shift between the pre-training data and the target dataset. To address this issue, we introduce VLM-CPL, a novel approach based on consensus pseudo labels that integrates two noisy label filtering techniques with a <b>semi-supervised</b> <b>learning</b> strategy. Specifically, we first obtain <b>prompt-based</b> pseudo labels with uncertainty estimation by <b>zero-shot</b> inference with the VLM using multiple augmented views of an input. Then, by leveraging the feature representation ability of VLM, we obtain feature-based pseudo labels via sample <b>clustering</b> in the feature space. <b>Prompt-feature</b> consensus is introduced to select reliable samples based on the consensus between the two types of pseudo labels. By rejecting low-quality pseudo labels, we further propose High-confidence Cross Supervision (HCS) to learn from samples with reliable pseudo labels and the remaining unlabeled samples. Experimental results showed that our method obtained an accuracy of 87.1% and 95.1% on the HPH and LC25K datasets, respectively, and it largely outperformed existing <b>zero-shot</b> classification and noisy label learning methods. The code is available at <a href=https://github.com/lanfz2000/VLM-CPL>https://github.com/lanfz2000/VLM-CPL</a>.</p></p class="citation"></blockquote><h3 id=628--24115-deep-domain-adaptation-a-sim2real-neural-approach-for-improving-eye-tracking-systems-viet-dung-nguyen-et-al-2024>(6/28 | 24/115) Deep Domain Adaptation: A Sim2Real Neural Approach for Improving Eye-Tracking Systems (Viet Dung Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Viet Dung Nguyen, Reynold Bailey, Gabriel J. Diaz, Chengyi Ma, Alexander Fix, Alexander Ororbia. (2024)<br><strong>Deep Domain Adaptation: A Sim2Real Neural Approach for Improving Eye-Tracking Systems</strong><br><button class=copy-to-clipboard title="Deep Domain Adaptation: A Sim2Real Neural Approach for Improving Eye-Tracking Systems" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2-0; I-2-6; I-4-6; I-4-5, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Supervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15947v1.pdf filename=2403.15947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Eye image segmentation is a critical step in eye tracking that has great influence over the final gaze estimate. Segmentation models trained using <b>supervised</b> machine learning can excel at this task, their effectiveness is determined by the degree of overlap between the narrow distributions of image properties defined by the target dataset and highly specific training datasets, of which there are few. Attempts to broaden the distribution of existing eye image datasets through the inclusion of synthetic eye images have found that a model trained on synthetic images will often fail to generalize back to real-world eye images. In remedy, we use dimensionality-reduction techniques to measure the overlap between the target eye images and synthetic training data, and to prune the training dataset in a manner that maximizes distribution overlap. We demonstrate that our methods result in robust, improved performance when tackling the discrepancy between <b>simulation</b> and real-world data samples.</p></p class="citation"></blockquote><h3 id=728--25115-once-for-both-single-stage-of-importance-and-sparsity-search-for-vision-transformer-compression-hancheng-ye-et-al-2024>(7/28 | 25/115) Once for Both: Single Stage of Importance and Sparsity Search for Vision Transformer Compression (Hancheng Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hancheng Ye, Chong Yu, Peng Ye, Renqiu Xia, Yansong Tang, Jiwen Lu, Tao Chen, Bo Zhang. (2024)<br><strong>Once for Both: Single Stage of Importance and Sparsity Search for Vision Transformer Compression</strong><br><button class=copy-to-clipboard title="Once for Both: Single Stage of Importance and Sparsity Search for Vision Transformer Compression" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Vision Transformer, Pruning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15835v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15835v1.pdf filename=2403.15835v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>Vision</b> <b>Transformer</b> Compression (VTC) works mainly follow a two-stage scheme, where the importance score of each model unit is first evaluated or preset in each submodule, followed by the sparsity score evaluation according to the target sparsity constraint. Such a separate evaluation process induces the gap between importance and sparsity score distributions, thus causing high search costs for VTC. In this work, for the first time, we investigate how to integrate the evaluations of importance and sparsity scores into a single stage, searching the optimal subnets in an efficient manner. Specifically, we present OFB, a cost-efficient approach that simultaneously evaluates both importance and sparsity scores, termed Once for Both (OFB), for VTC. First, a bi-mask scheme is developed by entangling the importance score and the differentiable sparsity score to jointly determine the <b>pruning</b> potential (prunability) of each unit. Such a bi-mask search strategy is further used together with a proposed adaptive one-hot loss to realize the progressive-and-efficient search for the most important subnet. Finally, Progressive Masked Image Modeling (PMIM) is proposed to regularize the feature space to be more representative during the search process, which may be degraded by the dimension reduction. Extensive experiments demonstrate that OFB can achieve superior compression performance over state-of-the-art searching-based and <b>pruning-based</b> methods under various <b>Vision</b> <b>Transformer</b> architectures, meanwhile promoting search efficiency significantly, e.g., costing one GPU search day for the compression of DeiT-S on ImageNet-1K.</p></p class="citation"></blockquote><h3 id=828--26115-in-context-matting-he-guo-et-al-2024>(8/28 | 26/115) In-Context Matting (He Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>He Guo, Zixuan Ye, Zhiguo Cao, Hao Lu. (2024)<br><strong>In-Context Matting</strong><br><button class=copy-to-clipboard title="In-Context Matting" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Diffusion Model, Benchmarking, Text2image, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15789v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15789v1.pdf filename=2403.15789v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce <b>in-context</b> matting, a novel task setting of image matting. Given a reference image of a certain foreground and guided priors such as points, scribbles, and masks, <b>in-context</b> matting enables automatic alpha estimation on a batch of target images of the same foreground category, without additional auxiliary input. This setting marries good performance in auxiliary input-based matting and ease of use in automatic matting, which finds a good trade-off between customization and automation. To overcome the key challenge of accurate foreground matching, we introduce IconMatting, an <b>in-context</b> matting model built upon a pre-trained <b>text-to-image</b> <b>diffusion</b> <b>model.</b> Conditioned on inter- and intra-similarity matching, IconMatting can make full use of reference context to generate accurate target alpha mattes. To <b>benchmark</b> the task, we also introduce a novel testing dataset ICM-$57$, covering 57 groups of real-world images. Quantitative and qualitative results on the ICM-57 testing set show that IconMatting rivals the accuracy of trimap-based matting while retaining the automation level akin to automatic matting. Code is available at <a href=https://github.com/tiny-smart/in-context-matting>https://github.com/tiny-smart/in-context-matting</a></p></p class="citation"></blockquote><h3 id=928--27115-idat-inverse-distillation-adapter-tuning-jiacheng-ruan-et-al-2024>(9/28 | 27/115) iDAT: inverse Distillation Adapter-Tuning (Jiacheng Ruan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacheng Ruan, Jingsheng Gao, Mingye Xie, Daize Dong, Suncheng Xiang, Ting Liu, Yuzhuo Fu. (2024)<br><strong>iDAT: inverse Distillation Adapter-Tuning</strong><br><button class=copy-to-clipboard title="iDAT: inverse Distillation Adapter-Tuning" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15750v1.pdf filename=2403.15750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adapter-Tuning (AT) method involves freezing a pre-trained model and introducing trainable adapter modules to acquire downstream <b>knowledge,</b> <b>thereby</b> calibrating the model for better adaptation to downstream tasks. This paper proposes a <b>distillation</b> framework for the AT method instead of crafting a carefully designed adapter module, which aims to improve <b>fine-tuning</b> performance. For the first time, we explore the possibility of combining the AT method with <b>knowledge</b> <b>distillation.</b> Via statistical analysis, we observe significant differences in the <b>knowledge</b> <b>acquisition</b> between adapter modules of different models. Leveraging these differences, we propose a simple yet effective framework called inverse <b>Distillation</b> Adapter-Tuning (iDAT). Specifically, we designate the smaller model as the teacher and the larger model as the student. The two are jointly trained, and online <b>knowledge</b> <b>distillation</b> is applied to inject <b>knowledge</b> <b>of</b> different perspective to student model, and significantly enhance the <b>fine-tuning</b> performance on downstream tasks. Extensive experiments on the VTAB-1K <b>benchmark</b> with 19 image classification tasks demonstrate the effectiveness of iDAT. The results show that using existing AT method within our iDAT framework can further yield a 2.66% performance gain, with only an additional 0.07M trainable parameters. Our approach compares favorably with state-of-the-arts without bells and whistles. Our code is available at <a href=https://github.com/JCruan519/iDAT>https://github.com/JCruan519/iDAT</a>.</p></p class="citation"></blockquote><h3 id=1028--28115-diffusion-based-aesthetic-qr-code-generation-via-scanning-robust-perceptual-guidance-jia-wei-liao-et-al-2024>(10/28 | 28/115) Diffusion-based Aesthetic QR Code Generation via Scanning-Robust Perceptual Guidance (Jia-Wei Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jia-Wei Liao, Winston Wang, Tzu-Sian Wang, Li-Xuan Peng, Cheng-Fu Chou, Jun-Cheng Chen. (2024)<br><strong>Diffusion-based Aesthetic QR Code Generation via Scanning-Robust Perceptual Guidance</strong><br><button class=copy-to-clipboard title="Diffusion-based Aesthetic QR Code Generation via Scanning-Robust Perceptual Guidance" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: ControlNet, Diffusion Model, Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15878v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15878v1.pdf filename=2403.15878v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>QR <b>codes,</b> <b>prevalent</b> in daily applications, lack visual appeal due to their conventional black-and-white design. Integrating aesthetics while maintaining scannability poses a challenge. In this paper, we introduce a novel <b>diffusion-model-based</b> <b>aesthetic</b> QR <b>code</b> <b>generation</b> pipeline, utilizing pre-trained <b>ControlNet</b> and guided iterative refinement via a novel classifier guidance (SRG) based on the proposed Scanning-Robust Loss (SRL) tailored with QR <b>code</b> <b>mechanisms,</b> which ensures both aesthetics and scannability. To further improve the scannability while preserving aesthetics, we propose a two-stage pipeline with Scanning-Robust Perceptual Guidance (SRPG). Moreover, we can further enhance the scannability of the generated QR <b>code</b> <b>by</b> post-processing it through the proposed Scanning-Robust Projected Gradient Descent (SRPGD) post-processing technique based on SRL with proven convergence. With extensive quantitative, qualitative, and subjective experiments, the results demonstrate that the proposed approach can generate diverse aesthetic QR <b>codes</b> <b>with</b> flexibility in detail. In addition, our pipelines outperforming existing models in terms of Scanning Success Rate (SSR) 86.67% (+40%) with comparable aesthetic scores. The pipeline combined with SRPGD further achieves 96.67% (+50%). Our <b>code</b> <b>will</b> be available <a href=https://github.com/jwliao1209/DiffQRCode>https://github.com/jwliao1209/DiffQRCode</a>.</p></p class="citation"></blockquote><h3 id=1128--29115-boosting-few-shot-learning-via-attentive-feature-regularization-xingyu-zhu-et-al-2024>(11/28 | 29/115) Boosting Few-Shot Learning via Attentive Feature Regularization (Xingyu Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyu Zhu, Shuo Wang, Jinda Lu, Yanbin Hao, Haifeng Liu, Xiangnan He. (2024)<br><strong>Boosting Few-Shot Learning via Attentive Feature Regularization</strong><br><button class=copy-to-clipboard title="Boosting Few-Shot Learning via Attentive Feature Regularization" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Few-shot, Few-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17025v1.pdf filename=2403.17025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>learning</b> (FSL) based on manifold regularization aims to improve the recognition capacity of novel objects with limited training samples by mixing two samples from different categories with a blending factor. However, this mixing operation weakens the feature representation due to the linear interpolation and the overlooking of the importance of specific channels. To solve these issues, this paper proposes attentive feature regularization (AFR) which aims to improve the feature representativeness and discriminability. In our approach, we first calculate the relations between different categories of semantic labels to pick out the related features used for regularization. Then, we design two attention-based calculations at both the instance and channel levels. These calculations enable the regularization procedure to focus on two crucial aspects: the feature complementarity through adaptive interpolation in related categories and the emphasis on specific feature channels. Finally, we combine these regularization strategies to significantly improve the classifier performance. Empirical studies on several popular FSL <b>benchmarks</b> demonstrate the effectiveness of AFR, which improves the recognition accuracy of novel categories without the need to retrain any feature extractor, especially in the 1-shot setting. Furthermore, the proposed AFR can seamlessly integrate into other FSL methods to improve classification performance.</p></p class="citation"></blockquote><h3 id=1228--30115-towards-human-like-machine-comprehension-few-shot-relational-learning-in-visually-rich-documents-hao-wang-et-al-2024>(12/28 | 30/115) Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents (Hao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wang, Tang Li, Chenhui Chu, Nengjun Zhu, Rui Wang, Pinpin Zhu. (2024)<br><strong>Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents</strong><br><button class=copy-to-clipboard title="Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-IR, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Few-shot, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15765v1.pdf filename=2403.15765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Key-value relations are prevalent in Visually-Rich Documents (VRDs), often depicted in distinct spatial regions accompanied by specific color and font styles. These non-textual cues serve as important indicators that greatly enhance human comprehension and acquisition of such relation triplets. However, current document AI approaches often fail to consider this valuable prior information related to visual and spatial features, resulting in suboptimal performance, particularly when dealing with limited examples. To address this limitation, our research focuses on <b>few-shot</b> relational learning, specifically targeting the extraction of key-value relation triplets in VRDs. Given the absence of a suitable dataset for this task, we introduce two new <b>few-shot</b> <b>benchmarks</b> built upon existing <b>supervised</b> <b>benchmark</b> datasets. Furthermore, we propose a variational approach that incorporates relational 2D-spatial priors and prototypical rectification techniques. This approach aims to generate relation representations that are more aware of the spatial context and unseen relation in a manner similar to human perception. Experimental results demonstrate the effectiveness of our proposed method by showcasing its ability to outperform existing methods. This study also opens up new possibilities for practical applications.</p></p class="citation"></blockquote><h3 id=1328--31115-x-portrait-expressive-portrait-animation-with-hierarchical-motion-attention-you-xie-et-al-2024>(13/28 | 31/115) X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention (You Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, Linjie Luo. (2024)<br><strong>X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention</strong><br><button class=copy-to-clipboard title="X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: ControlNet, Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15931v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15931v2.pdf filename=2403.15931v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose X-Portrait, an innovative conditional <b>diffusion</b> <b>model</b> tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained <b>diffusion</b> <b>model</b> as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of <b>ControlNet.</b> In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeball positions. Notably, to mitigate the identity leakage from the driving signals, we train our motion control modules with scaling-augmented cross-identity images, ensuring maximized disentanglement from the appearance reference modules. Experimental results demonstrate the universal effectiveness of X-Portrait across a diverse range of facial portraits and expressive driving sequences, and showcase its proficiency in generating captivating portrait animations with consistently maintained identity characteristics.</p></p class="citation"></blockquote><h3 id=1428--32115-time-series-initialization-and-conditioning-for-video-agnostic-stabilization-of-video-super-resolution-using-recurrent-networks-hiroshi-mori-et-al-2024>(14/28 | 32/115) Time-series Initialization and Conditioning for Video-agnostic Stabilization of Video Super-Resolution using Recurrent Networks (Hiroshi Mori et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hiroshi Mori, Norimichi Ukita. (2024)<br><strong>Time-series Initialization and Conditioning for Video-agnostic Stabilization of Video Super-Resolution using Recurrent Networks</strong><br><button class=copy-to-clipboard title="Time-series Initialization and Conditioning for Video-agnostic Stabilization of Video Super-Resolution using Recurrent Networks" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15832v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15832v1.pdf filename=2403.15832v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A <b>Recurrent</b> <b>Neural</b> <b>Network</b> <b>(RNN)</b> for Video Super Resolution (VSR) is generally trained with randomly clipped and cropped short videos extracted from original training videos due to various challenges in learning <b>RNNs.</b> However, since this <b>RNN</b> is optimized to super-resolve short videos, VSR of long videos is degraded due to the domain gap. Our preliminary experiments reveal that such degradation changes depending on the video properties, such as the video length and dynamics. To avoid this degradation, this paper proposes the training strategy of <b>RNN</b> for VSR that can work efficiently and stably independently of the video length and dynamics. The proposed training strategy stabilizes VSR by training a VSR network with various <b>RNN</b> hidden states changed depending on the video properties. Since computing such a variety of hidden states is time-consuming, this computational cost is reduced by reusing the hidden states for efficient training. In addition, training stability is further improved with frame-number conditioning. Our experimental results demonstrate that the proposed method performed better than base methods in videos with various lengths and dynamics.</p></p class="citation"></blockquote><h3 id=1528--33115-depth-estimation-fusing-image-and-radar-measurements-with-uncertain-directions-masaya-kotani-et-al-2024>(15/28 | 33/115) Depth Estimation fusing Image and Radar Measurements with Uncertain Directions (Masaya Kotani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masaya Kotani, Takeru Oba, Norimichi Ukita. (2024)<br><strong>Depth Estimation fusing Image and Radar Measurements with Uncertain Directions</strong><br><button class=copy-to-clipboard title="Depth Estimation fusing Image and Radar Measurements with Uncertain Directions" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15787v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15787v1.pdf filename=2403.15787v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a depth estimation method using radar-image fusion by addressing the uncertain vertical directions of sparse radar measurements. In prior radar-image fusion work, image features are merged with the uncertain sparse depths measured by radar through <b>convolutional</b> <b>layers.</b> This approach is disturbed by the features computed with the uncertain radar depths. Furthermore, since the features are computed with a fully <b>convolutional</b> <b>network,</b> the uncertainty of each depth corresponding to a pixel is spread out over its surrounding pixels. Our method avoids this problem by computing features only with an image and conditioning the features pixelwise with the radar depth. Furthermore, the set of possibly correct radar directions is identified with reliable LiDAR measurements, which are available only in the training stage. Our method improves training data by learning only these possibly correct radar directions, while the previous method trains raw radar measurements, including erroneous measurements. Experimental results demonstrate that our method can improve the quantitative and qualitative results compared with its base method using radar-image fusion.</p></p class="citation"></blockquote><h3 id=1628--34115-adversarial-defense-teacher-for-cross-domain-object-detection-under-poor-visibility-conditions-kaiwen-wang-et-al-2024>(16/28 | 34/115) Adversarial Defense Teacher for Cross-Domain Object Detection under Poor Visibility Conditions (Kaiwen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiwen Wang, Yinzhe Shen, Martin Lauer. (2024)<br><strong>Adversarial Defense Teacher for Cross-Domain Object Detection under Poor Visibility Conditions</strong><br><button class=copy-to-clipboard title="Adversarial Defense Teacher for Cross-Domain Object Detection under Poor Visibility Conditions" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15786v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15786v1.pdf filename=2403.15786v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>object</b> <b>detectors</b> encounter challenges in handling domain shifts between training and real-world data, particularly under poor visibility conditions like fog and night. Cutting-edge cross-domain <b>object</b> <b>detection</b> methods use teacher-student frameworks and compel teacher and student models to produce consistent predictions under weak and strong augmentations, respectively. In this paper, we reveal that manually crafted augmentations are insufficient for optimal teaching and present a simple yet effective framework named <b>Adversarial</b> <b>Defense</b> Teacher (ADT), leveraging <b>adversarial</b> <b>defense</b> to enhance teaching quality. Specifically, we employ <b>adversarial</b> <b>attacks,</b> encouraging the model to generalize on subtly perturbed inputs that effectively deceive the model. To address small <b>objects</b> <b>under</b> poor visibility conditions, we propose a Zoom-in Zoom-out strategy, which zooms-in images for better pseudo-labels and zooms-out images and pseudo-labels to learn refined features. Our results demonstrate that ADT achieves superior performance, reaching 54.5% mAP on Foggy Cityscapes, surpassing the previous state-of-the-art by 2.6% mAP.</p></p class="citation"></blockquote><h3 id=1728--35115-scenexprocedural-controllable-large-scale-scene-generation-via-large-language-models-mengqi-zhou-et-al-2024>(17/28 | 35/115) SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models (Mengqi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengqi Zhou, Jun Hou, Chuanchen Luo, Yuxi Wang, Zhaoxiang Zhang, Junran Peng. (2024)<br><strong>SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models</strong><br><button class=copy-to-clipboard title="SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15698v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15698v1.pdf filename=2403.15698v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to its great application potential, <b>large-scale</b> <b>scene</b> <b>generation</b> has drawn extensive attention in academia and industry. Recent research employs powerful generative models to create desired scenes and achieves promising results. However, most of these methods represent the scene using 3D primitives (e.g. point cloud or radiance field) incompatible with the industrial pipeline, which leads to a substantial gap between academic research and industrial deployment. Procedural Controllable Generation (PCG) is an efficient technique for creating scalable and high-quality assets, but it is unfriendly for ordinary users as it demands profound domain expertise. To address these issues, we resort to using the <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to drive the procedural modeling. In this paper, we introduce a <b>large-scale</b> <b>scene</b> <b>generation</b> framework, SceneX, which can automatically produce high-quality procedural models according to designers&rsquo; textual descriptions.Specifically, the proposed method comprises two components, PCGBench and PCGPlanner. The former encompasses an extensive collection of accessible procedural assets and thousands of hand-craft API documents. The latter aims to generate executable actions for Blender to produce controllable and precise 3D assets guided by the user&rsquo;s instructions. Our SceneX can generate a city spanning 2.5 km times 2.5 km with delicate layout and geometric structures, drastically reducing the time cost from several weeks for professional PCG engineers to just a few hours for an ordinary user. Extensive experiments demonstrated the capability of our method in controllable <b>large-scale</b> <b>scene</b> <b>generation</b> and editing, including asset placement and season translation.</p></p class="citation"></blockquote><h3 id=1828--36115-the-limits-of-perception-analyzing-inconsistencies-in-saliency-maps-in-xai-anna-stubbin-et-al-2024>(18/28 | 36/115) The Limits of Perception: Analyzing Inconsistencies in Saliency Maps in XAI (Anna Stubbin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Stubbin, Thompson Chyrikov, Jim Zhao, Christina Chajo. (2024)<br><strong>The Limits of Perception: Analyzing Inconsistencies in Saliency Maps in XAI</strong><br><button class=copy-to-clipboard title="The Limits of Perception: Analyzing Inconsistencies in Saliency Maps in XAI" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Recommendation, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15684v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15684v1.pdf filename=2403.15684v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Explainable artificial intelligence (XAI) plays an indispensable role in demystifying the decision-making processes of AI, especially within the healthcare industry. Clinicians rely heavily on detailed <b>reasoning</b> when making a diagnosis, often CT scans for specific features that distinguish between benign and malignant lesions. A comprehensive diagnostic approach includes an evaluation of imaging results, patient observations, and clinical tests. The surge in deploying deep learning models as support systems in medical diagnostics has been significant, offering advances that traditional methods could not. However, the complexity and opacity of these models present a double-edged sword. As they operate as &ldquo;black boxes,&rdquo; with their <b>reasoning</b> obscured and inaccessible, there&rsquo;s an increased risk of misdiagnosis, which can lead to patient harm. Hence, there is a pressing need to cultivate transparency within AI systems, ensuring that the rationale behind an AI&rsquo;s diagnostic <b>recommendations</b> is clear and understandable to medical practitioners. This shift towards transparency is not just beneficial &ndash; it&rsquo;s a critical step towards responsible AI integration in healthcare, ensuring that AI aids rather than hinders medical professionals in their crucial work.</p></p class="citation"></blockquote><h3 id=1928--37115-what-do-you-see-in-vehicle-comprehensive-vision-solution-for-in-vehicle-gaze-estimation-yihua-cheng-et-al-2024>(19/28 | 37/115) What Do You See in Vehicle? Comprehensive Vision Solution for In-Vehicle Gaze Estimation (Yihua Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihua Cheng, Yaning Zhu, Zongji Wang, Hongquan Hao, Yongwei Liu, Shiqing Cheng, Xi Wang, Hyung Jin Chang. (2024)<br><strong>What Do You See in Vehicle? Comprehensive Vision Solution for In-Vehicle Gaze Estimation</strong><br><button class=copy-to-clipboard title="What Do You See in Vehicle? Comprehensive Vision Solution for In-Vehicle Gaze Estimation" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15664v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15664v1.pdf filename=2403.15664v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Driver&rsquo;s eye gaze holds a wealth of cognitive and intentional cues crucial for intelligent vehicles. Despite its significance, research on in-vehicle gaze estimation remains limited due to the scarcity of comprehensive and well-annotated datasets in real driving scenarios. In this paper, we present three novel elements to advance in-vehicle gaze research. Firstly, we introduce IVGaze, a pioneering dataset capturing in-vehicle gaze, collected from 125 subjects and covering a large range of gaze and head poses within vehicles. Conventional gaze collection systems are inadequate for in-vehicle use. In this dataset, we propose a new vision-based solution for in-vehicle gaze collection, introducing a refined gaze target calibration method to tackle annotation challenges. Second, our research focuses on in-vehicle gaze estimation leveraging the IVGaze. In-vehicle face images often suffer from low resolution, <b>prompting</b> our introduction of a gaze pyramid <b>transformer</b> that leverages <b>transformer-based</b> multilevel features integration. Expanding upon this, we introduce the dual-stream gaze pyramid <b>transformer</b> (GazeDPTR). Employing perspective transformation, we rotate virtual cameras to normalize images, utilizing camera pose to merge normalized and original images for accurate gaze estimation. GazeDPTR shows state-of-the-art performance on the IVGaze dataset. Thirdly, we explore a novel strategy for gaze zone classification by extending the GazeDPTR. A foundational tri-plane and project gaze onto these planes are newly defined. Leveraging both positional features from the projection points and visual attributes from images, we achieve superior performance compared to relying solely on visual features, substantiating the advantage of gaze estimation. Our project is available at <a href=https://yihua.zone/work/ivgaze>https://yihua.zone/work/ivgaze</a>.</p></p class="citation"></blockquote><h3 id=2028--38115-spatio-temporal-bi-directional-cross-frame-memory-for-distractor-filtering-point-cloud-single-object-tracking-shaoyu-sun-et-al-2024>(20/28 | 38/115) Spatio-Temporal Bi-directional Cross-frame Memory for Distractor Filtering Point Cloud Single Object Tracking (Shaoyu Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaoyu Sun, Chunyang Wang, Xuelian Liu, Chunhao Shi, Yueyang Ding, Guan Xi. (2024)<br><strong>Spatio-Temporal Bi-directional Cross-frame Memory for Distractor Filtering Point Cloud Single Object Tracking</strong><br><button class=copy-to-clipboard title="Spatio-Temporal Bi-directional Cross-frame Memory for Distractor Filtering Point Cloud Single Object Tracking" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Graph, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15831v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15831v1.pdf filename=2403.15831v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D single object tracking within LIDAR point clouds is a pivotal task in computer vision, with profound implications for autonomous driving and robotics. However, existing methods, which depend solely on appearance matching via Siamese networks or utilize motion information from successive frames, encounter significant challenges. Issues such as similar objects nearby or occlusions can result in tracker drift. To mitigate these challenges, we design an innovative spatio-temporal bi-directional cross-frame distractor filtering tracker, named STMD-Tracker. Our first step involves the creation of a 4D multi-frame spatio-temporal <b>graph</b> <b>convolution</b> backbone. This design separates KNN <b>graph</b> spatial embedding and incorporates 1D temporal <b>convolution,</b> effectively capturing temporal fluctuations and spatio-temporal information. Subsequently, we devise a novel bi-directional cross-frame memory procedure. This integrates future and synthetic past frame memory to enhance the current memory, thereby improving the accuracy of iteration-based tracking. This iterative memory update mechanism allows our tracker to dynamically compensate for information in the current frame, effectively reducing tracker drift. Lastly, we construct spatially reliable Gaussian masks on the fused features to eliminate distractor points. This is further supplemented by an object-aware sampling strategy, which bolsters the efficiency and precision of object localization, thereby reducing tracking errors caused by distractors. Our extensive experiments on KITTI, NuScenes and Waymo datasets demonstrate that our approach significantly surpasses the current state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2128--39115-aocil-exemplar-free-analytic-online-class-incremental-learning-with-low-time-and-resource-consumption-huiping-zhuang-et-al-2024>(21/28 | 39/115) AOCIL: Exemplar-free Analytic Online Class Incremental Learning with Low Time and Resource Consumption (Huiping Zhuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huiping Zhuang, Yuchen Liu, Run He, Kai Tong, Ziqian Zeng, Cen Chen, Yi Wang, Lap-Pui Chau. (2024)<br><strong>AOCIL: Exemplar-free Analytic Online Class Incremental Learning with Low Time and Resource Consumption</strong><br><button class=copy-to-clipboard title="AOCIL: Exemplar-free Analytic Online Class Incremental Learning with Low Time and Resource Consumption" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15751v1.pdf filename=2403.15751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online Class Incremental Learning (OCIL) aims to train the model in a task-by-task manner, where data arrive in mini-batches at a time while previous data are not accessible. A significant challenge is known as Catastrophic Forgetting, i.e., loss of the previous knowledge on old data. To address this, replay-based methods show competitive results but invade data privacy, while exemplar-free methods protect data privacy but struggle for accuracy. In this paper, we proposed an exemplar-free approach &ndash; Analytic Online Class Incremental Learning (AOCIL). Instead of back-propagation, we design the Analytic Classifier (AC) updated by recursive least square, cooperating with a frozen backbone. AOCIL simultaneously achieves high accuracy, low resource consumption and data privacy protection. We conduct massive experiments on four existing <b>benchmark</b> datasets, and the results demonstrate the strong capability of handling OCIL scenarios. Codes will be ready.</p></p class="citation"></blockquote><h3 id=2228--40115-feature-manipulation-for-ddpm-based-change-detection-zhenglin-li-et-al-2024>(22/28 | 40/115) Feature Manipulation for DDPM based Change Detection (Zhenglin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenglin Li, Yangchen Huang, Mengran Zhu, Jingyu Zhang, JingHao Chang, Houze Liu. (2024)<br><strong>Feature Manipulation for DDPM based Change Detection</strong><br><button class=copy-to-clipboard title="Feature Manipulation for DDPM based Change Detection" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15943v1.pdf filename=2403.15943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Change Detection is a classic task of computer vision that receives a bi-temporal image pair as input and separates the semantically changed and unchanged regions of it. The <b>diffusion</b> <b>model</b> is used in image synthesis and as a feature extractor and has been applied to various downstream tasks. Using this, a feature map is extracted from the pre-trained <b>diffusion</b> <b>model</b> from the large-scale data set, and changes are detected through the additional network. On the one hand, the current <b>diffusion-based</b> <b>change</b> detection approach focuses only on extracting a good feature map using the <b>diffusion</b> <b>model.</b> It obtains and uses differences without further adjustment to the created feature map. Our method focuses on manipulating the feature map extracted from the <b>Diffusion</b> <b>Model</b> to be more semantically useful, and for this, we propose two methods: Feature Attention and FDAF. Our model with Feature Attention achieved a state-of-the-art F1 score (90.18) and IoU (83.86) on the LEVIR-CD dataset.</p></p class="citation"></blockquote><h3 id=2328--41115-centered-masking-for-language-image-pre-training-mingliang-liang-et-al-2024>(23/28 | 41/115) Centered Masking for Language-Image Pre-Training (Mingliang Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingliang Liang, Martha Larson. (2024)<br><strong>Centered Masking for Language-Image Pre-Training</strong><br><button class=copy-to-clipboard title="Centered Masking for Language-Image Pre-Training" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15837v1.pdf filename=2403.15837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel, straightforward, and effective technique for masking image patches during pre-training of a <b>vision-language</b> model. GLIP builds on Fast Language-Image Pre-Training (FLIP), which randomly masks image patches while training a CLIP model. GLIP replaces random masking with centered masking, that uses a Gaussian distribution and is inspired by the importance of image patches at the center of the image. GLIP retains the same computational savings as FLIP, while improving performance across a range of downstream datasets and tasks, as demonstrated by our experimental results. We show the benefits of GLIP to be easy to obtain, requiring no delicate tuning of the Gaussian, and also applicable to data sets containing images without an obvious center focus.</p></p class="citation"></blockquote><h3 id=2428--42115-contact-aware-human-motion-generation-from-textual-descriptions-sihan-ma-et-al-2024>(24/28 | 42/115) Contact-aware Human Motion Generation from Textual Descriptions (Sihan Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sihan Ma, Qiong Cao, Jing Zhang, Dacheng Tao. (2024)<br><strong>Contact-aware Human Motion Generation from Textual Descriptions</strong><br><button class=copy-to-clipboard title="Contact-aware Human Motion Generation from Textual Descriptions" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15709v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15709v1.pdf filename=2403.15709v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the problem of generating 3D interactive human motion from text. Given a textual description depicting the actions of different body parts in contact with objects, we synthesize sequences of 3D body poses that are visually natural and physically plausible. Yet, this task poses a significant challenge due to the inadequate consideration of interactions by physical contacts in both motion and textual descriptions, leading to unnatural and implausible sequences. To tackle this challenge, we create a novel dataset named RICH-CAT, representing ``Contact-Aware Texts&rsquo;&rsquo; constructed from the RICH dataset. RICH-CAT comprises high-quality motion, accurate human-object contact labels, and detailed textual descriptions, encompassing over 8,500 motion-text pairs across 26 indoor/outdoor actions. Leveraging RICH-CAT, we propose a novel approach named CATMO for text-driven interactive human motion synthesis that explicitly integrates human body contacts as evidence. We employ two VQ-VAE models to encode motion and body contact sequences into distinct yet complementary latent spaces and an intertwined <b>GPT</b> for generating human motions and contacts in a mutually conditioned manner. Additionally, we introduce a pre-trained text encoder to learn textual embeddings that better discriminate among various contact types, allowing for more precise control over synthesized motions and contacts. Our experiments demonstrate the superior performance of our approach compared to existing text-to-motion methods, producing stable, contact-aware motion sequences. Code and data will be available for research purposes.</p></p class="citation"></blockquote><h3 id=2528--43115-an-active-learning-model-to-classify-animal-species-in-hong-kong-gareth-lamb-et-al-2024>(25/28 | 43/115) An active learning model to classify animal species in Hong Kong (Gareth Lamb et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gareth Lamb, Ching Hei Lo, Jin Wu, Calvin K. F. Lee. (2024)<br><strong>An active learning model to classify animal species in Hong Kong</strong><br><button class=copy-to-clipboard title="An active learning model to classify animal species in Hong Kong" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15675v1.pdf filename=2403.15675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Camera traps are used by ecologists globally as an efficient and non-invasive method to monitor animals. While it is time-consuming to manually label the collected images, recent advances in deep learning and computer vision has made it possible to automating this process [1]. A major obstacle to this is the generalisability of these models when applying these images to independently collected data from other parts of the world [2]. Here, we use a deep <b>active</b> <b>learning</b> workflow [3], and train a model that is applicable to camera trap images collected in Hong Kong.</p></p class="citation"></blockquote><h3 id=2628--44115-maptracker-tracking-with-strided-memory-fusion-for-consistent-vector-hd-mapping-jiacheng-chen-et-al-2024>(26/28 | 44/115) MapTracker: Tracking with Strided Memory Fusion for Consistent Vector HD Mapping (Jiacheng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacheng Chen, Yuefan Wu, Jiaqi Tan, Hang Ma, Yasutaka Furukawa. (2024)<br><strong>MapTracker: Tracking with Strided Memory Fusion for Consistent Vector HD Mapping</strong><br><button class=copy-to-clipboard title="MapTracker: Tracking with Strided Memory Fusion for Consistent Vector HD Mapping" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 8<br>Keywords: Benchmarking, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15951v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15951v1.pdf filename=2403.15951v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a vector HD-mapping algorithm that formulates the mapping as a tracking task and uses a history of memory latents to ensure consistent reconstructions over time. Our method, MapTracker, accumulates a sensor stream into memory buffers of two latent representations: 1) Raster latents in the bird&rsquo;s-eye-view (BEV) space and 2) Vector latents over the road elements (i.e., pedestrian-crossings, lane-dividers, and road-boundaries). The approach borrows the query propagation paradigm from the tracking literature that explicitly associates tracked road elements from the previous frame to the current, while fusing a subset of memory latents selected with distance strides to further enhance temporal consistency. A vector latent is decoded to reconstruct the <b>geometry</b> of a road element. The paper further makes <b>benchmark</b> contributions by 1) Improving processing code for existing datasets to produce consistent ground truth with temporal alignments and 2) Augmenting existing mAP metrics with consistency checks. MapTracker significantly outperforms existing methods on both nuScenes and Agroverse2 datasets by over 8% and 19% on the conventional and the new consistency-aware metrics, respectively. The code will be available on our project page: <a href=https://map-tracker.github.io>https://map-tracker.github.io</a>.</p></p class="citation"></blockquote><h3 id=2728--45115-finding-needles-in-a-haystack-a-black-box-approach-to-invisible-watermark-detection-minzhou-pan-et-al-2024>(27/28 | 45/115) Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection (Minzhou Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minzhou Pan, Zhengting Wang, Xin Dong, Vikash Sehwag, Lingjuan Lyu, Xue Lin. (2024)<br><strong>Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection</strong><br><button class=copy-to-clipboard title="Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15955v1.pdf filename=2403.15955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose WaterMark Detection (WMD), the first invisible watermark detection method under a <b>black-box</b> <b>and</b> annotation-free setting. WMD is capable of detecting arbitrary watermarks within a given reference dataset using a clean non-watermarked dataset as a reference, without relying on specific decoding methods or prior knowledge of the watermarking techniques. We develop WMD using foundations of offset learning, where a clean non-watermarked dataset enables us to isolate the influence of only watermarked samples in the reference dataset. Our comprehensive evaluations demonstrate the effectiveness of WMD, significantly outperforming naive detection methods, which only yield AUC scores around 0.5. In contrast, WMD consistently achieves impressive detection AUC scores, surpassing 0.9 in most single-watermark datasets and exceeding 0.7 in more challenging multi-watermark scenarios across diverse datasets and watermarking methods. As invisible watermarks become increasingly prevalent, while specific decoding techniques remain undisclosed, our approach provides a versatile solution and establishes a path toward increasing accountability, transparency, and trust in our digital visual content.</p></p class="citation"></blockquote><h3 id=2828--46115-pnas-mot-multi-modal-object-tracking-with-pareto-neural-architecture-search-chensheng-peng-et-al-2024>(28/28 | 46/115) PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture Search (Chensheng Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chensheng Peng, Zhaoyu Zeng, Jinling Gao, Jundong Zhou, Masayoshi Tomizuka, Xinbing Wang, Chenghu Zhou, Nanyang Ye. (2024)<br><strong>PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture Search</strong><br><button class=copy-to-clipboard title="PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture Search" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15712v1.pdf filename=2403.15712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiple object tracking is a critical task in autonomous driving. Existing works primarily focus on the heuristic design of neural networks to obtain high accuracy. As tracking accuracy improves, however, neural networks become increasingly complex, posing challenges for their practical application in real driving scenarios due to the high level of latency. In this paper, we explore the use of the neural architecture search (NAS) methods to search for efficient architectures for tracking, aiming for low real-time latency while maintaining relatively high accuracy. Another challenge for object tracking is the unreliability of a single sensor, therefore, we propose a <b>multi-modal</b> framework to improve the robustness. Experiments demonstrate that our algorithm can run on edge devices within lower latency constraints, thus greatly reducing the computational requirements for <b>multi-modal</b> object tracking while keeping lower latency.</p></p class="citation"></blockquote><h2 id=csse-3>cs.SE (3)</h2><h3 id=13--47115-when-llm-based-code-generation-meets-the-software-development-process-feng-lin-et-al-2024>(1/3 | 47/115) When LLM-based Code Generation Meets the Software Development Process (Feng Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Lin, Dong Jae Kim, Tse-Husn, Chen. (2024)<br><strong>When LLM-based Code Generation Meets the Software Development Process</strong><br><button class=copy-to-clipboard title="When LLM-based Code Generation Meets the Software Development Process" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 73<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15852v1.pdf filename=2403.15852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Software process models play a pivotal role in fostering collaboration and communication within software teams, enabling them to tackle intricate development tasks effectively. This paper introduces LCG, a <b>code</b> <b>generation</b> framework inspired by established software engineering practices. LCG leverages multiple <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> agents to emulate various software process models, namely LCGWaterfall, LCGTDD, and LCGScrum. Each model assigns <b>LLM</b> agents specific roles such as requirement engineer, architect, developer, tester, and scrum master, mirroring typical development activities and communication patterns. Through collaborative efforts utilizing chain-of-thought and <b>prompt</b> composition techniques, the agents continuously refine themselves to enhance <b>code</b> <b>quality.</b> Utilizing <b>GPT3.5</b> as the underlying <b>LLM</b> and baseline <b>(GPT),</b> we evaluate LCG across four <b>code</b> <b>generation</b> <b>benchmarks:</b> HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Results indicate LCGScrum outperforms other models, achieving Pass@1 scores of 75.2, 65.5, 82.5, and 56.7 in HumanEval, HumanEval-ET, MBPP, and MBPP-ET, respectively - an average 15% improvement over <b>GPT.</b> Analysis reveals distinct impacts of development activities on generated <b>code,</b> <b>with</b> design and <b>code</b> <b>reviews</b> contributing to enhanced exception handling, while design, testing, and <b>code</b> <b>reviews</b> mitigate <b>code</b> <b>smells.</b> Furthermore, temperature values exhibit negligible influence on Pass@1 across all models. However, variations in Pass@1 are notable for different <b>GPT3.5</b> model versions, ranging from 5 to over 60 in HumanEval, highlighting the stability of LCG across model versions. This stability underscores the importance of adopting software process models to bolster the quality and consistency of <b>LLM-generated</b> code.</p></p class="citation"></blockquote><h3 id=23--48115-codeshell-technical-report-rui-xie-et-al-2024>(2/3 | 48/115) CodeShell Technical Report (Rui Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Xie, Zhengran Zeng, Zhuohao Yu, Chang Gao, Shikun Zhang, Wei Ye. (2024)<br><strong>CodeShell Technical Report</strong><br><button class=copy-to-clipboard title="CodeShell Technical Report" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: Foundation Model, GPT, GPT-2, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15747v1.pdf filename=2403.15747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Code <b>large</b> <b>language</b> <b>models</b> mark a pivotal breakthrough in artificial intelligence. They are specifically crafted to understand and generate programming languages, significantly boosting the efficiency of coding development workflows. In this technical report, we present CodeShell-Base, a seven billion-parameter <b>foundation</b> <b>model</b> with 8K context length, showcasing exceptional proficiency in code comprehension. By incorporating Grouped-Query Attention and Rotary Positional Embedding into <b>GPT-2,</b> CodeShell-Base integrates the structural merits of StarCoder and CodeLlama and forms its unique architectural design. We then carefully built a comprehensive data pre-processing process, including similar data deduplication, <b>perplexity-based</b> data filtering, and model-based data filtering. Through this process, We have curated 100 billion high-quality pre-training data from GitHub. Benefiting from the high-quality data, CodeShell-Base outperforms CodeLlama in Humaneval after training on just 500 billion tokens (5 epochs). We have conducted extensive experiments across multiple language datasets, including Python, Java, and C++, and the results indicate that our model possesses robust <b>foundational</b> <b>capabilities</b> in code comprehension and generation.</p></p class="citation"></blockquote><h3 id=33--49115-leveraging-large-language-models-for-preliminary-security-risk-analysis-a-mission-critical-case-study-matteo-esposito-et-al-2024>(3/3 | 49/115) Leveraging Large Language Models for Preliminary Security Risk Analysis: A Mission-Critical Case Study (Matteo Esposito et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Esposito, Francesco Palagiano. (2024)<br><strong>Leveraging Large Language Models for Preliminary Security Risk Analysis: A Mission-Critical Case Study</strong><br><button class=copy-to-clipboard title="Leveraging Large Language Models for Preliminary Security Risk Analysis: A Mission-Critical Case Study" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-CR, cs-CY, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15756v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15756v1.pdf filename=2403.15756v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Preliminary security risk analysis (PSRA) provides a quick approach to identify, evaluate and propose remeditation to potential risks in specific scenarios. The extensive expertise required for an effective PSRA and the substantial ammount of textual-related tasks hinder quick assessments in mission-critical contexts, where timely and <b>prompt</b> actions are essential. The speed and accuracy of human experts in PSRA significantly impact response time. A <b>large</b> <b>language</b> <b>model</b> can quickly summarise information in less time than a human. To our knowledge, no prior study has explored the capabilities of <b>fine-tuned</b> models (FTM) in PSRA. Our case study investigates the proficiency of FTM to assist practitioners in PSRA. We manually curated 141 representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years.We compared the proficiency of the FTM versus seven human experts. Within the industrial context, our approach has proven successful in reducing errors in PSRA, hastening security risk detection, and minimizing false positives and negatives. This translates to cost savings for the company by averting unnecessary expenses associated with implementing unwarranted countermeasures. Therefore, experts can focus on more comprehensive risk analysis, leveraging <b>LLMs</b> for an effective preliminary assessment within a condensed timeframe.</p></p class="citation"></blockquote><h2 id=eessiv-3>eess.IV (3)</h2><h3 id=13--50115-graph-image-prior-for-unsupervised-dynamic-mri-reconstruction-zhongsen-li-et-al-2024>(1/3 | 50/115) Graph Image Prior for Unsupervised Dynamic MRI Reconstruction (Zhongsen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongsen Li, Wenxuan Chen, Shuai Wang, Chuyu Liu, Rui Li. (2024)<br><strong>Graph Image Prior for Unsupervised Dynamic MRI Reconstruction</strong><br><button class=copy-to-clipboard title="Graph Image Prior for Unsupervised Dynamic MRI Reconstruction" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 73<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Convolutional Neural Network, Convolutional Neural Network, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15770v1.pdf filename=2403.15770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The inductive bias of the <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> can act as a strong prior for image restoration, which is known as the Deep Image Prior (DIP). In recent years, DIP has been utilized in <b>unsupervised</b> dynamic MRI reconstruction, which adopts a generative model from the latent space to the image space. However, existing methods usually utilize a single pyramid-shaped <b>CNN</b> architecture to parameterize the generator, which cannot effectively exploit the spatio-temporal correlations within the dynamic data. In this work, we propose a novel scheme to exploit the DIP prior for dynamic MRI reconstruction, named ``Graph Image Prior&rsquo;&rsquo; (GIP). The generative model is decomposed into two stages: image recovery and manifold discovery, which is bridged by a <b>graph</b> <b>convolutional</b> <b>network</b> <b>to</b> exploit the spatio-temporal correlations. In addition, we devise an ADMM algorithm to alternately optimize the images and the network parameters to further improve the reconstruction performance. Experimental results demonstrate that GIP outperforms compressed sensing methods and <b>unsupervised</b> methods over different sampling trajectories, and significantly reduces the performance gap with the state-of-art <b>supervised</b> deep-learning methods. Moreover, GIP displays superior generalization ability when transferred to a different reconstruction setting, without the need for any additional data.</p></p class="citation"></blockquote><h3 id=23--51115-an-edge-detection-based-deep-learning-approach-for-tear-meniscus-height-measurement-kesheng-wang-et-al-2024>(2/3 | 51/115) An edge detection-based deep learning approach for tear meniscus height measurement (Kesheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kesheng Wang, Kunhui Xu, Xiaoyu Chen, Chunlei He, Jianfeng Zhang, Dexing Kong, Qi Dai, Shoujun Huang. (2024)<br><strong>An edge detection-based deep learning approach for tear meniscus height measurement</strong><br><button class=copy-to-clipboard title="An edge detection-based deep learning approach for tear meniscus height measurement" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15853v1.pdf filename=2403.15853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic measurements of tear meniscus height (TMH) have been achieved by using deep learning techniques; however, annotation is significantly influenced by subjective factors and is both time-consuming and labor-intensive. In this paper, we introduce an automatic TMH measurement technique based on edge detection-assisted annotation within a deep learning framework. This method generates mask labels less affected by subjective factors with enhanced efficiency compared to previous annotation approaches. For improved segmentation of the pupil and tear meniscus areas, the <b>convolutional</b> <b>neural</b> <b>network</b> Inceptionv3 was first implemented as an image quality assessment model, effectively identifying higher-quality images with an accuracy of 98.224%. Subsequently, by using the generated labels, various algorithms, including Unet, ResUnet, Deeplabv3+FcnResnet101, Deeplabv3+FcnResnet50, FcnResnet50, and FcnResnet101 were trained, with Unet demonstrating the best performance. Finally, Unet was used for automatic pupil and tear meniscus segmentation to locate the center of the pupil and calculate TMH,respectively. An evaluation of the mask quality predicted by Unet indicated a Mean Intersection over Union of 0.9362, a recall of 0.9261, a precision of 0.9423, and an F1-Score of 0.9326. Additionally, the TMH predicted by the model was assessed, with the fitting curve represented as y= 0.982x-0.862, an overall correlation coefficient of r^2=0.961 , and an accuracy of 94.80% (237/250). In summary, the algorithm can automatically screen images based on their quality,segment the pupil and tear meniscus areas, and automatically measure TMH. Measurement results using the AI algorithm demonstrate a high level of consistency with manual measurements, offering significant support to clinical doctors in diagnosing dry eye disease.</p></p class="citation"></blockquote><h3 id=33--52115-3d-transunet-for-brain-metastases-segmentation-in-the-brats2023-challenge-siwei-yang-et-al-2024>(3/3 | 52/115) 3D-TransUNet for Brain Metastases Segmentation in the BraTS2023 Challenge (Siwei Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siwei Yang, Xianhang Li, Jieru Mei, Jieneng Chen, Cihang Xie, Yuyin Zhou. (2024)<br><strong>3D-TransUNet for Brain Metastases Segmentation in the BraTS2023 Challenge</strong><br><button class=copy-to-clipboard title="3D-TransUNet for Brain Metastases Segmentation in the BraTS2023 Challenge" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15735v1.pdf filename=2403.15735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Segmenting brain tumors is complex due to their diverse appearances and scales. Brain metastases, the most common type of brain tumor, are a frequent complication of cancer. Therefore, an effective segmentation model for brain metastases must adeptly capture local intricacies to delineate small tumor regions while also integrating global context to understand broader scan features. The TransUNet model, which combines <b>Transformer</b> <b>self-attention</b> with U-Net&rsquo;s localized information, emerges as a promising solution for this task. In this report, we address brain metastases segmentation by training the 3D-TransUNet model on the Brain Tumor Segmentation (BraTS-METS) 2023 challenge dataset. Specifically, we explored two architectural configurations: the Encoder-only 3D-TransUNet, employing <b>Transformers</b> solely in the encoder, and the Decoder-only 3D-TransUNet, utilizing <b>Transformers</b> exclusively in the decoder. For Encoder-only 3D-TransUNet, we note that Masked-Autoencoder pre-training is required for a better initialization of the <b>Transformer</b> Encoder and thus accelerates the training process. We identify that the Decoder-only 3D-TransUNet model should offer enhanced efficacy in the segmentation of brain metastases, as indicated by our 5-fold cross-validation on the training set. However, our use of the Encoder-only 3D-TransUNet model already yield notable results, with an average lesion-wise Dice score of 59.8% on the test set, securing second place in the BraTS-METS 2023 challenge.</p></p class="citation"></blockquote><h2 id=csai-10>cs.AI (10)</h2><h3 id=110--53115-using-large-language-models-for-ontoclean-based-ontology-refinement-yihang-zhao-et-al-2024>(1/10 | 53/115) Using Large Language Models for OntoClean-based Ontology Refinement (Yihang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihang Zhao, Neil Vetter, Kaveh Aryan. (2024)<br><strong>Using Large Language Models for OntoClean-based Ontology Refinement</strong><br><button class=copy-to-clipboard title="Using Large Language Models for OntoClean-based Ontology Refinement" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 70<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15864v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15864v1.pdf filename=2403.15864v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the integration of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> such as <b>GPT-3.5</b> and <b>GPT-4</b> into the ontology refinement process, specifically focusing on the OntoClean methodology. OntoClean, critical for assessing the metaphysical quality of ontologies, involves a two-step process of assigning meta-properties to classes and verifying a set of constraints. Manually conducting the first step proves difficult in practice, due to the need for philosophical expertise and lack of consensus among ontologists. By employing <b>LLMs</b> with two <b>prompting</b> strategies, the study demonstrates that high accuracy in the labelling process can be achieved. The findings suggest the potential for <b>LLMs</b> to enhance ontology refinement, proposing the development of plugin software for ontology tools to facilitate this integration.</p></p class="citation"></blockquote><h3 id=210--54115-lamper-language-model-and-prompt-engineering-for-zero-shot-time-series-classification-zhicheng-du-et-al-2024>(2/10 | 54/115) LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series classification (Zhicheng Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhicheng Du, Zhaotian Xie, Yan Tong, Peiwu Qin. (2024)<br><strong>LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series classification</strong><br><button class=copy-to-clipboard title="LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series classification" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 40<br>Keywords: Zero-shot, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15875v1.pdf filename=2403.15875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study constructs the LanguAge Model with <b>Prompt</b> EngineeRing (LAMPER) framework, designed to systematically evaluate the adaptability of <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> in accommodating diverse <b>prompts</b> and their integration in <b>zero-shot</b> time series (TS) classification. We deploy LAMPER in experimental assessments using 128 univariate TS datasets sourced from the UCR archive. Our findings indicate that the feature representation capacity of LAMPER is influenced by the maximum input token threshold imposed by <b>PLMs.</b></p></p class="citation"></blockquote><h3 id=310--55115-the-frontier-of-data-erasure-machine-unlearning-for-large-language-models-youyang-qu-et-al-2024>(3/10 | 55/115) The Frontier of Data Erasure: Machine Unlearning for Large Language Models (Youyang Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youyang Qu, Ming Ding, Nan Sun, Kanchana Thilakarathna, Tianqing Zhu, Dusit Niyato. (2024)<br><strong>The Frontier of Data Erasure: Machine Unlearning for Large Language Models</strong><br><button class=copy-to-clipboard title="The Frontier of Data Erasure: Machine Unlearning for Large Language Models" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: Machine Unlearning, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15779v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15779v1.pdf filename=2403.15779v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are foundational to AI advancements, facilitating applications like predictive <b>text</b> <b>generation.</b> Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. <b>Machine</b> <b>unlearning</b> emerges as a cutting-edge solution to mitigate these concerns, offering techniques for <b>LLMs</b> to selectively discard certain data. This paper reviews the latest in <b>machine</b> <b>unlearning</b> for <b>LLMs,</b> introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of <b>machine</b> <b>unlearning,</b> this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of <b>machine</b> <b>unlearning</b> in advancing responsible, ethical AI.</p></p class="citation"></blockquote><h3 id=410--56115-an-upload-efficient-scheme-for-transferring-knowledge-from-a-server-side-pre-trained-generator-to-clients-in-heterogeneous-federated-learning-jianqing-zhang-et-al-2024>(4/10 | 56/115) An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning (Jianqing Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianqing Zhang, Yang Liu, Yang Hua, Jian Cao. (2024)<br><strong>An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning</strong><br><button class=copy-to-clipboard title="An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-DC, cs.AI<br>Keyword Score: 40<br>Keywords: Convolutional Neural Network, Federated Learning, Knowledge Transfer, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15760v1.pdf filename=2403.15760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Heterogeneous <b>Federated</b> <b>Learning</b> (HtFL) enables collaborative learning on multiple clients with different model architectures while preserving privacy. Despite recent research progress, <b>knowledge</b> <b>sharing</b> in HtFL is still difficult due to data and model heterogeneity. To tackle this issue, we leverage the <b>knowledge</b> <b>stored</b> in pre-trained generators and propose a new upload-efficient <b>knowledge</b> <b>transfer</b> scheme called <b>Federated</b> <b>Knowledge-Transfer</b> <b>Loop</b> (FedKTL). Our FedKTL can produce client-task-related prototypical image-vector pairs via the generator&rsquo;s inference on the server. With these pairs, each client can transfer pre-existing <b>knowledge</b> <b>from</b> the generator to its local model through an additional <b>supervised</b> local task. We conduct extensive experiments on four datasets under two types of data heterogeneity with 14 kinds of models including <b>CNNs</b> and ViTs. Results show that our upload-efficient FedKTL surpasses seven state-of-the-art methods by up to 7.31% in accuracy. Moreover, our <b>knowledge</b> <b>transfer</b> scheme is applicable in scenarios with only one edge client. Code: <a href=https://github.com/TsingZ0/FedKTL>https://github.com/TsingZ0/FedKTL</a></p></p class="citation"></blockquote><h3 id=510--57115-mixred-a-mix-lingual-relation-extraction-dataset-lingxing-kong-et-al-2024>(5/10 | 57/115) MixRED: A Mix-lingual Relation Extraction Dataset (Lingxing Kong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingxing Kong, Yougang Chu, Zheng Ma, Jianbing Zhang, Liang He, Jiajun Chen. (2024)<br><strong>MixRED: A Mix-lingual Relation Extraction Dataset</strong><br><button class=copy-to-clipboard title="MixRED: A Mix-lingual Relation Extraction Dataset" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 40<br>Keywords: Supervised Learning, Relation Extraction, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15696v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15696v1.pdf filename=2403.15696v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Relation</b> <b>extraction</b> is a critical task in the field of natural language processing with numerous real-world applications. Existing research primarily focuses on monolingual <b>relation</b> <b>extraction</b> or cross-lingual enhancement for <b>relation</b> <b>extraction.</b> Yet, there remains a significant gap in understanding <b>relation</b> <b>extraction</b> in the mix-lingual (or code-switching) scenario, where individuals intermix contents from different languages within sentences, generating mix-lingual content. Due to the lack of a dedicated dataset, the effectiveness of existing <b>relation</b> <b>extraction</b> models in such a scenario is largely unexplored. To address this issue, we introduce a novel task of considering <b>relation</b> <b>extraction</b> in the mix-lingual scenario called MixRE and constructing the human-annotated dataset MixRED to support this task. In addition to constructing the MixRED dataset, we evaluate both state-of-the-art <b>supervised</b> models and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on MixRED, revealing their respective advantages and limitations in the mix-lingual scenario. Furthermore, we delve into factors influencing model performance within the MixRE task and uncover promising directions for enhancing the performance of both <b>supervised</b> models and <b>LLMs</b> in this novel task.</p></p class="citation"></blockquote><h3 id=610--58115-trustsql-a-reliability-benchmark-for-text-to-sql-models-with-diverse-unanswerable-questions-gyubok-lee-et-al-2024>(6/10 | 58/115) TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions (Gyubok Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gyubok Lee, Woosog Chay, Seonhee Cho, Edward Choi. (2024)<br><strong>TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions</strong><br><button class=copy-to-clipboard title="TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 33<br>Keywords: Benchmarking, Text2SQL, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15879v1.pdf filename=2403.15879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have led to significant improvements in translating natural language questions into SQL queries. While achieving high accuracy in SQL generation is crucial, little is known about the extent to which these <b>text-to-SQL</b> models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones. To explore this aspect, we present TrustSQL, a new <b>benchmark</b> designed to assess the reliability of <b>text-to-SQL</b> models in both single-database and cross-database settings. The <b>benchmark</b> tasks models with providing one of two outcomes: 1) SQL prediction; or 2) abstention from making a prediction, either when there is a potential error in the generated SQL or when faced with unanswerable questions. For model evaluation, we explore various modeling approaches specifically designed for this task. These include: 1) optimizing separate models for answerability detection, SQL generation, and error detection, which are then integrated into a single pipeline; and 2) developing a unified approach that optimizes a single model to address the proposed task. Experimental results using our new reliability score show that addressing this challenge involves many different areas of research and opens new avenues for model development. Nonetheless, none of the methods surpass the reliability performance of the naive baseline, which abstains from answering all questions.</p></p class="citation"></blockquote><h3 id=710--59115-matchseg-towards-better-segmentation-via-reference-image-matching-ruiqiang-xiao-et-al-2024>(7/10 | 59/115) MatchSeg: Towards Better Segmentation via Reference Image Matching (Ruiqiang Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiqiang Xiao, Jiayu Huo, Haotian Zheng, Yang Liu, Sebastien Ourselin, Rachel Sparks. (2024)<br><strong>MatchSeg: Towards Better Segmentation via Reference Image Matching</strong><br><button class=copy-to-clipboard title="MatchSeg: Towards Better Segmentation via Reference Image Matching" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CV, cs.AI<br>Keyword Score: 30<br>Keywords: Few-shot, Few-shot Learning, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15901v1.pdf filename=2403.15901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, automated medical image segmentation methods based on deep learning have achieved great success. However, they heavily rely on large annotated datasets, which are costly and time-consuming to acquire. <b>Few-shot</b> <b>learning</b> aims to overcome the need for annotated data by using a small labeled dataset, known as a support set, to guide predicting labels for new, unlabeled images, known as the query set. Inspired by this paradigm, we introduce MatchSeg, a novel framework that enhances medical image segmentation through strategic reference image matching. We leverage contrastive language-image pre-training (CLIP) to select highly relevant samples when defining the support set. Additionally, we design a joint attention module to strengthen the interaction between support and query features, facilitating a more effective <b>knowledge</b> <b>transfer</b> between support and query sets. We validated our method across four public datasets. Experimental results demonstrate superior segmentation performance and powerful domain generalization ability of MatchSeg against existing methods for domain-specific and cross-domain segmentation tasks. Our code is made available at <a href=https://github.com/keeplearning-again/MatchSeg>https://github.com/keeplearning-again/MatchSeg</a></p></p class="citation"></blockquote><h3 id=810--60115-multi-agent-transformer-accelerated-rl-for-satisfaction-of-stl-specifications-albin-larsson-forsberg-et-al-2024>(8/10 | 60/115) Multi-agent transformer-accelerated RL for satisfaction of STL specifications (Albin Larsson Forsberg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Albin Larsson Forsberg, Alexandros Nikou, Aneta Vulgarakis Feljan, Jana Tumova. (2024)<br><strong>Multi-agent transformer-accelerated RL for satisfaction of STL specifications</strong><br><button class=copy-to-clipboard title="Multi-agent transformer-accelerated RL for satisfaction of STL specifications" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15916v1.pdf filename=2403.15916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the main challenges in multi-agent <b>reinforcement</b> <b>learning</b> is scalability as the number of agents increases. This issue is further exacerbated if the problem considered is temporally dependent. State-of-the-art solutions today mainly follow centralized training with decentralized execution paradigm in order to handle the scalability concerns. In this paper, we propose time-dependent multi-agent <b>transformers</b> which can solve the temporally dependent multi-agent problem efficiently with a centralized approach via the use of <b>transformers</b> that proficiently handle the large input. We highlight the efficacy of this method on two problems and use tools from statistics to verify the probability that the trajectories generated under the policy satisfy the task. The experiments show that our approach has superior performance against the literature baseline algorithms in both cases.</p></p class="citation"></blockquote><h3 id=910--61115-sat-encoding-of-partial-ordering-models-for-graph-coloring-problems-daniel-faber-et-al-2024>(9/10 | 61/115) SAT Encoding of Partial Ordering Models for Graph Coloring Problems (Daniel Faber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Faber, Adalat Jabrayilov, Petra Mutzel. (2024)<br><strong>SAT Encoding of Partial Ordering Models for Graph Coloring Problems</strong><br><button class=copy-to-clipboard title="SAT Encoding of Partial Ordering Models for Graph Coloring Problems" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-DM, cs-DS, cs-LO, cs.AI<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15961v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15961v1.pdf filename=2403.15961v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we suggest new SAT encodings of the partial-ordering based ILP model for the <b>graph</b> coloring problem (GCP) and the bandwidth coloring problem (BCP). The GCP asks for the minimum number of colors that can be assigned to the vertices of a given <b>graph</b> such that each two adjacent vertices get different colors. The BCP is a generalization, where each edge has a weight that enforces a minimal &ldquo;distance&rdquo; between the assigned colors, and the goal is to minimize the &ldquo;largest&rdquo; color used. For the widely studied GCP, we experimentally compare our new SAT encoding to the state-of-the-art approaches on the DIMACS <b>benchmark</b> set. Our evaluation confirms that this SAT encoding is effective for sparse <b>graphs</b> and even outperforms the state-of-the-art on some DIMACS instances. For the BCP, our theoretical analysis shows that the partial-ordering based SAT and ILP formulations have an asymptotically smaller size than that of the classical assignment-based model. Our practical evaluation confirms not only a dominance compared to the assignment-based encodings but also to the state-of-the-art approaches on a set of <b>benchmark</b> instances. Up to our knowledge, we have solved several open instances of the BCP from the literature for the first time.</p></p class="citation"></blockquote><h3 id=1010--62115-understanding-domain-size-generalization-in-markov-logic-networks-florian-chen-et-al-2024>(10/10 | 62/115) Understanding Domain-Size Generalization in Markov Logic Networks (Florian Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Chen, Felix Weitkämper, Sagar Malhotra. (2024)<br><strong>Understanding Domain-Size Generalization in Markov Logic Networks</strong><br><button class=copy-to-clipboard title="Understanding Domain-Size Generalization in Markov Logic Networks" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15933v1.pdf filename=2403.15933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the generalization behavior of Markov Logic Networks (MLNs) across relational structures of different sizes. Multiple works have noticed that MLNs learned on a given domain generalize poorly across domains of different sizes. This behavior emerges from a lack of internal consistency within an MLN when used across different domain sizes. In this paper, we quantify this inconsistency and bound it in terms of the variance of the MLN parameters. The parameter variance also bounds the KL divergence between an MLN&rsquo;s marginal distributions taken from different domain sizes. We use these bounds to show that maximizing the data log-likelihood while simultaneously minimizing the parameter variance corresponds to two natural notions of generalization across domain sizes. Our theoretical results apply to Exponential Random <b>Graphs</b> and other Markov network based relational models. Finally, we observe that solutions known to decrease the variance of the MLN parameters, like regularization and Domain-Size Aware MLNs, increase the internal consistency of the MLNs. We empirically verify our results on four different datasets, with different methods to control parameter variance, showing that controlling parameter variance leads to better generalization.</p></p class="citation"></blockquote><h2 id=csro-7>cs.RO (7)</h2><h3 id=17--63115-explore-until-confident-efficient-exploration-for-embodied-question-answering-allen-z-ren-et-al-2024>(1/7 | 63/115) Explore until Confident: Efficient Exploration for Embodied Question Answering (Allen Z. Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Allen Z. Ren, Jaden Clark, Anushri Dixit, Masha Itkina, Anirudha Majumdar, Dorsa Sadigh. (2024)<br><strong>Explore until Confident: Efficient Exploration for Embodied Question Answering</strong><br><button class=copy-to-clipboard title="Explore until Confident: Efficient Exploration for Embodied Question Answering" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Simulation, Simulator, Question Answering, Reasoning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15941v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15941v1.pdf filename=2403.15941v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of Embodied <b>Question</b> <b>Answering</b> (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a <b>question.</b> <b>In</b> this work, we leverage the strong semantic <b>reasoning</b> capabilities of large <b>vision-language</b> models (VLMs) to efficiently explore and answer such <b>questions.</b> <b>However,</b> there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual <b>prompting</b> of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM&rsquo;s <b>question</b> <b>answering</b> confidence, allowing the robot to know when to stop exploration - leading to a more calibrated and efficient exploration strategy. To test our framework in <b>simulation,</b> we also contribute a new EQA dataset with diverse, realistic human-robot scenarios and scenes built upon the Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot experiments show our proposed approach improves the performance and efficiency over baselines that do no leverage VLM for exploration or do not calibrate its confidence. Webpage with experiment videos and code: <a href=https://explore-eqa.github.io/>https://explore-eqa.github.io/</a></p></p class="citation"></blockquote><h3 id=27--64115-ia-imperative-learning-based-a-search-for-pathfinding-xiangyu-chen-et-al-2024>(2/7 | 64/115) iA$^<em>$: Imperative Learning-based A$^</em>$ Search for Pathfinding (Xiangyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyu Chen, Fan Yang, Chen Wang. (2024)<br><strong>iA$^<em>$: Imperative Learning-based A$^</em>$ Search for Pathfinding</strong><br><button class=copy-to-clipboard title="iA$^*$: Imperative Learning-based A$^*$ Search for Pathfinding" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Self-supervised Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15870v1.pdf filename=2403.15870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pathfinding problem, which aims to identify a collision-free path between two points, is crucial for many applications, such as robot navigation and autonomous driving. Classic methods, such as A$^<em>$ search, perform well on small-scale maps but face difficulties scaling up. Conversely, data-driven approaches can improve pathfinding efficiency but require extensive data labeling and lack theoretical guarantees, making it challenging for practical applications. To combine the strengths of the two methods, we utilize the imperative learning (IL) strategy and propose a novel <b>self-supervised</b> pathfinding framework, termed imperative learning-based A$^</em>$ (iA$^<em>$). Specifically, iA$^</em>$ is a bilevel optimization process where the lower-level optimization is dedicated to finding the optimal path by a differentiable A$^<em>$ search module, and the upper-level optimization narrows down the search space to improve efficiency via setting suitable initial values from a data-driven model. Besides, the model within the upper-level optimization is a fully <b>convolutional</b> <b>network,</b> trained by the calculated loss in the lower-level optimization. Thus, the framework avoids extensive data labeling and can be applied in diverse environments. Our comprehensive experiments demonstrate that iA$^</em>$ surpasses both classical and data-driven methods in pathfinding efficiency and shows superior robustness among different tasks, validated with public datasets and <b>simulation</b> environments.</p></p class="citation"></blockquote><h3 id=37--65115-aro-large-language-model-supervised-robotics-text2skill-autonomous-learning-yiwen-chen-et-al-2024>(3/7 | 65/115) ARO: Large Language Model Supervised Robotics Text2Skill Autonomous Learning (Yiwen Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwen Chen, Yuyao Ye, Ziyi Chen, Chuheng Zhang, Marcelo H. Ang. (2024)<br><strong>ARO: Large Language Model Supervised Robotics Text2Skill Autonomous Learning</strong><br><button class=copy-to-clipboard title="ARO: Large Language Model Supervised Robotics Text2Skill Autonomous Learning" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Human Intervention, Reinforcement Learning, Supervised Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15834v1.pdf filename=2403.15834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotics learning highly relies on <b>human</b> <b>expertise</b> and efforts, such as demonstrations, design of reward functions in <b>reinforcement</b> <b>learning,</b> performance evaluation using <b>human</b> <b>feedback,</b> etc. However, reliance on <b>human</b> <b>assistance</b> can lead to expensive learning costs and make skill learning difficult to scale. In this work, we introduce the <b>Large</b> <b>Language</b> <b>Model</b> <b>Supervised</b> Robotics Text2Skill Autonomous Learning (ARO) framework, which aims to replace <b>human</b> <b>participation</b> in the robot skill learning process with <b>large-scale</b> <b>language</b> <b>models</b> that incorporate reward function design and performance evaluation. We provide evidence that our approach enables fully autonomous robot skill learning, capable of completing partial tasks without <b>human</b> <b>intervention.</b> Furthermore, we also analyze the limitations of this approach in task understanding and optimization stability.</p></p class="citation"></blockquote><h3 id=47--66115-driveenv-nerf-exploration-of-a-nerf-based-autonomous-driving-environment-for-real-world-performance-validation-mu-yi-shen-et-al-2024>(4/7 | 66/115) DriveEnv-NeRF: Exploration of A NeRF-Based Autonomous Driving Environment for Real-World Performance Validation (Mu-Yi Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mu-Yi Shen, Chia-Chi Hsu, Hao-Yu Hou, Yu-Chen Huang, Wei-Fang Sun, Chia-Che Chang, Yu-Lun Liu, Chun-Yi Lee. (2024)<br><strong>DriveEnv-NeRF: Exploration of A NeRF-Based Autonomous Driving Environment for Real-World Performance Validation</strong><br><button class=copy-to-clipboard title="DriveEnv-NeRF: Exploration of A NeRF-Based Autonomous Driving Environment for Real-World Performance Validation" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15791v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15791v1.pdf filename=2403.15791v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce the DriveEnv-NeRF framework, which leverages Neural Radiance Fields (NeRF) to enable the validation and faithful forecasting of the efficacy of autonomous driving agents in a targeted real-world scene. Standard simulator-based rendering often fails to accurately reflect real-world performance due to the sim-to-real gap, which represents the disparity between virtual <b>simulations</b> and real-world conditions. To mitigate this gap, we propose a workflow for building a high-fidelity <b>simulation</b> environment of the targeted real-world scene using NeRF. This approach is capable of rendering realistic images from novel viewpoints and constructing 3D meshes for emulating collisions. The validation of these capabilities through the comparison of success rates in both simulated and real environments demonstrates the benefits of using DriveEnv-NeRF as a real-world performance indicator. Furthermore, the DriveEnv-NeRF framework can serve as a training environment for autonomous driving agents under various lighting conditions. This approach enhances the robustness of the agents and reduces performance degradation when deployed to the target real scene, compared to agents fully trained using the standard simulator rendering pipeline.</p></p class="citation"></blockquote><h3 id=57--67115-distributed-robust-learning-based-formation-control-of-mobile-robots-based-on-bioinspired-neural-dynamics-zhe-xu-et-al-2024>(5/7 | 67/115) Distributed Robust Learning based Formation Control of Mobile Robots based on Bioinspired Neural Dynamics (Zhe Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhe Xu, Tao Yan, Simon X. Yang, S. Andrew Gadsden, Mohammad Biglarbegian. (2024)<br><strong>Distributed Robust Learning based Formation Control of Mobile Robots based on Bioinspired Neural Dynamics</strong><br><button class=copy-to-clipboard title="Distributed Robust Learning based Formation Control of Mobile Robots based on Bioinspired Neural Dynamics" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15716v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15716v1.pdf filename=2403.15716v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the challenges of distributed formation control in multiple mobile robots, introducing a novel approach that enhances real-world practicability. We first introduce a distributed estimator using a variable structure and cascaded design technique, eliminating the need for derivative information to improve the real time performance. Then, a kinematic tracking control method is developed utilizing a bioinspired neural dynamic-based approach aimed at providing smooth control inputs and effectively resolving the speed jump issue. Furthermore, to address the challenges for robots operating with completely unknown dynamics and disturbances, a learning-based robust dynamic controller is developed. This controller provides real time parameter estimates while maintaining its robustness against disturbances. The overall stability of the proposed method is proved with rigorous mathematical analysis. At last, multiple comprehensive <b>simulation</b> studies have shown the advantages and effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=67--68115-data-driven-predictive-control-for-robust-exoskeleton-locomotion-kejun-li-et-al-2024>(6/7 | 68/115) Data-Driven Predictive Control for Robust Exoskeleton Locomotion (Kejun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kejun Li, Jeeseop Kim, Xiaobin Xiong, Kaveh Akbari Hamed, Yisong Yue, Aaron D. Ames. (2024)<br><strong>Data-Driven Predictive Control for Robust Exoskeleton Locomotion</strong><br><button class=copy-to-clipboard title="Data-Driven Predictive Control for Robust Exoskeleton Locomotion" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15658v1.pdf filename=2403.15658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exoskeleton locomotion must be robust while being adaptive to different users with and without payloads. To address these challenges, this work introduces a data-driven predictive control (DDPC) framework to synthesize walking gaits for lower-body exoskeletons, employing Hankel matrices and a state transition matrix for its data-driven model. The proposed approach leverages DDPC through a multi-layer architecture. At the top layer, DDPC serves as a planner employing Hankel matrices and a state transition matrix to generate a data-driven model that can learn and adapt to varying users and payloads. At the lower layer, our method incorporates inverse kinematics and passivity-based control to map the planned trajectory from DDPC into the full-order states of the lower-body exoskeleton. We validate the effectiveness of this approach through numerical <b>simulations</b> and hardware experiments conducted on the Atalante lower-body exoskeleton with different payloads. Moreover, we conducted a comparative analysis against the model predictive control (MPC) framework based on the reduced-order linear inverted pendulum (LIP) model. Through this comparison, the paper demonstrates that DDPC enables robust bipedal walking at various velocities while accounting for model uncertainties and unknown perturbations.</p></p class="citation"></blockquote><h3 id=77--69115-risk-calibrated-human-robot-interaction-via-set-valued-intent-prediction-justin-lidard-et-al-2024>(7/7 | 69/115) Risk-Calibrated Human-Robot Interaction via Set-Valued Intent Prediction (Justin Lidard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Justin Lidard, Hang Pham, Ariel Bachman, Bryan Boateng, Anirudha Majumdar. (2024)<br><strong>Risk-Calibrated Human-Robot Interaction via Set-Valued Intent Prediction</strong><br><button class=copy-to-clipboard title="Risk-Calibrated Human-Robot Interaction via Set-Valued Intent Prediction" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY, math-OC<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15959v1.pdf filename=2403.15959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tasks where robots must cooperate with humans, such as navigating around a cluttered home or sorting everyday items, are challenging because they exhibit a wide range of valid actions that lead to similar outcomes. Moreover, <b>zero-shot</b> cooperation between human-robot partners is an especially challenging problem because it requires the robot to infer and adapt on the fly to a latent human intent, which could vary significantly from human to human. Recently, deep learned motion prediction models have shown promising results in predicting human intent but are prone to being confidently incorrect. In this work, we present Risk-Calibrated Interactive Planning (RCIP), which is a framework for measuring and calibrating risk associated with uncertain action selection in human-robot cooperation, with the fundamental idea that the robot should ask for human clarification when the risk associated with the uncertainty in the human&rsquo;s intent cannot be controlled. RCIP builds on the theory of set-valued risk calibration to provide a finite-sample statistical guarantee on the cumulative loss incurred by the robot while minimizing the cost of human clarification in complex multi-step settings. Our main insight is to frame the risk control problem as a sequence-level multi-hypothesis testing problem, allowing efficient calibration using a low-dimensional parameter that controls a pre-trained risk-aware policy. Experiments across a variety of simulated and real-world environments demonstrate RCIP&rsquo;s ability to predict and adapt to a diverse set of dynamic human intents.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--70115-spatio-temporal-graph-convolutional-network-combined-large-language-model-a-deep-learning-framework-for-bike-demand-forecasting-peisen-li-et-al-2024>(1/2 | 70/115) Spatio-Temporal Graph Convolutional Network Combined Large Language Model: A Deep Learning Framework for Bike Demand Forecasting (Peisen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peisen Li, Yizhe Pang, Junyu Ren. (2024)<br><strong>Spatio-Temporal Graph Convolutional Network Combined Large Language Model: A Deep Learning Framework for Bike Demand Forecasting</strong><br><button class=copy-to-clipboard title="Spatio-Temporal Graph Convolutional Network Combined Large Language Model: A Deep Learning Framework for Bike Demand Forecasting" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-CY, cs-SI, cs.SI<br>Keyword Score: 53<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15733v1.pdf filename=2403.15733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents a new deep learning framework, combining Spatio-Temporal <b>Graph</b> <b>Convolutional</b> <b>Network</b> (STGCN) with a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM),</b> for bike demand forecasting. Addressing challenges in transforming discrete datasets and integrating unstructured language data, the framework leverages <b>LLMs</b> to extract insights from Points of Interest (POI) text data. The proposed STGCN-L model demonstrates competitive performance compared to existing models, showcasing its potential in predicting bike demand. Experiments using Philadelphia datasets highlight the effectiveness of the hybrid model, emphasizing the need for further exploration and enhancements, such as incorporating additional features like weather data for improved accuracy.</p></p class="citation"></blockquote><h3 id=22--71115-model-analyze-and-comprehend-user-interactions-and-various-attributes-within-a-social-media-platform-md-kaykobad-reza-et-al-2024>(2/2 | 71/115) Model, Analyze, and Comprehend User Interactions and Various Attributes within a Social Media Platform (Md Kaykobad Reza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Kaykobad Reza, S M Maksudul Alam, Yiran Luo, Youzhe Liu. (2024)<br><strong>Model, Analyze, and Comprehend User Interactions and Various Attributes within a Social Media Platform</strong><br><button class=copy-to-clipboard title="Model, Analyze, and Comprehend User Interactions and Various Attributes within a Social Media Platform" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-IR, cs-SI, cs.SI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15937v1.pdf filename=2403.15937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How can we effectively model, analyze, and comprehend user interactions and various attributes within a social media platform based on post-comment relationship? In this study, we propose a novel <b>graph-based</b> approach to model and analyze user interactions within a social media platform based on post-comment relationship. We construct a user interaction <b>graph</b> from social media data and analyze it to gain insights into community dynamics, user behavior, and content preferences. Our investigation reveals that while 56.05% of the active users are strongly connected within the community, only 0.8% of them significantly contribute to its dynamics. Moreover, we observe temporal variations in community activity, with certain periods experiencing heightened engagement. Additionally, our findings highlight a correlation between user activity and popularity showing that more active users are generally more popular. Alongside these, a preference for positive and informative content is also observed where 82.41% users preferred positive and informative content. Overall, our study provides a comprehensive framework for understanding and managing online communities, leveraging <b>graph-based</b> techniques to gain valuable insights into user behavior and community dynamics.</p></p class="citation"></blockquote><h2 id=cslg-13>cs.LG (13)</h2><h3 id=113--72115-safe-reinforcement-learning-for-constrained-markov-decision-processes-with-stochastic-stopping-time-abhijit-mazumdar-et-al-2024>(1/13 | 72/115) Safe Reinforcement Learning for Constrained Markov Decision Processes with Stochastic Stopping Time (Abhijit Mazumdar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhijit Mazumdar, Rafal Wisniewski, Manuela L. Bujorianu. (2024)<br><strong>Safe Reinforcement Learning for Constrained Markov Decision Processes with Stochastic Stopping Time</strong><br><button class=copy-to-clipboard title="Safe Reinforcement Learning for Constrained Markov Decision Processes with Stochastic Stopping Time" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 40<br>Keywords: Online Reinforcement Learning, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15928v1.pdf filename=2403.15928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present an <b>online</b> <b>reinforcement</b> <b>learning</b> algorithm for constrained Markov decision processes with a safety constraint. Despite the necessary attention of the scientific community, considering stochastic stopping time, the problem of learning optimal policy without violating safety constraints during the learning phase is yet to be addressed. To this end, we propose an algorithm based on linear programming that does not require a process model. We show that the learned policy is safe with high confidence. We also propose a method to compute a safe baseline policy, which is central in developing algorithms that do not violate the safety constraints. Finally, we provide <b>simulation</b> results to show the efficacy of the proposed algorithm. Further, we demonstrate that efficient exploration can be achieved by defining a subset of the state-space called proxy set.</p></p class="citation"></blockquote><h3 id=213--73115-tablepuppet-a-generic-framework-for-relational-federated-learning-lijie-xu-et-al-2024>(2/13 | 73/115) TablePuppet: A Generic Framework for Relational Federated Learning (Lijie Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lijie Xu, Chulin Xie, Yiran Guo, Gustavo Alonso, Bo Li, Guoliang Li, Wei Wang, Wentao Wu, Ce Zhang. (2024)<br><strong>TablePuppet: A Generic Framework for Relational Federated Learning</strong><br><button class=copy-to-clipboard title="TablePuppet: A Generic Framework for Relational Federated Learning" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DB, cs-DC, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Federated Learning, Stochastic Gradient Descent, Stochastic Gradient Descent, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15839v1.pdf filename=2403.15839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>federated</b> <b>learning</b> (FL) approaches view decentralized training data as a single table, divided among participants either horizontally (by rows) or vertically (by columns). However, these approaches are inadequate for handling distributed relational tables across databases. This scenario requires intricate SQL operations like joins and unions to obtain the training data, which is either costly or restricted by privacy concerns. This raises the question: can we directly run FL on distributed relational tables? In this paper, we formalize this problem as relational <b>federated</b> <b>learning</b> (RFL). We propose TablePuppet, a generic framework for RFL that decomposes the learning process into two steps: (1) learning over join (LoJ) followed by (2) learning over union (LoU). In a nutshell, LoJ pushes learning down onto the vertical tables being joined, and LoU further pushes learning down onto the horizontal partitions of each vertical table. TablePuppet incorporates computation/communication optimizations to deal with the duplicate tuples introduced by joins, as well as <b>differential</b> <b>privacy</b> (DP) to protect against both feature and label leakages. We demonstrate the efficiency of TablePuppet in combination with two widely-used ML training algorithms, <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> and alternating direction method of multipliers (ADMM), and compare their computation/communication complexity. We evaluate the SGD/ADMM algorithms developed atop TablePuppet by training diverse ML models. Our experimental results show that TablePuppet achieves model accuracy comparable to the centralized baselines running directly atop the SQL results. Moreover, ADMM takes less communication time than <b>SGD</b> to converge to similar model accuracy.</p></p class="citation"></blockquote><h3 id=313--74115-boarding-for-iss-imbalanced-self-supervised-discovery-of-a-scaled-autoencoder-for-mixed-tabular-datasets-samuel-stocksieker-et-al-2024>(3/13 | 74/115) Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled Autoencoder for Mixed Tabular Datasets (Samuel Stocksieker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Stocksieker, Denys Pommeret, Arthur Charpentier. (2024)<br><strong>Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled Autoencoder for Mixed Tabular Datasets</strong><br><button class=copy-to-clipboard title="Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled Autoencoder for Mixed Tabular Datasets" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Autoencoder, Self-supervised Learning, Self-supervised Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15790v1.pdf filename=2403.15790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of imbalanced <b>self-supervised</b> <b>learning,</b> especially in the context of tabular data, has not been extensively studied. Existing research has predominantly focused on image datasets. This paper aims to fill this gap by examining the specific challenges posed by data imbalance in <b>self-supervised</b> <b>learning</b> in the domain of tabular data, with a primary focus on <b>autoencoders.</b> <b>Autoencoders</b> are widely employed for learning and constructing a new representation of a dataset, particularly for dimensionality reduction. They are also often used for generative model learning, as seen in <b>variational</b> <b>autoencoders.</b> When dealing with mixed tabular data, qualitative variables are often encoded using a one-hot encoder with a standard loss function (MSE or Cross Entropy). In this paper, we analyze the drawbacks of this approach, especially when categorical variables are imbalanced. We propose a novel metric to balance learning: a Multi-Supervised Balanced MSE. This approach reduces the reconstruction error by balancing the influence of variables. Finally, we empirically demonstrate that this new metric, compared to the standard MSE: i) outperforms when the dataset is imbalanced, especially when the learning process is insufficient, and ii) provides similar results in the opposite case.</p></p class="citation"></blockquote><h3 id=413--75115-role-of-locality-and-weight-sharing-in-image-based-tasks-a-sample-complexity-separation-between-cnns-lcns-and-fcns-aakash-lahoti-et-al-2024>(4/13 | 75/115) Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs (Aakash Lahoti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aakash Lahoti, Stefani Karp, Ezra Winston, Aarti Singh, Yuanzhi Li. (2024)<br><strong>Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs</strong><br><button class=copy-to-clipboard title="Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15707v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15707v1.pdf filename=2403.15707v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vision tasks are characterized by the properties of locality and translation invariance. The superior performance of <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> on these tasks is widely attributed to the inductive bias of locality and weight sharing baked into their architecture. Existing attempts to quantify the statistical benefits of these biases in <b>CNNs</b> over locally connected <b>convolutional</b> <b>neural</b> <b>networks</b> (LCNs) and fully connected neural networks (FCNs) fall into one of the following categories: either they disregard the optimizer and only provide uniform convergence upper bounds with no separating lower bounds, or they consider simplistic tasks that do not truly mirror the locality and translation invariance as found in real-world vision tasks. To address these deficiencies, we introduce the Dynamic Signal Distribution (DSD) classification task that models an image as consisting of $k$ patches, each of dimension $d$, and the label is determined by a $d$-sparse signal vector that can freely appear in any one of the $k$ patches. On this task, for any orthogonally equivariant algorithm like gradient descent, we prove that <b>CNNs</b> require $\tilde{O}(k+d)$ samples, whereas LCNs require $\Omega(kd)$ samples, establishing the statistical advantages of weight sharing in translation invariant tasks. Furthermore, LCNs need $\tilde{O}(k(k+d))$ samples, compared to $\Omega(k^2d)$ samples for FCNs, showcasing the benefits of locality in local tasks. Additionally, we develop information theoretic tools for analyzing randomized algorithms, which may be of interest for statistical research.</p></p class="citation"></blockquote><h3 id=513--76115-deep-gaussian-covariance-network-with-trajectory-sampling-for-data-efficient-policy-search-can-bogoclu-et-al-2024>(5/13 | 76/115) Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search (Can Bogoclu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Can Bogoclu, Robert Vosshall, Kevin Cremanns, Dirk Roos. (2024)<br><strong>Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search</strong><br><button class=copy-to-clipboard title="Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Probabilistic Model, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15908v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15908v1.pdf filename=2403.15908v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Probabilistic</b> <b>world</b> models increase data efficiency of model-based <b>reinforcement</b> <b>learning</b> (MBRL) by guiding the policy with their epistemic uncertainty to improve exploration and acquire new samples. Moreover, the uncertainty-aware learning procedures in <b>probabilistic</b> <b>approaches</b> lead to robust policies that are less sensitive to noisy observations compared to uncertainty unaware solutions. We propose to combine trajectory sampling and deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL problems in an optimal control setting. We compare trajectory sampling with density-based approximation for uncertainty propagation using three different <b>probabilistic</b> <b>world</b> models; Gaussian processes, Bayesian neural networks, and DGCNs. We provide empirical evidence using four different well-known test environments, that our method improves the sample-efficiency over other combinations of uncertainty propagation methods and <b>probabilistic</b> <b>models.</b> During our tests, we place particular emphasis on the robustness of the learned policies with respect to noisy initial states.</p></p class="citation"></blockquote><h3 id=613--77115-towards-low-energy-adaptive-personalization-for-resource-constrained-devices-yushan-huang-et-al-2024>(6/13 | 77/115) Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices (Yushan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yushan Huang, Josh Millar, Yuxuan Long, Yuchen Zhao, Hamed Hadaddi. (2024)<br><strong>Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices</strong><br><button class=copy-to-clipboard title="Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15905v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15905v2.pdf filename=2403.15905v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The personalization of machine learning (ML) models to address data drift is a significant challenge in the context of Internet of Things (IoT) applications. Presently, most approaches focus on <b>fine-tuning</b> either the full base model or its last few layers to adapt to new data, while often neglecting energy costs. However, various types of data drift exist, and <b>fine-tuning</b> the full base model or the last few layers may not result in optimal performance in certain scenarios. We propose Target Block <b>Fine-Tuning</b> (TBFT), a low-energy adaptive personalization framework designed for resource-constrained devices. We categorize data drift and personalization into three types: input-level, feature-level, and output-level. For each type, we <b>fine-tune</b> different blocks of the model to achieve optimal performance with reduced energy costs. Specifically, input-, feature-, and output-level correspond to <b>fine-tuning</b> the front, middle, and rear blocks of the model. We evaluate TBFT on a ResNet model, three datasets, three different training sizes, and a Raspberry Pi. Compared with the $Block Avg$, where each block is <b>fine-tuned</b> individually and their performance improvements are averaged, TBFT exhibits an improvement in model accuracy by an average of 15.30% whilst saving 41.57% energy consumption on average compared with full <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=713--78115-bend-bagging-deep-learning-training-based-on-efficient-neural-network-diffusion-jia-wei-et-al-2024>(7/13 | 78/115) BEND: Bagging Deep Learning Training Based on Efficient Neural Network Diffusion (Jia Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jia Wei, Xingjun Zhang, Witold Pedrycz. (2024)<br><strong>BEND: Bagging Deep Learning Training Based on Efficient Neural Network Diffusion</strong><br><button class=copy-to-clipboard title="BEND: Bagging Deep Learning Training Based on Efficient Neural Network Diffusion" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Diffusion Model, Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15766v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15766v1.pdf filename=2403.15766v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bagging has achieved great success in the field of machine learning by integrating multiple base classifiers to build a single strong classifier to reduce model variance. The performance improvement of bagging mainly relies on the number and diversity of base classifiers. However, traditional deep learning model training methods are expensive to train individually and difficult to train multiple models with low similarity in a restricted dataset. Recently, <b>diffusion</b> <b>models,</b> which have been tremendously successful in the fields of imaging and vision, have been found to be effective in generating neural network model weights and biases with diversity. We creatively propose a Bagging deep learning training algorithm based on Efficient Neural network <b>Diffusion</b> <b>(BEND).</b> The originality of BEND comes from the first use of a neural network <b>diffusion</b> <b>model</b> to efficiently build base classifiers for bagging. Our approach is simple but effective, first using multiple trained model weights and biases as inputs to train <b>autoencoder</b> and latent <b>diffusion</b> <b>model</b> to realize a <b>diffusion</b> <b>model</b> from noise to valid neural network parameters. Subsequently, we generate several base classifiers using the trained <b>diffusion</b> <b>model.</b> Finally, we integrate these ba se classifiers for various inference tasks using the Bagging method. Resulting experiments on multiple models and datasets show that our proposed BEND algorithm can consistently outperform the mean and median accuracies of both the original trained model and the diffused model. At the same time, new models diffused using the <b>diffusion</b> <b>model</b> have higher diversity and lower cost than multiple models trained using traditional methods. The BEND approach successfully introduces <b>diffusion</b> <b>models</b> into the new deep learning training domain and provides a new paradigm for future deep learning training and inference.</p></p class="citation"></blockquote><h3 id=813--79115-on-the-fragility-of-active-learners-abhishek-ghose-et-al-2024>(8/13 | 79/115) On the Fragility of Active Learners (Abhishek Ghose et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Ghose, Emma Nguyen. (2024)<br><strong>On the Fragility of Active Learners</strong><br><button class=copy-to-clipboard title="On the Fragility of Active Learners" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Active Learning, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15744v1.pdf filename=2403.15744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Active</b> <b>learning</b> (AL) techniques aim to maximally utilize a labeling budget by iteratively selecting instances that are most likely to improve prediction accuracy. However, their benefit compared to random sampling has not been consistent across various setups, e.g., different datasets, classifiers. In this empirical study, we examine how a combination of different factors might obscure any gains from an AL technique. Focusing on <b>text</b> <b>classification,</b> we rigorously evaluate AL techniques over around 1000 experiments that vary wrt the dataset, batch size, <b>text</b> <b>representation</b> and the classifier. We show that AL is only effective in a narrow set of circumstances. We also address the problem of using metrics that are better aligned with real world expectations. The impact of this study is in its insights for a practitioner: (a) the choice of <b>text</b> <b>representation</b> and classifier is as important as that of an AL technique, (b) choice of the right metric is critical in assessment of the latter, and, finally, (c) reported AL results must be holistically interpreted, accounting for variables other than just the query strategy.</p></p class="citation"></blockquote><h3 id=913--80115-identifiable-latent-neural-causal-models-yuhang-liu-et-al-2024>(9/13 | 80/115) Identifiable Latent Neural Causal Models (Yuhang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun Zhang, Javen Qinfeng Shi. (2024)<br><strong>Identifiable Latent Neural Causal Models</strong><br><button class=copy-to-clipboard title="Identifiable Latent Neural Causal Models" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ME, stat-ML<br>Keyword Score: 15<br>Keywords: Distribution Shift, Distribution Shift, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15711v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15711v1.pdf filename=2403.15711v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causal <b>representation</b> <b>learning</b> seeks to uncover latent, high-level causal <b>representations</b> <b>from</b> low-level observed data. It is particularly good at predictions under unseen <b>distribution</b> <b>shifts,</b> because these shifts can generally be interpreted as consequences of interventions. Hence leveraging {seen} <b>distribution</b> <b>shifts</b> becomes a natural strategy to help identifying causal <b>representations,</b> <b>which</b> in turn benefits predictions where <b>distributions</b> <b>are</b> previously {unseen}. Determining the types (or conditions) of such <b>distribution</b> <b>shifts</b> that do contribute to the identifiability of causal <b>representations</b> <b>is</b> critical. This work establishes a {sufficient} and {necessary} condition characterizing the types of <b>distribution</b> <b>shifts</b> for identifiability in the context of latent additive noise models. Furthermore, we present partial identifiability results when only a portion of <b>distribution</b> <b>shifts</b> meets the condition. In addition, we extend our findings to latent post-nonlinear causal models. We translate our findings into a practical algorithm, allowing for the acquisition of reliable latent causal <b>representations.</b> <b>Our</b> algorithm, guided by our underlying theory, has demonstrated outstanding performance across a diverse range of synthetic and real-world datasets. The empirical observations align closely with the theoretical findings, affirming the robustness and effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=1013--81115-sample-and-communication-efficient-fully-decentralized-marl-policy-evaluation-via-a-new-approach-local-td-update-fnu-hairi-et-al-2024>(10/13 | 81/115) Sample and Communication Efficient Fully Decentralized MARL Policy Evaluation via a New Approach: Local TD update (Fnu Hairi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fnu Hairi, Zifan Zhang, Jia Liu. (2024)<br><strong>Sample and Communication Efficient Fully Decentralized MARL Policy Evaluation via a New Approach: Local TD update</strong><br><button class=copy-to-clipboard title="Sample and Communication Efficient Fully Decentralized MARL Policy Evaluation via a New Approach: Local TD update" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-MA, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15935v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15935v1.pdf filename=2403.15935v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In actor-critic framework for fully decentralized multi-agent <b>reinforcement</b> <b>learning</b> (MARL), one of the key components is the MARL policy evaluation (PE) problem, where a set of $N$ agents work cooperatively to evaluate the value function of the global states for a given policy through communicating with their neighbors. In MARL-PE, a critical challenge is how to lower the sample and communication complexities, which are defined as the number of training samples and communication rounds needed to converge to some $\epsilon$-stationary point. To lower communication complexity in MARL-PE, a &ldquo;natural&rsquo;&rsquo; idea is to perform multiple local TD-update steps between each consecutive rounds of communication to reduce the communication frequency. However, the validity of the local TD-update approach remains unclear due to the potential &ldquo;agent-drift&rsquo;&rsquo; phenomenon resulting from heterogeneous rewards across agents in general. This leads to an interesting open question: Can the local TD-update approach entail low sample and communication complexities? In this paper, we make the first attempt to answer this fundamental question. We focus on the setting of MARL-PE with average reward, which is motivated by many multi-agent network optimization problems. Our theoretical and experimental results confirm that allowing multiple local TD-update steps is indeed an effective approach in lowering the sample and communication complexities of MARL-PE compared to consensus-based MARL-PE algorithms. Specifically, the local TD-update steps between two consecutive communication rounds can be as large as $\mathcal{O}(1/\epsilon^{1/2}\log{(1/\epsilon)})$ in order to converge to an $\epsilon$-stationary point of MARL-PE. Moreover, we show theoretically that in order to reach the optimal sample complexity, the communication complexity of local TD-update approach is $\mathcal{O}(1/\epsilon^{1/2}\log{(1/\epsilon)})$.</p></p class="citation"></blockquote><h3 id=1113--82115-initialisation-and-topology-effects-in-decentralised-federated-learning-arash-badie-modiri-et-al-2024>(11/13 | 82/115) Initialisation and Topology Effects in Decentralised Federated Learning (Arash Badie-Modiri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arash Badie-Modiri, Chiara Boldrini, Lorenzo Valerio, János Kertész, Márton Karsai. (2024)<br><strong>Initialisation and Topology Effects in Decentralised Federated Learning</strong><br><button class=copy-to-clipboard title="Initialisation and Topology Effects in Decentralised Federated Learning" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG, physics-soc-ph<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15855v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15855v1.pdf filename=2403.15855v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fully decentralised <b>federated</b> <b>learning</b> enables collaborative training of individual machine learning models on distributed devices on a network while keeping the training data localised. This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination. Our research highlights that the effectiveness of decentralised <b>federated</b> <b>learning</b> is significantly influenced by the network topology of connected devices. A simplified numerical model for studying the early behaviour of these systems leads us to an improved artificial neural network initialisation strategy, which leverages the distribution of eigenvector centralities of the nodes of the underlying network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy. This work paves the way for more efficient and scalable artificial neural network training in a distributed and uncoordinated environment, offering a deeper understanding of the intertwining roles of network structure and learning dynamics.</p></p class="citation"></blockquote><h3 id=1213--83115-convection-diffusion-equation-a-theoretically-certified-framework-for-neural-networks-tangjun-wang-et-al-2024>(12/13 | 83/115) Convection-Diffusion Equation: A Theoretically Certified Framework for Neural Networks (Tangjun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tangjun Wang, Chenglong Bao, Zuoqiang Shi. (2024)<br><strong>Convection-Diffusion Equation: A Theoretically Certified Framework for Neural Networks</strong><br><button class=copy-to-clipboard title="Convection-Diffusion Equation: A Theoretically Certified Framework for Neural Networks" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15726v1.pdf filename=2403.15726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study the partial differential equation models of neural networks. Neural network can be viewed as a map from a simple base model to a complicate function. Based on solid analysis, we show that this map can be formulated by a convection-diffusion equation. This theoretically certified framework gives mathematical foundation and more understanding of neural networks. Moreover, based on the convection-diffusion equation model, we design a novel network structure, which incorporates diffusion mechanism into network architecture. Extensive experiments on both <b>benchmark</b> datasets and real-world applications validate the performance of the proposed model.</p></p class="citation"></blockquote><h3 id=1313--84115-g-acil-analytic-learning-for-exemplar-free-generalized-class-incremental-learning-huiping-zhuang-et-al-2024>(13/13 | 84/115) G-ACIL: Analytic Learning for Exemplar-Free Generalized Class Incremental Learning (Huiping Zhuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huiping Zhuang, Yizhu Chen, Di Fang, Run He, Kai Tong, Hongxin Wei, Ziqian Zeng, Cen Chen. (2024)<br><strong>G-ACIL: Analytic Learning for Exemplar-Free Generalized Class Incremental Learning</strong><br><button class=copy-to-clipboard title="G-ACIL: Analytic Learning for Exemplar-Free Generalized Class Incremental Learning" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15706v1.pdf filename=2403.15706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Class incremental learning (CIL) trains a network on sequential tasks with separated categories but suffers from catastrophic forgetting, where models quickly lose previously learned knowledge when acquiring new tasks. The generalized CIL (GCIL) aims to address the CIL problem in a more real-world scenario, where incoming data have mixed data categories and unknown <b>sample</b> <b>size</b> distribution, leading to intensified forgetting. Existing attempts for the GCIL either have poor performance, or invade data privacy by saving historical exemplars. To address this, in this paper, we propose an exemplar-free generalized analytic class incremental learning (G-ACIL). The G-ACIL adopts analytic learning (a gradient-free training technique), and delivers an analytical solution (i.e., closed-form) to the GCIL scenario. This solution is derived via decomposing the incoming data into exposed and unexposed classes, allowing an equivalence between the incremental learning and its joint training, i.e., the weight-invariant property. Such an equivalence is theoretically validated through matrix analysis tools, and hence contributes interpretability in GCIL. It is also empirically evidenced by experiments on various datasets and settings of GCIL. The results show that the G-ACIL exhibits leading performance with high robustness compared with existing competitive GCIL methods. Codes will be ready at <a href=https://github.com/ZHUANGHP/Analytic-continual-learning>https://github.com/ZHUANGHP/Analytic-continual-learning</a>.</p></p class="citation"></blockquote><h2 id=eesssy-10>eess.SY (10)</h2><h3 id=110--85115-scaling-learning-based-policy-optimization-for-temporal-tasks-via-dropout-navid-hashemi-et-al-2024>(1/10 | 85/115) Scaling Learning based Policy Optimization for Temporal Tasks via Dropout (Navid Hashemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Navid Hashemi, Bardh Hoxha, Danil Prokhorov, Georgios Fainekos, Jyotirmoy Deshmukh. (2024)<br><strong>Scaling Learning based Policy Optimization for Temporal Tasks via Dropout</strong><br><button class=copy-to-clipboard title="Scaling Learning based Policy Optimization for Temporal Tasks via Dropout" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-LG, cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 40<br>Keywords: Discrete Time, Discrete Time, Stochastic Gradient Descent, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15826v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15826v1.pdf filename=2403.15826v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a model-based approach for training feedback controllers for an autonomous agent operating in a highly nonlinear environment. We desire the trained policy to ensure that the agent satisfies specific task objectives, expressed in <b>discrete-time</b> <b>Signal</b> Temporal Logic (DT-STL). One advantage for reformulation of a task via formal frameworks, like DT-STL, is that it permits quantitative satisfaction semantics. In other words, given a trajectory and a DT-STL formula, we can compute the robustness, which can be interpreted as an approximate signed distance between the trajectory and the set of trajectories satisfying the formula. We utilize feedback controllers, and we assume a feed forward neural network for learning these feedback controllers. We show how this learning problem is similar to training <b>recurrent</b> <b>neural</b> <b>networks</b> <b>(RNNs),</b> where the number of <b>recurrent</b> <b>units</b> <b>is</b> proportional to the temporal horizon of the agent&rsquo;s task objectives. This poses a challenge: <b>RNNs</b> are susceptible to vanishing and exploding gradients, and na"{i}ve gradient descent-based strategies to solve long-horizon task objectives thus suffer from the same problems. To tackle this challenge, we introduce a novel gradient approximation algorithm based on the idea of dropout or gradient sampling. We show that, the existing smooth semantics for robustness are inefficient regarding gradient computation when the specification becomes complex. To address this challenge, we propose a new smooth semantics for DT-STL that under-approximates the robustness value and scales well for backpropagation over a complex specification. We show that our control synthesis methodology, can be quite helpful for <b>stochastic</b> <b>gradient</b> <b>descent</b> to converge with less numerical issues, enabling scalable backpropagation over long time horizons and trajectories over high dimensional state spaces.</p></p class="citation"></blockquote><h3 id=210--86115-a-fairness-oriented-reinforcement-learning-approach-for-the-operation-and-control-of-shared-micromobility-services-luca-vittorio-piron-et-al-2024>(2/10 | 86/115) A Fairness-Oriented Reinforcement Learning Approach for the Operation and Control of Shared Micromobility Services (Luca Vittorio Piron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Vittorio Piron, Matteo Cederle, Marina Ceccon, Federico Chiariotti, Alessandro Fabris, Marco Fabris, Gian Antonio Susto. (2024)<br><strong>A Fairness-Oriented Reinforcement Learning Approach for the Operation and Control of Shared Micromobility Services</strong><br><button class=copy-to-clipboard title="A Fairness-Oriented Reinforcement Learning Approach for the Operation and Control of Shared Micromobility Services" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-CY, cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 40<br>Keywords: Fairness, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15780v1.pdf filename=2403.15780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As Machine Learning systems become increasingly popular across diverse application domains, including those with direct human implications, the imperative of equity and algorithmic <b>fairness</b> has risen to prominence in the Artificial Intelligence community. On the other hand, in the context of Shared Micromobility Systems, the exploration of <b>fairness-oriented</b> approaches remains limited. Addressing this gap, we introduce a pioneering investigation into the balance between performance optimization and algorithmic <b>fairness</b> in the operation and control of Shared Micromobility Services. Our study leverages the Q-Learning algorithm in <b>Reinforcement</b> <b>Learning,</b> benefiting from its convergence guarantees to ensure the robustness of our proposed approach. Notably, our methodology stands out for its ability to achieve equitable outcomes, as measured by the Gini index, across different station categories&ndash;central, peripheral, and remote. Through strategic rebalancing of vehicle distribution, our approach aims to maximize operator performance while simultaneously upholding <b>fairness</b> principles for users. In addition to theoretical insights, we substantiate our findings with a case study or <b>simulation</b> based on synthetic data, validating the efficacy of our approach. This paper underscores the critical importance of <b>fairness</b> considerations in shaping control strategies for Shared Micromobility Services, offering a pragmatic framework for enhancing equity in urban transportation systems.</p></p class="citation"></blockquote><h3 id=310--87115-improved-soft-k-means-clustering-algorithm-for-balancing-energy-consumption-in-wireless-sensor-networks-botao-zhu-et-al-2024>(3/10 | 87/115) Improved Soft-k-Means Clustering Algorithm for Balancing Energy Consumption in Wireless Sensor Networks (Botao Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Botao Zhu, Ebrahim Bedeer, Ha H. Nguyen, Robert Barton, Jerome Henry. (2024)<br><strong>Improved Soft-k-Means Clustering Algorithm for Balancing Energy Consumption in Wireless Sensor Networks</strong><br><button class=copy-to-clipboard title="Improved Soft-k-Means Clustering Algorithm for Balancing Energy Consumption in Wireless Sensor Networks" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Clustering, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15700v1.pdf filename=2403.15700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Energy load balancing is an essential issue in designing wireless sensor networks (WSNs). <b>Clustering</b> techniques are utilized as energy-efficient methods to balance the network energy and prolong its lifetime. In this paper, we propose an improved soft-k-means (IS-k-means) <b>clustering</b> algorithm to balance the energy consumption of nodes in WSNs. First, we use the idea of ``clustering by fast search and find of density peaks&rsquo;&rsquo; (CFSFDP) and kernel density estimation (KDE) to improve the selection of the initial cluster centers of the soft k-means <b>clustering</b> algorithm. Then, we utilize the flexibility of the soft-k-means and reassign member nodes considering their membership probabilities at the boundary of clusters to balance the number of nodes per cluster. Furthermore, the concept of multi-cluster heads is employed to balance the energy consumption within clusters. {Extensive <b>simulation</b> results under different network scenarios demonstrate that for small-scale WSNs with single-hop transmission}, the proposed algorithm can postpone the first node death, the half of nodes death, and the last node death on average when compared to various <b>clustering</b> algorithms from the literature.</p></p class="citation"></blockquote><h3 id=410--88115-from-raw-data-to-safety-reducing-conservatism-by-set-expansion-mohammad-bajelani-et-al-2024>(4/10 | 88/115) From Raw Data to Safety: Reducing Conservatism by Set Expansion (Mohammad Bajelani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Bajelani, Klaske van Heusden. (2024)<br><strong>From Raw Data to Safety: Reducing Conservatism by Set Expansion</strong><br><button class=copy-to-clipboard title="From Raw Data to Safety: Reducing Conservatism by Set Expansion" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15883v1.pdf filename=2403.15883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In response to safety concerns associated with learning-based algorithms, safety filters have been proposed as a modular technique. Generally, these filters heavily rely on the system&rsquo;s model, which is contradictory if they are intended to enhance a data-driven or end-to-end learning solution. This paper extends our previous work, a purely Data-Driven Safety Filter (DDSF) based on Willems&rsquo; lemma, to an extremely short-sighted and non-conservative solution. Specifically, we propose online and offline sample-based methods to expand the safe set of DDSF and reduce its conservatism. Since this method is defined in an input-output framework, it can systematically handle both unknown and time-delay LTI systems using only one single batch of data. To evaluate its performance, we apply the proposed method to a time-delay system under various settings. The <b>simulation</b> results validate the effectiveness of the set expansion algorithm in generating a notably large input-output safe set, resulting in safety filters that are not conservative, even with an extremely short prediction horizon.</p></p class="citation"></blockquote><h3 id=510--89115-a-modular-safety-filter-for-safety-certified-cyber-physical-systems-mohammad-bajelani-et-al-2024>(5/10 | 89/115) A Modular Safety Filter for Safety-Certified Cyber-Physical Systems (Mohammad Bajelani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Bajelani, Walter Lucia, Klaske van Heusden. (2024)<br><strong>A Modular Safety Filter for Safety-Certified Cyber-Physical Systems</strong><br><button class=copy-to-clipboard title="A Modular Safety Filter for Safety-Certified Cyber-Physical Systems" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15854v1.pdf filename=2403.15854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays, many control systems are networked and embed communication and computation capabilities. Such control architectures are prone to cyber attacks on the cyberinfrastructure. Consequently, there is an impellent need to develop solutions to preserve the plant&rsquo;s safety against potential attacks. To ensure safety, this paper introduces a modular safety filter approach that is effective for a variety of cyber-attack types. This solution can be implemented in combination with existing control and detection algorithms, effectively separating safety from performance. The safety filter does not require information on the reliability of the received command or the feature of the used anomaly detector. It can be implemented in conjunction with high-performance, resilient controllers, to achieve both high performance during normal operation and safety during an attack. As an illustrative example, we have shown the effectiveness of the proposed design considering a multi-agent formation task involving 20 mobile robots. The <b>simulation</b> results testify that the safety filter operates effectively during false data injection and intelligent attacks.</p></p class="citation"></blockquote><h3 id=610--90115-tjcct-a-two-timescale-approach-for-uav-assisted-mobile-edge-computing-zemin-sun-et-al-2024>(6/10 | 90/115) TJCCT: A Two-timescale Approach for UAV-assisted Mobile Edge Computing (Zemin Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zemin Sun, Geng Sun, Qingqing Wu, Long He, Shuang Liang, Hongyang Pan, Dusit Niyato, Chau Yuen, Victor C. M. Leung. (2024)<br><strong>TJCCT: A Two-timescale Approach for UAV-assisted Mobile Edge Computing</strong><br><button class=copy-to-clipboard title="TJCCT: A Two-timescale Approach for UAV-assisted Mobile Edge Computing" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15828v1.pdf filename=2403.15828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) is emerging as a promising paradigm to provide aerial-terrestrial computing services in close proximity to mobile devices (MDs). However, meeting the demands of computation-intensive and delay-sensitive tasks for MDs poses several challenges, including the demand-supply contradiction between MDs and MEC servers, the demand-supply heterogeneity between MDs and MEC servers, the trajectory control requirements on energy efficiency and timeliness, and the different time-scale dynamics of the network. To address these issues, we first present a hierarchical architecture by incorporating terrestrial-aerial computing capabilities and leveraging UAV flexibility. Furthermore, we formulate a joint computing resource allocation, computation offloading, and trajectory control problem to maximize the system utility. Since the problem is a non-convex and NP-hard mixed integer nonlinear programming (MINLP), we propose a two-timescale joint computing resource allocation, computation offloading, and trajectory control (TJCCT) approach for solving the problem. In the short timescale, we propose a price-incentive model for on-demand computing resource allocation and a matching mechanism-based method for computation offloading. In the long timescale, we propose a convex optimization-based method for UAV trajectory control. Besides, we theoretically prove the stability, optimality, and polynomial complexity of TJCCT. Extended <b>simulation</b> results demonstrate that the proposed TJCCT outperforms the comparative algorithms in terms of the system utility, average processing rate, average completion delay, and average completion ratio.</p></p class="citation"></blockquote><h3 id=710--91115-small-noise-analysis-of-non-parametric-closed-loop-identification-mohamed-abdalmoaty-et-al-2024>(7/10 | 91/115) Small Noise Analysis of Non-Parametric Closed-Loop Identification (Mohamed Abdalmoaty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Abdalmoaty, Roy S. Smith. (2024)<br><strong>Small Noise Analysis of Non-Parametric Closed-Loop Identification</strong><br><button class=copy-to-clipboard title="Small Noise Analysis of Non-Parametric Closed-Loop Identification" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15771v1.pdf filename=2403.15771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We revisit the problem of non-parametric closed-loop identification in frequency domain; we give a brief survey of the literature and provide a small noise analysis of the direct, indirect, and joint input-output methods when two independent experiments with identical excitation are used. The analysis is asymptotic in the noise variance (i.e., as the standard deviation of the innovations $\sigma \to 0$), for a finite data record of length $N$. We highlight the relationship between the estimators accuracy and the loop shape via asymptotic variance expressions given in terms of the sensitivity function. The results are illustrated using a numerical <b>simulation</b> example.</p></p class="citation"></blockquote><h3 id=810--92115-causal-tracking-of-distributions-in-wasserstein-space-a-model-predictive-control-scheme-max-emerick-et-al-2024>(8/10 | 92/115) Causal Tracking of Distributions in Wasserstein Space: A Model Predictive Control Scheme (Max Emerick et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Max Emerick, Jared Jonas, Bassam Bamieh. (2024)<br><strong>Causal Tracking of Distributions in Wasserstein Space: A Model Predictive Control Scheme</strong><br><button class=copy-to-clipboard title="Causal Tracking of Distributions in Wasserstein Space: A Model Predictive Control Scheme" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: 93C25, cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15702v1.pdf filename=2403.15702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of optimal swarm tracking which can be formulated as a tracking problem for distributions in the Wasserstein metric. Optimal control solutions to this problem are non-causal and require knowing the time-trajectory of the distribution to be tracked in advance. We propose a scheme where these non-causal solutions can be used together with a predictive model for the reference to achieve causal tracking control of a priori-unknown references. We develop the resulting model-predictive control scheme in the simple case where the reference is predicted to be constant-in-time. A computational algorithm based on particle methods and discrete optimal mass transport is presented, and numerical <b>simulations</b> are provided for various classes of reference signals. The results demonstrate that the proposed control algorithm achieves reasonable performance even when using simple predictive models.</p></p class="citation"></blockquote><h3 id=910--93115-real-time-reconfiguration-and-connectivity-maintenance-for-auvs-network-under-external-disturbances-using-distributed-nonlinear-model-predictive-control-nhat-minh-nguyen-et-al-2024>(9/10 | 93/115) Real-Time Reconfiguration and Connectivity Maintenance for AUVs Network Under External Disturbances using Distributed Nonlinear Model Predictive Control (Nhat Minh Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nhat Minh Nguyen, Stephen McIlvanna, Jack Close, Mien Van. (2024)<br><strong>Real-Time Reconfiguration and Connectivity Maintenance for AUVs Network Under External Disturbances using Distributed Nonlinear Model Predictive Control</strong><br><button class=copy-to-clipboard title="Real-Time Reconfiguration and Connectivity Maintenance for AUVs Network Under External Disturbances using Distributed Nonlinear Model Predictive Control" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15671v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15671v1.pdf filename=2403.15671v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advancements in underwater vehicle technology have significantly expanded the potential scope for deploying autonomous or remotely operated underwater vehicles in novel practical applications. However, the efficiency and maneuverability of these vehicles remain critical challenges, particularly in the dynamic aquatic environment. In this work, we propose a novel control scheme for creating multi-agent distributed formation control with limited communication between individual agents. In addition, the formation of the multi-agent can be reconfigured in real-time and the network connectivity can be maintained. The proposed use case for this scheme includes creating underwater mobile communication networks that can adapt to environmental or network conditions to maintain the quality of communication links for long-range exploration, seabed monitoring, or underwater infrastructure inspection. This work introduces a novel Distributed Nonlinear Model Predictive Control (DNMPC) strategy, integrating Control Lyapunov Functions (CLF) and Control Barrier Functions (CBF) with a relaxed decay rate, specifically tailored for 6-DOF underwater robotics. The effectiveness of our proposed DNMPC scheme was demonstrated through rigorous MATLAB <b>simulations</b> for trajectory tracking and formation reconfiguration in a dynamic environment. Our findings, supported by tests conducted using Software In The Loop (SITL) <b>simulation,</b> confirm the approach&rsquo;s applicability in real-time scenarios.</p></p class="citation"></blockquote><h3 id=1010--94115-safe-and-stable-formation-control-with-distributed-multi-agents-using-adaptive-control-and-control-barrier-functions-jose-a-solano-castellanos-et-al-2024>(10/10 | 94/115) Safe and Stable Formation Control with Distributed Multi-Agents Using Adaptive Control and Control Barrier Functions (Jose A. Solano-Castellanos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jose A. Solano-Castellanos, Anuradha Annaswamy. (2024)<br><strong>Safe and Stable Formation Control with Distributed Multi-Agents Using Adaptive Control and Control Barrier Functions</strong><br><button class=copy-to-clipboard title="Safe and Stable Formation Control with Distributed Multi-Agents Using Adaptive Control and Control Barrier Functions" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15674v1.pdf filename=2403.15674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This manuscript considers the problem of ensuring stability and safety during formation control with distributed multi-agent systems in the presence of parametric uncertainty in the dynamics and limited communication. We propose an integrative approach that combines Control Barrier Functions, Adaptive Control, and connected <b>graphs.</b> A reference model is designed so as to ensure a safe and stable formation control strategy. This is combined with a provably correct adaptive control design that includes a use of a CBF-based safety filter that suitably generates safe reference commands, and employs error-based relaxation (EBR) of Nagumo&rsquo;s Invariance Theorem. Together, it is shown to lead to a guarantee of boundedness, formation control, and forward invariance. Numerical examples are provided to support the theoretical derivations.</p></p class="citation"></blockquote><h2 id=csit-4>cs.IT (4)</h2><h3 id=14--95115-energy-efficient-design-of-active-star-ris-aided-swipt-systems-sajad-faramarzi-et-al-2024>(1/4 | 95/115) Energy Efficient Design of Active STAR-RIS-Aided SWIPT Systems (Sajad Faramarzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sajad Faramarzi, Hosein Zarini, Sepideh Javadi, Mohammad Robat Mili, Rui Zhang, George K. Karagiannidis, Naofal Al-Dhahir. (2024)<br><strong>Energy Efficient Design of Active STAR-RIS-Aided SWIPT Systems</strong><br><button class=copy-to-clipboard title="Energy Efficient Design of Active STAR-RIS-Aided SWIPT Systems" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 40<br>Keywords: Meta Learning, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15754v1.pdf filename=2403.15754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider the downlink transmission of a multi-antenna base station (BS) supported by an active simultaneously transmitting and reconfigurable intelligent surface (STAR-RIS) to serve single-antenna users via simultaneous wireless information and power transfer (SWIPT). In this context, we formulate an energy efficiency maximisation problem that jointly optimises the gain, element selection and phase shift matrices of the active STAR-RIS, the transmit beamforming of the BS and the power splitting ratio of the users. With respect to the highly coupled and non-convex form of this problem, an alternating optimisation solution approach is proposed, using tools from convex optimisation and <b>reinforcement</b> <b>learning.</b> Specifically, semi-definite relaxation (SDR), difference of concave functions (DC), and fractional programming techniques are employed to transform the non-convex optimisation problem into a convex form for optimising the BS beamforming vector and the power splitting ratio of the SWIPT. Then, by integrating <b>meta-learning</b> <b>with</b> the modified deep deterministic policy gradient (DDPG) and soft actor-critical (SAC) methods, a combinatorial <b>reinforcement</b> <b>learning</b> network is developed to optimise the element selection, gain and phase shift matrices of the active STAR-RIS. Our <b>simulations</b> show the effectiveness of the proposed resource allocation scheme. Furthermore, our proposed active STAR-RIS-based SWIPT system outperforms its passive counterpart by 57% on average.</p></p class="citation"></blockquote><h3 id=24--96115-block-orthogonal-sparse-superposition-codes-for--sfl3--communications-low-error-rate-low-latency-and-low-power-consumption-donghwa-han-et-al-2024>(2/4 | 96/115) Block Orthogonal Sparse Superposition Codes for $ \sf{L}^3 $ Communications: Low Error Rate, Low Latency, and Low Power Consumption (Donghwa Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donghwa Han, Bowhyung Lee, Min Jang, Donghun Lee, Seho Myung, Namyoon Lee. (2024)<br><strong>Block Orthogonal Sparse Superposition Codes for $ \sf{L}^3 $ Communications: Low Error Rate, Low Latency, and Low Power Consumption</strong><br><button class=copy-to-clipboard title="Block Orthogonal Sparse Superposition Codes for $ \sf{L}^3 $ Communications: Low Error Rate, Low Latency, and Low Power Consumption" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15692v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15692v1.pdf filename=2403.15692v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Block orthogonal sparse superposition (BOSS) code is a class of joint coded modulation methods, which can closely achieve the finite-blocklength capacity with a low-complexity decoder at a few coding rates under Gaussian channels. However, for fading channels, the code performance degrades considerably because coded symbols experience different channel fading effects. In this paper, we put forth novel joint demodulation and decoding methods for BOSS codes under fading channels. For a fast fading channel, we present a minimum mean square error approximate maximum a posteriori (MMSE-A-MAP) algorithm for the joint demodulation and decoding when channel state information is available at the receiver (CSIR). We also propose a joint demodulation and decoding method without using CSIR for a block fading channel scenario. We refer to this as the non-coherent sphere decoding (NSD) algorithm. <b>Simulation</b> results demonstrate that BOSS codes with MMSE-A-MAP decoding outperform CRC-aided polar codes, while NSD decoding achieves comparable performance to quasi-maximum likelihood decoding with significantly reduced complexity. Both decoding algorithms are suitable for parallelization, satisfying low-latency constraints. Additionally, real-time <b>simulations</b> on a software-defined radio testbed validate the feasibility of using BOSS codes for low-power transmission.</p></p class="citation"></blockquote><h3 id=34--97115-differentiable-information-bottleneck-for-deterministic-multi-view-clustering-xiaoqiang-yan-et-al-2024>(3/4 | 97/115) Differentiable Information Bottleneck for Deterministic Multi-view Clustering (Xiaoqiang Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoqiang Yan, Zhixiang Jin, Fengshou Han, Yangdong Ye. (2024)<br><strong>Differentiable Information Bottleneck for Deterministic Multi-view Clustering</strong><br><button class=copy-to-clipboard title="Differentiable Information Bottleneck for Deterministic Multi-view Clustering" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs.IT, math-IT<br>Keyword Score: 16<br>Keywords: Benchmarking, Clustering, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15681v1.pdf filename=2403.15681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent several years, the information bottleneck (IB) principle provides an information-theoretic framework for deep multi-view <b>clustering</b> (MVC) by compressing multi-view observations while preserving the relevant information of multiple views. Although existing IB-based deep MVC methods have achieved huge success, they rely on variational approximation and distribution assumption to estimate the lower bound of <b>mutual</b> <b>information,</b> which is a notoriously hard and impractical problem in high-dimensional multi-view spaces. In this work, we propose a new differentiable information bottleneck (DIB) method, which provides a deterministic and analytical MVC solution by fitting the <b>mutual</b> <b>information</b> without the necessity of variational approximation. Specifically, we first propose to directly fit the <b>mutual</b> <b>information</b> of high-dimensional spaces by leveraging normalized kernel Gram matrix, which does not require any auxiliary neural estimator to estimate the lower bound of <b>mutual</b> <b>information.</b> Then, based on the new <b>mutual</b> <b>information</b> measurement, a deterministic multi-view neural network with analytical gradients is explicitly trained to parameterize IB principle, which derives a deterministic compression of input variables from different views. Finally, a triplet consistency discovery mechanism is devised, which is capable of mining the feature consistency, cluster consistency and joint consistency based on the deterministic and compact representations. Extensive experimental results show the superiority of our DIB method on 6 <b>benchmarks</b> compared with 13 state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=44--98115-permutation-recovery-problem-against-deletion-errors-for-dna-data-storage-shubhransh-singhvi-et-al-2024>(4/4 | 98/115) Permutation Recovery Problem against Deletion Errors for DNA Data Storage (Shubhransh Singhvi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubhransh Singhvi, Charchit Gupta, Avital Boruchovsky, Yuval Goldberg, Han Mao Kiah, Eitan Yaakobi. (2024)<br><strong>Permutation Recovery Problem against Deletion Errors for DNA Data Storage</strong><br><button class=copy-to-clipboard title="Permutation Recovery Problem against Deletion Errors for DNA Data Storage" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15827v1.pdf filename=2403.15827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Owing to its immense storage density and durability, DNA has emerged as a promising storage medium. However, due to technological constraints, data can only be written onto many short DNA molecules called data blocks that are stored in an unordered way. To handle the unordered nature of DNA data storage systems, a unique address is typically prepended to each data block to form a DNA strand. However, DNA storage systems are prone to errors and generate multiple noisy copies of each strand called DNA reads. Thus, we study the permutation recovery problem against deletions errors for DNA data storage. The permutation recovery problem for DNA data storage requires one to reconstruct the addresses or in other words to uniquely identify the noisy reads. By successfully reconstructing the addresses, one can essentially determine the correct order of the data blocks, effectively solving the <b>clustering</b> problem. We first show that we can almost surely identify all the noisy reads under certain mild assumptions. We then propose a permutation recovery procedure and analyze its complexity.</p></p class="citation"></blockquote><h2 id=cshc-1>cs.HC (1)</h2><h3 id=11--99115-negotiating-the-shared-agency-between-humans--ai-in-the-recommender-system-mengke-wu-et-al-2024>(1/1 | 99/115) Negotiating the Shared Agency between Humans & AI in the Recommender System (Mengke Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengke Wu, Weizi Liu, Yanyun, Wang, Mike Zhengyu Yao. (2024)<br><strong>Negotiating the Shared Agency between Humans & AI in the Recommender System</strong><br><button class=copy-to-clipboard title="Negotiating the Shared Agency between Humans & AI in the Recommender System" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CY, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Explainable AI, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15919v1.pdf filename=2403.15919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Smart <b>recommendation</b> algorithms have revolutionized information dissemination, enhancing efficiency and reshaping content delivery across various domains. However, concerns about user agency have arisen due to the inherent opacity (information asymmetry) and the nature of one-way output (power asymmetry) on algorithms. While both issues have been criticized by scholars via advocating <b>explainable</b> <b>AI</b> (XAI) and human-AI collaborative decision-making (HACD), few research evaluates their integrated effects on users, and few HACD discussions in <b>recommender</b> <b>systems</b> beyond improving and filtering the results. This study proposes an incubating idea as a missing step in HACD that allows users to control the degrees of AI-recommended content. Then, we integrate it with existing XAI to a flow prototype aimed at assessing the enhancement of user agency. We seek to understand how types of agency impact user perception and experience, and bring empirical evidence to refine the guidelines and designs for human-AI interactive systems.</p></p class="citation"></blockquote><h2 id=cscr-1>cs.CR (1)</h2><h3 id=11--100115-a-hybrid-llm-workflow-can-help-identify-user-privilege-related-variables-in-programs-of-any-size-haizhou-wang-et-al-2024>(1/1 | 100/115) A hybrid LLM workflow can help identify user privilege related variables in programs of any size (Haizhou Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haizhou Wang, Zhilong Wang, Peng Liu. (2024)<br><strong>A hybrid LLM workflow can help identify user privilege related variables in programs of any size</strong><br><button class=copy-to-clipboard title="A hybrid LLM workflow can help identify user privilege related variables in programs of any size" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SE, cs.CR<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15723v1.pdf filename=2403.15723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many programs involves operations and logic manipulating user privileges, which is essential for the security of an organization. Therefore, one common malicious goal of attackers is to obtain or escalate the privileges, causing privilege leakage. To protect the program and the organization against privilege leakage attacks, it is important to eliminate the vulnerabilities which can be exploited to achieve such attacks. Unfortunately, while memory vulnerabilities are less challenging to find, logic vulnerabilities are much more imminent, harmful and difficult to identify. Accordingly, many analysts choose to find user privilege related (UPR) variables first as start points to investigate the code where the UPR variables may be used to see if there exists any vulnerabilities, especially the logic ones. In this paper, we introduce a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> workflow that can assist analysts in identifying such UPR variables, which is considered to be a very time-consuming task. Specifically, our tool will audit all the variables in a program and output a UPR score, which is the degree of relationship (closeness) between the variable and user privileges, for each variable. The proposed approach avoids the drawbacks introduced by directly <b>prompting</b> a <b>LLM</b> to find UPR variables by focusing on leverage the <b>LLM</b> at statement level instead of supplying <b>LLM</b> with very long code snippets. Those variables with high UPR scores are essentially potential UPR variables, which should be manually investigated. Our experiments show that using a typical UPR score threshold (i.e., UPR score >0.8), the false positive rate (FPR) is only 13.49%, while UPR variable found is significantly more than that of the heuristic based method.</p></p class="citation"></blockquote><h2 id=statme-2>stat.ME (2)</h2><h3 id=12--101115-optimized-model-selection-for-estimating-treatment-effects-from-costly-simulations-of-the-us-opioid-epidemic-abdulrahman-a-ahmed-et-al-2024>(1/2 | 101/115) Optimized Model Selection for Estimating Treatment Effects from Costly Simulations of the US Opioid Epidemic (Abdulrahman A. Ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdulrahman A. Ahmed, M. Amin Rahimian, Mark S. Roberts. (2024)<br><strong>Optimized Model Selection for Estimating Treatment Effects from Costly Simulations of the US Opioid Epidemic</strong><br><button class=copy-to-clipboard title="Optimized Model Selection for Estimating Treatment Effects from Costly Simulations of the US Opioid Epidemic" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-MA, cs-SI, stat-AP, stat-ME, stat.ME<br>Keyword Score: 23<br>Keywords: Sample Size, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15755v1.pdf filename=2403.15755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Agent-based <b>simulation</b> with a synthetic population can help us compare different treatment conditions while keeping everything else constant within the same population (i.e., as digital twins). Such population-scale <b>simulations</b> require large computational power (i.e., CPU resources) to get accurate estimates for treatment effects. We can use meta models of the <b>simulation</b> results to circumvent the need to simulate every treatment condition. Selecting the best estimating model at a given <b>sample</b> <b>size</b> (number of <b>simulation</b> runs) is a crucial problem. Depending on the <b>sample</b> <b>size,</b> the ability of the method to estimate accurately can change significantly. In this paper, we discuss different methods to explore what model works best at a specific <b>sample</b> <b>size.</b> In addition to the empirical results, we provide a mathematical analysis of the MSE equation and how its components decide which model to select and why a specific method behaves that way in a range of <b>sample</b> <b>sizes.</b> The analysis showed why the direction estimation method is better than model-based methods in larger <b>sample</b> <b>sizes</b> and how the between-group variation and the within-group variation affect the MSE equation.</p></p class="citation"></blockquote><h3 id=22--102115-supervised-learning-via-ensembles-of-diverse-functional-representations-the-functional-voting-classifier-donato-riccio-et-al-2024>(2/2 | 102/115) Supervised Learning via Ensembles of Diverse Functional Representations: the Functional Voting Classifier (Donato Riccio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donato Riccio, Fabrizio Maturo, Elvira Romano. (2024)<br><strong>Supervised Learning via Ensembles of Diverse Functional Representations: the Functional Voting Classifier</strong><br><button class=copy-to-clipboard title="Supervised Learning via Ensembles of Diverse Functional Representations: the Functional Voting Classifier" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: 46N30, 62-08, cs-LG, stat-ME, stat-ML, stat.ME<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15778v1.pdf filename=2403.15778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many conventional statistical and machine learning methods face challenges when applied directly to high dimensional temporal observations. In recent decades, Functional Data Analysis (FDA) has gained widespread popularity as a framework for modeling and analyzing data that are, by their nature, functions in the domain of time. Although <b>supervised</b> <b>classification</b> has been extensively explored in recent decades within the FDA literature, ensemble learning of functional classifiers has only recently emerged as a topic of significant interest. Thus, the latter subject presents unexplored facets and challenges from various statistical perspectives. The focal point of this paper lies in the realm of ensemble learning for functional data and aims to show how different functional data representations can be used to train ensemble members and how base model predictions can be combined through majority voting. The so-called Functional Voting Classifier (FVC) is proposed to demonstrate how different functional representations leading to augmented diversity can increase predictive accuracy. Many real-world datasets from several domains are used to display that the FVC can significantly enhance performance compared to individual models. The framework presented provides a foundation for voting ensembles with functional data and can stimulate a highly encouraging line of research in the FDA context.</p></p class="citation"></blockquote><h2 id=csni-3>cs.NI (3)</h2><h3 id=13--103115-loam-low-latency-communication-caching-and-computation-placement-in-data-intensive-computing-networks-jinkun-zhang-et-al-2024>(1/3 | 103/115) LOAM: Low-latency Communication, Caching, and Computation Placement in Data-Intensive Computing Networks (Jinkun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinkun Zhang, Edmund Yeh. (2024)<br><strong>LOAM: Low-latency Communication, Caching, and Computation Placement in Data-Intensive Computing Networks</strong><br><button class=copy-to-clipboard title="LOAM: Low-latency Communication, Caching, and Computation Placement in Data-Intensive Computing Networks" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-DC, cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15927v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15927v1.pdf filename=2403.15927v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deploying data- and computation-intensive applications such as large-scale AI into heterogeneous dispersed computing networks can significantly enhance application performance by mitigating bottlenecks caused by limited network resources, including bandwidth, storage, and computing power. However, current resource allocation methods in dispersed computing do not provide a comprehensive solution that considers arbitrary topology, elastic resource amount, reuse of computation results, and nonlinear congestion-dependent optimization objectives. In this paper, we propose LOAM, a low-latency joint communication, caching, and computation placement framework with a rigorous analytical foundation that incorporates the above aspects. We tackle the NP-hard aggregated cost minimization problem with two methods: an offline method with a 1/2 approximation and an online adaptive method with a bounded gap from the optimum. Through extensive <b>simulation,</b> the proposed framework outperforms multiple baselines in both synthesis and real-world network scenarios.</p></p class="citation"></blockquote><h3 id=23--104115-a-novel-non-terrestrial-networks-architecture-all-optical-leo-constellations-with-high-altitude-ground-stations-pablo-g-madoery-et-al-2024>(2/3 | 104/115) A Novel Non-Terrestrial Networks Architecture: All Optical LEO Constellations with High-Altitude Ground Stations (Pablo G. Madoery et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pablo G. Madoery, Juan A. Fraire, Jorge M. Finochietto, Halim Yanikomeroglu, Gunes Karabulut Kurt. (2024)<br><strong>A Novel Non-Terrestrial Networks Architecture: All Optical LEO Constellations with High-Altitude Ground Stations</strong><br><button class=copy-to-clipboard title="A Novel Non-Terrestrial Networks Architecture: All Optical LEO Constellations with High-Altitude Ground Stations" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15659v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15659v1.pdf filename=2403.15659v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of low Earth orbit (LEO) satellite mega-constellations is dynamically transforming the space sector. While free-space optical (FSO) links efficiently facilitate intersatellite data forwarding, they suffer from atmospheric/weather conditions in the space-to-ground link. This study delves into utilizing high-altitude platform stations (HAPS) as elevated relay stations strategically positioned above terrestrial ground stations. We introduce the concept of high-altitude ground stations (HAGS), an innovative approach to enabling the development of all optical LEO satellite constellations. The first contribution is an analysis of the HAGS-based network architecture where the LEO spacecraft only hosts FSO transceivers. Secondly, we execute an extensive <b>simulation</b> campaign to determine the gain of HAGS, including a new equivalency model with the traditional ground station approach. Finally, we examine the research challenges of implementing HAGS-based, all optical LEO mega-constellations.</p></p class="citation"></blockquote><h3 id=33--105115-delay-optimal-forwarding-and-computation-offloading-for-service-chain-tasks-jinkun-zhang-et-al-2024>(3/3 | 105/115) Delay-Optimal Forwarding and Computation Offloading for Service Chain Tasks (Jinkun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinkun Zhang, Yuezhou Liu, Edmund Yeh. (2024)<br><strong>Delay-Optimal Forwarding and Computation Offloading for Service Chain Tasks</strong><br><button class=copy-to-clipboard title="Delay-Optimal Forwarding and Computation Offloading for Service Chain Tasks" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Karush-Kuhn-Tucker<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15936v1.pdf filename=2403.15936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emerging edge computing paradigms enable heterogeneous devices to collaborate on complex computation applications. However, for congestible links and computing units, delay-optimal forwarding and offloading for service chain tasks (e.g., DNN with vertical split) in edge computing networks remains an open problem. In this paper, we formulate the service chain forwarding and offloading problem with arbitrary topology and heterogeneous transmission/computation capability, and aim to minimize the aggregated network cost. We consider congestion-aware nonlinear cost functions that cover various performance metrics and constraints, such as average queueing delay with limited processor capacity. We solve the non-convex optimization problem globally by analyzing the <b>KKT</b> condition and proposing a sufficient condition for optimality. We then propose a distributed algorithm that converges to the global optimum. The algorithm adapts to changes in input rates and network topology, and can be implemented as an online algorithm. Numerical evaluation shows that our method significantly outperforms baselines in multiple network instances, especially in congested scenarios.</p></p class="citation"></blockquote><h2 id=cscy-2>cs.CY (2)</h2><h3 id=12--106115-deep-learning-approach-to-forecasting-covid-19-cases-in-residential-buildings-of-hong-kong-public-housing-estates-the-role-of-environment-and-sociodemographics-e-leung-et-al-2024>(1/2 | 106/115) Deep Learning Approach to Forecasting COVID-19 Cases in Residential Buildings of Hong Kong Public Housing Estates: The Role of Environment and Sociodemographics (E. Leung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>E. Leung, J. Guan, KO. Kwok, CT. Hung, CC. Ching, KC. Chong, CHK. Yam, T. Sun, WH. Tsang, EK. Yeoh, A. Lee. (2024)<br><strong>Deep Learning Approach to Forecasting COVID-19 Cases in Residential Buildings of Hong Kong Public Housing Estates: The Role of Environment and Sociodemographics</strong><br><button class=copy-to-clipboard title="Deep Learning Approach to Forecasting COVID-19 Cases in Residential Buildings of Hong Kong Public Housing Estates: The Role of Environment and Sociodemographics" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15759v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15759v1.pdf filename=2403.15759v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Introduction: The current study investigates the complex association between COVID-19 and the studied districts&rsquo; socioecology (e.g. internal and external built environment, sociodemographic profiles, etc.) to quantify their contributions to the early outbreaks and epidemic resurgence of COVID-19. Methods: We aligned the analytic model&rsquo;s architecture with the hierarchical structure of the resident&rsquo;s socioecology using a multi-headed hierarchical <b>convolutional</b> <b>neural</b> <b>network</b> to structure the vast array of hierarchically related predictive features representing buildings&rsquo; internal and external built environments and residents&rsquo; sociodemographic profiles as model input. COVID-19 cases accumulated in buildings across three adjacent districts in HK, both before and during HK&rsquo;s epidemic resurgence, were modeled. A forward-chaining validation was performed to examine the model&rsquo;s performance in forecasting COVID-19 cases over the 3-, 7-, and 14-day horizons during the two months subsequent to when the model for COVID-19 resurgence was built to align with the forecasting needs in an evolving pandemic. Results: Different sets of factors were found to be linked to the earlier waves of COVID-19 outbreaks compared to the epidemic resurgence of the pandemic. Sociodemographic factors such as work hours, monthly household income, employment types, and the number of non-working adults or children in household populations were of high importance to the studied buildings&rsquo; COVID-19 case counts during the early waves of COVID-19. Factors constituting one&rsquo;s internal built environment, such as the number of distinct households in the buildings, the number of distinct households per floor, and the number of floors, corridors, and lifts, had the greatest unique contributions to the building-level COVID-19 case counts during epidemic resurgence.</p></p class="citation"></blockquote><h3 id=22--107115-optimal-hospital-capacity-management-during-demand-surges-felix-parker-et-al-2024>(2/2 | 107/115) Optimal Hospital Capacity Management During Demand Surges (Felix Parker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felix Parker, Fardin Ganjkhanloo, Diego A. Martínez, Kimia Ghobadi. (2024)<br><strong>Optimal Hospital Capacity Management During Demand Surges</strong><br><button class=copy-to-clipboard title="Optimal Hospital Capacity Management During Demand Surges" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY, math-OC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15738v1.pdf filename=2403.15738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective hospital capacity management is pivotal for enhancing patient care quality, operational efficiency, and healthcare system resilience, notably during demand spikes like those seen in the COVID-19 pandemic. However, devising optimal capacity strategies is complicated by fluctuating demand, conflicting objectives, and multifaceted practical constraints. This study presents a data-driven framework to optimize capacity management decisions within hospital systems during surge events. Two key decisions are optimized over a tactical planning horizon: allocating dedicated capacity to surge patients and transferring incoming patients between emergency departments (EDs) of hospitals to better distribute demand. The optimization models are formulated as robust mixed-integer linear programs, enabling efficient computation of optimal decisions that are robust against demand uncertainty. The models incorporate practical constraints and costs, including setup times and costs for adding surge capacity, restrictions on ED patient transfers, and relative costs of different decisions that reflect impacts on care quality and operational efficiency. The methodology is evaluated retrospectively in a hospital system during the height of the COVID-19 pandemic to demonstrate the potential impact of the recommended decisions. The results show that optimally allocating beds and transferring just 30 patients over a 63 day period around the peak, less than one transfer every two days, could have reduced the need for surge capacity in the hospital system by approximately 98%. Overall, this work introduces a practical tool to transform capacity management decision-making, enabling proactive planning and the use of data-driven <b>recommendations</b> to improve outcomes.</p></p class="citation"></blockquote><h2 id=cssy-1>cs.SY (1)</h2><h3 id=11--108115-passivity-based-attack-identification-and-mitigation-with-event-triggered-observer-feedback-and-switching-controller-pushkal-purohit-et-al-2024>(1/1 | 108/115) Passivity-based Attack Identification and Mitigation with Event-triggered Observer Feedback and Switching Controller (Pushkal Purohit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pushkal Purohit, Anoop Jain. (2024)<br><strong>Passivity-based Attack Identification and Mitigation with Event-triggered Observer Feedback and Switching Controller</strong><br><button class=copy-to-clipboard title="Passivity-based Attack Identification and Mitigation with Event-triggered Observer Feedback and Switching Controller" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SY<br>Categories: cs-SY, cs.SY, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15697v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15697v1.pdf filename=2403.15697v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the problem of output consensus in linear passive multi-agent systems under a False Data Injection (FDI) attack, considering the unavailability of complete state information. Our formulation relies on an event-based cryptographic authentication scheme for sensor integrity and considers FDI attacks at the actuator end, inspired by their practical nature and usages. For secure consensus, we propose (i) a passivity-based approach for detecting FDI attacks on the system and (ii) a Zeno-free event-triggered observer-based switching controller, which switches between the normal and the defense modes following an attack detection. We show that the closed-loop system achieves practical consensus under the controller&rsquo;s action in the defense mode. <b>Simulation</b> examples are provided to support the theoretical findings.</p></p class="citation"></blockquote><h2 id=csir-1>cs.IR (1)</h2><h3 id=11--109115-queryexplorer-an-interactive-query-generation-assistant-for-search-and-exploration-kaustubh-d-dhole-et-al-2024>(1/1 | 109/115) QueryExplorer: An Interactive Query Generation Assistant for Search and Exploration (Kaustubh D. Dhole et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaustubh D. Dhole, Shivam Bajaj, Ramraj Chandradevan, Eugene Agichtein. (2024)<br><strong>QueryExplorer: An Interactive Query Generation Assistant for Search and Exploration</strong><br><button class=copy-to-clipboard title="QueryExplorer: An Interactive Query Generation Assistant for Search and Exploration" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: human-in-the-loop, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15667v1.pdf filename=2403.15667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Formulating effective search queries remains a challenging task, particularly when users lack expertise in a specific domain or are not proficient in the language of the content. Providing example documents of interest might be easier for a user. However, such query-by-example scenarios are prone to concept drift, and the retrieval effectiveness is highly sensitive to the query generation method, without a clear way to incorporate user feedback. To enable exploration and to support <b>Human-In-The-Loop</b> experiments we propose QueryExplorer &ndash; an interactive query generation, reformulation, and retrieval interface with support for HuggingFace generation models and PyTerrier&rsquo;s retrieval pipelines and datasets, and extensive logging of human feedback. To allow users to create and modify effective queries, our demo supports complementary approaches of using <b>LLMs</b> interactively, assisting the user with edits and feedback at multiple stages of the query formulation process. With support for recording fine-grained interactions and user annotations, QueryExplorer can serve as a valuable experimental and research platform for annotation, qualitative evaluation, and conducting <b>Human-in-the-Loop</b> (HITL) experiments for complex search tasks where users struggle to formulate queries.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--110115-improved-methods-of-task-assignment-and-resource-allocation-with-preemption-in-edge-computing-systems-caroline-rublein-et-al-2024>(1/1 | 110/115) Improved Methods of Task Assignment and Resource Allocation with Preemption in Edge Computing Systems (Caroline Rublein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Caroline Rublein, Fidan Mehmeti, Mark Mahon, Thomas F. La Porta. (2024)<br><strong>Improved Methods of Task Assignment and Resource Allocation with Preemption in Edge Computing Systems</strong><br><button class=copy-to-clipboard title="Improved Methods of Task Assignment and Resource Allocation with Preemption in Edge Computing Systems" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15665v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15665v1.pdf filename=2403.15665v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Edge computing has become a very popular service that enables mobile devices to run complex tasks with the help of network-based computing resources. However, edge clouds are often resource-constrained, which makes resource allocation a challenging issue. In addition, edge cloud servers must make allocation decisions with only limited information available, since the arrival of future client tasks might be impossible to predict, and the states and behavior of neighboring servers might be obscured. We focus on a distributed resource allocation method in which servers operate independently and do not communicate with each other, but interact with clients (tasks) to make allocation decisions. We follow a two-round bidding approach to assign tasks to edge cloud servers, and servers are allowed to preempt previous tasks to allocate more useful ones. We evaluate the performance of our system using realistic <b>simulations</b> and real-world trace data from a high-performance computing cluster. Results show that our heuristic improves system-wide performance by $20-25%$ over previous work when accounting for the time taken by each approach. In this way, an ideal trade-off between performance and speed is achieved.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=11--111115-space-group-informed-transformer-for-crystalline-materials-generation-zhendong-cao-et-al-2024>(1/1 | 111/115) Space Group Informed Transformer for Crystalline Materials Generation (Zhendong Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhendong Cao, Xiaoshan Luo, Jian Lv, Lei Wang. (2024)<br><strong>Space Group Informed Transformer for Crystalline Materials Generation</strong><br><button class=copy-to-clipboard title="Space Group Informed Transformer for Crystalline Materials Generation" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-LG, physics-comp-ph<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15734v1.pdf filename=2403.15734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce CrystalFormer, a <b>transformer-based</b> autoregressive model specifically designed for space group-controlled generation of crystalline materials. The space group symmetry significantly simplifies the crystal space, which is crucial for data and compute efficient generative modeling of crystalline materials. Leveraging the prominent discrete and sequential nature of the Wyckoff positions, CrystalFormer learns to generate crystals by directly predicting the species and locations of symmetry-inequivalent atoms in the unit cell. Our results demonstrate that CrystalFormer matches state-of-the-art performance on standard <b>benchmarks</b> for both validity, novelty, and stability of the generated crystalline materials. Our analysis also shows that CrystalFormer ingests sensible solid-state chemistry information from data for generative modeling. The CrystalFormer unifies symmetry-based structure search and generative pre-training in the realm of crystalline materials. The simplicity, generality, and flexibility of CrystalFormer position it as a promising architecture to be the foundational model of the entire crystalline materials space, heralding a new era in materials modeling and discovery.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--112115-utilizing-motion-matching-with-deep-reinforcement-learning-for-target-location-tasks-jeongmin-lee-et-al-2024>(1/1 | 112/115) Utilizing Motion Matching with Deep Reinforcement Learning for Target Location Tasks (Jeongmin Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeongmin Lee, Taesoo Kwon, Hyunju Shin, Yoonsang Lee. (2024)<br><strong>Utilizing Motion Matching with Deep Reinforcement Learning for Target Location Tasks</strong><br><button class=copy-to-clipboard title="Utilizing Motion Matching with Deep Reinforcement Learning for Target Location Tasks" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15902v1.pdf filename=2403.15902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an approach using deep <b>reinforcement</b> <b>learning</b> (DRL) to directly generate motion matching queries for long-term tasks, particularly targeting the reaching of specific locations. By integrating motion matching and DRL, our method demonstrates the rapid learning of policies for target location tasks within minutes on a standard desktop, employing a simple reward design. Additionally, we propose a unique hit reward and obstacle curriculum scheme to enhance policy learning in environments with moving obstacles.</p></p class="citation"></blockquote><h2 id=mathna-1>math.NA (1)</h2><h3 id=11--113115-conservative-surrogate-models-for-optimization-with-the-active-subspace-method-philippe-andré-luneau-2024>(1/1 | 113/115) Conservative Surrogate Models for Optimization with the Active Subspace Method (Philippe-André Luneau, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philippe-André Luneau. (2024)<br><strong>Conservative Surrogate Models for Optimization with the Active Subspace Method</strong><br><button class=copy-to-clipboard title="Conservative Surrogate Models for Optimization with the Active Subspace Method" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65K99, 65C20, cs-NA, math-NA, math-OC, math.NA<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15678v1.pdf filename=2403.15678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We are interested in building low-dimensional surrogate models to reduce optimization costs, while having theoretical guarantees that the optimum will satisfy the constraints of the full-size model, by making conservative approximations. The surrogate model is constructed using a <b>Gaussian</b> <b>process</b> regression (GPR). To ensure conservativeness, two new approaches are proposed: the first one using bootstrapping, and the second one using concentration inequalities. Those two techniques are based on a stochastic argument and thus will only enforce conservativeness up to a user-defined probability threshold. The method has applications in the context of optimization using the active subspace method for dimensionality reduction of the objective function and the constraints, addressing recorded issues about constraint violations. The resulting algorithms are tested on a toy optimization problem in thermal design.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=11--114115-distance-adjustment-of-a-graph-drawing-stress-model-yosuke-onoue-2024>(1/1 | 114/115) Distance Adjustment of a Graph Drawing Stress Model (Yosuke Onoue, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yosuke Onoue. (2024)<br><strong>Distance Adjustment of a Graph Drawing Stress Model</strong><br><button class=copy-to-clipboard title="Distance Adjustment of a Graph Drawing Stress Model" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15811v1.pdf filename=2403.15811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stress models are a promising approach for <b>graph</b> drawing. They minimize the weighted sum of the squared errors of the Euclidean and desired distances for each node pair. The desired distance typically uses the <b>graph-theoretic</b> distances obtained from the all-node pair shortest path problem. In a minimized stress function, the obtained coordinates are affected by the non-Euclidean property and the high-dimensionality of the <b>graph-theoretic</b> distance matrix. Therefore, the <b>graph-theoretic</b> distances used in stress models may not necessarily be the best metric for determining the node coordinates. In this study, we propose two different methods of adjusting the <b>graph-theoretical</b> distance matrix to a distance matrix suitable for <b>graph</b> drawing while preserving its structure. The first method is the application of eigenvalue decomposition to the inner product matrix obtained from the distance matrix and the obtainment of a new distance matrix by setting some eigenvalues with small absolute values to zero. The second approach is the usage of a stress model modified by adding a term that minimizes the Frobenius norm between the adjusted and original distance matrices. We perform computational experiments using several <b>benchmark</b> <b>graphs</b> to demonstrate that the proposed method improves some quality metrics, including the node resolution and the Gabriel <b>graph</b> property, when compared to conventional stress models.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--115115-team-coordination-on-graphs-problem-analysis-and-algorithms-manshi-limbu-et-al-2024>(1/1 | 115/115) Team Coordination on Graphs: Problem, Analysis, and Algorithms (Manshi Limbu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manshi Limbu, Yanlin Zhou, Gregory Stein, Xuan Wang, Daigo Shishika, Xuesu Xiao. (2024)<br><strong>Team Coordination on Graphs: Problem, Analysis, and Algorithms</strong><br><button class=copy-to-clipboard title="Team Coordination on Graphs: Problem, Analysis, and Algorithms" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15946v1.pdf filename=2403.15946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Team Coordination on <b>Graphs</b> with Risky Edges (TCGRE) is a recently emerged problem, in which a robot team collectively reduces <b>graph</b> traversal cost through support from one robot to another when the latter traverses a risky edge. Resembling the traditional Multi-Agent Path Finding (MAPF) problem, both classical and learning-based methods have been proposed to solve TCGRE, however, they lacked either computation efficiency or optimality assurance. In this paper, we reformulate TCGRE as a constrained optimization and perform rigorous mathematical analysis. Our theoretical analysis shows the NP-hardness of TCGRE by reduction from the Maximum 3D Matching problem and that efficient decomposition is a key to tackle this combinatorial optimization problem. Further more, we design three classes of algorithms to solve TCGRE, i.e., Joint State <b>Graph</b> (JSG) based, coordination based, and receding-horizon sub-team based solutions. Each of these proposed algorithms enjoy different provable optimality and efficiency characteristics that are demonstrated in our extensive experiments.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.24</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.26</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-18>cs.CL (18)</a><ul><li><a href=#118--1115-leveraging-zero-shot-prompting-for-efficient-language-model-distillation-lukas-vöge-et-al-2024>(1/18 | 1/115) Leveraging Zero-Shot Prompting for Efficient Language Model Distillation (Lukas Vöge et al., 2024)</a></li><li><a href=#218--2115-eagle-a-domain-generalization-framework-for-ai-generated-text-detection-amrita-bhattacharjee-et-al-2024>(2/18 | 2/115) EAGLE: A Domain Generalization Framework for AI-generated Text Detection (Amrita Bhattacharjee et al., 2024)</a></li><li><a href=#318--3115-llambert-large-scale-low-cost-data-annotation-in-nlp-bálint-csanády-et-al-2024>(3/18 | 3/115) LlamBERT: Large-scale low-cost data annotation in NLP (Bálint Csanády et al., 2024)</a></li><li><a href=#418--4115-llms-instruct-llmsan-extraction-and-editing-method-xin-zhang-et-al-2024>(4/18 | 4/115) LLMs Instruct LLMs:An Extraction and Editing Method (Xin Zhang et al., 2024)</a></li><li><a href=#518--5115-edda-a-encoder-decoder-data-augmentation-framework-for-zero-shot-stance-detection-daijun-ding-et-al-2024>(5/18 | 5/115) EDDA: A Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance Detection (Daijun Ding et al., 2024)</a></li><li><a href=#618--6115-towards-a-rag-based-summarization-agent-for-the-electron-ion-collider-karthik-suresh-et-al-2024>(6/18 | 6/115) Towards a RAG-based Summarization Agent for the Electron-Ion Collider (Karthik Suresh et al., 2024)</a></li><li><a href=#718--7115-mrc-based-nested-medical-ner-with-co-prediction-and-adaptive-pre-training-xiaojing-du-et-al-2024>(7/18 | 7/115) MRC-based Nested Medical NER with Co-prediction and Adaptive Pre-training (Xiaojing Du et al., 2024)</a></li><li><a href=#818--8115-stentconv-predicting-disagreement-with-stance-detection-and-a-signed-graph-convolutional-network-isabelle-lorge-et-al-2024>(8/18 | 8/115) STEntConv: Predicting Disagreement with Stance Detection and a Signed Graph Convolutional Network (Isabelle Lorge et al., 2024)</a></li><li><a href=#918--9115-vlue-a-new-benchmark-and-multi-task-knowledge-transfer-learning-for-vietnamese-natural-language-understanding-phong-nguyen-thuan-do-et-al-2024>(9/18 | 9/115) VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for Vietnamese Natural Language Understanding (Phong Nguyen-Thuan Do et al., 2024)</a></li><li><a href=#1018--10115-ghost-sentence-a-tool-for-everyday-users-to-copyright-data-from-large-language-models-shuai-zhao-et-al-2024>(10/18 | 10/115) Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models (Shuai Zhao et al., 2024)</a></li><li><a href=#1118--11115-few-shot-dialogue-strategy-learning-for-motivational-interviewing-via-inductive-reasoning-zhouhang-xie-et-al-2024>(11/18 | 11/115) Few-shot Dialogue Strategy Learning for Motivational Interviewing via Inductive Reasoning (Zhouhang Xie et al., 2024)</a></li><li><a href=#1218--12115-modeling-unified-semantic-discourse-structure-for-high-quality-headline-generation-minghui-xu-et-al-2024>(12/18 | 12/115) Modeling Unified Semantic Discourse Structure for High-quality Headline Generation (Minghui Xu et al., 2024)</a></li><li><a href=#1318--13115-ai-for-biomedicine-in-the-era-of-large-language-models-zhenyu-bi-et-al-2024>(13/18 | 13/115) AI for Biomedicine in the Era of Large Language Models (Zhenyu Bi et al., 2024)</a></li><li><a href=#1418--14115-peace-a-chemistry-oriented-dataset-for-optical-character-recognition-on-scientific-documents-nan-zhang-et-al-2024>(14/18 | 14/115) PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on Scientific Documents (Nan Zhang et al., 2024)</a></li><li><a href=#1518--15115-computational-sentence-level-metrics-predicting-human-sentence-comprehension-kun-sun-et-al-2024>(15/18 | 15/115) Computational Sentence-level Metrics Predicting Human Sentence Comprehension (Kun Sun et al., 2024)</a></li><li><a href=#1618--16115-feel-a-framework-for-evaluating-emotional-support-capability-with-large-language-models-huaiwen-zhang-et-al-2024>(16/18 | 16/115) FEEL: A Framework for Evaluating Emotional Support Capability with Large Language Models (Huaiwen Zhang et al., 2024)</a></li><li><a href=#1718--17115-geotokens-and-geotransformers-eren-unlu-2024>(17/18 | 17/115) Geotokens and Geotransformers (Eren Unlu, 2024)</a></li><li><a href=#1818--18115-raamove-a-corpus-for-analyzing-moves-in-research-article-abstracts-hongzheng-li-et-al-2024>(18/18 | 18/115) RAAMove: A Corpus for Analyzing Moves in Research Article Abstracts (Hongzheng Li et al., 2024)</a></li></ul></li><li><a href=#cscv-28>cs.CV (28)</a><ul><li><a href=#128--19115-illusionvqa-a-challenging-optical-illusion-dataset-for-vision-language-models-haz-sameen-shahgir-et-al-2024>(1/28 | 19/115) IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models (Haz Sameen Shahgir et al., 2024)</a></li><li><a href=#228--20115-technical-report-masked-skeleton-sequence-modeling-for-learning-larval-zebrafish-behavior-latent-embeddings-lanxin-xu-et-al-2024>(2/28 | 20/115) Technical Report: Masked Skeleton Sequence Modeling for Learning Larval Zebrafish Behavior Latent Embeddings (Lanxin Xu et al., 2024)</a></li><li><a href=#328--21115-temporal-spatial-object-relations-modeling-for-vision-and-language-navigation-bowen-huang-et-al-2024>(3/28 | 21/115) Temporal-Spatial Object Relations Modeling for Vision-and-Language Navigation (Bowen Huang et al., 2024)</a></li><li><a href=#428--22115-an-embarrassingly-simple-defense-against-backdoor-attacks-on-ssl-aryan-satpathy-et-al-2024>(4/28 | 22/115) An Embarrassingly Simple Defense Against Backdoor Attacks On SSL (Aryan Satpathy et al., 2024)</a></li><li><a href=#528--23115-vlm-cpl-consensus-pseudo-labels-from-vision-language-models-for-human-annotation-free-pathological-image-classification-lanfeng-zhong-et-al-2024>(5/28 | 23/115) VLM-CPL: Consensus Pseudo Labels from Vision-Language Models for Human Annotation-Free Pathological Image Classification (Lanfeng Zhong et al., 2024)</a></li><li><a href=#628--24115-deep-domain-adaptation-a-sim2real-neural-approach-for-improving-eye-tracking-systems-viet-dung-nguyen-et-al-2024>(6/28 | 24/115) Deep Domain Adaptation: A Sim2Real Neural Approach for Improving Eye-Tracking Systems (Viet Dung Nguyen et al., 2024)</a></li><li><a href=#728--25115-once-for-both-single-stage-of-importance-and-sparsity-search-for-vision-transformer-compression-hancheng-ye-et-al-2024>(7/28 | 25/115) Once for Both: Single Stage of Importance and Sparsity Search for Vision Transformer Compression (Hancheng Ye et al., 2024)</a></li><li><a href=#828--26115-in-context-matting-he-guo-et-al-2024>(8/28 | 26/115) In-Context Matting (He Guo et al., 2024)</a></li><li><a href=#928--27115-idat-inverse-distillation-adapter-tuning-jiacheng-ruan-et-al-2024>(9/28 | 27/115) iDAT: inverse Distillation Adapter-Tuning (Jiacheng Ruan et al., 2024)</a></li><li><a href=#1028--28115-diffusion-based-aesthetic-qr-code-generation-via-scanning-robust-perceptual-guidance-jia-wei-liao-et-al-2024>(10/28 | 28/115) Diffusion-based Aesthetic QR Code Generation via Scanning-Robust Perceptual Guidance (Jia-Wei Liao et al., 2024)</a></li><li><a href=#1128--29115-boosting-few-shot-learning-via-attentive-feature-regularization-xingyu-zhu-et-al-2024>(11/28 | 29/115) Boosting Few-Shot Learning via Attentive Feature Regularization (Xingyu Zhu et al., 2024)</a></li><li><a href=#1228--30115-towards-human-like-machine-comprehension-few-shot-relational-learning-in-visually-rich-documents-hao-wang-et-al-2024>(12/28 | 30/115) Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents (Hao Wang et al., 2024)</a></li><li><a href=#1328--31115-x-portrait-expressive-portrait-animation-with-hierarchical-motion-attention-you-xie-et-al-2024>(13/28 | 31/115) X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention (You Xie et al., 2024)</a></li><li><a href=#1428--32115-time-series-initialization-and-conditioning-for-video-agnostic-stabilization-of-video-super-resolution-using-recurrent-networks-hiroshi-mori-et-al-2024>(14/28 | 32/115) Time-series Initialization and Conditioning for Video-agnostic Stabilization of Video Super-Resolution using Recurrent Networks (Hiroshi Mori et al., 2024)</a></li><li><a href=#1528--33115-depth-estimation-fusing-image-and-radar-measurements-with-uncertain-directions-masaya-kotani-et-al-2024>(15/28 | 33/115) Depth Estimation fusing Image and Radar Measurements with Uncertain Directions (Masaya Kotani et al., 2024)</a></li><li><a href=#1628--34115-adversarial-defense-teacher-for-cross-domain-object-detection-under-poor-visibility-conditions-kaiwen-wang-et-al-2024>(16/28 | 34/115) Adversarial Defense Teacher for Cross-Domain Object Detection under Poor Visibility Conditions (Kaiwen Wang et al., 2024)</a></li><li><a href=#1728--35115-scenexprocedural-controllable-large-scale-scene-generation-via-large-language-models-mengqi-zhou-et-al-2024>(17/28 | 35/115) SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models (Mengqi Zhou et al., 2024)</a></li><li><a href=#1828--36115-the-limits-of-perception-analyzing-inconsistencies-in-saliency-maps-in-xai-anna-stubbin-et-al-2024>(18/28 | 36/115) The Limits of Perception: Analyzing Inconsistencies in Saliency Maps in XAI (Anna Stubbin et al., 2024)</a></li><li><a href=#1928--37115-what-do-you-see-in-vehicle-comprehensive-vision-solution-for-in-vehicle-gaze-estimation-yihua-cheng-et-al-2024>(19/28 | 37/115) What Do You See in Vehicle? Comprehensive Vision Solution for In-Vehicle Gaze Estimation (Yihua Cheng et al., 2024)</a></li><li><a href=#2028--38115-spatio-temporal-bi-directional-cross-frame-memory-for-distractor-filtering-point-cloud-single-object-tracking-shaoyu-sun-et-al-2024>(20/28 | 38/115) Spatio-Temporal Bi-directional Cross-frame Memory for Distractor Filtering Point Cloud Single Object Tracking (Shaoyu Sun et al., 2024)</a></li><li><a href=#2128--39115-aocil-exemplar-free-analytic-online-class-incremental-learning-with-low-time-and-resource-consumption-huiping-zhuang-et-al-2024>(21/28 | 39/115) AOCIL: Exemplar-free Analytic Online Class Incremental Learning with Low Time and Resource Consumption (Huiping Zhuang et al., 2024)</a></li><li><a href=#2228--40115-feature-manipulation-for-ddpm-based-change-detection-zhenglin-li-et-al-2024>(22/28 | 40/115) Feature Manipulation for DDPM based Change Detection (Zhenglin Li et al., 2024)</a></li><li><a href=#2328--41115-centered-masking-for-language-image-pre-training-mingliang-liang-et-al-2024>(23/28 | 41/115) Centered Masking for Language-Image Pre-Training (Mingliang Liang et al., 2024)</a></li><li><a href=#2428--42115-contact-aware-human-motion-generation-from-textual-descriptions-sihan-ma-et-al-2024>(24/28 | 42/115) Contact-aware Human Motion Generation from Textual Descriptions (Sihan Ma et al., 2024)</a></li><li><a href=#2528--43115-an-active-learning-model-to-classify-animal-species-in-hong-kong-gareth-lamb-et-al-2024>(25/28 | 43/115) An active learning model to classify animal species in Hong Kong (Gareth Lamb et al., 2024)</a></li><li><a href=#2628--44115-maptracker-tracking-with-strided-memory-fusion-for-consistent-vector-hd-mapping-jiacheng-chen-et-al-2024>(26/28 | 44/115) MapTracker: Tracking with Strided Memory Fusion for Consistent Vector HD Mapping (Jiacheng Chen et al., 2024)</a></li><li><a href=#2728--45115-finding-needles-in-a-haystack-a-black-box-approach-to-invisible-watermark-detection-minzhou-pan-et-al-2024>(27/28 | 45/115) Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection (Minzhou Pan et al., 2024)</a></li><li><a href=#2828--46115-pnas-mot-multi-modal-object-tracking-with-pareto-neural-architecture-search-chensheng-peng-et-al-2024>(28/28 | 46/115) PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture Search (Chensheng Peng et al., 2024)</a></li></ul></li><li><a href=#csse-3>cs.SE (3)</a><ul><li><a href=#13--47115-when-llm-based-code-generation-meets-the-software-development-process-feng-lin-et-al-2024>(1/3 | 47/115) When LLM-based Code Generation Meets the Software Development Process (Feng Lin et al., 2024)</a></li><li><a href=#23--48115-codeshell-technical-report-rui-xie-et-al-2024>(2/3 | 48/115) CodeShell Technical Report (Rui Xie et al., 2024)</a></li><li><a href=#33--49115-leveraging-large-language-models-for-preliminary-security-risk-analysis-a-mission-critical-case-study-matteo-esposito-et-al-2024>(3/3 | 49/115) Leveraging Large Language Models for Preliminary Security Risk Analysis: A Mission-Critical Case Study (Matteo Esposito et al., 2024)</a></li></ul></li><li><a href=#eessiv-3>eess.IV (3)</a><ul><li><a href=#13--50115-graph-image-prior-for-unsupervised-dynamic-mri-reconstruction-zhongsen-li-et-al-2024>(1/3 | 50/115) Graph Image Prior for Unsupervised Dynamic MRI Reconstruction (Zhongsen Li et al., 2024)</a></li><li><a href=#23--51115-an-edge-detection-based-deep-learning-approach-for-tear-meniscus-height-measurement-kesheng-wang-et-al-2024>(2/3 | 51/115) An edge detection-based deep learning approach for tear meniscus height measurement (Kesheng Wang et al., 2024)</a></li><li><a href=#33--52115-3d-transunet-for-brain-metastases-segmentation-in-the-brats2023-challenge-siwei-yang-et-al-2024>(3/3 | 52/115) 3D-TransUNet for Brain Metastases Segmentation in the BraTS2023 Challenge (Siwei Yang et al., 2024)</a></li></ul></li><li><a href=#csai-10>cs.AI (10)</a><ul><li><a href=#110--53115-using-large-language-models-for-ontoclean-based-ontology-refinement-yihang-zhao-et-al-2024>(1/10 | 53/115) Using Large Language Models for OntoClean-based Ontology Refinement (Yihang Zhao et al., 2024)</a></li><li><a href=#210--54115-lamper-language-model-and-prompt-engineering-for-zero-shot-time-series-classification-zhicheng-du-et-al-2024>(2/10 | 54/115) LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series classification (Zhicheng Du et al., 2024)</a></li><li><a href=#310--55115-the-frontier-of-data-erasure-machine-unlearning-for-large-language-models-youyang-qu-et-al-2024>(3/10 | 55/115) The Frontier of Data Erasure: Machine Unlearning for Large Language Models (Youyang Qu et al., 2024)</a></li><li><a href=#410--56115-an-upload-efficient-scheme-for-transferring-knowledge-from-a-server-side-pre-trained-generator-to-clients-in-heterogeneous-federated-learning-jianqing-zhang-et-al-2024>(4/10 | 56/115) An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning (Jianqing Zhang et al., 2024)</a></li><li><a href=#510--57115-mixred-a-mix-lingual-relation-extraction-dataset-lingxing-kong-et-al-2024>(5/10 | 57/115) MixRED: A Mix-lingual Relation Extraction Dataset (Lingxing Kong et al., 2024)</a></li><li><a href=#610--58115-trustsql-a-reliability-benchmark-for-text-to-sql-models-with-diverse-unanswerable-questions-gyubok-lee-et-al-2024>(6/10 | 58/115) TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions (Gyubok Lee et al., 2024)</a></li><li><a href=#710--59115-matchseg-towards-better-segmentation-via-reference-image-matching-ruiqiang-xiao-et-al-2024>(7/10 | 59/115) MatchSeg: Towards Better Segmentation via Reference Image Matching (Ruiqiang Xiao et al., 2024)</a></li><li><a href=#810--60115-multi-agent-transformer-accelerated-rl-for-satisfaction-of-stl-specifications-albin-larsson-forsberg-et-al-2024>(8/10 | 60/115) Multi-agent transformer-accelerated RL for satisfaction of STL specifications (Albin Larsson Forsberg et al., 2024)</a></li><li><a href=#910--61115-sat-encoding-of-partial-ordering-models-for-graph-coloring-problems-daniel-faber-et-al-2024>(9/10 | 61/115) SAT Encoding of Partial Ordering Models for Graph Coloring Problems (Daniel Faber et al., 2024)</a></li><li><a href=#1010--62115-understanding-domain-size-generalization-in-markov-logic-networks-florian-chen-et-al-2024>(10/10 | 62/115) Understanding Domain-Size Generalization in Markov Logic Networks (Florian Chen et al., 2024)</a></li></ul></li><li><a href=#csro-7>cs.RO (7)</a><ul><li><a href=#17--63115-explore-until-confident-efficient-exploration-for-embodied-question-answering-allen-z-ren-et-al-2024>(1/7 | 63/115) Explore until Confident: Efficient Exploration for Embodied Question Answering (Allen Z. Ren et al., 2024)</a></li><li><a href=#27--64115-ia-imperative-learning-based-a-search-for-pathfinding-xiangyu-chen-et-al-2024>(2/7 | 64/115) iA$^<em>$: Imperative Learning-based A$^</em>$ Search for Pathfinding (Xiangyu Chen et al., 2024)</a></li><li><a href=#37--65115-aro-large-language-model-supervised-robotics-text2skill-autonomous-learning-yiwen-chen-et-al-2024>(3/7 | 65/115) ARO: Large Language Model Supervised Robotics Text2Skill Autonomous Learning (Yiwen Chen et al., 2024)</a></li><li><a href=#47--66115-driveenv-nerf-exploration-of-a-nerf-based-autonomous-driving-environment-for-real-world-performance-validation-mu-yi-shen-et-al-2024>(4/7 | 66/115) DriveEnv-NeRF: Exploration of A NeRF-Based Autonomous Driving Environment for Real-World Performance Validation (Mu-Yi Shen et al., 2024)</a></li><li><a href=#57--67115-distributed-robust-learning-based-formation-control-of-mobile-robots-based-on-bioinspired-neural-dynamics-zhe-xu-et-al-2024>(5/7 | 67/115) Distributed Robust Learning based Formation Control of Mobile Robots based on Bioinspired Neural Dynamics (Zhe Xu et al., 2024)</a></li><li><a href=#67--68115-data-driven-predictive-control-for-robust-exoskeleton-locomotion-kejun-li-et-al-2024>(6/7 | 68/115) Data-Driven Predictive Control for Robust Exoskeleton Locomotion (Kejun Li et al., 2024)</a></li><li><a href=#77--69115-risk-calibrated-human-robot-interaction-via-set-valued-intent-prediction-justin-lidard-et-al-2024>(7/7 | 69/115) Risk-Calibrated Human-Robot Interaction via Set-Valued Intent Prediction (Justin Lidard et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--70115-spatio-temporal-graph-convolutional-network-combined-large-language-model-a-deep-learning-framework-for-bike-demand-forecasting-peisen-li-et-al-2024>(1/2 | 70/115) Spatio-Temporal Graph Convolutional Network Combined Large Language Model: A Deep Learning Framework for Bike Demand Forecasting (Peisen Li et al., 2024)</a></li><li><a href=#22--71115-model-analyze-and-comprehend-user-interactions-and-various-attributes-within-a-social-media-platform-md-kaykobad-reza-et-al-2024>(2/2 | 71/115) Model, Analyze, and Comprehend User Interactions and Various Attributes within a Social Media Platform (Md Kaykobad Reza et al., 2024)</a></li></ul></li><li><a href=#cslg-13>cs.LG (13)</a><ul><li><a href=#113--72115-safe-reinforcement-learning-for-constrained-markov-decision-processes-with-stochastic-stopping-time-abhijit-mazumdar-et-al-2024>(1/13 | 72/115) Safe Reinforcement Learning for Constrained Markov Decision Processes with Stochastic Stopping Time (Abhijit Mazumdar et al., 2024)</a></li><li><a href=#213--73115-tablepuppet-a-generic-framework-for-relational-federated-learning-lijie-xu-et-al-2024>(2/13 | 73/115) TablePuppet: A Generic Framework for Relational Federated Learning (Lijie Xu et al., 2024)</a></li><li><a href=#313--74115-boarding-for-iss-imbalanced-self-supervised-discovery-of-a-scaled-autoencoder-for-mixed-tabular-datasets-samuel-stocksieker-et-al-2024>(3/13 | 74/115) Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled Autoencoder for Mixed Tabular Datasets (Samuel Stocksieker et al., 2024)</a></li><li><a href=#413--75115-role-of-locality-and-weight-sharing-in-image-based-tasks-a-sample-complexity-separation-between-cnns-lcns-and-fcns-aakash-lahoti-et-al-2024>(4/13 | 75/115) Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs (Aakash Lahoti et al., 2024)</a></li><li><a href=#513--76115-deep-gaussian-covariance-network-with-trajectory-sampling-for-data-efficient-policy-search-can-bogoclu-et-al-2024>(5/13 | 76/115) Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search (Can Bogoclu et al., 2024)</a></li><li><a href=#613--77115-towards-low-energy-adaptive-personalization-for-resource-constrained-devices-yushan-huang-et-al-2024>(6/13 | 77/115) Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices (Yushan Huang et al., 2024)</a></li><li><a href=#713--78115-bend-bagging-deep-learning-training-based-on-efficient-neural-network-diffusion-jia-wei-et-al-2024>(7/13 | 78/115) BEND: Bagging Deep Learning Training Based on Efficient Neural Network Diffusion (Jia Wei et al., 2024)</a></li><li><a href=#813--79115-on-the-fragility-of-active-learners-abhishek-ghose-et-al-2024>(8/13 | 79/115) On the Fragility of Active Learners (Abhishek Ghose et al., 2024)</a></li><li><a href=#913--80115-identifiable-latent-neural-causal-models-yuhang-liu-et-al-2024>(9/13 | 80/115) Identifiable Latent Neural Causal Models (Yuhang Liu et al., 2024)</a></li><li><a href=#1013--81115-sample-and-communication-efficient-fully-decentralized-marl-policy-evaluation-via-a-new-approach-local-td-update-fnu-hairi-et-al-2024>(10/13 | 81/115) Sample and Communication Efficient Fully Decentralized MARL Policy Evaluation via a New Approach: Local TD update (Fnu Hairi et al., 2024)</a></li><li><a href=#1113--82115-initialisation-and-topology-effects-in-decentralised-federated-learning-arash-badie-modiri-et-al-2024>(11/13 | 82/115) Initialisation and Topology Effects in Decentralised Federated Learning (Arash Badie-Modiri et al., 2024)</a></li><li><a href=#1213--83115-convection-diffusion-equation-a-theoretically-certified-framework-for-neural-networks-tangjun-wang-et-al-2024>(12/13 | 83/115) Convection-Diffusion Equation: A Theoretically Certified Framework for Neural Networks (Tangjun Wang et al., 2024)</a></li><li><a href=#1313--84115-g-acil-analytic-learning-for-exemplar-free-generalized-class-incremental-learning-huiping-zhuang-et-al-2024>(13/13 | 84/115) G-ACIL: Analytic Learning for Exemplar-Free Generalized Class Incremental Learning (Huiping Zhuang et al., 2024)</a></li></ul></li><li><a href=#eesssy-10>eess.SY (10)</a><ul><li><a href=#110--85115-scaling-learning-based-policy-optimization-for-temporal-tasks-via-dropout-navid-hashemi-et-al-2024>(1/10 | 85/115) Scaling Learning based Policy Optimization for Temporal Tasks via Dropout (Navid Hashemi et al., 2024)</a></li><li><a href=#210--86115-a-fairness-oriented-reinforcement-learning-approach-for-the-operation-and-control-of-shared-micromobility-services-luca-vittorio-piron-et-al-2024>(2/10 | 86/115) A Fairness-Oriented Reinforcement Learning Approach for the Operation and Control of Shared Micromobility Services (Luca Vittorio Piron et al., 2024)</a></li><li><a href=#310--87115-improved-soft-k-means-clustering-algorithm-for-balancing-energy-consumption-in-wireless-sensor-networks-botao-zhu-et-al-2024>(3/10 | 87/115) Improved Soft-k-Means Clustering Algorithm for Balancing Energy Consumption in Wireless Sensor Networks (Botao Zhu et al., 2024)</a></li><li><a href=#410--88115-from-raw-data-to-safety-reducing-conservatism-by-set-expansion-mohammad-bajelani-et-al-2024>(4/10 | 88/115) From Raw Data to Safety: Reducing Conservatism by Set Expansion (Mohammad Bajelani et al., 2024)</a></li><li><a href=#510--89115-a-modular-safety-filter-for-safety-certified-cyber-physical-systems-mohammad-bajelani-et-al-2024>(5/10 | 89/115) A Modular Safety Filter for Safety-Certified Cyber-Physical Systems (Mohammad Bajelani et al., 2024)</a></li><li><a href=#610--90115-tjcct-a-two-timescale-approach-for-uav-assisted-mobile-edge-computing-zemin-sun-et-al-2024>(6/10 | 90/115) TJCCT: A Two-timescale Approach for UAV-assisted Mobile Edge Computing (Zemin Sun et al., 2024)</a></li><li><a href=#710--91115-small-noise-analysis-of-non-parametric-closed-loop-identification-mohamed-abdalmoaty-et-al-2024>(7/10 | 91/115) Small Noise Analysis of Non-Parametric Closed-Loop Identification (Mohamed Abdalmoaty et al., 2024)</a></li><li><a href=#810--92115-causal-tracking-of-distributions-in-wasserstein-space-a-model-predictive-control-scheme-max-emerick-et-al-2024>(8/10 | 92/115) Causal Tracking of Distributions in Wasserstein Space: A Model Predictive Control Scheme (Max Emerick et al., 2024)</a></li><li><a href=#910--93115-real-time-reconfiguration-and-connectivity-maintenance-for-auvs-network-under-external-disturbances-using-distributed-nonlinear-model-predictive-control-nhat-minh-nguyen-et-al-2024>(9/10 | 93/115) Real-Time Reconfiguration and Connectivity Maintenance for AUVs Network Under External Disturbances using Distributed Nonlinear Model Predictive Control (Nhat Minh Nguyen et al., 2024)</a></li><li><a href=#1010--94115-safe-and-stable-formation-control-with-distributed-multi-agents-using-adaptive-control-and-control-barrier-functions-jose-a-solano-castellanos-et-al-2024>(10/10 | 94/115) Safe and Stable Formation Control with Distributed Multi-Agents Using Adaptive Control and Control Barrier Functions (Jose A. Solano-Castellanos et al., 2024)</a></li></ul></li><li><a href=#csit-4>cs.IT (4)</a><ul><li><a href=#14--95115-energy-efficient-design-of-active-star-ris-aided-swipt-systems-sajad-faramarzi-et-al-2024>(1/4 | 95/115) Energy Efficient Design of Active STAR-RIS-Aided SWIPT Systems (Sajad Faramarzi et al., 2024)</a></li><li><a href=#24--96115-block-orthogonal-sparse-superposition-codes-for--sfl3--communications-low-error-rate-low-latency-and-low-power-consumption-donghwa-han-et-al-2024>(2/4 | 96/115) Block Orthogonal Sparse Superposition Codes for $ \sf{L}^3 $ Communications: Low Error Rate, Low Latency, and Low Power Consumption (Donghwa Han et al., 2024)</a></li><li><a href=#34--97115-differentiable-information-bottleneck-for-deterministic-multi-view-clustering-xiaoqiang-yan-et-al-2024>(3/4 | 97/115) Differentiable Information Bottleneck for Deterministic Multi-view Clustering (Xiaoqiang Yan et al., 2024)</a></li><li><a href=#44--98115-permutation-recovery-problem-against-deletion-errors-for-dna-data-storage-shubhransh-singhvi-et-al-2024>(4/4 | 98/115) Permutation Recovery Problem against Deletion Errors for DNA Data Storage (Shubhransh Singhvi et al., 2024)</a></li></ul></li><li><a href=#cshc-1>cs.HC (1)</a><ul><li><a href=#11--99115-negotiating-the-shared-agency-between-humans--ai-in-the-recommender-system-mengke-wu-et-al-2024>(1/1 | 99/115) Negotiating the Shared Agency between Humans & AI in the Recommender System (Mengke Wu et al., 2024)</a></li></ul></li><li><a href=#cscr-1>cs.CR (1)</a><ul><li><a href=#11--100115-a-hybrid-llm-workflow-can-help-identify-user-privilege-related-variables-in-programs-of-any-size-haizhou-wang-et-al-2024>(1/1 | 100/115) A hybrid LLM workflow can help identify user privilege related variables in programs of any size (Haizhou Wang et al., 2024)</a></li></ul></li><li><a href=#statme-2>stat.ME (2)</a><ul><li><a href=#12--101115-optimized-model-selection-for-estimating-treatment-effects-from-costly-simulations-of-the-us-opioid-epidemic-abdulrahman-a-ahmed-et-al-2024>(1/2 | 101/115) Optimized Model Selection for Estimating Treatment Effects from Costly Simulations of the US Opioid Epidemic (Abdulrahman A. Ahmed et al., 2024)</a></li><li><a href=#22--102115-supervised-learning-via-ensembles-of-diverse-functional-representations-the-functional-voting-classifier-donato-riccio-et-al-2024>(2/2 | 102/115) Supervised Learning via Ensembles of Diverse Functional Representations: the Functional Voting Classifier (Donato Riccio et al., 2024)</a></li></ul></li><li><a href=#csni-3>cs.NI (3)</a><ul><li><a href=#13--103115-loam-low-latency-communication-caching-and-computation-placement-in-data-intensive-computing-networks-jinkun-zhang-et-al-2024>(1/3 | 103/115) LOAM: Low-latency Communication, Caching, and Computation Placement in Data-Intensive Computing Networks (Jinkun Zhang et al., 2024)</a></li><li><a href=#23--104115-a-novel-non-terrestrial-networks-architecture-all-optical-leo-constellations-with-high-altitude-ground-stations-pablo-g-madoery-et-al-2024>(2/3 | 104/115) A Novel Non-Terrestrial Networks Architecture: All Optical LEO Constellations with High-Altitude Ground Stations (Pablo G. Madoery et al., 2024)</a></li><li><a href=#33--105115-delay-optimal-forwarding-and-computation-offloading-for-service-chain-tasks-jinkun-zhang-et-al-2024>(3/3 | 105/115) Delay-Optimal Forwarding and Computation Offloading for Service Chain Tasks (Jinkun Zhang et al., 2024)</a></li></ul></li><li><a href=#cscy-2>cs.CY (2)</a><ul><li><a href=#12--106115-deep-learning-approach-to-forecasting-covid-19-cases-in-residential-buildings-of-hong-kong-public-housing-estates-the-role-of-environment-and-sociodemographics-e-leung-et-al-2024>(1/2 | 106/115) Deep Learning Approach to Forecasting COVID-19 Cases in Residential Buildings of Hong Kong Public Housing Estates: The Role of Environment and Sociodemographics (E. Leung et al., 2024)</a></li><li><a href=#22--107115-optimal-hospital-capacity-management-during-demand-surges-felix-parker-et-al-2024>(2/2 | 107/115) Optimal Hospital Capacity Management During Demand Surges (Felix Parker et al., 2024)</a></li></ul></li><li><a href=#cssy-1>cs.SY (1)</a><ul><li><a href=#11--108115-passivity-based-attack-identification-and-mitigation-with-event-triggered-observer-feedback-and-switching-controller-pushkal-purohit-et-al-2024>(1/1 | 108/115) Passivity-based Attack Identification and Mitigation with Event-triggered Observer Feedback and Switching Controller (Pushkal Purohit et al., 2024)</a></li></ul></li><li><a href=#csir-1>cs.IR (1)</a><ul><li><a href=#11--109115-queryexplorer-an-interactive-query-generation-assistant-for-search-and-exploration-kaustubh-d-dhole-et-al-2024>(1/1 | 109/115) QueryExplorer: An Interactive Query Generation Assistant for Search and Exploration (Kaustubh D. Dhole et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--110115-improved-methods-of-task-assignment-and-resource-allocation-with-preemption-in-edge-computing-systems-caroline-rublein-et-al-2024>(1/1 | 110/115) Improved Methods of Task Assignment and Resource Allocation with Preemption in Edge Computing Systems (Caroline Rublein et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#11--111115-space-group-informed-transformer-for-crystalline-materials-generation-zhendong-cao-et-al-2024>(1/1 | 111/115) Space Group Informed Transformer for Crystalline Materials Generation (Zhendong Cao et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--112115-utilizing-motion-matching-with-deep-reinforcement-learning-for-target-location-tasks-jeongmin-lee-et-al-2024>(1/1 | 112/115) Utilizing Motion Matching with Deep Reinforcement Learning for Target Location Tasks (Jeongmin Lee et al., 2024)</a></li></ul></li><li><a href=#mathna-1>math.NA (1)</a><ul><li><a href=#11--113115-conservative-surrogate-models-for-optimization-with-the-active-subspace-method-philippe-andré-luneau-2024>(1/1 | 113/115) Conservative Surrogate Models for Optimization with the Active Subspace Method (Philippe-André Luneau, 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#11--114115-distance-adjustment-of-a-graph-drawing-stress-model-yosuke-onoue-2024>(1/1 | 114/115) Distance Adjustment of a Graph Drawing Stress Model (Yosuke Onoue, 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--115115-team-coordination-on-graphs-problem-analysis-and-algorithms-manshi-limbu-et-al-2024>(1/1 | 115/115) Team Coordination on Graphs: Problem, Analysis, and Algorithms (Manshi Limbu et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>