<!doctype html><html><head><title>arXiv @ 2024.03.26</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240326000000/"><meta property="og:site_name" content="Akitenkrad's Blog"><meta property="og:title" content="arXiv @ 2024.03.26"><meta property="og:description" content="Primary Categories cs.AI (8) cs.CE (1) cs.CG (1) cs.CL (15) cs.CR (3) cs.CV (45) cs.CY (1) cs.DB (1) cs.DC (1) cs.DL (1) cs.DS (2) cs.HC (1) cs.IR (2) cs.IT (2) cs.LG (18) cs.MA (2) cs.NE (1) cs.NI (4) cs.RO (8) cs.SD (2) cs.SE (5) eess.IV (5) eess.SY (7) math.CT (1) math.DG (1) math.NA (1) physics.ao-ph (1) physics.flu-dyn (2) q-bio.PE (1) stat.ML (3) Keywords keyword cs.CL cs.CV cs.LG Adversarial Attack 1 1 Adversarial Learning 1 Adversarial Purification 1 Anomaly Detection 1 Autoencoder 1 1 Bandit Algorithm 1 Benchmarking 1 14 1 Black Box 1 1 ChatGPT 1 1 Clustering 1 1 1 Cohere 1 Contextual Embedding 1 Continual Learning 1 Continuous Time 2 Contrastive Learning 2 Convolution 4 4 Convolutional Neural Network 4 2 Data Augmentation 1 Differential Privacy 1 Diffusion Model 5 1 Disambiguation 1 Domain Adaptation 2 Emotion Recognition 1 Fairness 1 Fake News Detection 1 Federated Learning 1 1 Few-shot 1 1 Few-shot Learning 1 1 Fine-tuning 6 5 Foundation Model 1 1 GPT 1 GPT-3 1 GPT-3."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-26T00:00:00+00:00"><meta property="article:tag" content="ArXiv"><meta property="article:tag" content="Published:2024"><meta name=description content="arXiv @ 2024.03.26"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE")}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10">arXiv @ 2024.04.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/ title="arXiv @ 2024.04.11">arXiv @ 2024.04.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/ title="arXiv @ 2024.04.12">arXiv @ 2024.04.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/ title="arXiv @ 2024.04.13">arXiv @ 2024.04.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240414000000/ title="arXiv @ 2024.04.14">arXiv @ 2024.04.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/ title="arXiv @ 2024.04.15">arXiv @ 2024.04.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/ title="arXiv @ 2024.04.16">arXiv @ 2024.04.16</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240326000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Tuesday, Mar 26, 2024</p></div><div class=title><h1>arXiv @ 2024.03.26</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#csai-8>cs.AI (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#cscl-15>cs.CL (15)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#cscr-3>cs.CR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#cscv-45>cs.CV (45)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#csdl-1>cs.DL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#cshc-1>cs.HC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#csir-2>cs.IR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#cslg-18>cs.LG (18)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#csni-4>cs.NI (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#csro-8>cs.RO (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#csse-5>cs.SE (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#eessiv-5>eess.IV (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#eesssy-7>eess.SY (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#mathct-1>math.CT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#mathdg-1>math.DG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#mathna-1>math.NA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#physicsao-ph-1>physics.ao-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#physicsflu-dyn-2>physics.flu-dyn (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#q-biope-1>q-bio.PE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td>1</td><td>1</td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Purification</td><td></td><td>1</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>1</td><td></td></tr><tr><td>Autoencoder</td><td></td><td>1</td><td>1</td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>1</td></tr><tr><td>Benchmarking</td><td>1</td><td>14</td><td>1</td></tr><tr><td>Black Box</td><td></td><td>1</td><td>1</td></tr><tr><td>ChatGPT</td><td>1</td><td>1</td><td></td></tr><tr><td>Clustering</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Cohere</td><td>1</td><td></td><td></td></tr><tr><td>Contextual Embedding</td><td>1</td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td>1</td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td>2</td></tr><tr><td>Contrastive Learning</td><td></td><td>2</td><td></td></tr><tr><td>Convolution</td><td></td><td>4</td><td>4</td></tr><tr><td>Convolutional Neural Network</td><td></td><td>4</td><td>2</td></tr><tr><td>Data Augmentation</td><td></td><td>1</td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td>1</td></tr><tr><td>Diffusion Model</td><td></td><td>5</td><td>1</td></tr><tr><td>Disambiguation</td><td>1</td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>2</td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td>1</td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td>1</td></tr><tr><td>Fake News Detection</td><td>1</td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td>1</td><td>1</td></tr><tr><td>Few-shot</td><td>1</td><td>1</td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>6</td><td>5</td><td></td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td>1</td></tr><tr><td>GPT</td><td>1</td><td></td><td></td></tr><tr><td>GPT-3</td><td>1</td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td>2</td><td></td></tr><tr><td>Graph</td><td>2</td><td>4</td><td>6</td></tr><tr><td>Graph Classification</td><td></td><td></td><td>1</td></tr><tr><td>Graph Convolutional Network</td><td></td><td>1</td><td>1</td></tr><tr><td>Graph Neural Network</td><td></td><td>1</td><td>3</td></tr><tr><td>Image2text</td><td></td><td>3</td><td></td></tr><tr><td>In-context Learning</td><td>2</td><td></td><td></td></tr><tr><td>Instruction Following</td><td>2</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>2</td><td></td></tr><tr><td>Knowledge Graph</td><td>1</td><td></td><td>1</td></tr><tr><td>Knowledge Transfer</td><td></td><td>1</td><td></td></tr><tr><td>LLaMA</td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>13</td><td>3</td><td>2</td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td>1</td></tr><tr><td>Meta Learning</td><td></td><td>1</td><td></td></tr><tr><td>Multi-modal</td><td></td><td>13</td><td></td></tr><tr><td>Named Entity Recognition</td><td>2</td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>2</td></tr><tr><td>Object Detection</td><td></td><td>2</td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>1</td><td>2</td></tr><tr><td>Out-of-domain</td><td></td><td>1</td><td></td></tr><tr><td>Parameter Sharing</td><td></td><td></td><td>1</td></tr><tr><td>Perplexity</td><td>1</td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>2</td><td></td></tr><tr><td>Prompt</td><td>4</td><td>3</td><td></td></tr><tr><td>Pruning</td><td></td><td>1</td><td></td></tr><tr><td>Question Answering</td><td>2</td><td></td><td></td></tr><tr><td>Recommendation</td><td></td><td>2</td><td></td></tr><tr><td>Recommender System</td><td></td><td></td><td>1</td></tr><tr><td>Reconstruction Loss</td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td>1</td><td>2</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td></td><td>2</td></tr><tr><td>Representation Learning</td><td></td><td>1</td><td>1</td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>3</td><td>1</td></tr><tr><td>Self-Distillation</td><td></td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td>2</td><td>6</td><td>4</td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td>1</td></tr><tr><td>Semi-Supervised Learning</td><td></td><td>1</td><td></td></tr><tr><td>Simulation</td><td></td><td>1</td><td></td></tr><tr><td>Simulator</td><td></td><td>1</td><td></td></tr><tr><td>Summarization</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Supervised Learning</td><td>2</td><td>1</td><td>2</td></tr><tr><td>Text Embedding</td><td></td><td>2</td><td></td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>3</td><td></td></tr><tr><td>Tokenization</td><td></td><td></td><td>1</td></tr><tr><td>Transformer</td><td>1</td><td>8</td><td>3</td></tr><tr><td>Unsupervised Learning</td><td>1</td><td>6</td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td>8</td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>3</td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td></td></tr><tr><td>Word Sense Disambiguation</td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>3</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=csai-8>cs.AI (8)</h2><h3 id=18--1146-can-language-models-pretend-solvers-logic-code-simulation-with-llms-minyu-chen-et-al-2024>(1/8 | 1/146) Can Language Models Pretend Solvers? Logic Code Simulation with LLMs (Minyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minyu Chen, Guoqiang Li, Ling-I Wu, Ruibang Liu, Yuxin Su, Xi Chang, Jianxin Xue. (2024)<br><strong>Can Language Models Pretend Solvers? Logic Code Simulation with LLMs</strong><br><button class=copy-to-clipboard title="Can Language Models Pretend Solvers? Logic Code Simulation with LLMs" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LO, cs-SE, cs.AI<br>Keyword Score: 100<br>Keywords: Simulation, Simulator, GPT, GPT-4, GPT-4 turbo, Transformer, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16097v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16097v2.pdf filename=2403.16097v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated significant potential in addressing logic problems. capitalizing on the great capabilities of <b>LLMs</b> for code-related activities, several frameworks leveraging logical solvers for logic <b>reasoning</b> have been proposed recently. While existing research predominantly focuses on viewing <b>LLMs</b> as natural language logic solvers or translators, their roles as logic code interpreters and executors have received limited attention. This study delves into a novel aspect, namely logic code <b>simulation,</b> which forces <b>LLMs</b> to emulate logical solvers in predicting the results of logical programs. To further investigate this novel task, we formulate our three research questions: Can <b>LLMs</b> efficiently simulate the outputs of logic codes? What strength arises along with logic code simulation? And what pitfalls? To address these inquiries, we curate three novel datasets tailored for the logic code <b>simulation</b> task and undertake thorough experiments to establish the baseline performance of <b>LLMs</b> in code <b>simulation.</b> Subsequently, we introduce a pioneering <b>LLM-based</b> code <b>simulation</b> technique, Dual Chains of Logic (DCoL). This technique advocates a dual-path thinking approach for <b>LLMs,</b> which has demonstrated state-of-the-art performance compared to other <b>LLM</b> <b>prompt</b> strategies, achieving a notable improvement in accuracy by 7.06% with <b>GPT-4-Turbo.</b></p></p class="citation"></blockquote><h3 id=28--2146-sshpool-the-separated-subgraph-based-hierarchical-pooling-zhuo-xu-et-al-2024>(2/8 | 2/146) SSHPool: The Separated Subgraph-based Hierarchical Pooling (Zhuo Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuo Xu, Lixin Cui, Yue Wang, Hangyuan Du, Lu Bai, Edwin R. Hancock. (2024)<br><strong>SSHPool: The Separated Subgraph-based Hierarchical Pooling</strong><br><button class=copy-to-clipboard title="SSHPool: The Separated Subgraph-based Hierarchical Pooling" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 43<br>Keywords: Graph Classification, Graph, Graph Neural Network, Graph Neural Network, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16133v1.pdf filename=2403.16133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we develop a novel local <b>graph</b> <b>pooling</b> <b>method,</b> namely the Separated Subgraph-based Hierarchical Pooling (SSHPool), for <b>graph</b> <b>classification.</b> <b>To</b> this end, we commence by assigning the nodes of a sample <b>graph</b> <b>into</b> <b>different</b> clusters, resulting in a family of separated subgraphs. We individually employ a local <b>graph</b> <b>convolution</b> <b>units</b> as the local structure to further compress each subgraph into a coarsened node, transforming the original <b>graph</b> <b>into</b> <b>a</b> coarsened <b>graph.</b> <b>Since</b> <b>these</b> subgraphs are separated by different clusters and the structural information cannot be propagated between them, the local <b>convolution</b> operation can significantly avoid the over-smoothing problem arising in most existing <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs).</b> By hierarchically performing the proposed procedures on the resulting coarsened <b>graph,</b> <b>the</b> <b>proposed</b> SSHPool can effectively extract the hierarchical global feature of the original <b>graph</b> <b>structure,</b> <b>encapsulating</b> rich intrinsic structural characteristics. Furthermore, we develop an end-to-end <b>GNN</b> framework associated with the proposed SSHPool module for <b>graph</b> <b>classification.</b> <b>Experimental</b> results demonstrate the superior performance of the proposed model on real-world datasets, significantly outperforming state-of-the-art <b>GNN</b> methods in terms of the classification accuracies.</p></p class="citation"></blockquote><h3 id=38--3146-engineering-safety-requirements-for-autonomous-driving-with-large-language-models-ali-nouri-et-al-2024>(3/8 | 3/146) Engineering Safety Requirements for Autonomous Driving with Large Language Models (Ali Nouri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Nouri, Beatriz Cabrero-Daniel, Fredrik Törner, Hȧkan Sivencrona, Christian Berger. (2024)<br><strong>Engineering Safety Requirements for Autonomous Driving with Large Language Models</strong><br><button class=copy-to-clipboard title="Engineering Safety Requirements for Autonomous Driving with Large Language Models" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: Natural Language Understanding, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16289v1.pdf filename=2403.16289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Changes and updates in the requirement artifacts, which can be frequent in the automotive domain, are a challenge for SafetyOps. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> with their impressive <b>natural</b> <b>language</b> <b>understanding</b> and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update. In this study, we propose a prototype of a pipeline of <b>prompts</b> and <b>LLMs</b> that receives an item definition and outputs solutions in the form of safety requirements. This pipeline also performs a review of the requirement dataset and identifies redundant or contradictory requirements. We first identified the necessary characteristics for performing HARA and then defined tests to assess an <b>LLM&rsquo;s</b> capability in meeting these criteria. We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively. Finally, the prototype was implemented at a case company and the responsible team evaluated its efficiency.</p></p class="citation"></blockquote><h3 id=48--4146-rumor-detection-with-a-novel-graph-neural-network-approach-tianrui-liu-et-al-2024>(4/8 | 4/146) Rumor Detection with a novel graph neural network approach (Tianrui Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianrui Liu, Qi Cai, Changxin Xu, Bo Hong, Fanghao Ni, Yuxin Qiao, Tsungwei Yang. (2024)<br><strong>Rumor Detection with a novel graph neural network approach</strong><br><button class=copy-to-clipboard title="Rumor Detection with a novel graph neural network approach" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16206v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16206v2.pdf filename=2403.16206v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The wide spread of rumors on social media has caused a negative impact on people&rsquo;s daily life, leading to potential panic, fear, and mental health problems for the public. How to debunk rumors as early as possible remains a challenging problem. Existing studies mainly leverage information propagation structure to detect rumors, while very few works focus on correlation among users that they may coordinate to spread rumors in order to gain large popularity. In this paper, we propose a new detection model, that jointly learns both the representations of user correlation and information propagation to detect rumors on social media. Specifically, we leverage <b>graph</b> <b>neural</b> <b>networks</b> to learn the representations of user correlation from a bipartite <b>graph</b> <b>that</b> <b>describes</b> the correlations between users and source tweets, and the representations of information propagation with a tree structure. Then we combine the learned representations from these two modules to classify the rumors. Since malicious users intend to subvert our model after deployment, we further develop a greedy attack scheme to analyze the cost of three <b>adversarial</b> <b>attacks:</b> <b>graph</b> <b>attack,</b> <b>comment</b> attack, and joint attack. Evaluation results on two public datasets illustrate that the proposed MODEL outperforms the state-of-the-art rumor detection models. We also demonstrate our method performs well for early rumor detection. Moreover, the proposed detection method is more robust to <b>adversarial</b> <b>attacks</b> compared to the best existing method. Importantly, we show that it requires a high cost for attackers to subvert user correlation pattern, demonstrating the importance of considering user correlation for rumor detection.</p></p class="citation"></blockquote><h3 id=58--5146-a-temporal-graph-network-framework-for-dynamic-recommendation-yejin-kim-et-al-2024>(5/8 | 5/146) A Temporal Graph Network Framework for Dynamic Recommendation (Yejin Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yejin Kim, Youngbin Lee, Vincent Yuan, Annika Lee, Yongjae Lee. (2024)<br><strong>A Temporal Graph Network Framework for Dynamic Recommendation</strong><br><button class=copy-to-clipboard title="A Temporal Graph Network Framework for Dynamic Recommendation" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 23<br>Keywords: Graph, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16066v1.pdf filename=2403.16066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommender</b> <b>systems,</b> crucial for user engagement on platforms like e-commerce and streaming services, often lag behind users&rsquo; evolving preferences due to static data reliance. After Temporal <b>Graph</b> Networks (TGNs) were proposed, various studies have shown that TGN can significantly improve situations where the features of nodes and edges dynamically change over time. However, despite its promising capabilities, it has not been directly applied in <b>recommender</b> <b>systems</b> to date. Our study bridges this gap by directly implementing Temporal <b>Graph</b> Networks (TGN) in <b>recommender</b> <b>systems,</b> a first in this field. Using real-world datasets and a range of <b>graph</b> and history embedding methods, we show TGN&rsquo;s adaptability, confirming its effectiveness in dynamic <b>recommendation</b> scenarios.</p></p class="citation"></blockquote><h3 id=68--6146-cyber-security-knowledge-graph-generation-by-hierarchical-nonnegative-matrix-factorization-ryan-barron-et-al-2024>(6/8 | 6/146) Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix Factorization (Ryan Barron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan Barron, Maksim E. Eren, Manish Bhattarai, Selma Wanna, Nicholas Solovyev, Kim Rasmussen, Boian S. Alexandrov, Charles Nicholas, Cynthia Matuszek. (2024)<br><strong>Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix Factorization</strong><br><button class=copy-to-clipboard title="Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix Factorization" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 16<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16222v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16222v2.pdf filename=2403.16222v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Much of human <b>knowledge</b> <b>in</b> cybersecurity is encapsulated within the ever-growing volume of scientific papers. As this textual data continues to expand, the importance of document organization methods becomes increasingly crucial for extracting actionable insights hidden within large text datasets. <b>Knowledge</b> <b>Graphs</b> <b>(KGs)</b> serve as a means to store factual information in a structured manner, providing explicit, interpretable <b>knowledge</b> <b>that</b> includes domain-specific information from the cybersecurity scientific literature. One of the challenges in constructing a <b>KG</b> from scientific literature is the extraction of ontology from unstructured text. In this paper, we address this topic and introduce a method for building a <b>multi-modal</b> <b>KG</b> by extracting structured ontology from scientific papers. We demonstrate this concept in the cybersecurity domain. One modality of the <b>KG</b> represents observable information from the papers, such as the categories in which they were published or the authors. The second modality uncovers latent (hidden) patterns of text extracted through hierarchical and semantic non-negative matrix factorization (NMF), such as named entities, topics or clusters, and keywords. We illustrate this concept by consolidating more than two million scientific papers uploaded to arXiv into the cyber-domain, using hierarchical and semantic NMF, and by building a cyber-domain-specific <b>KG.</b></p></p class="citation"></blockquote><h3 id=78--7146-evaluating-fairness-metrics-across-borders-from-human-perceptions-yuya-sasaki-et-al-2024>(7/8 | 7/146) Evaluating Fairness Metrics Across Borders from Human Perceptions (Yuya Sasaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuya Sasaki, Sohei Tokuno, Haruka Maeda, Osamu Sakura. (2024)<br><strong>Evaluating Fairness Metrics Across Borders from Human Perceptions</strong><br><button class=copy-to-clipboard title="Evaluating Fairness Metrics Across Borders from Human Perceptions" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16101v1.pdf filename=2403.16101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Which <b>fairness</b> metrics are appropriately applicable in your contexts? There may be instances of discordance regarding the perception of <b>fairness,</b> even when the outcomes comply with established <b>fairness</b> metrics. Several surveys have been conducted to evaluate <b>fairness</b> metrics with human perceptions of <b>fairness.</b> However, these surveys were limited in scope, including only a few hundred participants within a single country. In this study, we conduct an international survey to evaluate the appropriateness of various <b>fairness</b> metrics in decision-making scenarios. We collected responses from 1,000 participants in each of China, France, Japan, and the United States, amassing a total of 4,000 responses, to analyze the preferences of <b>fairness</b> metrics. Our survey consists of three distinct scenarios paired with four <b>fairness</b> metrics, and each participant answers their preference for the <b>fairness</b> metric in each case. This investigation explores the relationship between personal attributes and the choice of <b>fairness</b> metrics, uncovering a significant influence of national context on these preferences.</p></p class="citation"></blockquote><h3 id=88--8146-landmark-guided-cross-speaker-lip-reading-with-mutual-information-regularization-linzhi-wu-et-al-2024>(8/8 | 8/146) Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization (Linzhi Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linzhi Wu, Xingyu Zhang, Yakun Zhang, Changyan Zheng, Tiejun Liu, Liang Xie, Ye Yan, Erwei Yin. (2024)<br><strong>Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization</strong><br><button class=copy-to-clipboard title="Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CV, cs-MM, cs.AI<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16071v1.pdf filename=2403.16071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lip reading, the process of interpreting silent speech from visual lip movements, has gained rising attention for its wide range of realistic applications. Deep learning approaches greatly improve current lip reading systems. However, lip reading in cross-speaker scenarios where the speaker identity changes, poses a challenging problem due to inter-speaker variability. A well-trained lip reading system may perform poorly when handling a brand new speaker. To learn a speaker-robust lip reading model, a key insight is to reduce visual variations across speakers, avoiding the model overfitting to specific speakers. In this work, in view of both input visual clues and latent representations based on a hybrid CTC/attention architecture, we propose to exploit the lip landmark-guided fine-grained visual clues instead of frequently-used mouth-cropped images as input features, diminishing speaker-specific appearance characteristics. Furthermore, a max-min <b>mutual</b> <b>information</b> regularization approach is proposed to capture speaker-insensitive latent representations. Experimental evaluations on public lip reading datasets demonstrate the effectiveness of the proposed approach under the intra-speaker and inter-speaker conditions.</p></p class="citation"></blockquote><h2 id=csse-5>cs.SE (5)</h2><h3 id=15--9146-combining-fine-tuning-and-llm-based-agents-for-intuitive-smart-contract-auditing-with-justifications-wei-ma-et-al-2024>(1/5 | 9/146) Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications (Wei Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Ma, Daoyuan Wu, Yuqiang Sun, Tianwen Wang, Shangqing Liu, Jian Zhang, Yue Xue, Yang Liu. (2024)<br><strong>Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications</strong><br><button class=copy-to-clipboard title="Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 100<br>Keywords: Fine-tuning, Fine-tuning, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16073v1.pdf filename=2403.16073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Smart contracts are decentralized applications built atop blockchains like Ethereum. Recent research has shown that <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have potential in auditing smart contracts, but the state-of-the-art indicates that even <b>GPT-4</b> can achieve only 30% precision (when both decision and justification are correct). This is likely because off-the-shelf <b>LLMs</b> were primarily pre-trained on a general text/code corpus and not <b>fine-tuned</b> on the specific domain of Solidity smart contract auditing. In this paper, we propose TrustLLM, a general framework that combines <b>fine-tuning</b> and <b>LLM-based</b> agents for intuitive smart contract auditing with justifications. Specifically, TrustLLM is inspired by the observation that expert human auditors first perceive what could be wrong and then perform a detailed analysis of the code to identify the cause. As such, TrustLLM employs a two-stage <b>fine-tuning</b> approach: it first tunes a Detector model to make decisions and then tunes a Reasoner model to generate causes of vulnerabilities. However, <b>fine-tuning</b> alone faces challenges in accurately identifying the optimal cause of a vulnerability. Therefore, we introduce two <b>LLM-based</b> agents, the Ranker and Critic, to iteratively select and debate the most suitable cause of vulnerability based on the output of the <b>fine-tuned</b> Reasoner model. To evaluate TrustLLM, we collected a balanced dataset with 1,734 positive and 1,810 negative samples to <b>fine-tune</b> TrustLLM. We then compared it with traditional <b>fine-tuned</b> models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) as well as <b>prompt</b> <b>learning-based</b> <b>LLMs</b> <b>(GPT4,</b> <b>GPT-3.5,</b> and CodeLlama-13b/34b). On a dataset of 263 real smart contract vulnerabilities, TrustLLM achieves an F1 score of 91.21% and an accuracy of 91.11%. The causes generated by TrustLLM achieved a consistency of about 38% compared to the ground truth causes.</p></p class="citation"></blockquote><h3 id=25--10146-llms-as-compiler-for-arabic-programming-language-serry-sibaee-et-al-2024>(2/5 | 10/146) LLMs as Compiler for Arabic Programming Language (Serry Sibaee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Serry Sibaee, Omar Najar, Lahouri Ghouti, Anis Koubaa. (2024)<br><strong>LLMs as Compiler for Arabic Programming Language</strong><br><button class=copy-to-clipboard title="LLMs as Compiler for Arabic Programming Language" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-LG, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16087v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16087v1.pdf filename=2403.16087v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we introduce APL (Arabic Programming Language) that uses <b>Large</b> <b>language</b> <b>models</b> <b>(LLM)</b> as semi-compiler to covert Arabic text code to python code then run the code. Designing a full pipeline from the structure of the APL text then a <b>prompt</b> (using <b>prompt</b> engineering) then running the prodcued python code using PyRunner. This project has a three parts first python library, a playground with simple interface and this research paper.</p></p class="citation"></blockquote><h3 id=35--11146-coupled-requirements-driven-testing-of-cps-from-simulation-to-reality-ankit-agrawal-et-al-2024>(3/5 | 11/146) Coupled Requirements-driven Testing of CPS: From Simulation To Reality (Ankit Agrawal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ankit Agrawal, Philipp Zech, Michael Vierhauser. (2024)<br><strong>Coupled Requirements-driven Testing of CPS: From Simulation To Reality</strong><br><button class=copy-to-clipboard title="Coupled Requirements-driven Testing of CPS: From Simulation To Reality" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16287v1.pdf filename=2403.16287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Failures in safety-critical Cyber-Physical Systems (CPS), both software and hardware-related, can lead to severe incidents impacting physical infrastructure or even harming humans. As a result, extensive <b>simulations</b> and field tests need to be conducted, as part of the verification and validation of system requirements, to ensure system safety. However, current <b>simulation</b> and field testing practices, particularly in the domain of small Unmanned Aerial Systems (sUAS), are ad-hoc and lack a thorough, structured testing process. Furthermore, there is a dearth of standard processes and methodologies to inform the design of comprehensive <b>simulation</b> and field tests. This gap in the testing process leads to the deployment of sUAS applications that are: (a) tested in <b>simulation</b> environments which do not adequately capture the real-world complexity, such as environmental factors, due to a lack of tool support; (b) not subjected to a comprehensive range of scenarios during <b>simulation</b> testing to validate the system requirements, due to the absence of a process defining the relationship between requirements and <b>simulation</b> tests; and (c) not analyzed through standard safety analysis processes, because of missing traceability between <b>simulation</b> testing artifacts and safety analysis artifacts. To address these issues, we have developed an initial framework for validating CPS, specifically focusing on sUAS and robotic applications. We demonstrate the suitability of our framework by applying it to an example from the sUAS domain. Our preliminary results confirm the applicability of our framework. We conclude with a research roadmap to outline our next research goals along with our current proposal.</p></p class="citation"></blockquote><h3 id=45--12146-coverup-coverage-guided-llm-based-test-generation-juan-altmayer-pizzorno-et-al-2024>(4/5 | 12/146) CoverUp: Coverage-Guided LLM-Based Test Generation (Juan Altmayer Pizzorno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan Altmayer Pizzorno, Emery D. Berger. (2024)<br><strong>CoverUp: Coverage-Guided LLM-Based Test Generation</strong><br><button class=copy-to-clipboard title="CoverUp: Coverage-Guided LLM-Based Test Generation" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-LG, cs-PL, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16218v1.pdf filename=2403.16218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and <b>large-language</b> <b>models</b> <b>(LLMs).</b> CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the <b>LLM</b> to focus its attention on as yet uncovered lines and branches. The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid <b>LLM</b> / search-based software testing system, CoverUp substantially improves coverage across the board. On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%). We show that CoverUp&rsquo;s iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes.</p></p class="citation"></blockquote><h3 id=55--13146-finewave-fine-grained-warning-verification-of-bugs-for-automated-static-analysis-tools-han-liu-et-al-2024>(5/5 | 13/146) FineWAVE: Fine-Grained Warning Verification of Bugs for Automated Static Analysis Tools (Han Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Liu, Jian Zhang, Cen Zhang, Xiaohan Zhang, Kaixuan Li, Sen Chen, Shang-Wei Lin, Yixiang Chen, Xinhua Li, Yang Liu. (2024)<br><strong>FineWAVE: Fine-Grained Warning Verification of Bugs for Automated Static Analysis Tools</strong><br><button class=copy-to-clipboard title="FineWAVE: Fine-Grained Warning Verification of Bugs for Automated Static Analysis Tools" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16032v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16032v1.pdf filename=2403.16032v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The continual expansion of software size and complexity has led to an increased focus on reducing defects and bugs during development. Although Automated Static Analysis Tools (ASATs) offer help, in practice, the significant number of false positives can impede developers&rsquo; productivity and confidence in the tools. Therefore, previous research efforts have explored learning-based methods to validate the reported warnings. Nevertheless, there are still some limitations. (1) The granularity of prior research is coarse, as it focuses on identifying either actionable warnings throughout extensive development histories or potential true warnings at the function level. These approaches lack specificity regarding individual bugs and warnings. (2) Machine learning-based approaches need much manual effort for feature engineering while existing deep learning-based approaches ignore key semantics between source code and warnings. (3) The small number of selected projects hinders the comprehensive evaluation of these approaches. In this paper, we proposed a fine-grained warning verification approach that is sensitive to bugs for improving the results of ASATs, namely \ourtool. Specifically, we design a novel <b>LSTM-based</b> model that captures both fine-grained semantics of source code and warnings from ASATs and highlights their correlations with cross-attention. To tackle the data scarcity of training and evaluation, we collected a large-scale dataset of 280,273 warnings, namely FineWA. It is ten times larger than the existing largest dataset. Then, we conducted extensive experiments on the dataset to evaluate FineWAVE. The experimental results demonstrate the effectiveness of our approach, with an F1-score of 97.79% for reducing false alarms and 67.06% for confirming actual warnings, which also significantly outperforms all baselines.</p></p class="citation"></blockquote><h2 id=cslg-18>cs.LG (18)</h2><h3 id=118--14146-node-classification-via-semantic-structural-attention-enhanced-graph-convolutional-networks-hongyin-zhu-2024>(1/18 | 14/146) Node Classification via Semantic-Structural Attention-Enhanced Graph Convolutional Networks (Hongyin Zhu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyin Zhu. (2024)<br><strong>Node Classification via Semantic-Structural Attention-Enhanced Graph Convolutional Networks</strong><br><button class=copy-to-clipboard title="Node Classification via Semantic-Structural Attention-Enhanced Graph Convolutional Networks" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs-SI, cs.LG<br>Keyword Score: 88<br>Keywords: Graph Convolutional Network, Node Classification, Graph, Graph Neural Network, Convolution, Convolutional Neural Network, Knowledge Graph, Supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16033v1.pdf filename=2403.16033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>data,</b> <b>also</b> known as complex network data, is omnipresent across various domains and applications. Prior <b>graph</b> <b>neural</b> <b>network</b> models primarily focused on extracting task-specific structural features through <b>supervised</b> <b>learning</b> objectives, but they fell short in capturing the inherent semantic and structural features of the entire <b>graph.</b> <b>In</b> <b>this</b> paper, we introduce the semantic-structural attention-enhanced <b>graph</b> <b>convolutional</b> <b>network</b> (SSA-GCN), which not only models the <b>graph</b> <b>structure</b> <b>but</b> also extracts generalized <b>unsupervised</b> features to enhance vertex classification performance. The SSA-GCN&rsquo;s key contributions lie in three aspects: firstly, it derives semantic information through <b>unsupervised</b> feature extraction from a <b>knowledge</b> <b>graph</b> <b>perspective;</b> <b>secondly,</b> it obtains structural information through <b>unsupervised</b> feature extraction from a complex network perspective; and finally, it integrates these features through a cross-attention mechanism. By leveraging these features, we augment the <b>graph</b> <b>convolutional</b> <b>network,</b> thereby enhancing the model&rsquo;s generalization capabilities. Our experiments on the Cora and CiteSeer datasets demonstrate the performance improvements achieved by our proposed method. Furthermore, our approach also exhibits excellent accuracy under privacy settings, making it a robust and effective solution for <b>graph</b> <b>data</b> <b>analysis.</b></p></p class="citation"></blockquote><h3 id=218--15146-a-survey-on-self-supervised-pre-training-of-graph-foundation-models-a-knowledge-based-perspective-ziwen-zhao-et-al-2024>(2/18 | 15/146) A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective (Ziwen Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziwen Zhao, Yuhua Li, Yixiong Zou, Ruixuan Li, Rui Zhang. (2024)<br><strong>A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective</strong><br><button class=copy-to-clipboard title="A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 83<br>Keywords: Graph, Graph Neural Network, Foundation Model, Self-supervised Learning, Self-supervised Learning, Self-supervised Pre-training, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16137v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16137v1.pdf filename=2403.16137v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>self-supervised</b> <b>learning</b> is now a go-to method for pre-training <b>graph</b> <b>foundation</b> <b>models,</b> including <b>graph</b> <b>neural</b> <b>networks,</b> <b>graph</b> <b>transformers,</b> <b>and</b> more recent <b>large</b> <b>language</b> <b>model</b> <b>(LLM)-based</b> <b>graph</b> <b>models.</b> <b>There</b> is a wide variety of knowledge patterns embedded in the structure and properties of <b>graphs</b> <b>which</b> <b>may</b> be used for pre-training, but we lack a systematic overview of <b>self-supervised</b> <b>pre-training</b> tasks from the perspective of <b>graph</b> <b>knowledge.</b> <b>In</b> this paper, we comprehensively survey and analyze the pre-training tasks of <b>graph</b> <b>foundation</b> <b>models</b> from a knowledge-based perspective, consisting of microscopic (nodes, links, etc) and macroscopic knowledge (clusters, global structure, etc). It covers a total of 9 knowledge categories and 25 pre-training tasks, as well as various downstream task adaptation strategies. Furthermore, an extensive list of the related papers with detailed metadata is provided at <a href=https://github.com/Newiz430/Pretext>https://github.com/Newiz430/Pretext</a>.</p></p class="citation"></blockquote><h3 id=318--16146-vcr-graphormer-a-mini-batch-graph-transformer-via-virtual-connections-dongqi-fu-et-al-2024>(3/18 | 16/146) VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections (Dongqi Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongqi Fu, Zhigang Hua, Yan Xie, Jin Fang, Si Zhang, Kaan Sancak, Hao Wu, Andrey Malevich, Jingrui He, Bo Long. (2024)<br><strong>VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections</strong><br><button class=copy-to-clipboard title="VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Graph, Convolution, Convolutional Neural Network, Transformer, Tokenization, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16030v1.pdf filename=2403.16030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>transformer</b> has been proven as an effective <b>graph</b> learning method for its adoption of attention mechanism that is capable of capturing expressive representations from complex topological and feature information of <b>graphs.</b> <b>Graph</b> <b>transformer</b> conventionally performs dense attention (or global attention) for every pair of nodes to learn node representation vectors, resulting in quadratic computational costs that are unaffordable for large-scale <b>graph</b> data. Therefore, mini-batch training for <b>graph</b> <b>transformers</b> is a promising direction, but limited samples in each mini-batch can not support effective dense attention to encode informative representations. Facing this bottleneck, (1) we start by assigning each node a token list that is sampled by personalized PageRank (PPR) and then apply standard multi-head <b>self-attention</b> only on this list to compute its node representations. This PPR <b>tokenization</b> method decouples model training from complex <b>graph</b> topological information and makes heavy feature engineering offline and independent, such that mini-batch training of <b>graph</b> <b>transformers</b> is possible by loading each node&rsquo;s token list in batches. We further prove this PPR <b>tokenization</b> is viable as a <b>graph</b> <b>convolution</b> <b>network</b> with a fixed polynomial filter and jumping knowledge. However, only using personalized PageRank may limit information carried by a token list, which could not support different <b>graph</b> inductive biases for model training. To this end, (2) we rewire <b>graphs</b> by introducing multiple types of virtual connections through structure- and content-based super nodes that enable PPR <b>tokenization</b> to encode local and global contexts, long-range interaction, and heterophilous information into each node&rsquo;s token list, and then formalize our Virtual Connection Ranking based <b>Graph</b> <b>Transformer</b> (VCR-Graphormer).</p></p class="citation"></blockquote><h3 id=418--17146-a-federated-parameter-aggregation-method-for-node-classification-tasks-with-different-graph-network-structures-hao-song-et-al-2024>(4/18 | 17/146) A Federated Parameter Aggregation Method for Node Classification Tasks with Different Graph Network Structures (Hao Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Song, Jiacheng Yao, Zhengxi Li, Shaocong Xu, Shibo Jin, Jiajun Zhou, Chenbo Fu, Qi Xuan, Shanqing Yu. (2024)<br><strong>A Federated Parameter Aggregation Method for Node Classification Tasks with Different Graph Network Structures</strong><br><button class=copy-to-clipboard title="A Federated Parameter Aggregation Method for Node Classification Tasks with Different Graph Network Structures" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Node Classification, Graph, Graph Neural Network, Federated Learning, Parameter Sharing, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16004v1.pdf filename=2403.16004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past few years, <b>federated</b> <b>learning</b> has become widely used in various classical machine learning fields because of its collaborative ability to train data from multiple sources without compromising privacy. However, in the area of <b>graph</b> <b>neural</b> <b>networks,</b> the <b>nodes</b> <b>and</b> network structures of <b>graphs</b> <b>held</b> <b>by</b> clients are different in many practical applications, and the aggregation method that directly shares model gradients cannot be directly applied to this scenario. Therefore, this work proposes a <b>federated</b> <b>aggregation</b> method FLGNN applied to various <b>graph</b> <b>federation</b> <b>scenarios</b> and investigates the aggregation effect of <b>parameter</b> <b>sharing</b> at each layer of the <b>graph</b> <b>neural</b> <b>network</b> model. The effectiveness of the <b>federated</b> <b>aggregation</b> method FLGNN is verified by experiments on real datasets. Additionally, for the privacy security of FLGNN, this paper designs membership inference attack experiments and <b>differential</b> <b>privacy</b> defense experiments. The results show that FLGNN performs good robustness, and the success rate of privacy theft is further reduced by adding <b>differential</b> <b>privacy</b> defense methods.</p></p class="citation"></blockquote><h3 id=518--18146-the-n-implementation-details-of-rlhf-with-ppo-a-case-study-on-tldr-summarization-shengyi-huang-et-al-2024>(5/18 | 18/146) The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization (Shengyi Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengyi Huang, Michael Noukhovitch, Arian Hosseini, Kashif Rasul, Weixun Wang, Lewis Tunstall. (2024)<br><strong>The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization</strong><br><button class=copy-to-clipboard title="The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17031v1.pdf filename=2403.17031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work is the first to openly reproduce the <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> scaling behaviors reported in OpenAI&rsquo;s seminal TL;DR <b>summarization</b> work. We create an <b>RLHF</b> pipeline from scratch, enumerate over 20 key implementation details, and share key insights during the reproduction. Our <b>RLHF-trained</b> Pythia models demonstrate significant gains in response quality that scale with model size, with our 2.8B, 6.9B models outperforming OpenAI&rsquo;s released 1.3B checkpoint. We publicly release the trained model checkpoints and code to facilitate further research and accelerate progress in the field (\url{https://github.com/vwxyzjn/summarize_from_feedback_details}).</p></p class="citation"></blockquote><h3 id=618--19146-ibcb-efficient-inverse-batched-contextual-bandit-for-behavioral-evolution-history-yi-xu-et-al-2024>(6/18 | 19/146) IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral Evolution History (Yi Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Xu, Weiran Shen, Xiao Zhang, Jun Xu. (2024)<br><strong>IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral Evolution History</strong><br><button class=copy-to-clipboard title="IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral Evolution History" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Out-of-distribution, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16075v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16075v1.pdf filename=2403.16075v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional imitation learning focuses on modeling the behavioral mechanisms of experts, which requires a large amount of interaction history generated by some fixed expert. However, in many streaming applications, such as streaming <b>recommender</b> <b>systems,</b> online decision-makers typically engage in online learning during the decision-making process, meaning that the interaction history generated by online decision-makers includes their behavioral evolution from novice expert to experienced expert. This poses a new challenge for existing imitation learning approaches that can only utilize data from experienced experts. To address this issue, this paper proposes an inverse batched contextual <b>bandit</b> (IBCB) framework that can efficiently perform estimations of environment reward parameters and learned policy based on the expert&rsquo;s behavioral evolution history. Specifically, IBCB formulates the inverse problem into a simple quadratic programming problem by utilizing the behavioral evolution history of the batched contextual <b>bandit</b> with inaccessible rewards. We demonstrate that IBCB is a unified framework for both deterministic and randomized <b>bandit</b> policies. The experimental results indicate that IBCB outperforms several existing imitation learning algorithms on synthetic and real-world data and significantly reduces running time. Additionally, empirical analyses reveal that IBCB exhibits better <b>out-of-distribution</b> generalization and is highly effective in learning the <b>bandit</b> policy from the interaction history of novice experts.</p></p class="citation"></blockquote><h3 id=718--20146-akbr-learning-adaptive-kernel-based-representations-for-graph-classification-feifei-qian-et-al-2024>(7/18 | 20/146) AKBR: Learning Adaptive Kernel-based Representations for Graph Classification (Feifei Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feifei Qian, Lixin Cui, Yue Wang, Hangyuan Du, Lu Bai, Edwin R. Hancock. (2024)<br><strong>AKBR: Learning Adaptive Kernel-based Representations for Graph Classification</strong><br><button class=copy-to-clipboard title="AKBR: Learning Adaptive Kernel-based Representations for Graph Classification" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 21<br>Keywords: Graph Classification, Graph, Benchmarking, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16130v1.pdf filename=2403.16130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a new model to learn Adaptive Kernel-based <b>Representations</b> <b>(AKBR)</b> for <b>graph</b> <b>classification.</b> Unlike state-of-the-art R-convolution <b>graph</b> <b>kernels</b> that are defined by merely counting any pair of isomorphic substructures between <b>graphs</b> <b>and</b> cannot provide an end-to-end learning mechanism for the classifier, the proposed AKBR approach aims to define an end-to-end <b>representation</b> <b>learning</b> model to construct an adaptive kernel matrix for <b>graphs.</b> <b>To</b> this end, we commence by leveraging a novel feature-channel attention mechanism to capture the interdependencies between different substructure invariants of original <b>graphs.</b> <b>The</b> proposed AKBR model can thus effectively identify the structural importance of different substructures, and compute the R-convolution kernel between pairwise <b>graphs</b> <b>associated</b> with the more significant substructures specified by their structural attentions. Since each row of the resulting kernel matrix can be theoretically seen as the embedding vector of a sample <b>graph,</b> <b>the</b> proposed AKBR model is able to directly employ the resulting kernel matrix as the <b>graph</b> <b>feature</b> matrix and input it into the classifier for classification (i.e., the SoftMax layer), naturally providing an end-to-end learning architecture between the kernel computation as well as the classifier. Experimental results show that the proposed AKBR model outperforms existing state-of-the-art <b>graph</b> <b>kernels</b> and deep learning methods on standard <b>graph</b> <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=818--21146-one-masked-model-is-all-you-need-for-sensor-fault-detection-isolation-and-accommodation-yiwei-fu-et-al-2024>(8/18 | 21/146) One Masked Model is All You Need for Sensor Fault Detection, Isolation and Accommodation (Yiwei Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwei Fu, Weizhong Yan. (2024)<br><strong>One Masked Model is All You Need for Sensor Fault Detection, Isolation and Accommodation</strong><br><button class=copy-to-clipboard title="One Masked Model is All You Need for Sensor Fault Detection, Isolation and Accommodation" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16153v1.pdf filename=2403.16153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate and reliable sensor measurements are critical for ensuring the safety and longevity of complex engineering systems such as wind turbines. In this paper, we propose a novel framework for sensor fault detection, isolation, and accommodation (FDIA) using masked models and <b>self-supervised</b> <b>learning.</b> Our proposed approach is a general time series modeling approach that can be applied to any neural network (NN) model capable of sequence modeling, and captures the complex spatio-temporal relationships among different sensors. During training, the proposed masked approach creates a random mask, which acts like a fault, for one or more sensors, making the training and inference task unified: finding the faulty sensors and correcting them. We validate our proposed technique on both a public dataset and a real-world dataset from GE offshore wind turbines, and demonstrate its effectiveness in detecting, diagnosing and correcting sensor faults. The masked model not only simplifies the overall FDIA pipeline, but also outperforms existing approaches. Our proposed technique has the potential to significantly improve the accuracy and reliability of sensor measurements in complex engineering systems in real-time, and could be applied to other types of sensors and engineering systems in the future. We believe that our proposed framework can contribute to the development of more efficient and effective FDIA techniques for a wide range of applications.</p></p class="citation"></blockquote><h3 id=918--22146-stochastic-parameter-reduced-order-model-based-on-hybrid-machine-learning-approaches-cheng-fang-et-al-2024>(9/18 | 22/146) Stochastic parameter reduced-order model based on hybrid machine learning approaches (Cheng Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Fang, Jinqiao Duan. (2024)<br><strong>Stochastic parameter reduced-order model based on hybrid machine learning approaches</strong><br><button class=copy-to-clipboard title="Stochastic parameter reduced-order model based on hybrid machine learning approaches" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Autoencoder, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17032v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17032v1.pdf filename=2403.17032v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Establishing appropriate mathematical models for complex systems in natural phenomena not only helps deepen our understanding of nature but can also be used for state estimation and prediction. However, the extreme complexity of natural phenomena makes it extremely challenging to develop full-order models (FOMs) and apply them to studying many quantities of interest. In contrast, appropriate reduced-order models (ROMs) are favored due to their high computational efficiency and ability to describe the key dynamics and statistical characteristics of natural phenomena. Taking the viscous Burgers equation as an example, this paper constructs a <b>Convolutional</b> <b>Autoencoder-Reservoir</b> Computing-Normalizing Flow algorithm framework, where the <b>Convolutional</b> <b>Autoencoder</b> is used to construct latent space representations, and the Reservoir Computing-Normalizing Flow framework is used to characterize the evolution of latent state variables. In this way, a data-driven stochastic parameter reduced-order model is constructed to describe the complex system and its dynamic behavior.</p></p class="citation"></blockquote><h3 id=1018--23146-interpretable-modeling-of-deep-reinforcement-learning-driven-scheduling-boyang-li-et-al-2024>(10/18 | 23/146) Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling (Boyang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyang Li, Zhiling Lan, Michael E. Papka. (2024)<br><strong>Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling</strong><br><button class=copy-to-clipboard title="Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Black Box, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16293v1.pdf filename=2403.16293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of high-performance computing (HPC), there has been recent exploration into the use of deep <b>reinforcement</b> <b>learning</b> for cluster scheduling (DRL scheduling), which has demonstrated promising outcomes. However, a significant challenge arises from the lack of interpretability in deep neural networks (DNN), rendering them as <b>black-box</b> <b>models</b> to system managers. This lack of model interpretability hinders the practical deployment of DRL scheduling. In this work, we present a framework called IRL (Interpretable <b>Reinforcement</b> <b>Learning)</b> to address the issue of interpretability of DRL scheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a decision tree by utilizing imitation learning. Unlike DNN, decision tree models are non-parametric and easily comprehensible to humans. To extract an effective and efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger) algorithm and introduces the notion of critical state to prune the derived decision tree. Through trace-based experiments, we demonstrate that IRL is capable of converting a <b>black-box</b> <b>DNN</b> policy into an interpretable rulebased decision tree while maintaining comparable scheduling performance. Additionally, IRL can contribute to the setting of rewards in DRL scheduling.</p></p class="citation"></blockquote><h3 id=1118--24146-from-discrete-to-continuous-deep-fair-clustering-with-transferable-representations-xiang-zhang-2024>(11/18 | 24/146) From Discrete to Continuous: Deep Fair Clustering With Transferable Representations (Xiang Zhang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Zhang. (2024)<br><strong>From Discrete to Continuous: Deep Fair Clustering With Transferable Representations</strong><br><button class=copy-to-clipboard title="From Discrete to Continuous: Deep Fair Clustering With Transferable Representations" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-CY, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16201v1.pdf filename=2403.16201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of deep fair <b>clustering,</b> which partitions data into clusters via the representations extracted by deep neural networks while hiding sensitive data attributes. To achieve <b>fairness,</b> existing methods present a variety of <b>fairness-related</b> objective functions based on the group <b>fairness</b> criterion. However, these works typically assume that the sensitive attributes are discrete and do not work for continuous sensitive variables, such as the proportion of the female population in an area. Besides, the potential of the representations learned from <b>clustering</b> tasks to improve performance on other tasks is ignored by existing works. In light of these limitations, we propose a flexible deep fair <b>clustering</b> method that can handle discrete and continuous sensitive attributes simultaneously. Specifically, we design an information bottleneck style objective function to learn fair and <b>clustering-friendly</b> representations. Furthermore, we explore for the first time the transferability of the extracted representations to other downstream tasks. Unlike existing works, we impose <b>fairness</b> at the representation level, which could guarantee <b>fairness</b> for the transferred task regardless of <b>clustering</b> results. To verify the effectiveness of the proposed method, we perform extensive experiments on datasets with discrete and continuous sensitive attributes, demonstrating the advantage of our method in comparison with state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=1218--25146-enhancing-demand-prediction-in-open-systems-by-cartogram-aided-deep-learning-sangjoon-park-et-al-2024>(12/18 | 25/146) Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep Learning (Sangjoon Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sangjoon Park, Yongsung Kwon, Hyungjoon Soh, Mi Jin Lee, Seung-Woo Son. (2024)<br><strong>Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep Learning</strong><br><button class=copy-to-clipboard title="Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep Learning" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-soc-ph<br>Keyword Score: 13<br>Keywords: Graph, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16049v1.pdf filename=2403.16049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting temporal patterns across various domains poses significant challenges due to their nuanced and often nonlinear trajectories. To address this challenge, prediction frameworks have been continuously refined, employing data-driven statistical methods, mathematical models, and machine learning. Recently, as one of the challenging systems, shared transport systems such as public bicycles have gained prominence due to urban constraints and environmental concerns. Predicting rental and return patterns at bicycle stations remains a formidable task due to the system&rsquo;s openness and imbalanced usage patterns across stations. In this study, we propose a deep learning framework to predict rental and return patterns by leveraging cartogram approaches. The cartogram approach facilitates the prediction of demand for newly installed stations with no training data as well as long-period prediction, which has not been achieved before. We apply this method to public bicycle rental-and-return data in Seoul, South Korea, employing a spatial-temporal <b>convolutional</b> <b>graph</b> attention network. Our improved architecture incorporates batch attention and modified node feature updates for better prediction accuracy across different time scales. We demonstrate the effectiveness of our framework in predicting temporal patterns and its potential applications.</p></p class="citation"></blockquote><h3 id=1318--26146-out-of-distribution-detection-via-deep-multi-comprehension-ensemble-chenhui-xu-et-al-2024>(13/18 | 26/146) Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble (Chenhui Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenhui Xu, Fuxun Yu, Zirui Xu, Nathan Inkawhich, Xiang Chen. (2024)<br><strong>Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble</strong><br><button class=copy-to-clipboard title="Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16260v1.pdf filename=2403.16260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research underscores the pivotal role of the <b>Out-of-Distribution</b> (OOD) feature representation field scale in determining the efficacy of models in OOD detection. Consequently, the adoption of model ensembles has emerged as a prominent strategy to augment this feature representation field, capitalizing on anticipated model diversity. However, our introduction of novel qualitative and quantitative model ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and the Self-Coupling Index, reveals a critical drawback in existing ensemble methods. We find that these methods incorporate weights that are affine-transformable, exhibiting limited variability and thus failing to achieve the desired diversity in feature representation. To address this limitation, we elevate the dimensions of traditional model ensembles, incorporating various factors such as different weight initializations, data holdout, etc., into distinct supervision tasks. This innovative approach, termed Multi-Comprehension (MC) Ensemble, leverages diverse training tasks to generate distinct comprehensions of the data and labels, thereby extending the feature representation field. Our experimental results demonstrate the superior performance of the MC Ensemble strategy in OOD detection compared to both the naive Deep Ensemble method and a standalone model of comparable size. This underscores the effectiveness of our proposed approach in enhancing the model&rsquo;s capability to detect instances outside its training distribution.</p></p class="citation"></blockquote><h3 id=1418--27146-partially-blinded-unlearning-class-unlearning-for-deep-networks-a-bayesian-perspective-subhodip-panda-et-al-2024>(14/18 | 27/146) Partially Blinded Unlearning: Class Unlearning for Deep Networks a Bayesian Perspective (Subhodip Panda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subhodip Panda, Shashwat Sourav, Prathosh A. P. (2024)<br><strong>Partially Blinded Unlearning: Class Unlearning for Deep Networks a Bayesian Perspective</strong><br><button class=copy-to-clipboard title="Partially Blinded Unlearning: Class Unlearning for Deep Networks a Bayesian Perspective" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Machine Unlearning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16246v1.pdf filename=2403.16246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In order to adhere to regulatory standards governing individual data privacy and safety, <b>machine</b> <b>learning</b> models must systematically eliminate information derived from specific subsets of a user&rsquo;s training data that can no longer be utilized. The emerging discipline of <b>Machine</b> <b>Unlearning</b> has arisen as a pivotal area of research, facilitating the process of selectively discarding information designated to specific sets or classes of data from a pre-trained model, thereby eliminating the necessity for extensive retraining from scratch. The principal aim of this study is to formulate a methodology tailored for the purposeful elimination of information linked to a specific class of data from a pre-trained classification network. This intentional removal is crafted to degrade the model&rsquo;s performance specifically concerning the unlearned data class while concurrently minimizing any detrimental impacts on the model&rsquo;s performance in other classes. To achieve this goal, we frame the class unlearning problem from a Bayesian perspective, which yields a loss function that minimizes the log-likelihood associated with the unlearned data with a stability regularization in parameter space. This stability regularization incorporates Mohalanobis distance with respect to the Fisher Information matrix and $l_2$ distance from the pre-trained model parameters. Our novel approach, termed \textbf{Partially-Blinded Unlearning (PBU)}, surpasses existing state-of-the-art class unlearning methods, demonstrating superior effectiveness. Notably, PBU achieves this efficacy without requiring awareness of the entire training dataset but only to the unlearned data points, marking a distinctive feature of its performance.</p></p class="citation"></blockquote><h3 id=1518--28146-systematic-construction-of-continuous-time-neural-networks-for-linear-dynamical-systems-chinmay-datar-et-al-2024>(15/18 | 28/146) Systematic construction of continuous-time neural networks for linear dynamical systems (Chinmay Datar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chinmay Datar, Adwait Datar, Felix Dietrich, Wil Schilders. (2024)<br><strong>Systematic construction of continuous-time neural networks for linear dynamical systems</strong><br><button class=copy-to-clipboard title="Systematic construction of continuous-time neural networks for linear dynamical systems" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 93B17, 65L70, 68T07, I-2-m; G-1-3; G-1-7, cs-LG, cs-NA, cs.LG, math-DS, math-NA<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16215v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16215v1.pdf filename=2403.16215v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Discovering a suitable neural network architecture for modeling complex dynamical systems poses a formidable challenge, often involving extensive trial and error and navigation through a high-dimensional hyper-parameter space. In this paper, we discuss a systematic approach to constructing neural architectures for modeling a subclass of dynamical systems, namely, Linear Time-Invariant (LTI) systems. We use a variant of <b>continuous-time</b> <b>neural</b> networks in which the output of each neuron evolves continuously as a solution of a first-order or second-order Ordinary Differential Equation (ODE). Instead of deriving the network architecture and parameters from data, we propose a gradient-free algorithm to compute sparse architecture and network parameters directly from the given LTI system, leveraging its properties. We bring forth a novel neural architecture paradigm featuring horizontal hidden layers and provide insights into why employing conventional neural architectures with vertical hidden layers may not be favorable. We also provide an upper bound on the numerical errors of our neural networks. Finally, we demonstrate the high accuracy of our constructed networks on three numerical examples.</p></p class="citation"></blockquote><h3 id=1618--29146-subspace-defense-discarding-adversarial-perturbations-by-learning-a-subspace-for-clean-signals-rui-zheng-et-al-2024>(16/18 | 29/146) Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals (Rui Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Zheng, Yuhao Zhou, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang. (2024)<br><strong>Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals</strong><br><button class=copy-to-clipboard title="Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16176v1.pdf filename=2403.16176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks (DNNs) are notoriously vulnerable to <b>adversarial</b> <b>attacks</b> that place carefully crafted perturbations on normal examples to fool DNNs. To better understand such attacks, a characterization of the features carried by <b>adversarial</b> <b>examples</b> is needed. In this paper, we tackle this challenge by inspecting the subspaces of sample features through spectral analysis. We first empirically show that the features of either clean signals or <b>adversarial</b> <b>perturbations</b> are redundant and span in low-dimensional linear subspaces respectively with minimal overlap, and the classical low-dimensional subspace projection can suppress perturbation features out of the subspace of clean signals. This makes it possible for DNNs to learn a subspace where only features of clean signals exist while those of perturbations are discarded, which can facilitate the distinction of <b>adversarial</b> <b>examples.</b> To prevent the residual perturbations that is inevitable in subspace learning, we propose an independence criterion to disentangle clean signals from perturbations. Experimental results show that the proposed strategy enables the model to inherently suppress adversaries, which not only boosts model robustness but also motivates new directions of effective <b>adversarial</b> <b>defense.</b></p></p class="citation"></blockquote><h3 id=1718--30146-a-transformer-approach-for-electricity-price-forecasting-oscar-llorente-gonzalez-et-al-2024>(17/18 | 30/146) A Transformer approach for Electricity Price Forecasting (Oscar Llorente Gonzalez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oscar Llorente Gonzalez, Jose Portela. (2024)<br><strong>A Transformer approach for Electricity Price Forecasting</strong><br><button class=copy-to-clipboard title="A Transformer approach for Electricity Price Forecasting" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16108v1.pdf filename=2403.16108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel approach to electricity price forecasting (EPF) using a pure <b>Transformer</b> model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the <b>Transformer</b> model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.</p></p class="citation"></blockquote><h3 id=1818--31146-a-unified-module-for-accelerating-stable-diffusion-lcm-lora-ayush-thakur-et-al-2024>(18/18 | 31/146) A Unified Module for Accelerating STABLE-DIFFUSION: LCM-LORA (Ayush Thakur et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayush Thakur, Rashmi Vashisth. (2024)<br><strong>A Unified Module for Accelerating STABLE-DIFFUSION: LCM-LORA</strong><br><button class=copy-to-clipboard title="A Unified Module for Accelerating STABLE-DIFFUSION: LCM-LORA" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-GR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16024v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16024v1.pdf filename=2403.16024v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a comprehensive study on the unified module for accelerating stable-diffusion processes, specifically focusing on the lcm-lora module. Stable-diffusion processes play a crucial role in various scientific and engineering domains, and their acceleration is of paramount importance for efficient computational performance. The standard iterative procedures for solving fixed-source discrete ordinates problems often exhibit slow convergence, particularly in optically thick scenarios. To address this challenge, unconditionally stable <b>diffusion-acceleration</b> <b>methods</b> have been developed, aiming to enhance the computational efficiency of transport equations and discrete ordinates problems. This study delves into the theoretical foundations and numerical results of unconditionally stable <b>diffusion</b> <b>synthetic</b> acceleration methods, providing insights into their stability and performance for model discrete ordinates problems. Furthermore, the paper explores recent advancements in <b>diffusion</b> <b>model</b> acceleration, including on device acceleration of large <b>diffusion</b> <b>models</b> via gpu aware optimizations, highlighting the potential for significantly improved inference latency. The results and analyses in this study provide important insights into stable <b>diffusion</b> <b>processes</b> and have important ramifications for the creation and application of acceleration methods specifically, the lcm-lora module in a variety of computing environments.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-2>physics.flu-dyn (2)</h2><h3 id=12--32146-predicting-energy-budgets-in-droplet-dynamics-a-recurrent-neural-network-approach-diego-a-de-aguiar-et-al-2024>(1/2 | 32/146) Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural Network Approach (Diego A. de Aguiar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Diego A. de Aguiar, Hugo L. França, Cassio M. Oishi. (2024)<br><strong>Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural Network Approach</strong><br><button class=copy-to-clipboard title="Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural Network Approach" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-LG, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 70<br>Keywords: Simulation, Simulator, LSTM, LSTM, LSTM, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16144v1.pdf filename=2403.16144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural networks in fluid mechanics offer an efficient approach for exploring complex flows, including multiphase and free surface flows. The <b>recurrent</b> <b>neural</b> <b>network,</b> particularly the <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> model, proves attractive for learning mappings from transient inputs to dynamic outputs. This study applies <b>LSTM</b> to predict transient and static outputs for fluid flows under surface tension effects. Specifically, we explore two distinct droplet dynamic scenarios: droplets with diverse initial shapes impacting with solid surfaces, as well as the coalescence of two droplets following collision. Using only dimensionless numbers and geometric time series data from numerical <b>simulations,</b> <b>LSTM</b> predicts the energy budget. The marker-and-cell front-tracking methodology combined with a marker-and-cell finite-difference strategy is adopted for simulating the droplet dynamics. Using a <b>recurrent</b> <b>neural</b> <b>network</b> <b>(RNN)</b> architecture fed with time series data derived from geometrical parameters, as for example droplet diameter variation, our study shows the accuracy of our approach in predicting energy budgets, as for instance the kinetic, dissipation, and surface energy trends, across a range of Reynolds and Weber numbers in droplet dynamic problems. Finally, a two-phase sequential neural network using only geometric data, which is readily available in experimental settings, is employed to predict the energies and then use them to estimate static parameters, such as the Reynolds and Weber numbers. While our methodology has been primarily validated with <b>simulation</b> data, its adaptability to experimental datasets is a promising avenue for future exploration. We hope that our strategy can be useful for diverse applications, spanning from inkjet printing to combustion engines, where the prediction of energy budgets or dissipation energies is crucial.</p></p class="citation"></blockquote><h3 id=22--33146-explicit-form-of-simplified-grads-13-moments-distribution-function-based-moment-gas-kinetic-solver-with-unstructured-meshes-for-the-multiscale-rarefied-flow-w-liu-et-al-2024>(2/2 | 33/146) Explicit form of simplified Grad&rsquo;s 13 moments distribution function-based moment gas kinetic solver with unstructured meshes for the multiscale rarefied flow (W. Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>W. Liu, C. Shu, Z. J. Liu. (2024)<br><strong>Explicit form of simplified Grad&rsquo;s 13 moments distribution function-based moment gas kinetic solver with unstructured meshes for the multiscale rarefied flow</strong><br><button class=copy-to-clipboard title="Explicit form of simplified Grad's 13 moments distribution function-based moment gas kinetic solver with unstructured meshes for the multiscale rarefied flow" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-NA, math-NA, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16014v1.pdf filename=2403.16014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is essential to efficiently solve multiscale flows covering the continuum regime to the rarefied regime. The explicit form of Grad&rsquo;s 13 moments distribution function-based moment gas kinetic solver (G13-MGKS) has been proposed in our previous work [Comput. Math. Appl., 137 (2023), pp. 112-125], which demonstrates the potential for efficiently simulating continuum flows accurately and presenting reasonable predictions for rarefied flows at moderate Knudsen numbers on structured meshes. To further extend the solver&rsquo;s applicability to unstructured meshes, we propose the simplified version of the Grad&rsquo;s 13 moments distribution function-based moment gas kinetic solver (SG13-MGKS) with an explicit form of the numerical flux in the present paper. The Shakhov collision model has been adopted and validated within the framework of SG13-MGKS to ensure the correct Prandtl number in the <b>simulation.</b> Additionally, a simplified treatment for the numerical fluxes has been adopted to minimize the need for complex calculations of the gradient of integral coefficients. The performance of SG13-MGKS has been evaluated in numerical cases of Couette flow with temperature differences, flow passing through a NACA0012 airfoil, and pressure-driven flow in a variable-diameter circular pipe. Our results demonstrate that SG13-MGKS can achieve reasonably accurate computational results at Knudsen numbers below 0.2. Benefiting from the avoidance of discretization in velocity space, G13-MGKS is able to be two orders of magnitude faster compared to the conventional discrete velocity method. Furthermore, the simplified form of numerical fluxes and the fewer gradients of integration coefficients enable the performance of SG13-MGKS on unstructured grids with a saving of about 4 times the computation time and 3 times the memory cost compared to the previous version of G13-MGKS.</p></p class="citation"></blockquote><h2 id=cscv-45>cs.CV (45)</h2><h3 id=145--34146-cross-domain-multi-modal-few-shot-object-detection-via-rich-text-zeyu-shangguan-et-al-2024>(1/45 | 34/146) Cross-domain Multi-modal Few-shot Object Detection via Rich Text (Zeyu Shangguan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Shangguan, Daniel Seita, Mohammad Rostami. (2024)<br><strong>Cross-domain Multi-modal Few-shot Object Detection via Rich Text</strong><br><button class=copy-to-clipboard title="Cross-domain Multi-modal Few-shot Object Detection via Rich Text" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Object Detection, Few-shot, Few-shot Learning, Meta Learning, Multi-modal, Domain Adaptation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16188v1.pdf filename=2403.16188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-modal feature extraction and integration have led to steady performance improvements in <b>few-shot</b> <b>learning</b> tasks due to generating richer features. However, existing <b>multi-modal</b> <b>object</b> <b>detection</b> (MM-OD) methods degrade when facing significant <b>domain-shift</b> <b>and</b> are sample insufficient. We hypothesize that rich text information could more effectively help the model to build a knowledge relationship between the vision instance and its language description and can help mitigate <b>domain</b> <b>shift.</b> Specifically, we study the Cross-Domain <b>few-shot</b> <b>generalization</b> of MM-OD (CDMM-FSOD) and propose a <b>meta-learning</b> <b>based</b> <b>multi-modal</b> <b>few-shot</b> <b>object</b> <b>detection</b> method that utilizes rich text semantic information as an auxiliary modality to achieve <b>domain</b> <b>adaptation</b> in the context of FSOD. Our proposed network contains (i) a <b>multi-modal</b> feature aggregation module that aligns the vision and language support feature embeddings and (ii) a rich text semantic rectify module that utilizes bidirectional text feature generation to reinforce <b>multi-modal</b> feature alignment and thus to enhance the model&rsquo;s language understanding capability. We evaluate our model on common standard cross-domain <b>object</b> <b>detection</b> datasets and demonstrate that our approach considerably outperforms existing FSOD methods.</p></p class="citation"></blockquote><h3 id=245--35146-robust-diffusion-models-for-adversarial-purification-guang-lin-et-al-2024>(2/45 | 35/146) Robust Diffusion Models for Adversarial Purification (Guang Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guang Lin, Zerui Tao, Jianhai Zhang, Toshihisa Tanaka, Qibin Zhao. (2024)<br><strong>Robust Diffusion Models for Adversarial Purification</strong><br><button class=copy-to-clipboard title="Robust Diffusion Models for Adversarial Purification" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Diffusion Model, Adversarial Learning, Fine-tuning, Fine-tuning, Adversarial Attack, Adversarial Purification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16067v1.pdf filename=2403.16067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> (DMs) based <b>adversarial</b> <b>purification</b> (AP) has shown to be the most powerful alternative to <b>adversarial</b> <b>training</b> (AT). However, these methods neglect the fact that pre-trained <b>diffusion</b> <b>models</b> themselves are not robust to <b>adversarial</b> <b>attacks</b> as well. Additionally, the <b>diffusion</b> <b>process</b> can easily destroy semantic information and generate a high quality image but totally different from the original input image after the reverse process, leading to degraded standard accuracy. To overcome these issues, a natural idea is to harness <b>adversarial</b> <b>training</b> strategy to retrain or <b>fine-tune</b> the pre-trained <b>diffusion</b> <b>model,</b> which is computationally prohibitive. We propose a novel robust reverse process with <b>adversarial</b> <b>guidance,</b> which is independent of given pre-trained DMs and avoids retraining or <b>fine-tuning</b> the DMs. This robust guidance can not only ensure to generate purified examples retaining more semantic content but also mitigate the accuracy-robustness trade-off of DMs for the first time, which also provides DM-based AP an efficient adaptive ability to new attacks. Extensive experiments are conducted to demonstrate that our method achieves the state-of-the-art results and exhibits generalization against different attacks.</p></p class="citation"></blockquote><h3 id=345--36146-papr-training-free-one-step-patch-pruning-with-lightweight-convnets-for-faster-inference-tanvir-mahmud-et-al-2024>(3/45 | 36/146) PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for Faster Inference (Tanvir Mahmud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanvir Mahmud, Burhaneddin Yaman, Chun-Hao Liu, Diana Marculescu. (2024)<br><strong>PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for Faster Inference</strong><br><button class=copy-to-clipboard title="PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for Faster Inference" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Pruning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16020v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16020v1.pdf filename=2403.16020v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As deep neural networks evolve from <b>convolutional</b> <b>neural</b> <b>networks</b> (ConvNets) to advanced <b>vision</b> <b>transformers</b> (ViTs), there is an increased need to eliminate redundant data for faster processing without compromising accuracy. Previous methods are often architecture-specific or necessitate re-training, restricting their applicability with frequent model updates. To solve this, we first introduce a novel property of lightweight ConvNets: their ability to identify key discriminative patch regions in images, irrespective of model&rsquo;s final accuracy or size. We demonstrate that fully-connected layers are the primary bottleneck for ConvNets performance, and their suppression with simple weight recalibration markedly enhances discriminative patch localization performance. Using this insight, we introduce PaPr, a method for substantially <b>pruning</b> redundant patches with minimal accuracy loss using lightweight ConvNets across a variety of deep learning architectures, including ViTs, ConvNets, and hybrid <b>transformers,</b> without any re-training. Moreover, the simple early-stage one-step patch <b>pruning</b> with PaPr enhances existing patch reduction methods. Through extensive testing on diverse architectures, PaPr achieves significantly higher accuracy over state-of-the-art patch reduction methods with similar FLOP count reduction. More specifically, PaPr reduces about 70% of redundant patches in videos with less than 0.8% drop in accuracy, and up to 3.7x FLOPs reduction, which is a 15% more reduction with 2.5% higher accuracy.</p></p class="citation"></blockquote><h3 id=445--37146-pose-guided-self-training-with-two-stage-clustering-for-unsupervised-landmark-discovery-siddharth-tourani-et-al-2024>(4/45 | 37/146) Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised Landmark Discovery (Siddharth Tourani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siddharth Tourani, Ahmed Alwheibi, Arif Mahmood, Muhammad Haris Khan. (2024)<br><strong>Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised Landmark Discovery</strong><br><button class=copy-to-clipboard title="Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised Landmark Discovery" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Diffusion Model, Benchmarking, Clustering, Self-supervised Learning, Self-supervised Learning, Unsupervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16194v1.pdf filename=2403.16194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> landmarks discovery (ULD) for an object category is a challenging computer vision problem. In pursuit of developing a robust ULD framework, we explore the potential of a recent paradigm of <b>self-supervised</b> <b>learning</b> algorithms, known as <b>diffusion</b> <b>models.</b> Some recent works have shown that these models implicitly contain important correspondence cues. Towards harnessing the potential of <b>diffusion</b> <b>models</b> for the ULD task, we make the following core contributions. First, we propose a <b>ZeroShot</b> ULD baseline based on simple <b>clustering</b> of random pixel locations with nearest neighbour matching. It delivers better results than existing ULD methods. Second, motivated by the <b>ZeroShot</b> performance, we develop a ULD algorithm based on <b>diffusion</b> <b>features</b> using self-training and <b>clustering</b> which also outperforms prior methods by notable margins. Third, we introduce a new proxy task based on generating latent pose codes and also propose a two-stage <b>clustering</b> mechanism to facilitate effective pseudo-labeling, resulting in a significant performance improvement. Overall, our approach consistently outperforms state-of-the-art methods on four challenging <b>benchmarks</b> AFLW, MAFL, CatHeads and LS3D by significant margins.</p></p class="citation"></blockquote><h3 id=545--38146-multi-scale-spatio-temporal-graph-convolutional-network-for-facial-expression-spotting-yicheng-deng-et-al-2024>(5/45 | 38/146) Multi-Scale Spatio-Temporal Graph Convolutional Network for Facial Expression Spotting (Yicheng Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yicheng Deng, Hideaki Hayashi, Hajime Nagahara. (2024)<br><strong>Multi-Scale Spatio-Temporal Graph Convolutional Network for Facial Expression Spotting</strong><br><button class=copy-to-clipboard title="Multi-Scale Spatio-Temporal Graph Convolutional Network for Facial Expression Spotting" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Graph Convolutional Network, Graph, Contrastive Learning, Convolution, Convolutional Neural Network, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15994v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15994v1.pdf filename=2403.15994v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial expression spotting is a significant but challenging task in facial expression analysis. The accuracy of expression spotting is affected not only by irrelevant facial movements but also by the difficulty of perceiving subtle motions in micro-expressions. In this paper, we propose a Multi-Scale Spatio-Temporal <b>Graph</b> <b>Convolutional</b> <b>Network</b> (SpoT-GCN) for facial expression spotting. To extract more robust motion features, we track both short- and long-term motion of facial muscles in compact sliding windows whose window length adapts to the temporal receptive field of the network. This strategy, termed the receptive field adaptive sliding window strategy, effectively magnifies the motion features while alleviating the problem of severe head movement. The subtle motion features are then converted to a facial <b>graph</b> <b>representation,</b> <b>whose</b> spatio-temporal <b>graph</b> <b>patterns</b> <b>are</b> learned by a <b>graph</b> <b>convolutional</b> <b>network.</b> This network learns both local and global features from multiple scales of facial <b>graph</b> <b>structures</b> <b>using</b> our proposed facial local <b>graph</b> <b>pooling</b> <b>(FLGP).</b> Furthermore, we introduce <b>supervised</b> <b>contrastive</b> <b>learning</b> to enhance the discriminative capability of our model for difficult-to-classify frames. The experimental results on the SAMM-LV and CAS(ME)^2 datasets demonstrate that our method achieves state-of-the-art performance, particularly in micro-expression spotting. Ablation studies further verify the effectiveness of our proposed modules.</p></p class="citation"></blockquote><h3 id=645--39146-sdstrack-self-distillation-symmetric-adapter-learning-for-multi-modal-visual-object-tracking-xiaojun-hou-et-al-2024>(6/45 | 39/146) SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal Visual Object Tracking (Xiaojun Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaojun Hou, Jiazheng Xing, Yijie Qian, Yaowei Guo, Shuo Xin, Junhao Chen, Kai Tang, Mengmeng Wang, Zhengkai Jiang, Liang Liu, Yong Liu. (2024)<br><strong>SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal Visual Object Tracking</strong><br><button class=copy-to-clipboard title="SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal Visual Object Tracking" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Fine-tuning, Knowledge Distillation, Multi-modal, Multi-modal, Self-Distillation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16002v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16002v2.pdf filename=2403.16002v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> Visual Object Tracking (VOT) has recently gained significant attention due to its robustness. Early research focused on fully <b>fine-tuning</b> RGB-based trackers, which was inefficient and lacked generalized representation due to the scarcity of <b>multimodal</b> data. Therefore, recent studies have utilized <b>prompt</b> tuning to transfer pre-trained RGB-based trackers to <b>multimodal</b> data. However, the modality gap limits pre-trained knowledge recall, and the dominance of the RGB modality persists, preventing the full utilization of information from other modalities. To address these issues, we propose a novel symmetric <b>multimodal</b> tracking framework called SDSTrack. We introduce lightweight adaptation for efficient <b>fine-tuning,</b> which directly transfers the feature extraction ability from RGB to other domains with a small number of trainable parameters and integrates <b>multimodal</b> features in a balanced, symmetric manner. Furthermore, we design a complementary masked patch <b>distillation</b> strategy to enhance the robustness of trackers in complex environments, such as extreme weather, poor imaging, and sensor failure. Extensive experiments demonstrate that SDSTrack outperforms state-of-the-art methods in various <b>multimodal</b> tracking scenarios, including RGB+Depth, RGB+Thermal, and RGB+Event tracking, and exhibits impressive results in extreme conditions. Our source code is available at <a href=https://github.com/hoqolo/SDSTrack>https://github.com/hoqolo/SDSTrack</a>.</p></p class="citation"></blockquote><h3 id=745--40146-a-general-and-efficient-federated-split-learning-with-pre-trained-image-transformers-for-heterogeneous-data-yifan-shi-et-al-2024>(7/45 | 40/146) A General and Efficient Federated Split Learning with Pre-trained Image Transformers for Heterogeneous Data (Yifan Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Shi, Yuhui Zhang, Ziyue Huang, Xiaofeng Yang, Li Shen, Wei Chen, Xueqian Wang. (2024)<br><strong>A General and Efficient Federated Split Learning with Pre-trained Image Transformers for Heterogeneous Data</strong><br><button class=copy-to-clipboard title="A General and Efficient Federated Split Learning with Pre-trained Image Transformers for Heterogeneous Data" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Vision Transformer, Black Box, Federated Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16050v1.pdf filename=2403.16050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Split</b> Learning (FSL) is a promising distributed learning paradigm in practice, which gathers the strengths of both <b>Federated</b> <b>Learning</b> (FL) and Split Learning (SL) paradigms, to ensure model privacy while diminishing the resource overhead of each client, especially on large <b>transformer</b> models in a resource-constrained environment, e.g., Internet of Things (IoT). However, almost all works merely investigate the performance with simple neural network models in FSL. Despite the minor efforts focusing on incorporating <b>Vision</b> <b>Transformers</b> (ViT) as model architectures, they train ViT from scratch, thereby leading to enormous training overhead in each device with limited resources. Therefore, in this paper, we harness Pre-trained Image <b>Transformers</b> (PITs) as the initial model, coined FES-PIT, to accelerate the training process and improve model robustness. Furthermore, we propose FES-PTZO to hinder the gradient inversion attack, especially having the capability compatible with <b>black-box</b> <b>scenarios,</b> where the gradient information is unavailable. Concretely, FES-PTZO approximates the server gradient by utilizing a zeroth-order (ZO) optimization, which replaces the backward propagation with just one forward process. Empirically, we are the first to provide a systematic evaluation of FSL methods with PITs in real-world datasets, different partial device participations, and heterogeneous data splits. Our experiments verify the effectiveness of our algorithms.</p></p class="citation"></blockquote><h3 id=845--41146-bimcv-r-a-landmark-dataset-for-3d-ct-text-image-retrieval-yinda-chen-et-al-2024>(8/45 | 41/146) BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval (Yinda Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinda Chen, Che Liu, Xiaoyu Liu, Rossella Arcucci, Zhiwei Xiong. (2024)<br><strong>BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval</strong><br><button class=copy-to-clipboard title="BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Image2text, Text2image, Text2image, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15992v1.pdf filename=2403.15992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The burgeoning integration of 3D medical imaging into healthcare has led to a substantial increase in the workload of medical professionals. To assist clinicians in their diagnostic processes and alleviate their workload, the development of a robust system for retrieving similar case studies presents a viable solution. While the concept holds great promise, the field of 3D medical <b>text-image</b> retrieval is currently limited by the absence of robust evaluation <b>benchmarks</b> and curated datasets. To remedy this, our study presents a groundbreaking dataset, BIMCV-R (This dataset will be released upon acceptance.), which includes an extensive collection of 8,069 3D CT volumes, encompassing over 2 million slices, paired with their respective radiological reports. Expanding upon the foundational work of our dataset, we craft a retrieval strategy, MedFinder. This approach employs a dual-stream network architecture, harnessing the potential of <b>large</b> <b>language</b> <b>models</b> to advance the field of medical image retrieval beyond existing <b>text-image</b> retrieval solutions. It marks our preliminary step towards developing a system capable of facilitating <b>text-to-image,</b> <b>image-to-text,</b> and keyword-based retrieval tasks.</p></p class="citation"></blockquote><h3 id=945--42146-exploiting-semantic-reconstruction-to-mitigate-hallucinations-in-vision-language-models-minchan-kim-et-al-2024>(9/45 | 42/146) Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models (Minchan Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru Chang. (2024)<br><strong>Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models</strong><br><button class=copy-to-clipboard title="Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Unsupervised Learning, Unsupervised Learning, Image2text, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16167v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16167v2.pdf filename=2403.16167v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hallucinations in <b>vision-language</b> models pose a significant challenge to their reliability, particularly in the generation of long captions. Current methods fall short of accurately identifying and mitigating these hallucinations. To address this issue, we introduce ESREAL, a novel <b>unsupervised</b> <b>learning</b> framework designed to suppress the generation of hallucinations through accurate localization and penalization of hallucinated tokens. Initially, ESREAL creates a reconstructed image based on the generated caption and aligns its corresponding regions with those of the original image. This semantic reconstruction aids in identifying both the presence and type of token-level hallucinations within the generated caption. Subsequently, ESREAL computes token-level hallucination scores by assessing the semantic similarity of aligned regions based on the type of hallucination. Finally, ESREAL employs a proximal policy optimization algorithm, where it selectively penalizes hallucinated tokens according to their token-level hallucination scores. Our framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2 by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved solely through signals derived from the image itself, without the need for any <b>image-text</b> pairs.</p></p class="citation"></blockquote><h3 id=1045--43146-enhancing-video-transformers-for-action-understanding-with-vlm-aided-training-hui-lu-et-al-2024>(10/45 | 43/146) Enhancing Video Transformers for Action Understanding with VLM-aided Training (Hui Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hui Lu, Hu Jian, Ronald Poppe, Albert Ali Salah. (2024)<br><strong>Enhancing Video Transformers for Action Understanding with VLM-aided Training</strong><br><button class=copy-to-clipboard title="Enhancing Video Transformers for Action Understanding with VLM-aided Training" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Vision Transformer, Transformer, Prompt, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16128v1.pdf filename=2403.16128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Owing to their ability to extract relevant spatio-temporal video embeddings, <b>Vision</b> <b>Transformers</b> (ViTs) are currently the best performing models in video action understanding. However, their generalization over domains or datasets is somewhat limited. In contrast, Visual Language Models (VLMs) have demonstrated exceptional generalization performance, but are currently unable to process videos. Consequently, they cannot extract spatio-temporal patterns that are crucial for action understanding. In this paper, we propose the Four-tiered <b>Prompts</b> (FTP) framework that takes advantage of the complementary strengths of ViTs and VLMs. We retain ViTs&rsquo; strong spatio-temporal representation ability but improve the visual encodings to be more comprehensive and general by aligning them with VLM outputs. The FTP framework adds four feature processors that focus on specific aspects of human action in videos: action category, action components, action description, and context information. The VLMs are only employed during training, and inference incurs a minimal computation cost. Our approach consistently yields state-of-the-art performance. For instance, we achieve remarkable top-1 accuracy of 93.8% on Kinetics-400 and 83.4% on Something-Something V2, surpassing VideoMAEv2 by 2.8% and 2.6%, respectively.</p></p class="citation"></blockquote><h3 id=1145--44146-enhancing-visual-continual-learning-with-language-guided-supervision-bolin-ni-et-al-2024>(11/45 | 44/146) Enhancing Visual Continual Learning with Language-Guided Supervision (Bolin Ni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bolin Ni, Hongbo Zhao, Chenghao Zhang, Ke Hu, Gaofeng Meng, Zhaoxiang Zhang, Shiming Xiang. (2024)<br><strong>Enhancing Visual Continual Learning with Language-Guided Supervision</strong><br><button class=copy-to-clipboard title="Enhancing Visual Continual Learning with Language-Guided Supervision" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Continual Learning, Knowledge Transfer, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16124v1.pdf filename=2403.16124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> (CL) aims to empower models to learn new tasks without forgetting previously acquired <b>knowledge.</b> <b>Most</b> prior works concentrate on the techniques of architectures, replay data, regularization, \etc. However, the category name of each class is largely neglected. Existing methods commonly utilize the one-hot labels and randomly initialize the classifier head. We argue that the scarce semantic information conveyed by the one-hot labels hampers the effective <b>knowledge</b> <b>transfer</b> across tasks. In this paper, we revisit the role of the classifier head within the CL paradigm and replace the classifier with semantic <b>knowledge</b> <b>from</b> <b>pretrained</b> <b>language</b> <b>models</b> <b>(PLMs).</b> Specifically, we use <b>PLMs</b> to generate semantic targets for each class, which are frozen and serve as supervision signals during training. Such targets fully consider the semantic correlation between all classes across tasks. Empirical studies show that our approach mitigates forgetting by alleviating representation drifting and facilitating <b>knowledge</b> <b>transfer</b> across tasks. The proposed method is simple to implement and can seamlessly be plugged into existing methods with negligible adjustments. Extensive experiments based on eleven mainstream baselines demonstrate the effectiveness and generalizability of our approach to various protocols. For example, under the class-incremental learning setting on ImageNet-100, our method significantly improves the Top-1 accuracy by 3.2% to 6.1% while reducing the forgetting rate by 2.6% to 13.1%.</p></p class="citation"></blockquote><h3 id=1245--45146-segment-anything-model-for-road-network-graph-extraction-congrui-hetang-et-al-2024>(12/45 | 45/146) Segment Anything Model for Road Network Graph Extraction (Congrui Hetang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Congrui Hetang, Haoru Xue, Cindy Le, Tianwei Yue, Wenping Wang, Yihui He. (2024)<br><strong>Segment Anything Model for Road Network Graph Extraction</strong><br><button class=copy-to-clipboard title="Segment Anything Model for Road Network Graph Extraction" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 38<br>Keywords: Graph, Graph Neural Network, Fine-tuning, Geometry, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16051v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16051v1.pdf filename=2403.16051v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose SAM-Road, an adaptation of the Segment Anything Model (SAM) for extracting large-scale, vectorized road network <b>graphs</b> <b>from</b> <b>satellite</b> imagery. To predict <b>graph</b> <b>geometry,</b> <b>we</b> formulate it as a dense semantic segmentation task, leveraging the inherent strengths of SAM. The image encoder of SAM is <b>fine-tuned</b> to produce probability masks for roads and intersections, from which the <b>graph</b> <b>vertices</b> <b>are</b> extracted via simple non-maximum suppression. To predict <b>graph</b> <b>topology,</b> <b>we</b> designed a lightweight <b>transformer-based</b> <b>graph</b> <b>neural</b> <b>network,</b> which leverages the SAM image embeddings to estimate the edge existence probabilities between vertices. Our approach directly predicts the <b>graph</b> <b>vertices</b> <b>and</b> edges for large regions without expensive and complex post-processing heuristics, and is capable of building complete road network <b>graphs</b> <b>spanning</b> <b>multiple</b> square kilometers in a matter of seconds. With its simple, straightforward, and minimalist design, SAM-Road achieves comparable accuracy with the state-of-the-art method RNGDet++, while being 40 times faster on the City-scale dataset. We thus demonstrate the power of a foundational vision model when applied to a <b>graph</b> <b>learning</b> <b>task.</b> The code is available at <a href=https://github.com/htcr/sam_road>https://github.com/htcr/sam_road</a>.</p></p class="citation"></blockquote><h3 id=1345--46146-constricting-normal-latent-space-for-anomaly-detection-with-normal-only-training-data-marcella-astrid-et-al-2024>(13/45 | 46/146) Constricting Normal Latent Space for Anomaly Detection with Normal-only Training Data (Marcella Astrid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcella Astrid, Muhammad Zaigham Zaheer, Seung-Ik Lee. (2024)<br><strong>Constricting Normal Latent Space for Anomaly Detection with Normal-only Training Data</strong><br><button class=copy-to-clipboard title="Constricting Normal Latent Space for Anomaly Detection with Normal-only Training Data" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Anomaly Detection, Autoencoder, Benchmarking, Reconstruction Loss<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16270v1.pdf filename=2403.16270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In order to devise an <b>anomaly</b> <b>detection</b> model using only normal training data, an <b>autoencoder</b> (AE) is typically trained to reconstruct the data. As a result, the AE can extract normal representations in its latent space. During test time, since AE is not trained using real anomalies, it is expected to poorly reconstruct the anomalous data. However, several researchers have observed that it is not the case. In this work, we propose to limit the <b>reconstruction</b> <b>capability</b> of AE by introducing a novel latent constriction loss, which is added to the existing <b>reconstruction</b> <b>loss.</b> By using our method, no extra computational cost is added to the AE during test time. Evaluations using three video <b>anomaly</b> <b>detection</b> <b>benchmark</b> datasets, i.e., Ped2, Avenue, and ShanghaiTech, demonstrate the effectiveness of our method in limiting the <b>reconstruction</b> <b>capability</b> of AE, which leads to a better <b>anomaly</b> <b>detection</b> model.</p></p class="citation"></blockquote><h3 id=1445--47146-adversarially-masked-video-consistency-for-unsupervised-domain-adaptation-xiaoyu-zhu-et-al-2024>(14/45 | 47/146) Adversarially Masked Video Consistency for Unsupervised Domain Adaptation (Xiaoyu Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyu Zhu, Junwei Liang, Po-Yao Huang, Alex Hauptmann. (2024)<br><strong>Adversarially Masked Video Consistency for Unsupervised Domain Adaptation</strong><br><button class=copy-to-clipboard title="Adversarially Masked Video Consistency for Unsupervised Domain Adaptation" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Unsupervised Learning, Transformer, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16242v1.pdf filename=2403.16242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of <b>unsupervised</b> <b>domain</b> <b>adaptation</b> for egocentric videos. We propose a <b>transformer-based</b> model to learn class-discriminative and <b>domain-invariant</b> <b>feature</b> representations. It consists of two novel designs. The first module is called Generative Adversarial <b>Domain</b> <b>Alignment</b> Network with the aim of learning <b>domain-invariant</b> <b>representations.</b> It simultaneously learns a mask generator and a <b>domain-invariant</b> <b>encoder</b> in an adversarial way. The <b>domain-invariant</b> <b>encoder</b> is trained to minimize the distance between the source and target <b>domain.</b> <b>The</b> masking generator, conversely, aims at producing challenging masks by maximizing the <b>domain</b> <b>distance.</b> The second is a Masked Consistency Learning module to learn class-discriminative representations. It enforces the prediction consistency between the masked target videos and their full forms. To better evaluate the effectiveness of <b>domain</b> <b>adaptation</b> methods, we construct a more challenging <b>benchmark</b> for egocentric videos, U-Ego4D. Our method achieves state-of-the-art performance on the Epic-Kitchen and the proposed U-Ego4D <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=1545--48146-knowledge-enhanced-dual-stream-zero-shot-composed-image-retrieval-yucheng-suo-et-al-2024>(15/45 | 48/146) Knowledge-Enhanced Dual-stream Zero-shot Composed Image Retrieval (Yucheng Suo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yucheng Suo, Fan Ma, Linchao Zhu, Yi Yang. (2024)<br><strong>Knowledge-Enhanced Dual-stream Zero-shot Composed Image Retrieval</strong><br><button class=copy-to-clipboard title="Knowledge-Enhanced Dual-stream Zero-shot Composed Image Retrieval" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Zero-shot, Image2text, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16005v1.pdf filename=2403.16005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the <b>zero-shot</b> Composed Image Retrieval (ZS-CIR) task, which is to retrieve the target image given a reference image and a description without training on the triplet datasets. Previous works generate pseudo-word tokens by projecting the reference image features to the <b>text</b> <b>embedding</b> space. However, they focus on the global visual representation, ignoring the representation of detailed attributes, e.g., color, object number and layout. To address this challenge, we propose a Knowledge-Enhanced Dual-stream <b>zero-shot</b> composed image retrieval framework (KEDs). KEDs implicitly models the attributes of the reference images by incorporating a database. The database enriches the pseudo-word tokens by providing relevant images and captions, emphasizing shared attribute information in various aspects. In this way, KEDs recognizes the reference image from diverse perspectives. Moreover, KEDs adopts an extra stream that aligns pseudo-word tokens with textual concepts, leveraging pseudo-triplets mined from <b>image-text</b> pairs. The pseudo-word tokens generated in this stream are explicitly aligned with fine-grained semantics in the <b>text</b> <b>embedding</b> space. Extensive experiments on widely used <b>benchmarks,</b> i.e. ImageNet-R, COCO object, Fashion-IQ and CIRR, show that KEDs outperforms previous <b>zero-shot</b> composed image retrieval methods.</p></p class="citation"></blockquote><h3 id=1645--49146-l-mae-longitudinal-masked-auto-encoder-with-time-and-severity-aware-encoding-for-diabetic-retinopathy-progression-prediction-rachid-zeghlache-et-al-2024>(16/45 | 49/146) L-MAE: Longitudinal masked auto-encoder with time and severity-aware encoding for diabetic retinopathy progression prediction (Rachid Zeghlache et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Yihao Li, Alireza Rezaei, Hugo Le Boité, Ramin Tadayoni, Pascal Massin, Béatrice Cochener, Ikram Brahim, Gwenolé Quellec, Mathieu Lamard. (2024)<br><strong>L-MAE: Longitudinal masked auto-encoder with time and severity-aware encoding for diabetic retinopathy progression prediction</strong><br><button class=copy-to-clipboard title="L-MAE: Longitudinal masked auto-encoder with time and severity-aware encoding for diabetic retinopathy progression prediction" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Self-supervised Learning, Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16272v1.pdf filename=2403.16272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-training strategies based on <b>self-supervised</b> <b>learning</b> (SSL) have proven to be effective pretext tasks for many downstream tasks in computer vision. Due to the significant disparity between medical and natural images, the application of typical SSL is not straightforward in medical imaging. Additionally, those pretext tasks often lack context, which is critical for computer-aided clinical decision support. In this paper, we developed a longitudinal masked auto-encoder (MAE) based on the well-known <b>Transformer-based</b> MAE. In particular, we explored the importance of time-aware position embedding as well as disease progression-aware masking. Taking into account the time between examinations instead of just scheduling them offers the benefit of capturing temporal changes and trends. The masking strategy, for its part, evolves during follow-up to better capture pathological changes, ensuring a more accurate assessment of disease progression. Using OPHDIAT, a large follow-up screening dataset targeting diabetic retinopathy (DR), we evaluated the pre-trained weights on a longitudinal task, which is to predict the severity label of the next visit within 3 years based on the past time series examinations. Our results demonstrated the relevancy of both time-aware position embedding and masking strategies based on disease progression knowledge. Compared to popular baseline models and standard longitudinal <b>Transformers,</b> these simple yet effective extensions significantly enhance the predictive ability of deep classification models.</p></p class="citation"></blockquote><h3 id=1745--50146-towards-online-real-time-memory-based-video-inpainting-transformers-guillaume-thiry-et-al-2024>(17/45 | 50/146) Towards Online Real-Time Memory-based Video Inpainting Transformers (Guillaume Thiry et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guillaume Thiry, Hao Tang, Radu Timofte, Luc Van Gool. (2024)<br><strong>Towards Online Real-Time Memory-based Video Inpainting Transformers</strong><br><button class=copy-to-clipboard title="Towards Online Real-Time Memory-based Video Inpainting Transformers" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16161v1.pdf filename=2403.16161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video inpainting tasks have seen significant improvements in recent years with the rise of deep neural networks and, in particular, <b>vision</b> <b>transformers.</b> Although these models show promising reconstruction quality and temporal consistency, they are still unsuitable for live videos, one of the last steps to make them completely convincing and usable. The main limitations are that these state-of-the-art models inpaint using the whole video (offline processing) and show an insufficient frame rate. In our approach, we propose a framework to adapt existing inpainting <b>transformers</b> to these constraints by memorizing and refining redundant computations while maintaining a decent inpainting quality. Using this framework with some of the most recent inpainting models, we show great online results with a consistent throughput above 20 frames per second. The code and pretrained models will be made available upon acceptance.</p></p class="citation"></blockquote><h3 id=1845--51146-eva-zero-shot-accurate-attributes-and-multi-object-video-editing-xiangpeng-yang-et-al-2024>(18/45 | 51/146) EVA: Zero-shot Accurate Attributes and Multi-Object Video Editing (Xiangpeng Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangpeng Yang, Linchao Zhu, Hehe Fan, Yi Yang. (2024)<br><strong>EVA: Zero-shot Accurate Attributes and Multi-Object Video Editing</strong><br><button class=copy-to-clipboard title="EVA: Zero-shot Accurate Attributes and Multi-Object Video Editing" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Zero-shot, Self-Attention, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16111v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16111v1.pdf filename=2403.16111v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current diffusion-based video editing primarily focuses on local editing (\textit{e.g.,} object/background editing) or global style editing by utilizing various dense correspondences. However, these methods often fail to accurately edit the foreground and background simultaneously while preserving the original layout. We find that the crux of the issue stems from the imprecise distribution of attention weights across designated regions, including inaccurate <b>text-to-attribute</b> <b>control</b> and attention leakage. To tackle this issue, we introduce EVA, a \textbf{zero-shot} and \textbf{multi-attribute} video editing framework tailored for human-centric videos with complex motions. We incorporate a Spatial-Temporal Layout-Guided Attention mechanism that leverages the intrinsic positive and negative correspondences of cross-frame diffusion features. To avoid attention leakage, we utilize these correspondences to boost the attention scores of tokens within the same attribute across all video frames while limiting interactions between tokens of different attributes in the <b>self-attention</b> layer. For precise <b>text-to-attribute</b> <b>manipulation,</b> we use discrete <b>text</b> <b>embeddings</b> focused on specific layout areas within the cross-attention layer. Benefiting from the precise attention weight distribution, EVA can be easily generalized to multi-object editing scenarios and achieves accurate identity mapping. Extensive experiments demonstrate EVA achieves state-of-the-art results in real-world scenarios. Full results are provided at <a href=https://knightyxp.github.io/EVA/>https://knightyxp.github.io/EVA/</a></p></p class="citation"></blockquote><h3 id=1945--52146-are-nerfs-ready-for-autonomous-driving-towards-closing-the-real-to-simulation-gap-carl-lindström-et-al-2024>(19/45 | 52/146) Are NeRFs ready for autonomous driving? Towards closing the real-to-simulation gap (Carl Lindström et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carl Lindström, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson. (2024)<br><strong>Are NeRFs ready for autonomous driving? Towards closing the real-to-simulation gap</strong><br><button class=copy-to-clipboard title="Are NeRFs ready for autonomous driving? Towards closing the real-to-simulation gap" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 30<br>Keywords: Data Augmentation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16092v1.pdf filename=2403.16092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop <b>simulation</b> and <b>data</b> <b>augmentation</b> capabilities. However, to trust the results achieved in <b>simulation,</b> one needs to ensure that AD systems perceive real and rendered <b>data</b> <b>in</b> the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated <b>data</b> <b>gap.</b> Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real <b>data.</b> <b>Moreover,</b> we conduct the first large-scale investigation into the real-to-simulated <b>data</b> <b>gap</b> in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated <b>data,</b> <b>and</b> study the effects of different pre-training strategies. Our results show notable improvements in model robustness to simulated <b>data,</b> <b>even</b> improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators.</p></p class="citation"></blockquote><h3 id=2045--53146-autoinst-automatic-instance-based-segmentation-of-lidar-3d-scans-cedric-perauer-et-al-2024>(20/45 | 53/146) AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans (Cedric Perauer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cedric Perauer, Laurenz Adrian Heidrich, Haifan Zhang, Matthias Nießner, Anastasiia Kornilova, Alexey Artemov. (2024)<br><strong>AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans</strong><br><button class=copy-to-clipboard title="AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-6; I-2-9, cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Graph, Benchmarking, Multi-modal, Self-supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16318v1.pdf filename=2403.16318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, progress in acquisition equipment such as LiDAR sensors has enabled sensing increasingly spacious outdoor 3D environments. Making sense of such 3D acquisitions requires fine-grained scene understanding, such as constructing instance-based 3D scene segmentations. Commonly, a neural network is trained for this task; however, this requires access to a large, densely annotated dataset, which is widely known to be challenging to obtain. To address this issue, in this work we propose to predict instance segmentations for 3D scenes in an <b>unsupervised</b> way, without relying on ground-truth annotations. To this end, we construct a learning framework consisting of two components: (1) a pseudo-annotation scheme for generating initial <b>unsupervised</b> pseudo-labels; and (2) a self-training algorithm for instance segmentation to fit robust, accurate instances from initial noisy proposals. To enable generating 3D instance mask proposals, we construct a weighted proxy-graph by connecting 3D points with edges integrating <b>multi-modal</b> image- and point-based <b>self-supervised</b> features, and perform <b>graph-cuts</b> to isolate individual pseudo-instances. We then build on a state-of-the-art point-based architecture and train a 3D instance segmentation model, resulting in significant refinement of initial proposals. To scale to arbitrary complexity 3D scenes, we design our algorithm to operate on local 3D point chunks and construct a merging step to generate scene-level instance segmentations. Experiments on the challenging SemanticKITTI <b>benchmark</b> demonstrate the potential of our approach, where it attains 13.3% higher Average Precision and 9.1% higher F1 score compared to the best-performing baseline. The code will be made publicly available at <a href=https://github.com/artonson/autoinst>https://github.com/artonson/autoinst</a>.</p></p class="citation"></blockquote><h3 id=2145--54146-avicuna-audio-visual-llm-with-interleaver-and-context-boundary-alignment-for-temporal-referential-dialogue-yunlong-tang-et-al-2024>(21/45 | 54/146) AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue (Yunlong Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunlong Tang, Daiki Shimada, Jing Bi, Chenliang Xu. (2024)<br><strong>AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue</strong><br><button class=copy-to-clipboard title="AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16276v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16276v1.pdf filename=2403.16276v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In everyday communication, humans frequently use speech and gestures to refer to specific areas or objects, a process known as Referential Dialogue (RD). While prior studies have investigated RD through <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> or <b>Large</b> <b>Multimodal</b> <b>Models</b> (LMMs) in static contexts, the exploration of Temporal Referential Dialogue (TRD) within audio-visual media remains limited. Two primary challenges hinder progress in this field: (1) the absence of comprehensive, untrimmed audio-visual video datasets with precise temporal annotations, and (2) the need for methods to integrate complex temporal auditory and visual cues effectively. To address these challenges, we introduce a novel framework to generate PU-VALOR, an extensive audio-visual dataset comprising over 114,000 untrimmed videos with accurate temporal demarcations. We also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI) that ensures the temporal alignment of audio-visual information. Additionally, we develop the A5-222K dataset, encompassing more than 200,000 audio-text pairings, to facilitate the audio and text alignments. Our experiments demonstrate that AVicuna can effectively handle TRD in audio-visual videos and achieve state-of-the-art performance on various audio-visual video understanding tasks, particularly in untrimmed videos. We further investigate the optimal audio-interleaving rate for interleaved audio-visual inputs, which maximizes performance on the Audio-Visual Event Dense Localization task.</p></p class="citation"></blockquote><h3 id=2245--55146-unlearning-backdoor-threats-enhancing-backdoor-defense-in-multimodal-contrastive-learning-via-local-token-unlearning-siyuan-liang-et-al-2024>(22/45 | 55/146) Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning (Siyuan Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyuan Liang, Kuanrong Liu, Jiajun Gong, Jiawei Liang, Yuan Xun, Ee-Chien Chang, Xiaochun Cao. (2024)<br><strong>Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning</strong><br><button class=copy-to-clipboard title="Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Contrastive Learning, Fine-tuning, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16257v1.pdf filename=2403.16257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>contrastive</b> <b>learning</b> has emerged as a powerful paradigm for building high-quality features using the complementary strengths of various data modalities. However, the open nature of such systems inadvertently increases the possibility of backdoor attacks. These attacks subtly embed malicious behaviors within the model during training, which can be activated by specific triggers in the inference phase, posing significant security risks. Despite existing countermeasures through <b>fine-tuning</b> that reduce the adverse impacts of such attacks, these defenses often degrade the clean accuracy and necessitate the construction of extensive clean training pairs. In this paper, we explore the possibility of a less-cost defense from the perspective of model unlearning, that is, whether the model can be made to quickly \textbf{u}nlearn \textbf{b}ackdoor \textbf{t}hreats (UBT) by constructing a small set of poisoned samples. Specifically, we strengthen the backdoor shortcuts to discover suspicious samples through overfitting training prioritized by weak similarity samples. Building on the initial identification of suspicious samples, we introduce an innovative token-based localized forgetting training regime. This technique specifically targets the poisoned aspects of the model, applying a focused effort to unlearn the backdoor associations and trying not to damage the integrity of the overall model. Experimental results show that our method not only ensures a minimal success rate for attacks, but also preserves the model&rsquo;s high clean accuracy.</p></p class="citation"></blockquote><h3 id=2345--56146-opportunities-and-challenges-in-the-application-of-large-artificial-intelligence-models-in-radiology-liangrui-pan-et-al-2024>(23/45 | 56/146) Opportunities and challenges in the application of large artificial intelligence models in radiology (Liangrui Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liangrui Pan, Zhenyu Zhao, Ying Lu, Kewei Tang, Liyong Fu, Qingchun Liang, Shaoliang Peng. (2024)<br><strong>Opportunities and challenges in the application of large artificial intelligence models in radiology</strong><br><button class=copy-to-clipboard title="Opportunities and challenges in the application of large artificial intelligence models in radiology" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, ChatGPT, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16112v1.pdf filename=2403.16112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Influenced by <b>ChatGPT,</b> artificial intelligence (AI) large models have witnessed a global upsurge in large model research and development. As people enjoy the convenience by this AI large model, more and more large models in subdivided fields are gradually being proposed, especially large models in radiology imaging field. This article first introduces the development history of large models, technical details, workflow, working principles of <b>multimodal</b> large models and working principles of video generation large models. Secondly, we <b>summarize</b> the latest research progress of AI large models in radiology education, radiology report generation, applications of unimodal and <b>multimodal</b> radiology. Finally, this paper also <b>summarizes</b> some of the challenges of large AI models in radiology, with the aim of better promoting the rapid revolution in the field of radiography.</p></p class="citation"></blockquote><h3 id=2445--57146-object-detectors-in-the-open-environment-challenges-solutions-and-outlook-siyuan-liang-et-al-2024>(24/45 | 57/146) Object Detectors in the Open Environment: Challenges, Solutions, and Outlook (Siyuan Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyuan Liang, Wei Wang, Ruoyu Chen, Aishan Liu, Boxi Wu, Ee-Chien Chang, Xiaochun Cao, Dacheng Tao. (2024)<br><strong>Object Detectors in the Open Environment: Challenges, Solutions, and Outlook</strong><br><button class=copy-to-clipboard title="Object Detectors in the Open Environment: Challenges, Solutions, and Outlook" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Foundation Model, Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16271v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16271v2.pdf filename=2403.16271v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the emergence of <b>foundation</b> <b>models,</b> deep learning-based object detectors have shown practical usability in closed set scenarios. However, for real-world tasks, object detectors often operate in open environments, where crucial factors (e.g., data distribution, objective) that influence model learning are often changing. The dynamic and intricate nature of the open environment poses novel and formidable challenges to object detectors. Unfortunately, current research on object detectors in open environments lacks a comprehensive analysis of their distinctive characteristics, challenges, and corresponding solutions, which hinders their secure deployment in critical real-world scenarios. This paper aims to bridge this gap by conducting a comprehensive review and analysis of object detectors in open environments. We initially identified limitations of key structural components within the existing detection pipeline and propose the open environment object detector challenge framework that includes four quadrants (i.e., <b>out-of-domain,</b> out-of-category, robust learning, and incremental learning) based on the dimensions of the data / target changes. For each quadrant of challenges in the proposed framework, we present a detailed description and systematic analysis of the overarching goals and core difficulties, systematically review the corresponding solutions, and <b>benchmark</b> their performance over multiple widely adopted datasets. In addition, we engage in a discussion of open problems and potential avenues for future research. This paper aims to provide a fresh, comprehensive, and systematic understanding of the challenges and solutions associated with open-environment object detectors, thus catalyzing the development of more solid applications in real-world scenarios. A project related to this survey can be found at <a href=https://github.com/LiangSiyuan21/OEOD_Survey>https://github.com/LiangSiyuan21/OEOD_Survey</a>.</p></p class="citation"></blockquote><h3 id=2545--58146-emotion-recognition-from-the-perspective-of-activity-recognition-savinay-nagendra-et-al-2024>(25/45 | 58/146) Emotion Recognition from the perspective of Activity Recognition (Savinay Nagendra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Savinay Nagendra, Prapti Panigrahi. (2024)<br><strong>Emotion Recognition from the perspective of Activity Recognition</strong><br><button class=copy-to-clipboard title="Emotion Recognition from the perspective of Activity Recognition" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Emotion Recognition, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16263v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16263v1.pdf filename=2403.16263v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Applications of an efficient <b>emotion</b> <b>recognition</b> system can be found in several domains such as medicine, driver fatigue surveillance, social robotics, and human-computer interaction. Appraising human <b>emotional</b> <b>states,</b> behaviors, and reactions displayed in real-world settings can be accomplished using latent continuous dimensions. Continuous dimensional models of human affect, such as those based on valence and arousal are more accurate in describing a broad range of spontaneous everyday <b>emotions</b> <b>than</b> more traditional models of discrete stereotypical <b>emotion</b> <b>categories</b> (e.g. happiness, surprise). Most of the prior work on estimating valence and arousal considers laboratory settings and acted data. But, for <b>emotion</b> <b>recognition</b> systems to be deployed and integrated into real-world mobile and computing devices, we need to consider data collected in the world. Action recognition is a domain of Computer Vision that involves capturing complementary information on appearance from still frames and motion between frames. In this paper, we treat <b>emotion</b> <b>recognition</b> from the perspective of action recognition by exploring the application of deep learning architectures specifically designed for action recognition, for continuous affect recognition. We propose a novel three-stream end-to-end deep learning regression pipeline with an attention mechanism, which is an ensemble design based on sub-modules of multiple state-of-the-art action recognition systems. The pipeline constitutes a novel data pre-processing approach with a spatial <b>self-attention</b> mechanism to extract keyframes. The optical flow of high-attention regions of the face is extracted to capture temporal context. AFEW-VA in-the-wild dataset has been used to conduct comparative experiments. Quantitative analysis shows that the proposed model outperforms multiple standard baselines of both <b>emotion</b> <b>recognition</b> and action recognition models.</p></p class="citation"></blockquote><h3 id=2645--59146-skull-to-face-anatomy-guided-3d-facial-reconstruction-and-editing-yongqing-liang-et-al-2024>(26/45 | 59/146) Skull-to-Face: Anatomy-Guided 3D Facial Reconstruction and Editing (Yongqing Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongqing Liang, Congyi Zhang, Junli Zhao, Wenping Wang, Xin Li. (2024)<br><strong>Skull-to-Face: Anatomy-Guided 3D Facial Reconstruction and Editing</strong><br><button class=copy-to-clipboard title="Skull-to-Face: Anatomy-Guided 3D Facial Reconstruction and Editing" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-CY, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16207v1.pdf filename=2403.16207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deducing the 3D face from a skull is an essential but challenging task in forensic science and archaeology. Existing methods for automated facial reconstruction yield inaccurate results, suffering from the non-determinative nature of the problem that a skull with a sparse set of tissue depth cannot fully determine the skinned face. Additionally, their texture-less results require further post-processing stages to achieve a photo-realistic appearance. This paper proposes an end-to-end 3D face reconstruction and exploration tool, providing textured 3D faces for reference. With the help of state-of-the-art <b>text-to-image</b> <b>diffusion</b> <b>models</b> and image-based facial reconstruction techniques, we generate an initial reference 3D face, whose biological profile aligns with the given skull. We then adapt these initial faces to meet the statistical expectations of extruded anatomical landmarks on the skull through an optimization process. The joint statistical distribution of tissue depths is learned on a small set of anatomical landmarks on the skull. To support further adjustment, we propose an efficient face adaptation tool to assist users in tuning tissue depths, either globally or at local regions, while observing plausible visual feedback. Experiments conducted on a real skull-face dataset demonstrated the effectiveness of our proposed pipeline in terms of reconstruction accuracy, diversity, and stability.</p></p class="citation"></blockquote><h3 id=2745--60146-fh-sstnet-forehead-creases-based-user-verification-using-spatio-spatial-temporal-network-geetanjali-sharma-et-al-2024>(27/45 | 60/146) FH-SSTNet: Forehead Creases based User Verification using Spatio-Spatial Temporal Network (Geetanjali Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Geetanjali Sharma, Gaurav Jaswal, Aditya Nigam, Raghavendra Ramachandra. (2024)<br><strong>FH-SSTNet: Forehead Creases based User Verification using Spatio-Spatial Temporal Network</strong><br><button class=copy-to-clipboard title="FH-SSTNet: Forehead Creases based User Verification using Spatio-Spatial Temporal Network" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16202v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16202v1.pdf filename=2403.16202v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Biometric authentication, which utilizes contactless features, such as forehead patterns, has become increasingly important for identity verification and access management. The proposed method is based on learning a 3D spatio-spatial temporal <b>convolution</b> to create detailed pictures of forehead patterns. We introduce a new <b>CNN</b> model called the Forehead Spatio-Spatial Temporal Network (FH-SSTNet), which utilizes a 3D <b>CNN</b> architecture with triplet loss to capture distinguishing features. We enhance the model&rsquo;s discrimination capability using Arcloss in the network&rsquo;s head. Experimentation on the Forehead Creases version 1 (FH-V1) dataset, containing 247 unique subjects, demonstrates the superior performance of FH-SSTNet compared to existing methods and pre-trained <b>CNNs</b> like ResNet50, especially for forehead-based user verification. The results demonstrate the superior performance of FH-SSTNet for forehead-based user verification, confirming its effectiveness in identity authentication.</p></p class="citation"></blockquote><h3 id=2845--61146-salience-detr-enhancing-detection-transformer-with-hierarchical-salience-filtering-refinement-xiuquan-hou-et-al-2024>(28/45 | 61/146) Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement (Xiuquan Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiuquan Hou, Meiqin Liu, Senlin Zhang, Ping Wei, Badong Chen. (2024)<br><strong>Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement</strong><br><button class=copy-to-clipboard title="Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16131v1.pdf filename=2403.16131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>DETR-like methods have significantly increased detection performance in an end-to-end manner. The mainstream two-stage frameworks of them perform dense <b>self-attention</b> and select a fraction of queries for sparse cross-attention, which is proven effective for improving performance but also introduces a heavy computational burden and high dependence on stable query selection. This paper demonstrates that suboptimal two-stage selection strategies result in scale bias and redundancy due to the mismatch between selected queries and objects in two-stage initialization. To address these issues, we propose hierarchical salience filtering refinement, which performs <b>transformer</b> encoding only on filtered discriminative queries, for a better trade-off between computational efficiency and precision. The filtering process overcomes scale bias through a novel scale-independent salience supervision. To compensate for the semantic misalignment among queries, we introduce elaborate query refinement modules for stable two-stage initialization. Based on above improvements, the proposed Salience DETR achieves significant improvements of +4.0% AP, +0.2% AP, +4.4% AP on three challenging task-specific detection datasets, as well as 49.2% AP on COCO 2017 with less FLOPs. The code is available at <a href=https://github.com/xiuqhou/Salience-DETR>https://github.com/xiuqhou/Salience-DETR</a>.</p></p class="citation"></blockquote><h3 id=2945--62146-self-supervised-multi-frame-neural-scene-flow-dongrui-liu-et-al-2024>(29/45 | 62/146) Self-Supervised Multi-Frame Neural Scene Flow (Dongrui Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongrui Liu, Daqi Liu, Xueqian Li, Sihao Lin, Hongwei xie, Bing Wang, Xiaojun Chang, Lei Chu. (2024)<br><strong>Self-Supervised Multi-Frame Neural Scene Flow</strong><br><button class=copy-to-clipboard title="Self-Supervised Multi-Frame Neural Scene Flow" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Out-of-distribution, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16116v1.pdf filename=2403.16116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Scene Flow Prior (NSFP) and Fast Neural Scene Flow (FNSF) have shown remarkable adaptability in the context of large <b>out-of-distribution</b> autonomous driving. Despite their success, the underlying reasons for their astonishing generalization capabilities remain unclear. Our research addresses this gap by examining the generalization capabilities of NSFP through the lens of uniform stability, revealing that its performance is inversely proportional to the number of input point clouds. This finding sheds light on NSFP&rsquo;s effectiveness in handling large-scale point cloud scene flow estimation tasks. Motivated by such theoretical insights, we further explore the improvement of scene flow estimation by leveraging historical point clouds across multiple frames, which inherently increases the number of point clouds. Consequently, we propose a simple and effective method for multi-frame point cloud scene flow estimation, along with a theoretical evaluation of its generalization abilities. Our analysis confirms that the proposed method maintains a limited generalization error, suggesting that adding multiple frames to the scene flow optimization process does not detract from its generalizability. Extensive experimental results on large-scale autonomous driving Waymo Open and Argoverse lidar datasets demonstrate that the proposed method achieves state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=3045--63146-mars-spectrometry-2-gas-chromatography----second-place-solution-dmitry-a-konovalov-2024>(30/45 | 63/146) Mars Spectrometry 2: Gas Chromatography &ndash; Second place solution (Dmitry A. Konovalov, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dmitry A. Konovalov. (2024)<br><strong>Mars Spectrometry 2: Gas Chromatography &ndash; Second place solution</strong><br><button class=copy-to-clipboard title="Mars Spectrometry 2: Gas Chromatography -- Second place solution" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15990v1.pdf filename=2403.15990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Mars Spectrometry 2: Gas Chromatography challenge was sponsored by NASA and run on the DrivenData competition platform in 2022. This report describes the solution which achieved the second-best score on the competition&rsquo;s test dataset. The solution utilized two-dimensional, image-like representations of the competition&rsquo;s chromatography data samples. A number of different <b>Convolutional</b> <b>Neural</b> <b>Network</b> models were trained and ensembled for the final submission.</p></p class="citation"></blockquote><h3 id=3145--64146-towards-two-stream-foveation-based-active-vision-learning-timur-ibrayev-et-al-2024>(31/45 | 64/146) Towards Two-Stream Foveation-based Active Vision Learning (Timur Ibrayev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timur Ibrayev, Amitangshu Mukherjee, Sai Aparna Aketi, Kaushik Roy. (2024)<br><strong>Towards Two-Stream Foveation-based Active Vision Learning</strong><br><button class=copy-to-clipboard title="Towards Two-Stream Foveation-based Active Vision Learning" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15977v1.pdf filename=2403.15977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural network (DNN) based machine perception frameworks process the entire input in a one-shot manner to provide answers to both &ldquo;what object is being observed&rdquo; and &ldquo;where it is located&rdquo;. In contrast, the &ldquo;two-stream hypothesis&rdquo; from neuroscience explains the neural processing in the human visual cortex as an active vision system that utilizes two separate regions of the brain to answer the what and the where questions. In this work, we propose a machine learning framework inspired by the &ldquo;two-stream hypothesis&rdquo; and explore the potential benefits that it offers. Specifically, the proposed framework models the following mechanisms: 1) ventral (what) stream focusing on the input regions perceived by the fovea part of an eye (foveation), 2) dorsal (where) stream providing visual guidance, and 3) iterative processing of the two streams to calibrate visual focus and process the sequence of focused image patches. The training of the proposed framework is accomplished by label-based DNN training for the ventral stream model and <b>reinforcement</b> <b>learning</b> for the dorsal stream model. We show that the two-stream foveation-based learning is applicable to the challenging task of <b>weakly-supervised</b> object localization (WSOL), where the training data is limited to the object class or its attributes. The framework is capable of both predicting the properties of an object and successfully localizing it by predicting its bounding box. We also show that, due to the independent nature of the two streams, the dorsal model can be applied on its own to unseen images to localize objects from different datasets.</p></p class="citation"></blockquote><h3 id=3245--65146-edit3k-universal-representation-learning-for-video-editing-components-xin-gu-et-al-2024>(32/45 | 65/146) Edit3K: Universal Representation Learning for Video Editing Components (Xin Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Gu, Libo Zhang, Fan Chen, Longyin Wen, Yufei Wang, Tiejian Luo, Sijie Zhu. (2024)<br><strong>Edit3K: Universal Representation Learning for Video Editing Components</strong><br><button class=copy-to-clipboard title="Edit3K: Universal Representation Learning for Video Editing Components" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 18<br>Keywords: Benchmarking, Recommendation, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16048v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16048v1.pdf filename=2403.16048v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on understanding the predominant video creation pipeline, i.e., compositional video editing with six main types of editing components, including video effects, animation, transition, filter, sticker, and text. In contrast to existing visual <b>representation</b> <b>learning</b> of visual materials (i.e., images/videos), we aim to learn visual <b>representations</b> <b>of</b> editing actions/components that are generally applied on raw materials. We start by proposing the first large-scale dataset for editing components of video creation, which covers about $3,094$ editing components with $618,800$ videos. Each video in our dataset is rendered by various image/video materials with a single editing component, which supports atomic visual understanding of different editing components. It can also benefit several downstream tasks, e.g., editing component <b>recommendation,</b> editing component recognition/retrieval, etc. Existing visual <b>representation</b> <b>methods</b> perform poorly because it is difficult to disentangle the visual appearance of editing components from raw materials. To that end, we <b>benchmark</b> popular alternative solutions and propose a novel method that learns to attend to the appearance of editing components regardless of raw materials. Our method achieves favorable results on editing component retrieval/recognition compared to the alternative solutions. A user study is also conducted to show that our <b>representations</b> <b>cluster</b> visually similar editing components better than other alternatives. Furthermore, our learned <b>representations</b> <b>used</b> to transition <b>recommendation</b> tasks achieve state-of-the-art results on the AutoTransition dataset. The code and dataset will be released for academic use.</p></p class="citation"></blockquote><h3 id=3345--66146-blur2blur-blur-conversion-for-unsupervised-image-deblurring-on-unknown-domains-bang-dang-pham-et-al-2024>(33/45 | 66/146) Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown Domains (Bang-Dang Pham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bang-Dang Pham, Phong Tran, Anh Tran, Cuong Pham, Rang Nguyen, Minh Hoai. (2024)<br><strong>Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown Domains</strong><br><button class=copy-to-clipboard title="Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown Domains" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16205v1.pdf filename=2403.16205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an innovative framework designed to train an image deblurring algorithm tailored to a specific camera device. This algorithm works by transforming a blurry input image, which is challenging to deblur, into another blurry image that is more amenable to deblurring. The transformation process, from one blurry state to another, leverages unpaired data consisting of sharp and blurry images captured by the target camera device. Learning this blur-to-blur transformation is inherently simpler than direct blur-to-sharp conversion, as it primarily involves modifying blur patterns rather than the intricate task of reconstructing fine image details. The efficacy of the proposed approach has been demonstrated through comprehensive experiments on various <b>benchmarks,</b> where it significantly outperforms state-of-the-art methods both quantitatively and qualitatively. Our code and data are available at <a href=https://zero1778.github.io/blur2blur/>https://zero1778.github.io/blur2blur/</a></p></p class="citation"></blockquote><h3 id=3445--67146-improving-scene-graph-generation-with-relation-words-debiasing-in-vision-language-models-yuxuan-wang-et-al-2024>(34/45 | 67/146) Improving Scene Graph Generation with Relation Words&rsquo; Debiasing in Vision-Language Models (Yuxuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Wang, Xiaoyuan Liu. (2024)<br><strong>Improving Scene Graph Generation with Relation Words&rsquo; Debiasing in Vision-Language Models</strong><br><button class=copy-to-clipboard title="Improving Scene Graph Generation with Relation Words' Debiasing in Vision-Language Models" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Graph, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16184v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16184v1.pdf filename=2403.16184v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene <b>Graph</b> Generation (SGG) provides basic language representation of visual scenes, requiring models to grasp complex and diverse semantics between various objects. However, this complexity and diversity in SGG also leads to underrepresentation, where part of test triplets are rare or even unseen during training, resulting in imprecise predictions. To tackle this, we propose using the SGG models with pretrained <b>vision-language</b> models (VLMs) to enhance representation. However, due to the gap between the pretraining and SGG, directly ensembling the pretrained VLMs leads to severe biases across relation words. Thus, we introduce LM Estimation to approximate the words&rsquo; distribution underlies in the pretraining language sets, and then use the distribution for debiasing. After that, we ensemble VLMs with SGG models to enhance representation. Considering that each model may represent better at different samples, we use a certainty-aware indicator to score each sample and dynamically adjust the ensemble weights. Our method effectively addresses the words biases, enhances SGG&rsquo;s representation, and achieve markable performance enhancements. It is training-free and integrates well with existing SGG models.</p></p class="citation"></blockquote><h3 id=3545--68146-gaze-guided-hand-object-interaction-synthesis-benchmark-and-method-jie-tian-et-al-2024>(35/45 | 68/146) Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method (Jie Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Tian, Lingxiao Yang, Ran Ji, Yuexin Ma, Lan Xu, Jingyi Yu, Ye Shi, Jingya Wang. (2024)<br><strong>Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method</strong><br><button class=copy-to-clipboard title="Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16169v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16169v3.pdf filename=2403.16169v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gaze plays a crucial role in revealing human attention and intention, shedding light on the cognitive processes behind human actions. The integration of gaze guidance with the dynamics of hand-object interactions boosts the accuracy of human motion prediction. However, the lack of datasets that capture the intricate relationship and consistency among gaze, hand, and object movements remains a substantial hurdle. In this paper, we introduce the first Gaze-guided Hand-Object Interaction dataset, GazeHOI, and present a novel task for synthesizing gaze-guided hand-object interactions. Our dataset, GazeHOI, features simultaneous 3D modeling of gaze, hand, and object interactions, comprising 479 sequences with an average duration of 19.1 seconds, 812 sub-sequences, and 33 objects of various sizes. We propose a hierarchical framework centered on a gaze-guided hand-object interaction <b>diffusion</b> <b>model,</b> named GHO-Diffusion. In the pre-diffusion phase, we separate gaze conditions into spatial-temporal features and goal pose conditions at different levels of information granularity. During the <b>diffusion</b> <b>phase,</b> two gaze-conditioned <b>diffusion</b> <b>models</b> are stacked to simplify the complex synthesis of hand-object motions. Here, the object motion <b>diffusion</b> <b>model</b> generates sequences of object motions based on gaze conditions, while the hand motion <b>diffusion</b> <b>model</b> produces hand motions based on the generated object motion. To improve fine-grained goal pose alignment, we introduce a Spherical Gaussian constraint to guide the denoising step. In the subsequent post-diffusion phase, we optimize the generated hand motions using contact consistency. Our extensive experiments highlight the uniqueness of our dataset and the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=3645--69146-sm2c-boost-the-semi-supervised-segmentation-for-medical-image-by-using-meta-pseudo-labels-and-mixed-images-yifei-wang-et-al-2024>(36/45 | 69/146) SM2C: Boost the Semi-supervised Segmentation for Medical Image by using Meta Pseudo Labels and Mixed Images (Yifei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Wang, Chuhong Zhu. (2024)<br><strong>SM2C: Boost the Semi-supervised Segmentation for Medical Image by using Meta Pseudo Labels and Mixed Images</strong><br><button class=copy-to-clipboard title="SM2C: Boost the Semi-supervised Segmentation for Medical Image by using Meta Pseudo Labels and Mixed Images" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16009v1.pdf filename=2403.16009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, machine learning-based semantic segmentation algorithms have demonstrated their potential to accurately segment regions and contours in medical images, allowing the precise location of anatomical structures and abnormalities. Although medical images are difficult to acquire and annotate, <b>semi-supervised</b> <b>learning</b> methods are efficient in dealing with the scarcity of labeled data. However, overfitting is almost inevitable due to the limited images for training. Furthermore, the intricate shapes of organs and lesions in medical images introduce additional complexity in different cases, preventing networks from acquiring a strong ability to generalize. To this end, we introduce a novel method called Scaling-up Mix with Multi-Class (SM2C). This method uses three strategies - scaling-up image size, multi-class mixing, and object shape jittering - to improve the ability to learn semantic features within medical images. By diversifying the shape of the segmentation objects and enriching the semantic information within each sample, the SM2C demonstrates its potential, especially in the training of unlabelled data. Extensive experiments demonstrate the effectiveness of the SM2C on three <b>benchmark</b> medical image segmentation datasets. The proposed framework shows significant improvements over state-of-the-art counterparts.</p></p class="citation"></blockquote><h3 id=3745--70146-image-captioning-in-news-report-scenario-tianrui-liu-et-al-2024>(37/45 | 70/146) Image Captioning in news report scenario (Tianrui Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianrui Liu, Qi Cai, Changxin Xu, Bo Hong, Jize Xiong, Yuxin Qiao, Tsungwei Yang. (2024)<br><strong>Image Captioning in news report scenario</strong><br><button class=copy-to-clipboard title="Image Captioning in news report scenario" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16209v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16209v2.pdf filename=2403.16209v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image captioning strives to generate pertinent captions for specified images, situating itself at the crossroads of Computer Vision (CV) and Natural Language Processing (NLP). This endeavor is of paramount importance with far-reaching applications in <b>recommendation</b> systems, news outlets, social media, and beyond. Particularly within the realm of news reporting, captions are expected to encompass detailed information, such as the identities of celebrities captured in the images. However, much of the existing body of work primarily centers around understanding scenes and actions. In this paper, we explore the realm of image captioning specifically tailored for celebrity photographs, illustrating its broad potential for enhancing news industry practices. This exploration aims to augment automated news content generation, thereby facilitating a more nuanced dissemination of information. Our endeavor shows a broader horizon, enriching the narrative in news reporting through a more intuitive image captioning framework.</p></p class="citation"></blockquote><h3 id=3845--71146-diffusion-model-is-a-good-pose-estimator-from-3d-rf-vision-junqiao-fan-et-al-2024>(38/45 | 71/146) Diffusion Model is a Good Pose Estimator from 3D RF-Vision (Junqiao Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junqiao Fan, Jianfei Yang, Yuecong Xu, Lihua Xie. (2024)<br><strong>Diffusion Model is a Good Pose Estimator from 3D RF-Vision</strong><br><button class=copy-to-clipboard title="Diffusion Model is a Good Pose Estimator from 3D RF-Vision" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16198v1.pdf filename=2403.16198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human pose estimation (HPE) from Radio Frequency vision (RF-vision) performs human sensing using RF signals that penetrate obstacles without revealing privacy (e.g., facial information). Recently, mmWave radar has emerged as a promising RF-vision sensor, providing radar point clouds by processing RF signals. However, the mmWave radar has a limited resolution with severe noise, leading to inaccurate and inconsistent human pose estimation. This work proposes mmDiff, a novel <b>diffusion-based</b> <b>pose</b> estimator tailored for noisy radar data. Our approach aims to provide reliable guidance as conditions to <b>diffusion</b> <b>models.</b> Two key challenges are addressed by mmDiff: (1) miss-detection of parts of human bodies, which is addressed by a module that isolates feature extraction from different body parts, and (2) signal inconsistency due to environmental interference, which is tackled by incorporating prior knowledge of body structure and motion. Several modules are designed to achieve these goals, whose features work as the conditions for the subsequent <b>diffusion</b> <b>model,</b> eliminating the miss-detection and instability of HPE based on RF-vision. Extensive experiments demonstrate that mmDiff outperforms existing methods significantly, achieving state-of-the-art performances on public datasets.</p></p class="citation"></blockquote><h3 id=3945--72146-semantic-is-enough-only-semantic-information-for-nerf-reconstruction-ruibo-wang-et-al-2024>(39/45 | 72/146) Semantic Is Enough: Only Semantic Information For NeRF Reconstruction (Ruibo Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruibo Wang, Song Zhang, Ping Huang, Donghai Zhang, Wei Yan. (2024)<br><strong>Semantic Is Enough: Only Semantic Information For NeRF Reconstruction</strong><br><button class=copy-to-clipboard title="Semantic Is Enough: Only Semantic Information For NeRF Reconstruction" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16043v1.pdf filename=2403.16043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research that combines implicit 3D representation with semantic information, like Semantic-NeRF, has proven that NeRF model could perform excellently in rendering 3D structures with semantic labels. This research aims to extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing solely on semantic output and removing the RGB output component. We reformulate the model and its training procedure to leverage only the cross-entropy loss between the model semantic output and the ground truth semantic images, removing the colour data traditionally used in the original Semantic-NeRF approach. We then conduct a series of identical experiments using the original and the modified Semantic-NeRF model. Our primary objective is to obverse the impact of this modification on the model performance by Semantic-NeRF, focusing on tasks such as scene understanding, <b>object</b> <b>detection,</b> and segmentation. The results offer valuable insights into the new way of rendering the scenes and provide an avenue for further research and development in semantic-focused 3D scene understanding.</p></p class="citation"></blockquote><h3 id=4045--73146-exploring-the-impact-of-dataset-bias-on-dataset-distillation-yao-lu-et-al-2024>(40/45 | 73/146) Exploring the Impact of Dataset Bias on Dataset Distillation (Yao Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Lu, Jianyang Gu, Xuguang Chen, Saeed Vahidian, Qi Xuan. (2024)<br><strong>Exploring the Impact of Dataset Bias on Dataset Distillation</strong><br><button class=copy-to-clipboard title="Exploring the Impact of Dataset Bias on Dataset Distillation" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16028v1.pdf filename=2403.16028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dataset <b>Distillation</b> (DD) is a promising technique to synthesize a smaller dataset that preserves essential information from the original dataset. This synthetic dataset can serve as a substitute for the original large-scale one, and help alleviate the training workload. However, current DD methods typically operate under the assumption that the dataset is unbiased, overlooking potential bias issues within the dataset itself. To fill in this blank, we systematically investigate the influence of dataset bias on DD. To the best of our knowledge, this is the first exploration in the DD domain. Given that there are no suitable biased datasets for DD, we first construct two biased datasets, CMNIST-DD and CCIFAR10-DD, to establish a foundation for subsequent analysis. Then we utilize existing DD methods to generate synthetic datasets on CMNIST-DD and CCIFAR10-DD, and evaluate their performance following the standard process. Experiments demonstrate that biases present in the original dataset significantly impact the performance of the synthetic dataset in most cases, which highlights the necessity of identifying and mitigating biases in the original datasets during DD. Finally, we reformulate DD within the context of a biased dataset. Our code along with biased datasets are available at <a href=https://github.com/yaolu-zjut/Biased-DD>https://github.com/yaolu-zjut/Biased-DD</a>.</p></p class="citation"></blockquote><h3 id=4145--74146-fill-in-the-____-a-diffusion-based-image-inpainting-pipeline-eyoel-gebre-et-al-2024>(41/45 | 74/146) Fill in the ____ (a Diffusion-based Image Inpainting Pipeline) (Eyoel Gebre et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eyoel Gebre, Krishna Saxena, Timothy Tran. (2024)<br><strong>Fill in the ____ (a Diffusion-based Image Inpainting Pipeline)</strong><br><button class=copy-to-clipboard title="Fill in the ____ (a Diffusion-based Image Inpainting Pipeline)" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16016v1.pdf filename=2403.16016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image inpainting is the process of taking an image and generating lost or intentionally occluded portions. Inpainting has countless applications including restoring previously damaged pictures, restoring the quality of images that have been degraded due to compression, and removing unwanted objects/text. Modern inpainting techniques have shown remarkable ability in generating sensible completions for images with mask occlusions. In our paper, an overview of the progress of inpainting techniques will be provided, along with identifying current leading approaches, focusing on their strengths and weaknesses. A critical gap in these existing models will be addressed, focusing on the ability to <b>prompt</b> and control what exactly is generated. We will additionally justify why we think this is the natural next progressive step that inpainting models must take, and provide multiple approaches to implementing this functionality. Finally, we will evaluate the results of our approaches by qualitatively checking whether they generate high-quality images that correctly inpaint regions with the objects that they are instructed to produce.</p></p class="citation"></blockquote><h3 id=4245--75146-egoexolearn-a-dataset-for-bridging-asynchronous-ego--and-exo-centric-view-of-procedural-activities-in-real-world-yifei-huang-et-al-2024>(42/45 | 75/146) EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World (Yifei Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Huang, Guo Chen, Jilan Xu, Mingfang Zhang, Lijin Yang, Baoqi Pei, Hongjie Zhang, Lu Dong, Yali Wang, Limin Wang, Yu Qiao. (2024)<br><strong>EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World</strong><br><button class=copy-to-clipboard title="EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 9<br>Keywords: Benchmarking, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16182v1.pdf filename=2403.16182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Being able to map the activities of others into one&rsquo;s own point of view is one fundamental human skill even from a very early age. Taking a step toward understanding this human ability, we introduce EgoExoLearn, a large-scale dataset that emulates the human demonstration following process, in which individuals record egocentric videos as they execute tasks guided by demonstration videos. Focusing on the potential applications in daily assistance and professional support, EgoExoLearn contains egocentric and demonstration video data spanning 120 hours captured in daily life scenarios and specialized laboratories. Along with the videos we record high-quality gaze data and provide detailed <b>multimodal</b> annotations, formulating a playground for modeling the human ability to bridge asynchronous procedural actions from different viewpoints. To this end, we present <b>benchmarks</b> such as cross-view association, cross-view action planning, and cross-view referenced skill assessment, along with detailed analysis. We expect EgoExoLearn can serve as an important resource for bridging the actions across views, thus paving the way for creating AI agents capable of seamlessly learning by observing humans in the real world. Code and data can be found at: <a href=https://github.com/OpenGVLab/EgoExoLearn>https://github.com/OpenGVLab/EgoExoLearn</a></p></p class="citation"></blockquote><h3 id=4345--76146-v2x-real-a-largs-scale-dataset-for-vehicle-to-everything-cooperative-perception-hao-xiang-et-al-2024>(43/45 | 76/146) V2X-Real: a Largs-Scale Dataset for Vehicle-to-Everything Cooperative Perception (Hao Xiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Xiang, Zhaoliang Zheng, Xin Xia, Runsheng Xu, Letian Gao, Zewei Zhou, Xu Han, Xinkai Ji, Mingxi Li, Zonglin Meng, Li Jin, Mingyue Lei, Zhaoyang Ma, Zihang He, Haoxuan Ma, Yunshuang Yuan, Yingqian Zhao, Jiaqi Ma. (2024)<br><strong>V2X-Real: a Largs-Scale Dataset for Vehicle-to-Everything Cooperative Perception</strong><br><button class=copy-to-clipboard title="V2X-Real: a Largs-Scale Dataset for Vehicle-to-Everything Cooperative Perception" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16034v1.pdf filename=2403.16034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in Vehicle-to-Everything (V2X) technologies have enabled autonomous vehicles to share sensing information to see through occlusions, greatly boosting the perception capability. However, there are no real-world datasets to facilitate the real V2X cooperative perception research &ndash; existing datasets either only support Vehicle-to-Infrastructure cooperation or Vehicle-to-Vehicle cooperation. In this paper, we propose a dataset that has a mixture of multiple vehicles and smart infrastructure simultaneously to facilitate the V2X cooperative perception development with multi-modality sensing data. Our V2X-Real is collected using two connected automated vehicles and two smart infrastructures, which are all equipped with <b>multi-modal</b> sensors including LiDAR sensors and multi-view cameras. The whole dataset contains 33K LiDAR frames and 171K camera data with over 1.2M annotated bounding boxes of 10 categories in very challenging urban scenarios. According to the collaboration mode and ego perspective, we derive four types of datasets for Vehicle-Centric, Infrastructure-Centric, Vehicle-to-Vehicle, and Infrastructure-to-Infrastructure cooperative perception. Comprehensive multi-class multi-agent <b>benchmarks</b> of SOTA cooperative perception methods are provided. The V2X-Real dataset and <b>benchmark</b> codes will be released.</p></p class="citation"></blockquote><h3 id=4445--77146-inverse-rendering-of-glossy-objects-via-the-neural-plenoptic-function-and-radiance-fields-haoyuan-wang-et-al-2024>(44/45 | 77/146) Inverse Rendering of Glossy Objects via the Neural Plenoptic Function and Radiance Fields (Haoyuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyuan Wang, Wenbo Hu, Lei Zhu, Rynson W. H. Lau. (2024)<br><strong>Inverse Rendering of Glossy Objects via the Neural Plenoptic Function and Radiance Fields</strong><br><button class=copy-to-clipboard title="Inverse Rendering of Glossy Objects via the Neural Plenoptic Function and Radiance Fields" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16224v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16224v1.pdf filename=2403.16224v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inverse rendering aims at recovering both <b>geometry</b> and materials of objects. It provides a more compatible reconstruction for conventional rendering engines, compared with the neural radiance fields (NeRFs). On the other hand, existing NeRF-based inverse rendering methods cannot handle glossy objects with local light interactions well, as they typically oversimplify the illumination as a 2D environmental map, which assumes infinite lights only. Observing the superiority of NeRFs in recovering radiance fields, we propose a novel 5D Neural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more accurate lighting-object interactions can be formulated via the rendering equation. We also design a material-aware cone sampling strategy to efficiently integrate lights inside the BRDF lobes with the help of pre-filtered radiance fields. Our method has two stages: the <b>geometry</b> of the target object and the pre-filtered environmental radiance fields are reconstructed in the first stage, and materials of the target object are estimated in the second stage with the proposed NeP and material-aware cone sampling strategy. Extensive experiments on the proposed real-world and synthetic datasets demonstrate that our method can reconstruct high-fidelity geometry/materials of challenging glossy objects with complex lighting interactions from nearby objects. Project webpage: <a href=https://whyy.site/paper/nep>https://whyy.site/paper/nep</a></p></p class="citation"></blockquote><h3 id=4545--78146-pku-dymvhumans-a-multi-view-video-benchmark-for-high-fidelity-dynamic-human-modeling-xiaoyun-zheng-et-al-2024>(45/45 | 78/146) PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling (Xiaoyun Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyun Zheng, Liwei Liao, Xufeng Li, Jianbo Jiao, Rongjie Wang, Feng Gao, Shiqi Wang, Ronggang Wang. (2024)<br><strong>PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling</strong><br><button class=copy-to-clipboard title="PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16080v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16080v2.pdf filename=2403.16080v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics. Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses. In part, this is due to the challenges of acquiring high-quality human datasets. To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos. It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios. These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion. Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and <b>benchmark</b> on PKU-DyMVHumans dataset. It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene. Extensive studies are performed on the <b>benchmark,</b> demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data. The dataset is available at: <a href=https://pku-dymvhumans.github.io>https://pku-dymvhumans.github.io</a>.</p></p class="citation"></blockquote><h2 id=eessiv-5>eess.IV (5)</h2><h3 id=15--79146-enhancing-mri-based-classification-of-alzheimers-disease-with-explainable-3d-hybrid-compact-convolutional-transformers-arindam-majee-et-al-2024>(1/5 | 79/146) Enhancing MRI-Based Classification of Alzheimer&rsquo;s Disease with Explainable 3D Hybrid Compact Convolutional Transformers (Arindam Majee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arindam Majee, Avisek Gupta, Sourav Raha, Swagatam Das. (2024)<br><strong>Enhancing MRI-Based Classification of Alzheimer&rsquo;s Disease with Explainable 3D Hybrid Compact Convolutional Transformers</strong><br><button class=copy-to-clipboard title="Enhancing MRI-Based Classification of Alzheimer's Disease with Explainable 3D Hybrid Compact Convolutional Transformers" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 63<br>Keywords: Vision Transformer, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16175v1.pdf filename=2403.16175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Alzheimer&rsquo;s disease (AD), characterized by progressive cognitive decline and memory loss, presents a formidable global health challenge, underscoring the critical importance of early and precise diagnosis for timely interventions and enhanced patient outcomes. While MRI scans provide valuable insights into brain structures, traditional analysis methods often struggle to discern intricate 3D patterns crucial for AD identification. Addressing this challenge, we introduce an alternative end-to-end deep learning model, the 3D Hybrid Compact <b>Convolutional</b> <b>Transformers</b> <b>3D</b> (HCCT). By synergistically combining <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> and <b>vision</b> <b>transformers</b> (ViTs), the 3D HCCT adeptly captures both local features and long-range relationships within 3D MRI scans. Extensive evaluations on prominent AD <b>benchmark</b> dataset, ADNI, demonstrate the 3D HCCT&rsquo;s superior performance, surpassing state of the art <b>CNN</b> and <b>transformer-based</b> methods in classification accuracy. Its robust generalization capability and interpretability marks a significant stride in AD classification from 3D MRI scans, promising more accurate and reliable diagnoses for improved patient care and superior clinical outcomes.</p></p class="citation"></blockquote><h3 id=25--80146-leveraging-deep-learning-and-xception-architecture-for-high-accuracy-mri-classification-in-alzheimer-diagnosis-shaojie-li-et-al-2024>(2/5 | 80/146) Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI Classification in Alzheimer Diagnosis (Shaojie Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaojie Li, Haichen Qu, Xinqi Dong, Bo Dang, Hengyi Zang, Yulu Gong. (2024)<br><strong>Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI Classification in Alzheimer Diagnosis</strong><br><button class=copy-to-clipboard title="Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI Classification in Alzheimer Diagnosis" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16212v1.pdf filename=2403.16212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploring the application of deep learning technologies in the field of medical diagnostics, Magnetic Resonance Imaging (MRI) provides a unique perspective for observing and diagnosing complex neurodegenerative diseases such as Alzheimer Disease (AD). With advancements in deep learning, particularly in <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and the Xception network architecture, we are now able to analyze and classify vast amounts of MRI data with unprecedented accuracy. The progress of this technology not only enhances our understanding of brain structural changes but also opens up new avenues for monitoring disease progression through non-invasive means and potentially allows for precise diagnosis in the early stages of the disease. This study aims to classify MRI images using deep learning models to identify different stages of Alzheimer Disease through a series of innovative data processing and model construction steps. Our experimental results show that the deep learning framework based on the Xception model achieved a 99.6% accuracy rate in the multi-class MRI image classification task, demonstrating its potential application value in assistive diagnosis. Future research will focus on expanding the dataset, improving model interpretability, and clinical validation to further promote the application of deep learning technology in the medical field, with the hope of bringing earlier diagnosis and more personalized treatment plans to Alzheimer Disease patients.</p></p class="citation"></blockquote><h3 id=35--81146-laplacian-guided-entropy-model-in-neural-codec-with-blur-dissipated-synthesis-atefeh-khoshkhahtinat-et-al-2024>(3/5 | 81/146) Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis (Atefeh Khoshkhahtinat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atefeh Khoshkhahtinat, Ali Zafari, Piyush M. Mehta, Nasser M. Nasrabadi. (2024)<br><strong>Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis</strong><br><button class=copy-to-clipboard title="Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-IT, cs-LG, eess-IV, eess.IV, math-IT<br>Keyword Score: 20<br>Keywords: Diffusion Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16258v1.pdf filename=2403.16258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While replacing Gaussian decoders with a conditional <b>diffusion</b> <b>model</b> enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels. To address this limitation, we adopt a non-isotropic <b>diffusion</b> <b>model</b> at the decoder side. This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images. Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step. This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk. The global spatial context is built upon the <b>Transformer,</b> which is specifically designed for image compression tasks. The designed <b>Transformer</b> employs a Laplacian-shaped positional encoding, the learnable parameters of which are adaptively adjusted for each channel cluster. Our experiments demonstrate that our proposed framework yields better perceptual quality compared to cutting-edge generative-based codecs, and the proposed entropy model contributes to notable bitrate savings.</p></p class="citation"></blockquote><h3 id=45--82146-cfat-unleashing-triangularwindows-for-image-super-resolution-abhisek-ray-et-al-2024>(4/5 | 82/146) CFAT: Unleashing TriangularWindows for Image Super-resolution (Abhisek Ray et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhisek Ray, Gaurav Kumar, Maheshkumar H. Kolekar. (2024)<br><strong>CFAT: Unleashing TriangularWindows for Image Super-resolution</strong><br><button class=copy-to-clipboard title="CFAT: Unleashing TriangularWindows for Image Super-resolution" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, cs-MM, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16143v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16143v1.pdf filename=2403.16143v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> models have revolutionized the field of image super-resolution (SR) by harnessing their inherent ability to capture complex contextual features. The overlapping rectangular shifted window technique used in <b>transformer</b> architecture nowadays is a common practice in super-resolution models to improve the quality and robustness of image upscaling. However, it suffers from distortion at the boundaries and has limited unique shifting modes. To overcome these weaknesses, we propose a non-overlapping triangular window technique that synchronously works with the rectangular one to mitigate boundary-level distortion and allows the model to access more unique sifting modes. In this paper, we propose a Composite Fusion Attention <b>Transformer</b> (CFAT) that incorporates triangular-rectangular window-based local attention with a channel-based global attention technique in image super-resolution. As a result, CFAT enables attention mechanisms to be activated on more image pixels and captures long-range, multi-scale features to improve SR performance. The extensive experimental results and ablation study demonstrate the effectiveness of CFAT in the SR domain. Our proposed model shows a significant 0.7 dB performance improvement over other state-of-the-art SR architectures.</p></p class="citation"></blockquote><h3 id=55--83146-hemoset-the-first-blood-segmentation-dataset-for-automation-of-hemostasis-management-albert-j-miao-shan-lin-et-al-2024>(5/5 | 83/146) HemoSet: The First Blood Segmentation Dataset for Automation of Hemostasis Management (Albert J. Miao Shan Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Albert J. Miao Shan Lin, Jingpei Lu, Florian Richter, Benjamin Ostrander, Emily K. Funk, Ryan K. Orosco, Michael C. Yip. (2024)<br><strong>HemoSet: The First Blood Segmentation Dataset for Automation of Hemostasis Management</strong><br><button class=copy-to-clipboard title="HemoSet: The First Blood Segmentation Dataset for Automation of Hemostasis Management" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16286v1.pdf filename=2403.16286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hemorrhaging occurs in surgeries of all types, forcing surgeons to quickly adapt to the visual interference that results from blood rapidly filling the surgical field. Introducing automation into the crucial surgical task of hemostasis management would offload mental and physical tasks from the surgeon and surgical assistants while simultaneously increasing the efficiency and safety of the operation. The first step in automation of hemostasis management is detection of blood in the surgical field. To propel the development of blood detection algorithms in surgeries, we present HemoSet, the first blood segmentation dataset based on bleeding during a live animal robotic surgery. Our dataset features vessel hemorrhage scenarios where turbulent flow leads to abnormal pooling geometries in surgical fields. These pools are formed in conditions endemic to surgical procedures &ndash; uneven heterogeneous tissue, under glossy lighting conditions and rapid tool movement. We <b>benchmark</b> several state-of-the-art segmentation models and provide insight into the difficulties specific to blood detection. We intend for HemoSet to spur development of autonomous blood suction tools by providing a platform for training and refining blood segmentation models, addressing the precision needed for such robotics.</p></p class="citation"></blockquote><h2 id=eesssy-7>eess.SY (7)</h2><h3 id=17--84146-ann-based-adaptive-nmpc-for-uranium-extraction-scrubbing-operation-in-spent-nuclear-fuel-treatment-process-duc-tri-vo-et-al-2024>(1/7 | 84/146) ANN-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in Spent Nuclear Fuel Treatment Process (Duc-Tri Vo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duc-Tri Vo, Ionela Prodan, Laurent Lefèvre, Vincent Vanel, Sylvain Costenoble, Binh Dinh. (2024)<br><strong>ANN-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in Spent Nuclear Fuel Treatment Process</strong><br><button class=copy-to-clipboard title="ANN-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in Spent Nuclear Fuel Treatment Process" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 60<br>Keywords: Logistic Regression, Simulation, Simulator, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16307v1.pdf filename=2403.16307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the particularities in optimal control of the uranium extraction-scrubbing operation in the PUREX process. The control problem requires optimally stabilizing the system at a desired solvent saturation level, guaranteeing constraints, disturbance rejection, and adapting to set point variations. A qualified simulator named PAREX was developed by the French Alternative Energies and Atomic Energy Commission (CEA) to simulate liquid-liquid extraction operations in the PUREX process. However, since the mathematical model is complex and is described by a system of nonlinear, stiff, high-dimensional differential-algebraic equations (DAE), applying optimal control methods will lead to a large-scale nonlinear programming problem with a huge computational burden. The solution we propose in this work is to train a neural network to predict the process outputs using the measurement history. This neural network architecture, which employs the <b>long</b> <b>short-term</b> <b>memory</b> <b>(LSTM),</b> linear regression and <b>logistic</b> <b>regression</b> networks, allows reducing the number of state variables, thus reducing the complexity of the optimization problems in the control scheme. Furthermore, nonlinear model predictive control (NMPC) and moving horizon estimation (MHE) problems are developed and solved using the PSO (Particle Swarm Optimization) algorithm. <b>Simulation</b> results show that the proposed adaptive optimal control scheme satisfies the requirements of the control problem and provides promise for experimental testing.</p></p class="citation"></blockquote><h3 id=27--85146-voltage-regulation-in-polymer-electrolyte-fuel-cell-systems-using-gaussian-process-model-predictive-control-xiufei-li-et-al-2024>(2/7 | 85/146) Voltage Regulation in Polymer Electrolyte Fuel Cell Systems Using Gaussian Process Model Predictive Control (Xiufei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiufei Li, Miao Zhang, Yuanxin Qi, Miao Yang. (2024)<br><strong>Voltage Regulation in Polymer Electrolyte Fuel Cell Systems Using Gaussian Process Model Predictive Control</strong><br><button class=copy-to-clipboard title="Voltage Regulation in Polymer Electrolyte Fuel Cell Systems Using Gaussian Process Model Predictive Control" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16170v1.pdf filename=2403.16170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces a novel approach utilizing <b>Gaussian</b> <b>process</b> model predictive control (MPC) to stabilize the output voltage of a polymer electrolyte fuel cell (PEFC) system by simultaneously regulating hydrogen and airflow rates. Two <b>Gaussian</b> <b>process</b> models are developed to capture PEFC dynamics, taking into account constraints including hydrogen pressure and input change rates, thereby aiding in mitigating errors inherent to PEFC predictive control. The dynamic performance of the physical model and <b>Gaussian</b> <b>process</b> MPC in constraint handling and system inputs is compared and analyzed. <b>Simulation</b> outcomes demonstrate that the proposed <b>Gaussian</b> <b>process</b> MPC effectively maintains the voltage at the target 48 V while adhering to safety constraints, even amidst workload disturbances ranging from 110-120 A. In comparison to traditional MPC using detailed system models, <b>Gaussian</b> <b>process</b> MPC exhibits a 43% higher overshoot and 25% slower response time. Nonetheless, it offers the advantage of not requiring the underlying true system model and needing less system information.</p></p class="citation"></blockquote><h3 id=37--86146-data-driven-sliding-mode-control-for-partially-unknown-nonlinear-systems-jianglin-lan-et-al-2024>(3/7 | 86/146) Data-Driven Sliding Mode Control for Partially Unknown Nonlinear Systems (Jianglin Lan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianglin Lan, Xianxian Zhao, Congcong Sun. (2024)<br><strong>Data-Driven Sliding Mode Control for Partially Unknown Nonlinear Systems</strong><br><button class=copy-to-clipboard title="Data-Driven Sliding Mode Control for Partially Unknown Nonlinear Systems" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16136v1.pdf filename=2403.16136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a new design method for data-driven control of nonlinear systems with partially unknown dynamics and unknown bounded disturbance. Since it is not possible to achieve exact nonlinearity cancellation in the presence of unknown disturbance, this paper adapts the idea of sliding mode control (SMC) to ensure system stability and robustness without assuming that the nonlinearity goes to zero faster than the state as in the existing methods. The SMC consists of a data-dependent robust controller ensuring the system state trajectory reach and remain on the sliding surface and a nominal controller solved from a data-dependent semidefinite program (SDP) ensuring robust stability of the state trajectory on the sliding surface. Numerical <b>simulation</b> results demonstrate effectiveness of the proposed data-driven SMC and its superior in terms of robust stability over the existing data-driven control that also uses approximate nonlinearity cancellation.</p></p class="citation"></blockquote><h3 id=47--87146-bi-level-control-of-weaving-sections-in-mixed-traffic-environments-with-connected-and-automated-vehicles-longhao-yan-et-al-2024>(4/7 | 87/146) Bi-Level Control of Weaving Sections in Mixed Traffic Environments with Connected and Automated Vehicles (Longhao Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Longhao Yan, Jinhao Liang, Kaidi Yang. (2024)<br><strong>Bi-Level Control of Weaving Sections in Mixed Traffic Environments with Connected and Automated Vehicles</strong><br><button class=copy-to-clipboard title="Bi-Level Control of Weaving Sections in Mixed Traffic Environments with Connected and Automated Vehicles" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16225v1.pdf filename=2403.16225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Connected and automated vehicles (CAVs) can be beneficial for improving the operation of highway bottlenecks such as weaving sections. This paper proposes a bi-level control approach based on an upper-level deep <b>reinforcement</b> <b>learning</b> controller and a lower-level model predictive controller to coordinate the lane-changings of a mixed fleet of CAVs and human-driven vehicles (HVs) in weaving sections. The upper level represents a roadside controller that collects vehicular information from the entire weaving section and determines the control weights used in the lower-level controller. The lower level is implemented within each CAV, which takes the control weights from the upper-level controller and generates the acceleration and steering angle for individual CAVs based on the local situation. The lower-level controller further incorporates an HV trajectory predictor, which is capable of handling the dynamic topology of vehicles in weaving scenarios with intensive mandatory lane changes. The case study inspired by a real weaving section in Basel, Switzerland, shows that our method consistently outperforms state-of-the-art <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=57--88146-semi-automatic-line-system-provisioning-with-integrated-physical-parameter-aware-methodology-field-verification-and-operational-feasibility-hideki-nishizawa-et-al-2024>(5/7 | 88/146) Semi-Automatic Line-System Provisioning with Integrated Physical-Parameter-Aware Methodology: Field Verification and Operational Feasibility (Hideki Nishizawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hideki Nishizawa, Giacomo Borraccini, Takeo Sasai, Yue-Kai Huang, Toru Mano, Kazuya Anazawa, Masatoshi Namiki, Soichiroh Usui, Tatsuya Matsumura, Yoshiaki Sone, Zehao Wang, Seiji Okamoto, Takeru Inoue, Ezra Ip, Andrea D&rsquo;Amico, Tingjun Chen, Vittorio Curri, Ting Wang, Koji Asahi, Koichi Takasugi. (2024)<br><strong>Semi-Automatic Line-System Provisioning with Integrated Physical-Parameter-Aware Methodology: Field Verification and Operational Feasibility</strong><br><button class=copy-to-clipboard title="Semi-Automatic Line-System Provisioning with Integrated Physical-Parameter-Aware Methodology: Field Verification and Operational Feasibility" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16281v1.pdf filename=2403.16281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose methods and an architecture to conduct measurements and optimize newly installed optical fiber line systems semi-automatically using integrated physics-aware technologies in a data center interconnection (DCI) transmission scenario. We demonstrate, for the first time, digital longitudinal monitoring (DLM) and optical line system (OLS) physical parameter calibration working together in real-time to extract physical link parameters for transmission performance optimization. Our methodology has the following advantages over traditional design: a minimized footprint at user sites, accurate estimation of the necessary optical network characteristics via complementary telemetry technologies, and the capability to conduct all operation work remotely. The last feature is crucial, as it enables remote operation to implement network design settings for immediate response to quality of transmission (QoT) degradation and reversion in the case of unforeseen problems. We successfully performed semi-automatic line system provisioning over field fiber networks facilities at Duke University, Durham, NC. The tasks of parameter retrieval, equipment setting optimization, and system setup/provisioning were completed within 1 hour. The field operation was <b>supervised</b> by on-duty personnel who could access the system remotely from different time zones. By comparing Q-factor estimates calculated from the extracted link parameters with measured results from 400G transceivers, we confirmed that our methodology has a reduction in the QoT prediction errors (+-0.3 dB) over existing design (+-10.6 dB).</p></p class="citation"></blockquote><h3 id=67--89146-digital-control-of-negative-imaginary-systems-a-discrete-time-hybrid-integrator-gain-system-approach-kanghong-shi-et-al-2024>(6/7 | 89/146) Digital control of negative imaginary systems: a discrete-time hybrid integrator-gain system approach (Kanghong Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kanghong Shi, Ian R. Petersen. (2024)<br><strong>Digital control of negative imaginary systems: a discrete-time hybrid integrator-gain system approach</strong><br><button class=copy-to-clipboard title="Digital control of negative imaginary systems: a discrete-time hybrid integrator-gain system approach" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16046v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16046v1.pdf filename=2403.16046v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A hybrid integrator-gain system (HIGS) is a control element that switches between an integrator and a gain, which overcomes some inherent limitations of linear controllers. In this paper, we consider using <b>discrete-time</b> <b>HIGS</b> controllers for the digital control of negative imaginary (NI) systems. We show that the <b>discrete-time</b> <b>HIGS</b> themselves are step-advanced negative imaginary systems. For a minimal linear NI system, there always exists a HIGS controller that can asymptotically stablize it. An illustrative example is provided, where we use the proposed HIGS control method to stabilize a <b>discrete-time</b> <b>mass-spring</b> system.</p></p class="citation"></blockquote><h3 id=77--90146-fisher-information-approach-for-masking-the-sensing-plan-applications-in-multifunction-radars-shashwat-jain-et-al-2024>(7/7 | 90/146) Fisher Information Approach for Masking the Sensing Plan: Applications in Multifunction Radars (Shashwat Jain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shashwat Jain, Vikram Krishnamurthy, Muralidhar Rangaswamy, Bosung Kang, Sandeep Gogineni. (2024)<br><strong>Fisher Information Approach for Masking the Sensing Plan: Applications in Multifunction Radars</strong><br><button class=copy-to-clipboard title="Fisher Information Approach for Masking the Sensing Plan: Applications in Multifunction Radars" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15966v1.pdf filename=2403.15966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How to design a <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP) based radar controller that makes small sacrifices in performance to mask its sensing plan from an adversary? The radar controller purposefully minimizes the Fisher information of its emissions so that an adversary cannot identify the controller&rsquo;s model parameters accurately. Unlike classical open loop statistical inference, where the Fisher information serves as a lower bound for the achievable covariance, this paper employs the Fisher information as a design constraint for a closed loop radar controller to mask its sensing plan. We analytically derive a closed-form expression for the determinant of the Fisher Information Matrix (FIM) pertaining to the parameters of the MDP-based controller. Subsequently, we constrain the MDP with respect to the determinant of the FIM. Numerical results show that the introduction of minor perturbations to the MDP&rsquo;s transition kernel and the total operation cost can reduce the Fisher Information of the emissions. Consequently, this reduction amplifies the variability in policy and transition kernel estimation errors, thwarting the adversary&rsquo;s accuracy in estimating the controller&rsquo;s sensing plan.</p></p class="citation"></blockquote><h2 id=cscl-15>cs.CL (15)</h2><h3 id=115--91146-sql-encoder-improving-nl2sql-in-context-learning-through-a-context-aware-encoder-mohammadreza-pourreza-et-al-2024>(1/15 | 91/146) SQL-Encoder: Improving NL2SQL In-Context Learning Through a Context-Aware Encoder (Mohammadreza Pourreza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammadreza Pourreza, Davood Rafiei, Yuxi Feng, Raymond Li, Zhenan Fan, Weiwei Zhang. (2024)<br><strong>SQL-Encoder: Improving NL2SQL In-Context Learning Through a Context-Aware Encoder</strong><br><button class=copy-to-clipboard title="SQL-Encoder: Improving NL2SQL In-Context Learning Through a Context-Aware Encoder" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-DB, cs-HC, cs.CL<br>Keyword Score: 60<br>Keywords: Cohere, GPT, GPT-3, GPT-3.5, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16204v1.pdf filename=2403.16204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting structural similarity between queries is essential for selecting examples in <b>in-context</b> <b>learning</b> models. However, assessing structural similarity based solely on the natural language expressions of queries, without considering SQL queries, presents a significant challenge. This paper explores the significance of this similarity metric and proposes a model for accurately estimating it. To achieve this, we leverage a dataset comprising 170k question pairs, meticulously curated to train a similarity prediction model. Our comprehensive evaluation demonstrates that the proposed model adeptly captures the structural similarity between questions, as evidenced by improvements in Kendall-Tau distance and precision@k metrics. Notably, our model outperforms strong competitive embedding models from OpenAI and <b>Cohere.</b> Furthermore, compared to these competitive models, our proposed encoder enhances the downstream performance of NL2SQL models in 1-shot <b>in-context</b> <b>learning</b> scenarios by 1-2% for <b>GPT-3.5-turbo,</b> 4-8% for CodeLlama-7B, and 2-3% for CodeLlama-13B.</p></p class="citation"></blockquote><h3 id=215--92146-a-little-leak-will-sink-a-great-ship-survey-of-transparency-for-large-language-models-from-start-to-finish-masahiro-kaneko-et-al-2024>(2/15 | 92/146) A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish (Masahiro Kaneko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masahiro Kaneko, Timothy Baldwin. (2024)<br><strong>A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish</strong><br><button class=copy-to-clipboard title="A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Few-shot, Few-shot Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16139v1.pdf filename=2403.16139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are trained on massive web-crawled corpora. This poses risks of leakage, including personal information, copyrighted texts, and <b>benchmark</b> datasets. Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance. We establish the following three criteria concerning the leakage issues: (1) leakage rate: the proportion of leaked data in training data, (2) output rate: the ease of generating leaked data, and (3) detection rate: the detection performance of leaked versus non-leaked data. Despite the leakage rate being the origin of data leakage issues, it is not understood how it affects the output rate and detection rate. In this paper, we conduct an experimental survey to elucidate the relationship between the leakage rate and both the output rate and detection rate for personal information, copyrighted texts, and <b>benchmark</b> data. Additionally, we propose a self-detection approach that uses <b>few-shot</b> <b>learning</b> in which <b>LLMs</b> detect whether instances are present or absent in their training data, in contrast to previous methods that do not employ explicit learning. To explore the ease of generating leaked information, we create a dataset of <b>prompts</b> designed to elicit personal information, copyrighted text, and <b>benchmarks</b> from <b>LLMs.</b> Our experiments reveal that <b>LLMs</b> produce leaked information in most cases despite less such data in their training set. This indicates even small amounts of leaked data can greatly affect outputs. Our self-detection method showed superior performance compared to existing detection methods.</p></p class="citation"></blockquote><h3 id=315--93146-qibo-a-large-language-model-for-traditional-chinese-medicine-heyi-zhang-et-al-2024>(3/15 | 93/146) Qibo: A Large Language Model for Traditional Chinese Medicine (Heyi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heyi Zhang, Xin Wang, Zhaopeng Meng, Yongzhe Jia, Dawei Xu. (2024)<br><strong>Qibo: A Large Language Model for Traditional Chinese Medicine</strong><br><button class=copy-to-clipboard title="Qibo: A Large Language Model for Traditional Chinese Medicine" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Supervised Learning, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16056v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16056v1.pdf filename=2403.16056v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of Artificial Intelligence, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance. However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of <b>LLMs</b> is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources. In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the <b>large</b> <b>model</b> <b>with</b> professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on <b>LLaMA,</b> which is the first <b>LLM</b> in the field of TCM to undergo a complete training process from pre-training to <b>Supervised</b> <b>Fine-Tuning</b> (SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of <b>LLMs,</b> which is a specialized tool for evaluating the performance of <b>LLMs</b> in the TCM domain. This tool will provide an important basis for quantifying and comparing the understanding and application capabilities of different models in the field of traditional Chinese medicine, and provide guidance for future research directions and practical applications of intelligent assistants for traditional Chinese medicine. Finally, we conducted sufficient experiments to prove that Qibo has good performance in the field of traditional Chinese medicine.</p></p class="citation"></blockquote><h3 id=415--94146-monotonic-paraphrasing-improves-generalization-of-language-model-prompting-qin-liu-et-al-2024>(4/15 | 94/146) Monotonic Paraphrasing Improves Generalization of Language Model Prompting (Qin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qin Liu, Fei Wang, Nan Xu, Tianyi Yan, Tao Meng, Muhao Chen. (2024)<br><strong>Monotonic Paraphrasing Improves Generalization of Language Model Prompting</strong><br><button class=copy-to-clipboard title="Monotonic Paraphrasing Improves Generalization of Language Model Prompting" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Zero-shot, Large Language Model, Large Language Model, Perplexity, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16038v1.pdf filename=2403.16038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Performance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> may vary with different <b>prompts</b> or instructions of even the same task. One commonly recognized factor for this phenomenon is the model&rsquo;s familiarity with the given <b>prompt</b> or instruction, which is typically estimated by its <b>perplexity.</b> However, finding the <b>prompt</b> with the lowest <b>perplexity</b> is challenging, given the enormous space of possible <b>prompting</b> phrases. In this paper, we propose monotonic paraphrasing (MonoPara), an end-to-end decoding strategy that paraphrases given <b>prompts</b> or instructions into their lower <b>perplexity</b> counterparts based on an ensemble of a paraphrase LM for <b>prompt</b> (or instruction) rewriting, and a target LM (i.e. the <b>prompt</b> or instruction executor) that constrains the generation for lower <b>perplexity.</b> The ensemble decoding process can efficiently paraphrase the original <b>prompt</b> without altering its semantic meaning, while monotonically decreasing the <b>perplexity</b> of each generation as calculated by the target LM. We explore in detail both greedy and search-based decoding as two alternative decoding schemes of MonoPara. Notably, MonoPara does not require any training and can monotonically lower the <b>perplexity</b> of the paraphrased <b>prompt</b> or instruction, leading to improved performance of <b>zero-shot</b> LM <b>prompting</b> as evaluated on a wide selection of tasks. In addition, MonoPara is also shown to effectively improve LMs&rsquo; generalization on perturbed and unseen task instructions.</p></p class="citation"></blockquote><h3 id=515--95146-cbt-llm-a-chinese-large-language-model-for-cognitive-behavioral-therapy-based-mental-health-question-answering-hongbin-na-2024>(5/15 | 95/146) CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering (Hongbin Na, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongbin Na. (2024)<br><strong>CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering</strong><br><button class=copy-to-clipboard title="CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Question Answering, Question Answering, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16008v1.pdf filename=2403.16008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent advancements in artificial intelligence highlight the potential of language models in psychological health support. While models trained on data from mental health service platform have achieved preliminary success, challenges persist in areas such as data scarcity, quality, and ensuring a solid foundation in psychological techniques. To address these challenges, this study introduces a novel approach to enhance the precision and efficacy of psychological support through <b>large</b> <b>language</b> <b>models.</b> Specifically, we design a specific <b>prompt</b> derived from principles of Cognitive Behavioral Therapy (CBT) and have generated the CBT <b>QA</b> dataset, specifically for Chinese psychological health Q&amp;A based on CBT structured intervention strategies. Unlike previous methods, our dataset emphasizes professional and structured response. Utilizing this dataset, we <b>fine-tuned</b> the <b>large</b> <b>language</b> <b>model,</b> giving birth to CBT-LLM, the <b>large-scale</b> <b>language</b> <b>model</b> specifically designed for Cognitive Behavioral Therapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in generating structured, professional, and highly relevant responses in psychological health support tasks, showcasing its practicality and quality. The model is available on Hugging Face: <a href=https://huggingface.co/Hongbin37/CBT-LLM>https://huggingface.co/Hongbin37/CBT-LLM</a>.</p></p class="citation"></blockquote><h3 id=615--96146-connecting-the-dots-inferring-patent-phrase-similarity-with-retrieved-phrase-graphs-zhuoyi-peng-et-al-2024>(6/15 | 96/146) Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs (Zhuoyi Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoyi Peng, Yi Yang. (2024)<br><strong>Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs</strong><br><button class=copy-to-clipboard title="Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Graph, Self-supervised Learning, Self-supervised Learning, Supervised Learning, Contextual Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16265v1.pdf filename=2403.16265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the patent phrase similarity inference task, which measures the semantic similarity between two patent phrases. As patent documents employ legal and highly technical language, existing semantic textual similarity methods that use localized <b>contextual</b> <b>information</b> do not perform satisfactorily in inferring patent phrase similarity. To address this, we introduce a <b>graph-augmented</b> approach to amplify the global <b>contextual</b> <b>information</b> of the patent phrases. For each patent phrase, we construct a phrase <b>graph</b> that links to its focal patents and a list of patents that are either cited by or cite these focal patents. The augmented phrase embedding is then derived from combining its localized <b>contextual</b> <b>embedding</b> with its global embedding within the phrase <b>graph.</b> We further propose a <b>self-supervised</b> <b>learning</b> objective that capitalizes on the retrieved topology to refine both the contextualized embedding and the <b>graph</b> parameters in an end-to-end manner. Experimental results from a unique patent phrase similarity dataset demonstrate that our approach significantly enhances the representation of patent phrases, resulting in marked improvements in similarity inference in a <b>self-supervised</b> <b>fashion.</b> Substantial improvements are also observed in the <b>supervised</b> setting, underscoring the potential benefits of leveraging retrieved phrase <b>graph</b> augmentation.</p></p class="citation"></blockquote><h3 id=715--97146-large-language-models-offer-an-alternative-to-the-traditional-approach-of-topic-modelling-yida-mu-et-al-2024>(7/15 | 97/146) Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling (Yida Mu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yida Mu, Chun Dong, Kalina Bontcheva, Xingyi Song. (2024)<br><strong>Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling</strong><br><button class=copy-to-clipboard title="Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Clustering, Unsupervised Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16248v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16248v2.pdf filename=2403.16248v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Topic modelling, as a well-established <b>unsupervised</b> technique, has found extensive use in automatically detecting significant topics within a corpus of documents. However, classic topic modelling approaches (e.g., LDA) have certain drawbacks, such as the lack of semantic understanding and the presence of overlapping topics. In this work, we investigate the untapped potential of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> as an alternative for uncovering the underlying topics within extensive text corpora. To this end, we introduce a framework that <b>prompts</b> <b>LLMs</b> to generate topics from a given set of documents and establish evaluation protocols to assess the <b>clustering</b> efficacy of <b>LLMs.</b> Our findings indicate that <b>LLMs</b> with appropriate <b>prompts</b> can stand out as a viable alternative, capable of generating relevant topic titles and adhering to human guidelines to refine and merge topics. Through in-depth experiments and evaluation, we summarise the advantages and constraints of employing <b>LLMs</b> in topic extraction.</p></p class="citation"></blockquote><h3 id=815--98146-argument-quality-assessment-in-the-age-of-instruction-following-large-language-models-henning-wachsmuth-et-al-2024>(8/15 | 98/146) Argument Quality Assessment in the Age of Instruction-Following Large Language Models (Henning Wachsmuth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Henning Wachsmuth, Gabriella Lapesa, Elena Cabrio, Anne Lauscher, Joonsuk Park, Eva Maria Vecchi, Serena Villata, Timon Ziegenbein. (2024)<br><strong>Argument Quality Assessment in the Age of Instruction-Following Large Language Models</strong><br><button class=copy-to-clipboard title="Argument Quality Assessment in the Age of Instruction-Following Large Language Models" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Instruction Following, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16084v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16084v1.pdf filename=2403.16084v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The computational treatment of arguments on controversial issues has been subject to extensive NLP research, due to its envisioned impact on opinion formation, decision making, writing education, and the like. A critical task in any such application is the assessment of an argument&rsquo;s quality - but it is also particularly challenging. In this position paper, we start from a brief survey of argument quality research, where we identify the diversity of quality notions and the subjectiveness of their perception as the main hurdles towards substantial progress on argument quality assessment. We argue that the capabilities of <b>instruction-following</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to leverage knowledge across contexts enable a much more reliable assessment. Rather than just <b>fine-tuning</b> <b>LLMs</b> towards leaderboard chasing on assessment tasks, they need to be instructed systematically with argumentation theories and scenarios as well as with ways to solve argument-related problems. We discuss the real-world opportunities and ethical issues emerging thereby.</p></p class="citation"></blockquote><h3 id=915--99146-a-survey-on-lexical-ambiguity-detection-and-word-sense-disambiguation-miuru-abeysiriwardana-et-al-2024>(9/15 | 99/146) A Survey on Lexical Ambiguity Detection and Word Sense Disambiguation (Miuru Abeysiriwardana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miuru Abeysiriwardana, Deshan Sumanathilaka. (2024)<br><strong>A Survey on Lexical Ambiguity Detection and Word Sense Disambiguation</strong><br><button class=copy-to-clipboard title="A Survey on Lexical Ambiguity Detection and Word Sense Disambiguation" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 38<br>Keywords: Graph, Knowledge Graph, Disambiguation, Word Sense Disambiguation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16129v1.pdf filename=2403.16129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores techniques that focus on understanding and resolving ambiguity in language within the field of natural language processing (NLP), highlighting the complexity of linguistic phenomena such as polysemy and homonymy and their implications for computational models. Focusing extensively on <b>Word</b> <b>Sense</b> <b>Disambiguation</b> (WSD), it outlines diverse approaches ranging from deep learning techniques to leveraging lexical resources and <b>knowledge</b> <b>graphs</b> like WordNet. The paper introduces cutting-edge methodologies like <b>word</b> <b>sense</b> <b>extension</b> (WSE) and neuromyotonic approaches, enhancing <b>disambiguation</b> accuracy by predicting new <b>word</b> <b>senses.</b> <b>It</b> examines specific applications in biomedical <b>disambiguation</b> and language specific optimisation and discusses the significance of cognitive metaphors in discourse analysis. The research identifies persistent challenges in the field, such as the scarcity of sense annotated corpora and the complexity of informal clinical texts. It concludes by suggesting future directions, including using <b>large</b> <b>language</b> <b>models,</b> visual WSD, and multilingual WSD systems, emphasising the ongoing evolution in addressing lexical complexities in NLP. This thinking perspective highlights the advancement in this field to enable computers to understand language more accurately.</p></p class="citation"></blockquote><h3 id=1015--100146-lexdrafter-terminology-drafting-for-legislative-documents-using-retrieval-augmented-generation-ashish-chouhan-et-al-2024>(10/15 | 100/146) LexDrafter: Terminology Drafting for Legislative Documents using Retrieval Augmented Generation (Ashish Chouhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashish Chouhan, Michael Gertz. (2024)<br><strong>LexDrafter: Terminology Drafting for Legislative Documents using Retrieval Augmented Generation</strong><br><button class=copy-to-clipboard title="LexDrafter: Terminology Drafting for Legislative Documents using Retrieval Augmented Generation" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16295v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16295v1.pdf filename=2403.16295v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increase in legislative documents at the EU, the number of new terms and their definitions is increasing as well. As per the Joint Practical Guide of the European Parliament, the Council and the Commission, terms used in legal documents shall be consistent, and identical concepts shall be expressed without departing from their meaning in ordinary, legal, or technical language. Thus, while drafting a new legislative document, having a framework that provides insights about existing definitions and helps define new terms based on a document&rsquo;s context will support such harmonized legal definitions across different regulations and thus avoid ambiguities. In this paper, we present LexDrafter, a framework that assists in drafting Definitions articles for legislative documents using <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)</b> and existing term definitions present in different legislative documents. For this, definition elements are built by extracting definitions from existing documents. Using definition elements and <b>RAG,</b> a Definitions article can be suggested on demand for a legislative document that is being drafted. We demonstrate and evaluate the functionality of LexDrafter using a collection of EU documents from the energy domain. The code for LexDrafter framework is available at <a href=https://github.com/achouhan93/LexDrafter>https://github.com/achouhan93/LexDrafter</a>.</p></p class="citation"></blockquote><h3 id=1115--101146-improving-sequence-to-sequence-models-for-abstractive-text-summarization-using-meta-heuristic-approaches-aditya-saxena-et-al-2024>(11/15 | 101/146) Improving Sequence-to-Sequence Models for Abstractive Text Summarization Using Meta Heuristic Approaches (Aditya Saxena et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aditya Saxena, Ashutosh Ranjan. (2024)<br><strong>Improving Sequence-to-Sequence Models for Abstractive Text Summarization Using Meta Heuristic Approaches</strong><br><button class=copy-to-clipboard title="Improving Sequence-to-Sequence Models for Abstractive Text Summarization Using Meta Heuristic Approaches" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs-NE, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Text Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16247v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16247v1.pdf filename=2403.16247v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As human society transitions into the information age, reduction in our attention span is a contingency, and people who spend time reading lengthy news articles are decreasing rapidly and the need for succinct information is higher than ever before. Therefore, it is essential to provide a quick overview of important news by concisely summarizing the top news article and the most intuitive headline. When humans try to make summaries, they extract the essential information from the source and add useful phrases and grammatical annotations from the original extract. Humans have a unique ability to create abstractions. However, automatic <b>summarization</b> is a complicated problem to solve. The use of sequence-to-sequence (seq2seq) models for neural abstractive <b>text</b> <b>summarization</b> has been ascending as far as prevalence. Numerous innovative strategies have been proposed to develop the current seq2seq models further, permitting them to handle different issues like saliency, familiarity, and human lucidness and create excellent synopses. In this article, we aimed toward enhancing the present architectures and models for abstractive <b>text</b> <b>summarization.</b> The modifications have been aimed at <b>fine-tuning</b> hyper-parameters, attempting specific encoder-decoder combinations. We examined many experiments on an extensively used CNN/DailyMail dataset to check the effectiveness of various models.</p></p class="citation"></blockquote><h3 id=1215--102146-alora-allocating-low-rank-adaptation-for-fine-tuning-large-language-models-zequan-liu-et-al-2024>(12/15 | 102/146) ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models (Zequan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zequan Liu, Jiawen Lyn, Wei Zhu, Xing Tian, Yvette Graham. (2024)<br><strong>ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models</strong><br><button class=copy-to-clipboard title="ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16187v1.pdf filename=2403.16187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient <b>fine-tuning</b> (PEFT) is widely studied for its effectiveness and efficiency in the era of <b>large</b> <b>language</b> <b>models.</b> Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method. However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks. Recognizing the need for more flexible downstream task adaptation, we extend the methodology of LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. First, we propose a novel method, AB-LoRA, that can effectively estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important <b>Transformer</b> modules needing higher ranks. We have conducted experiments on various tasks, and the experimental results demonstrate that our ALoRA method can outperform the recent baselines with comparable tunable parameters.</p></p class="citation"></blockquote><h3 id=1315--103146-korean-bio-medical-corpus-kbmc-for-medical-named-entity-recognition-sungjoo-byun-et-al-2024>(13/15 | 103/146) Korean Bio-Medical Corpus (KBMC) for Medical Named Entity Recognition (Sungjoo Byun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sungjoo Byun, Jiseung Hong, Sumin Park, Dongjun Jang, Jean Seo, Minseok Kim, Chaeyoung Oh, Hyopil Shin. (2024)<br><strong>Korean Bio-Medical Corpus (KBMC) for Medical Named Entity Recognition</strong><br><button class=copy-to-clipboard title="Korean Bio-Medical Corpus (KBMC) for Medical Named Entity Recognition" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: ChatGPT, Named Entity Recognition, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16158v1.pdf filename=2403.16158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> plays a pivotal role in medical Natural Language Processing (NLP). Yet, there has not been an open-source medical <b>NER</b> dataset specifically for the Korean language. To address this, we utilized <b>ChatGPT</b> to assist in constructing the KBMC (Korean Bio-Medical Corpus), which we are now presenting to the public. With the KBMC dataset, we noticed an impressive 20% increase in medical <b>NER</b> performance compared to models trained on general Korean <b>NER</b> datasets. This research underscores the significant benefits and importance of using specialized tools and datasets, like <b>ChatGPT,</b> to enhance language processing in specialized fields such as healthcare.</p></p class="citation"></blockquote><h3 id=1415--104146-wangchanlion-and-wangchanx-mrc-eval-wannaphong-phatthiyaphaibun-et-al-2024>(14/15 | 104/146) WangchanLion and WangchanX MRC Eval (Wannaphong Phatthiyaphaibun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wannaphong Phatthiyaphaibun, Surapon Nonesung, Patomporn Payoungkhamdee, Peerat Limkonchotiwat, Can Udomcharoenchaikit, Ekapol Chuangsuwanich, Sarana Nutanong. (2024)<br><strong>WangchanLion and WangchanX MRC Eval</strong><br><button class=copy-to-clipboard title="WangchanLion and WangchanX MRC Eval" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Instruction Following<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16127v1.pdf filename=2403.16127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This technical report describes the development of WangchanLion, an <b>instruction</b> <b>fine-tuned</b> model focusing on Machine Reading Comprehension (MRC) in the Thai language. Our model is based on SEA-LION and a collection of <b>instruction</b> <b>following</b> datasets. To promote open research and reproducibility, we publically release all training data, code, and the final model weights under the Apache-2 license. To assess the contextual understanding capability, we conducted extensive experimental studies using two Thai MRC datasets, XQuAD and Iapp_wiki_qa_squad. Experimental results demonstrate the model&rsquo;s ability to comprehend the context and produce an answer faithful to the reference one in 0-shot and 1-shot settings. In addition, our evaluation goes beyond the traditional MRC. We propose a new evaluation scheme assessing the answer&rsquo;s correctness, helpfulness, conciseness, and contextuality. Evaluation results provide insight into how we can improve our model in the future. Our code is public at <a href=https://github.com/vistec-AI/WangchanLion>https://github.com/vistec-AI/WangchanLion</a>.</p></p class="citation"></blockquote><h3 id=1515--105146-a-multi-label-dataset-of-french-fake-news-human-and-machine-insights-benjamin-icard-et-al-2024>(15/15 | 105/146) A Multi-Label Dataset of French Fake News: Human and Machine Insights (Benjamin Icard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Icard, François Maine, Morgane Casanova, Géraud Faye, Julien Chanson, Guillaume Gadek, Ghislain Atemezing, François Bancilhon, Paul Égré. (2024)<br><strong>A Multi-Label Dataset of French Fake News: Human and Machine Insights</strong><br><button class=copy-to-clipboard title="A Multi-Label Dataset of French Fake News: Human and Machine Insights" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16099v1.pdf filename=2403.16099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators. By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of <b>fake</b> <b>news,</b> and compare them to the predictions of automated classifiers. We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus. We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label <b>Fake</b> <b>News.</b> The annotated dataset is available online at the following url: <a href=https://github.com/obs-info/obsinfox>https://github.com/obs-info/obsinfox</a> Keywords: <b>Fake</b> <b>News,</b> Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press</p></p class="citation"></blockquote><h2 id=cscr-3>cs.CR (3)</h2><h3 id=13--106146-eg-conmix-an-intrusion-detection-method-based-on-graph-contrastive-learning-lijin-wu-et-al-2024>(1/3 | 106/146) EG-ConMix: An Intrusion Detection Method based on Graph Contrastive Learning (Lijin Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lijin Wu, Shanshan Lei, Feilong Liao, Yuanjun Zheng, Yuxin Liu, Wentao Fu, Hao Song, Jiajun Zhou. (2024)<br><strong>EG-ConMix: An Intrusion Detection Method based on Graph Contrastive Learning</strong><br><button class=copy-to-clipboard title="EG-ConMix: An Intrusion Detection Method based on Graph Contrastive Learning" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 43<br>Keywords: E-GraphSAGE, Graph, Graph Contrastive Learning, Contrastive Learning, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17980v1.pdf filename=2403.17980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the number of IoT devices increases, security concerns become more prominent. The impact of threats can be minimized by deploying Network Intrusion Detection System (NIDS) by monitoring network traffic, detecting and discovering intrusions, and issuing security alerts promptly. Most intrusion detection research in recent years has been directed towards the pair of traffic itself without considering the interrelationships among them, thus limiting the monitoring of complex IoT network attack events. Besides, anomalous traffic in real networks accounts for only a small fraction, which leads to a severe imbalance problem in the dataset that makes algorithmic learning and prediction extremely difficult. In this paper, we propose an EG-ConMix method based on <b>E-GraphSAGE,</b> incorporating a <b>data</b> <b>augmentation</b> module to fix the problem of <b>data</b> <b>imbalance.</b> In addition, we incorporate <b>contrastive</b> <b>learning</b> to discern the difference between normal and malicious traffic samples, facilitating the extraction of key features. Extensive experiments on two publicly available datasets demonstrate the superior intrusion detection performance of EG-ConMix compared to state-of-the-art methods. Remarkably, it exhibits significant advantages in terms of training speed and accuracy for large-scale <b>graphs.</b></p></p class="citation"></blockquote><h3 id=23--107146-is-watermarking-llm-generated-code-robust-tarun-suresh-et-al-2024>(2/3 | 107/146) Is Watermarking LLM-Generated Code Robust? (Tarun Suresh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tarun Suresh, Shubham Ugare, Gagandeep Singh, Sasa Misailovic. (2024)<br><strong>Is Watermarking LLM-Generated Code Robust?</strong><br><button class=copy-to-clipboard title="Is Watermarking LLM-Generated Code Robust?" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17983v1.pdf filename=2403.17983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the first study of the robustness of existing watermarking techniques on Python code generated by <b>large</b> <b>language</b> <b>models.</b> Although existing works showed that watermarking can be robust for natural language, we show that it is easy to remove these watermarks on code by semantic-preserving transformations.</p></p class="citation"></blockquote><h3 id=33--108146-a-survey-on-consumer-iot-traffic-security-and-privacy-yan-jia-et-al-2024>(3/3 | 108/146) A Survey on Consumer IoT Traffic: Security and Privacy (Yan Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Jia, Yuxin Song, Zihou Liu, Qingyin Tan, Fangming Wang, Yu Zhang, Zheli Liu. (2024)<br><strong>A Survey on Consumer IoT Traffic: Security and Privacy</strong><br><button class=copy-to-clipboard title="A Survey on Consumer IoT Traffic: Security and Privacy" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16149v1.pdf filename=2403.16149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people&rsquo;s daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and <b>summarized</b> the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious traffic analysis, security analysis, and measurement. At last, we discuss the new challenges and future research directions.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--109146-modal-adaptive-knowledge-enhanced-graph-based-financial-prediction-from-monetary-policy-conference-calls-with-llm-kun-ouyang-et-al-2024>(1/1 | 109/146) Modal-adaptive Knowledge-enhanced Graph-based Financial Prediction from Monetary Policy Conference Calls with LLM (Kun Ouyang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Ouyang, Yi Liu, Shicheng Li, Ruihan Bao, Keiko Harimoto, Xu Sun. (2024)<br><strong>Modal-adaptive Knowledge-enhanced Graph-based Financial Prediction from Monetary Policy Conference Calls with LLM</strong><br><button class=copy-to-clipboard title="Modal-adaptive Knowledge-enhanced Graph-based Financial Prediction from Monetary Policy Conference Calls with LLM" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 39<br>Keywords: Graph, Multi-modal, Multi-modal, BERT, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16055v1.pdf filename=2403.16055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Financial prediction from Monetary Policy Conference (MPC) calls is a new yet challenging task, which targets at predicting the price movement and volatility for specific financial assets by analyzing <b>multimodal</b> information including text, video, and audio. Although the existing work has achieved great success using cross-modal <b>transformer</b> blocks, it overlooks the potential external financial knowledge, the varying contributions of different modalities to financial prediction, as well as the innate relations among different financial assets. To tackle these limitations, we propose a novel Modal-Adaptive kNowledge-enhAnced <b>Graph-basEd</b> financial pRediction scheme, named MANAGER. Specifically, MANAGER resorts to FinDKG to obtain the external related knowledge for the input text. Meanwhile, MANAGER adopts BEiT-3 and Hidden-unit <b>BERT</b> (HuBERT) to extract the video and audio features, respectively. Thereafter, MANAGER introduces a novel knowledge-enhanced cross-modal <b>graph</b> that fully characterizes the semantic relations among text, external knowledge, video and audio, to adaptively utilize the information in different modalities, with ChatGLM2 as the backbone. Extensive experiments on a publicly available dataset Monopoly verify the superiority of our model over cutting-edge methods.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=12--110146-ultra-low-cost-two-stage-multimodal-system-for-non-normative-behavior-detection-albert-lu-et-al-2024>(1/2 | 110/146) Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior Detection (Albert Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Albert Lu, Stephen Cranefield. (2024)<br><strong>Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior Detection</strong><br><button class=copy-to-clipboard title="Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior Detection" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-IR, cs-MA, cs.MA<br>Keyword Score: 36<br>Keywords: Fine-tuning, Logistic Regression, Multi-modal, Multi-modal, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16151v1.pdf filename=2403.16151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The online community has increasingly been inundated by a toxic wave of harmful comments. In response to this growing challenge, we introduce a two-stage ultra-low-cost <b>multimodal</b> harmful behavior detection method designed to identify harmful comments and images with high precision and recall rates. We first utilize the CLIP-ViT model to transform tweets and images into embeddings, effectively capturing the intricate interplay of semantic meaning and subtle contextual clues within texts and images. Then in the second stage, the system feeds these embeddings into a conventional machine learning classifier like SVM or <b>logistic</b> <b>regression,</b> enabling the system to be trained rapidly and to perform inference at an ultra-low cost. By converting tweets into rich <b>multimodal</b> embeddings through the CLIP-ViT model and utilizing them to train conventional machine learning classifiers, our system is not only capable of detecting harmful textual information with near-perfect performance, achieving precision and recall rates above 99% but also demonstrates the ability to <b>zero-shot</b> harmful images without additional training, thanks to its <b>multimodal</b> embedding input. This capability empowers our system to identify unseen harmful images without requiring extensive and costly image datasets. Additionally, our system quickly adapts to new harmful content; if a new harmful content pattern is identified, we can <b>fine-tune</b> the classifier with the corresponding tweets&rsquo; embeddings to promptly update the system. This makes it well suited to addressing the ever-evolving nature of online harmfulness, providing online communities with a robust, generalizable, and cost-effective tool to safeguard their communities.</p></p class="citation"></blockquote><h3 id=22--111146-social-deliberation-vs-social-contracts-in-self-governing-voluntary-organisations-matthew-scott-et-al-2024>(2/2 | 111/146) Social Deliberation vs. Social Contracts in Self-Governing Voluntary Organisations (Matthew Scott et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew Scott, Asimina Mertzani, Ciske Smit, Stefan Sarkadi, Jeremy Pitt. (2024)<br><strong>Social Deliberation vs. Social Contracts in Self-Governing Voluntary Organisations</strong><br><button class=copy-to-clipboard title="Social Deliberation vs. Social Contracts in Self-Governing Voluntary Organisations" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: I-2-11, cs-MA, cs.MA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16329v1.pdf filename=2403.16329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Self-organising multi-agent systems regulate their components&rsquo; behaviour voluntarily, according to a set of socially-constructed, mutually-agreed, and mutable social arrangements. In some systems, these arrangements may be applied with a frequency, at a scale and within implicit cost constraints such that performance becomes a pressing issue. This paper introduces the \textit{Megabike Scenario}, which consists of a negotiated agreement on a relatively &rsquo;large&rsquo; set of conventional rules, &lsquo;frequent&rsquo; &lsquo;democratic&rsquo; decision-making according to those rules, and a resource-bounded imperative to reach &lsquo;correct&rsquo; decisions. A formalism is defined for effective rule representation and processing in the scenario, and is evaluated against five interleaved socio-functional requirements. System performance is also evaluated empirically through <b>simulation.</b> We conclude that to self-organise their social arrangements, agents need some awareness of their own limitations and the value of compromise.</p></p class="citation"></blockquote><h2 id=csro-8>cs.RO (8)</h2><h3 id=18--112146-mqe-unleashing-the-power-of-interaction-with-multi-agent-quadruped-environment-ziyan-xiong-et-al-2024>(1/8 | 112/146) MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment (Ziyan Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyan Xiong, Bo Chen, Shiyu Huang, Wei-Wei Tu, Zhaofeng He, Yang Gao. (2024)<br><strong>MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment</strong><br><button class=copy-to-clipboard title="MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Benchmarking, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16015v1.pdf filename=2403.16015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of deep <b>reinforcement</b> <b>learning</b> (DRL) has significantly advanced the field of robotics, particularly in the control and coordination of quadruped robots. However, the complexity of real-world tasks often necessitates the deployment of multi-robot systems capable of sophisticated interaction and collaboration. To address this need, we introduce the Multi-agent Quadruped Environment (MQE), a novel platform designed to facilitate the development and evaluation of multi-agent <b>reinforcement</b> <b>learning</b> (MARL) algorithms in realistic and dynamic scenarios. MQE emphasizes complex interactions between robots and objects, hierarchical policy structures, and challenging evaluation scenarios that reflect real-world applications. We present a series of collaborative and competitive tasks within MQE, ranging from simple coordination to complex adversarial interactions, and <b>benchmark</b> state-of-the-art MARL algorithms. Our findings indicate that hierarchical <b>reinforcement</b> <b>learning</b> can simplify task learning, but also highlight the need for advanced algorithms capable of handling the intricate dynamics of multi-agent interactions. MQE serves as a stepping stone towards bridging the gap between <b>simulation</b> and practical deployment, offering a rich environment for future research in multi-agent systems and robot learning. For open-sourced code and more details of MQE, please refer to <a href=https://ziyanx02.github.io/multiagent-quadruped-environment/>https://ziyanx02.github.io/multiagent-quadruped-environment/</a> .</p></p class="citation"></blockquote><h3 id=28--113146-guessing-human-intentions-to-avoid-dangerous-situations-in-caregiving-robots-noé-zapata-et-al-2024>(2/8 | 113/146) Guessing human intentions to avoid dangerous situations in caregiving robots (Noé Zapata et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Noé Zapata, Gerardo Pérez, Lucas Bonilla, Pedro Núñez, Pilar Bachiller, Pablo Bustos. (2024)<br><strong>Guessing human intentions to avoid dangerous situations in caregiving robots</strong><br><button class=copy-to-clipboard title="Guessing human intentions to avoid dangerous situations in caregiving robots" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16291v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16291v2.pdf filename=2403.16291v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For robots to interact socially, they must interpret human intentions and anticipate their potential outcomes accurately. This is particularly important for social robots designed for human care, which may face potentially dangerous situations for people, such as unseen obstacles in their way, that should be avoided. This paper explores the Artificial Theory of Mind (ATM) approach to inferring and interpreting human intentions. We propose an algorithm that detects risky situations for humans, selecting a robot action that removes the danger in real time. We use the <b>simulation-based</b> approach to ATM and adopt the &rsquo;like-me&rsquo; policy to assign intentions and actions to people. Using this strategy, the robot can detect and act with a high rate of success under time-constrained situations. The algorithm has been implemented as part of an existing robotics cognitive architecture and tested in <b>simulation</b> scenarios. Three experiments have been conducted to test the implementation&rsquo;s robustness, precision and real-time response, including a simulated scenario, a <b>human-in-the-loop</b> hybrid configuration and a real-world scenario.</p></p class="citation"></blockquote><h3 id=38--114146-rpmart-towards-robust-perception-and-manipulation-for-articulated-objects-junbo-wang-et-al-2024>(3/8 | 114/146) RPMArt: Towards Robust Perception and Manipulation for Articulated Objects (Junbo Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junbo Wang, Wenhai Liu, Qiaojun Yu, Yang You, Liu Liu, Weiming Wang, Cewu Lu. (2024)<br><strong>RPMArt: Towards Robust Perception and Manipulation for Articulated Objects</strong><br><button class=copy-to-clipboard title="RPMArt: Towards Robust Perception and Manipulation for Articulated Objects" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16023v1.pdf filename=2403.16023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Articulated objects are commonly found in daily life. It is essential that robots can exhibit robust perception and manipulation skills for articulated objects in real-world robotic applications. However, existing methods for articulated objects insufficiently address noise in point clouds and struggle to bridge the gap between <b>simulation</b> and reality, thus limiting the practical deployment in real-world scenarios. To tackle these challenges, we propose a framework towards Robust Perception and Manipulation for Articulated Objects (RPMArt), which learns to estimate the articulation parameters and manipulate the articulation part from the noisy point cloud. Our primary contribution is a Robust Articulation Network (RoArtNet) that is able to predict both joint parameters and affordable points robustly by local feature learning and point tuple voting. Moreover, we introduce an articulation-aware classification scheme to enhance its ability for sim-to-real transfer. Finally, with the estimated affordable point and articulation joint constraint, the robot can generate robust actions to manipulate articulated objects. After learning only from synthetic data, RPMArt is able to transfer <b>zero-shot</b> to real-world articulated objects. Experimental results confirm our approach&rsquo;s effectiveness, with our framework achieving state-of-the-art performance in both noise-added <b>simulation</b> and real-world environments. The code and data will be open-sourced for reproduction. More results are published on the project website at <a href=https://r-pmart.github.io>https://r-pmart.github.io</a> .</p></p class="citation"></blockquote><h3 id=48--115146-robust-locomotion-by-logic-perturbation-resilient-bipedal-locomotion-via-signal-temporal-logic-guided-model-predictive-control-zhaoyuan-gu-et-al-2024>(4/8 | 115/146) Robust-Locomotion-by-Logic: Perturbation-Resilient Bipedal Locomotion via Signal Temporal Logic Guided Model Predictive Control (Zhaoyuan Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoyuan Gu, Yuntian Zhao, Yipu Chen, Rongming Guo, Jennifer K. Leestma, Gregory S. Sawicki, Ye Zhao. (2024)<br><strong>Robust-Locomotion-by-Logic: Perturbation-Resilient Bipedal Locomotion via Signal Temporal Logic Guided Model Predictive Control</strong><br><button class=copy-to-clipboard title="Robust-Locomotion-by-Logic: Perturbation-Resilient Bipedal Locomotion via Signal Temporal Logic Guided Model Predictive Control" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15993v1.pdf filename=2403.15993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces a robust planning framework that utilizes a model predictive control (MPC) approach, enhanced by incorporating signal temporal logic (STL) specifications. This marks the first-ever study to apply STL-guided trajectory optimization for bipedal locomotion, specifically designed to handle both translational and orientational perturbations. Existing recovery strategies often struggle with <b>reasoning</b> complex task logic and evaluating locomotion robustness systematically, making them susceptible to failures caused by inappropriate recovery strategies or lack of robustness. To address these issues, we design an analytical robustness metric for bipedal locomotion and quantify this metric using STL specifications, which guide the generation of recovery trajectories to achieve maximum locomotion robustness. To enable safe and computational-efficient crossed-leg maneuver, we design data-driven self-leg-collision constraints that are $1000$ times faster than the traditional inverse-kinematics-based approach. Our framework outperforms a state-of-the-art locomotion controller, a standard MPC without STL, and a linear-temporal-logic-based planner in a high-fidelity dynamic <b>simulation,</b> especially in scenarios involving crossed-leg maneuvers. Additionally, the Cassie bipedal robot achieves robust performance under horizontal and orientational perturbations such as those observed in ship motions. These environments are validated in <b>simulations</b> and deployed on hardware. Furthermore, our proposed method demonstrates versatility on stepping stones and terrain-agnostic features on inclined terrains.</p></p class="citation"></blockquote><h3 id=58--116146-realtime-robust-shape-estimation-of-deformable-linear-object-jiaming-zhang-et-al-2024>(5/8 | 116/146) Realtime Robust Shape Estimation of Deformable Linear Object (Jiaming Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaming Zhang, Zhaomeng Zhang, Yihao Liu, Yaqian Chen, Amir Kheradmand, Mehran Armand. (2024)<br><strong>Realtime Robust Shape Estimation of Deformable Linear Object</strong><br><button class=copy-to-clipboard title="Realtime Robust Shape Estimation of Deformable Linear Object" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16146v1.pdf filename=2403.16146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Realtime shape estimation of continuum objects and manipulators is essential for developing accurate planning and control paradigms. The existing methods that create dense point clouds from camera images, and/or use distinguishable markers on a deformable body have limitations in realtime tracking of large continuum objects/manipulators. The physical occlusion of markers can often compromise accurate shape estimation. We propose a robust method to estimate the shape of linear deformable objects in realtime using scattered and unordered key points. By utilizing a robust probability-based labeling algorithm, our approach identifies the true order of the detected key points and then reconstructs the shape using piecewise spline interpolation. The approach only relies on knowing the number of the key points and the interval between two neighboring points. We demonstrate the robustness of the method when key points are partially occluded. The proposed method is also integrated into a <b>simulation</b> in Unity for tracking the shape of a cable with a length of 1m and a radius of 5mm. The <b>simulation</b> results show that our proposed approach achieves an average length error of 1.07% over the continuum&rsquo;s centerline and an average cross-section error of 2.11mm. The real-world experiments of tracking and estimating a heavy-load cable prove that the proposed approach is robust under occlusion and complex entanglement scenarios.</p></p class="citation"></blockquote><h3 id=68--117146-ht-lip-model-based-robust-control-of-quadrupedal-robot-locomotion-under-unknown-vertical-ground-motion-amir-iqbal-et-al-2024>(6/8 | 117/146) HT-LIP Model based Robust Control of Quadrupedal Robot Locomotion under Unknown Vertical Ground Motion (Amir Iqbal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Iqbal, Sushant Veer, Christopher Niezrecki, Yan Gu. (2024)<br><strong>HT-LIP Model based Robust Control of Quadrupedal Robot Locomotion under Unknown Vertical Ground Motion</strong><br><button class=copy-to-clipboard title="HT-LIP Model based Robust Control of Quadrupedal Robot Locomotion under Unknown Vertical Ground Motion" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16262v1.pdf filename=2403.16262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a hierarchical control framework that enables robust quadrupedal locomotion on a dynamic rigid surface (DRS) with general and unknown vertical motions. The key novelty of the framework lies in its higher layer, which is a <b>discrete-time,</b> <b>provably</b> stabilizing footstep controller. The basis of the footstep controller is a new hybrid, time-varying, linear inverted pendulum (HT-LIP) model that is low-dimensional and accurately captures the essential robot dynamics during DRS locomotion. A new set of sufficient stability conditions are then derived to directly guide the controller design for ensuring the asymptotic stability of the HT-LIP model under general, unknown, vertical DRS motions. Further, the footstep controller is cast as a computationally efficient quadratic program that incorporates the proposed HT-LIP model and stability conditions. The middle layer takes the desired footstep locations generated by the higher layer as input to produce kinematically feasible full-body reference trajectories, which are then accurately tracked by a lower-layer torque controller. Hardware experiments on a Unitree Go1 quadrupedal robot confirm the robustness of the proposed framework under various unknown, aperiodic, vertical DRS motions and uncertainties (e.g., slippery and uneven surfaces, solid and liquid loads, and sudden pushes).</p></p class="citation"></blockquote><h3 id=78--118146-combined-task-and-motion-planning-via-sketch-decompositions-extended-version-with-supplementary-material-magí-dalmau-moreno-et-al-2024>(7/8 | 118/146) Combined Task and Motion Planning Via Sketch Decompositions (Extended Version with Supplementary Material) (Magí Dalmau-Moreno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Magí Dalmau-Moreno, Néstor García, Vicenç Gómez, Héctor Geffner. (2024)<br><strong>Combined Task and Motion Planning Via Sketch Decompositions (Extended Version with Supplementary Material)</strong><br><button class=copy-to-clipboard title="Combined Task and Motion Planning Via Sketch Decompositions (Extended Version with Supplementary Material)" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16277v1.pdf filename=2403.16277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The challenge in combined task and motion planning (TAMP) is the effective integration of a search over a combinatorial space, usually carried out by a task planner, and a search over a continuous configuration space, carried out by a motion planner. Using motion planners for testing the feasibility of task plans and filling out the details is not effective because it makes the geometrical constraints play a passive role. This work introduces a new interleaved approach for integrating the two dimensions of TAMP that makes use of sketches, a recent simple but powerful language for expressing the decomposition of problems into subproblems. A sketch has width 1 if it decomposes the problem into subproblems that can be solved greedily in linear time. In the paper, a general sketch is introduced for several classes of TAMP problems which has width 1 under suitable assumptions. While sketch decompositions have been developed for classical planning, they offer two important benefits in the context of TAMP. First, when a task plan is found to be unfeasible due to the geometric constraints, the combinatorial search resumes in a specific sub-problem. Second, the sampling of object configurations is not done once, globally, at the start of the search, but locally, at the start of each subproblem. Optimizations of this basic setting are also considered and experimental results over existing and new pick-and-place <b>benchmarks</b> are reported.</p></p class="citation"></blockquote><h3 id=88--119146-kitchen-a-real-world-benchmark-and-dataset-for-6d-object-pose-estimation-in-kitchen-environments-abdelrahman-younes-et-al-2024>(8/8 | 119/146) KITchen: A Real-World Benchmark and Dataset for 6D Object Pose Estimation in Kitchen Environments (Abdelrahman Younes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdelrahman Younes, Tamim Asfour. (2024)<br><strong>KITchen: A Real-World Benchmark and Dataset for 6D Object Pose Estimation in Kitchen Environments</strong><br><button class=copy-to-clipboard title="KITchen: A Real-World Benchmark and Dataset for 6D Object Pose Estimation in Kitchen Environments" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16238v1.pdf filename=2403.16238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the recent progress on 6D object pose estimation methods for robotic grasping, a substantial performance gap persists between the capabilities of these methods on existing datasets and their efficacy in real-world mobile manipulation tasks, particularly when robots rely solely on their monocular egocentric field of view (FOV). Existing real-world datasets primarily focus on table-top grasping scenarios, where a robotic arm is placed in a fixed position and the objects are centralized within the FOV of fixed external camera(s). Assessing performance on such datasets may not accurately reflect the challenges encountered in everyday mobile manipulation tasks within kitchen environments such as retrieving objects from higher shelves, sinks, dishwashers, ovens, refrigerators, or microwaves. To address this gap, we present Kitchen, a novel <b>benchmark</b> designed specifically for estimating the 6D poses of objects located in diverse positions within kitchen settings. For this purpose, we recorded a comprehensive dataset comprising around 205k real-world RGBD images for 111 kitchen objects captured in two distinct kitchens, utilizing one humanoid robot with its egocentric perspectives. Subsequently, we developed a semi-automated annotation pipeline, to streamline the labeling process of such datasets, resulting in the generation of 2D object labels, 2D object segmentation masks, and 6D object poses with minimized human effort. The <b>benchmark,</b> the dataset, and the annotation pipeline are available at <a href=https://kitchen-dataset.github.io/KITchen>https://kitchen-dataset.github.io/KITchen</a>.</p></p class="citation"></blockquote><h2 id=csir-2>cs.IR (2)</h2><h3 id=12--120146-knowledge-aware-dual-side-attribute-enhanced-recommendation-taotian-pang-et-al-2024>(1/2 | 120/146) Knowledge-aware Dual-side Attribute-enhanced Recommendation (Taotian Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taotian Pang, Xingyu Lou, Fei Zhao, Zhen Wu, Kuiyao Dong, Qiuying Peng, Yue Qi, Xinyu Dai. (2024)<br><strong>Knowledge-aware Dual-side Attribute-enhanced Recommendation</strong><br><button class=copy-to-clipboard title="Knowledge-aware Dual-side Attribute-enhanced Recommendation" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 31<br>Keywords: Graph, Graph Neural Network, Benchmarking, Knowledge Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16037v1.pdf filename=2403.16037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>\textit{Knowledge-aware} <b>recommendation</b> methods (KGR) based on \textit{graph neural networks} <b>(GNNs)</b> and \textit{contrastive learning} (CL) have achieved promising performance. However, they fall short in modeling fine-grained user preferences and further fail to leverage the \textit{preference-attribute connection} to make predictions, leading to sub-optimal performance. To address the issue, we propose a method named \textit{\textbf{K}nowledge-aware \textbf{D}ual-side \textbf{A}ttribute-enhanced \textbf{R}ecommendation} (KDAR). Specifically, we build \textit{user preference representations} and \textit{attribute fusion representations} upon the attribute information in <b>knowledge</b> <b>graphs,</b> which are utilized to enhance \textit{collaborative filtering} (CF) based user and item representations, respectively. To discriminate the contribution of each attribute in these two types of attribute-based representations, a \textit{multi-level collaborative alignment contrasting} mechanism is proposed to align the importance of attributes with CF signals. Experimental results on four <b>benchmark</b> datasets demonstrate the superiority of KDAR over several state-of-the-art baselines. Further analyses verify the effectiveness of our method. The code of KDAR is released at: \href{https://github.com/TJTP/KDAR}{https://github.com/TJTP/KDAR}.</p></p class="citation"></blockquote><h3 id=22--121146-complementary-recommendation-in-e-commerce-definition-approaches-and-future-directions-linyue-li-et-al-2024>(2/2 | 121/146) Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions (Linyue Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linyue Li, Zhijuan Du. (2024)<br><strong>Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions</strong><br><button class=copy-to-clipboard title="Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs-LG, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16135v1.pdf filename=2403.16135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, complementary <b>recommendation</b> has received extensive attention in the e-commerce domain. In this paper, we comprehensively <b>summarize</b> and compare 34 representative studies conducted between 2009 and 2024. Firstly, we compare the data and methods used for modeling complementary relationships between products, including simple complementarity and more complex scenarios such as asymmetric complementarity, the coexistence of substitution and complementarity relationships between products, and varying degrees of complementarity between different pairs of products. Next, we classify and compare the models based on the research problems of complementary <b>recommendation,</b> such as diversity, personalization, and cold-start. Furthermore, we provide a comparative analysis of experimental results from different studies conducted on the same dataset, which helps identify the strengths and weaknesses of the research. Compared to previous surveys, this paper provides a more updated and comprehensive summary of the research, discusses future research directions, and contributes to the advancement of this field.</p></p class="citation"></blockquote><h2 id=csdl-1>cs.DL (1)</h2><h3 id=11--122146-large-language-models-in-biomedical-and-health-informatics-a-bibliometric-review-huizi-yu-et-al-2024>(1/1 | 122/146) Large Language Models in Biomedical and Health Informatics: A Bibliometric Review (Huizi Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huizi Yu, Lizhou Fan, Lingyao Li, Jiayan Zhou, Zihui Ma, Lu Xian, Wenyue Hua, Sijia He, Mingyu Jin, Yongfeng Zhang, Ashvin Gandhi, Xin Ma. (2024)<br><strong>Large Language Models in Biomedical and Health Informatics: A Bibliometric Review</strong><br><button class=copy-to-clipboard title="Large Language Models in Biomedical and Health Informatics: A Bibliometric Review" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-AI, cs-CL, cs-DL, cs-SI, cs.DL<br>Keyword Score: 30<br>Keywords: Recommendation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16303v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16303v2.pdf filename=2403.16303v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research. This bibliometric review aims to provide a panoramic view of how <b>LLMs</b> have been used in BHI by examining research articles and collaboration networks from 2022 to 2023. It further explores how <b>LLMs</b> can improve Natural Language Processing (NLP) applications in various BHI areas like medical diagnosis, patient engagement, electronic health record management, and personalized medicine. To do this, our bibliometric review identifies key trends, maps out research networks, and highlights major developments in this fast-moving field. Lastly, it discusses the ethical concerns and practical challenges of using <b>LLMs</b> in BHI, such as data privacy and reliable medical <b>recommendations.</b> Looking ahead, we consider how <b>LLMs</b> could further transform biomedical research as well as healthcare delivery and patient outcomes. This bibliometric review serves as a resource for stakeholders in healthcare, including researchers, clinicians, and policymakers, to understand the current state and future potential of <b>LLMs</b> in BHI.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--123146-mrsch-multi-resource-scheduling-for-hpc-boyang-li-et-al-2024>(1/1 | 123/146) MRSch: Multi-Resource Scheduling for HPC (Boyang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyang Li, Yuping Fan, Matthew Dearing, Zhiling Lan, Paul Richy, William Allcocky, Michael Papka. (2024)<br><strong>MRSch: Multi-Resource Scheduling for HPC</strong><br><button class=copy-to-clipboard title="MRSch: Multi-Resource Scheduling for HPC" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16298v1.pdf filename=2403.16298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emerging workloads in high-performance computing (HPC) are embracing significant changes, such as having diverse resource requirements instead of being CPU-centric. This advancement forces cluster schedulers to consider multiple schedulable resources during decision-making. Existing scheduling studies rely on heuristic or optimization methods, which are limited by an inability to adapt to new scenarios for ensuring long-term scheduling performance. We present an intelligent scheduling agent named MRSch for multi-resource scheduling in HPC that leverages direct future prediction (DFP), an advanced multi-objective <b>reinforcement</b> <b>learning</b> algorithm. While DFP demonstrated outstanding performance in a gaming competition, it has not been previously explored in the context of HPC scheduling. Several key techniques are developed in this study to tackle the challenges involved in multi-resource scheduling. These techniques enable MRSch to learn an appropriate scheduling policy automatically and dynamically adapt its policy in response to workload changes via dynamic resource prioritizing. We compare MRSch with existing scheduling methods through extensive tracebase <b>simulations.</b> Our results demonstrate that MRSch improves scheduling performance by up to 48% compared to the existing scheduling methods.</p></p class="citation"></blockquote><h2 id=csni-4>cs.NI (4)</h2><h3 id=14--124146-study-of-workload-interference-with-intelligent-routing-on-dragonfly-yao-kang-et-al-2024>(1/4 | 124/146) Study of Workload Interference with Intelligent Routing on Dragonfly (Yao Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Kang, Xin Wang, Zhiling Lan. (2024)<br><strong>Study of Workload Interference with Intelligent Routing on Dragonfly</strong><br><button class=copy-to-clipboard title="Study of Workload Interference with Intelligent Routing on Dragonfly" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-DC, cs-NI, cs.NI<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16288v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16288v1.pdf filename=2403.16288v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dragonfly interconnect is a crucial network technology for supercomputers. To support exascale systems, network resources are shared such that links and routers are not dedicated to any node pair. While link utilization is increased, workload performance is often offset by network contention. Recently, intelligent routing built on <b>reinforcement</b> <b>learning</b> demonstrates higher network throughput with lower packet latency. However, its effectiveness in reducing workload interference is unknown. In this work, we present extensive network <b>simulations</b> to study multi-workload contention under different routing mechanisms, intelligent routing and adaptive routing, on a large-scale Dragonfly system. We develop an enhanced network <b>simulation</b> toolkit, along with a suite of workloads with distinctive communication patterns. We also present two metrics to characterize application communication intensity. Our analysis focuses on examining how different workloads interfere with each other under different routing mechanisms by inspecting both application-level and network-level metrics. Several key insights are made from the analysis.</p></p class="citation"></blockquote><h3 id=24--125146-digital-twin-assisted-intelligent-network-management-for-vehicular-applications-kaige-qu-et-al-2024>(2/4 | 125/146) Digital Twin Assisted Intelligent Network Management for Vehicular Applications (Kaige Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaige Qu, Weihua Zhuang. (2024)<br><strong>Digital Twin Assisted Intelligent Network Management for Vehicular Applications</strong><br><button class=copy-to-clipboard title="Digital Twin Assisted Intelligent Network Management for Vehicular Applications" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 13<br>Keywords: Benchmarking, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16021v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16021v1.pdf filename=2403.16021v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emerging data-driven methods based on artificial intelligence (AI) have paved the way for intelligent, flexible, and adaptive network management in vehicular applications. To enhance network management towards network automation, this article presents a digital twin (DT) assisted two-tier learning framework, which facilitates the automated life-cycle management of machine learning based intelligent network management functions (INMFs). Specifically, at a high tier, <b>meta</b> <b>learning</b> is employed to capture different levels of general features for the INMFs under nonstationary network conditions. At a low tier, individual learning models are customized for local networks based on fast model adaptation. Hierarchical DTs are deployed at the edge and cloud servers to assist the two-tier learning process, through closed-loop interactions with the physical network domain. Finally, a case study demonstrates the fast and accurate model adaptation ability of <b>meta</b> <b>learning</b> in comparison with <b>benchmark</b> schemes.</p></p class="citation"></blockquote><h3 id=34--126146-q-adaptive-a-multi-agent-reinforcement-learning-based-routing-on-dragonfly-network-yao-kang-et-al-2024>(3/4 | 126/146) Q-adaptive: A Multi-Agent Reinforcement Learning Based Routing on Dragonfly Network (Yao Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Kang, Xin Wang, Zhiling Lan. (2024)<br><strong>Q-adaptive: A Multi-Agent Reinforcement Learning Based Routing on Dragonfly Network</strong><br><button class=copy-to-clipboard title="Q-adaptive: A Multi-Agent Reinforcement Learning Based Routing on Dragonfly Network" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-DC, cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16301v1.pdf filename=2403.16301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>on adaptive routing to balance network traffic for optimum performance. Ideally, adaptive routing attempts to forward packets between minimal and non-minimal paths with the least congestion. In practice, current adaptive routing algorithms estimate routing path congestion based on local information such as output queue occupancy. Using local information to estimate global path congestion is inevitably inaccurate because a router has no precise knowledge of link states a few hops away. This inaccuracy could lead to interconnect congestion. In this study, we present Q-adaptive routing, a multi-agent <b>reinforcement</b> <b>learning</b> routing scheme for Dragonfly systems. Q-adaptive routing enables routers to learn to route autonomously by leveraging advanced <b>reinforcement</b> <b>learning</b> technology. The proposed Q-adaptive routing is highly scalable thanks to its fully distributed nature without using any shared information between routers. Furthermore, a new two-level Q-table is designed for Q-adaptive to make it computational lightly and saves 50% of router memory usage compared with the previous Q-routing. We implement the proposed Q-adaptive routing in SST/Merlin simulator. Our evaluation results show that Q-adaptive routing achieves up to 10.5% system throughput improvement and 5.2x average packet latency reduction compared with adaptive routing algorithms. Remarkably, Q-adaptive can even outperform the optimal VALn non-minimal routing under the ADV+1 adversarial traffic pattern with up to 3% system throughput improvement and 75% average packet latency reduction.</p></p class="citation"></blockquote><h3 id=44--127146-interference-management-for-integrated-sensing-and-communication-systems-a-survey-yangyang-niu-et-al-2024>(4/4 | 127/146) Interference Management for Integrated Sensing and Communication Systems: A Survey (Yangyang Niu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yangyang Niu, Zhiqing Wei, Lin Wang, Huici Wu, Zhiyong Feng. (2024)<br><strong>Interference Management for Integrated Sensing and Communication Systems: A Survey</strong><br><button class=copy-to-clipboard title="Interference Management for Integrated Sensing and Communication Systems: A Survey" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16189v1.pdf filename=2403.16189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emerging applications such as autonomous driving and Internet of things (IoT) services put forward the demand for simutaneous sensing and communication functions in the same system. Integrated sensing and communication (ISAC) has the potential to meet the demands of ubiquitous communication and high-precision sensing due to the advantages of spectrum and hardware resource sharing, as well as the mutual enhancement of sensing and communication. However, ISAC system faces severe interference requiring effective interference suppression, avoidance, and exploitation techniques. This article provides a comprehensive survey on the interference management techniques in ISAC systems, involving network architecture, system design, signal processing, and resource allocation. We first review the channel modeling and performance metrics of the ISAC system. Then, the methods for managing self-interference (SI), mutual interference (MI), and clutter in a single base station (BS) system are <b>summarized,</b> including interference suppression, interference avoidance and interference exploitation methods. Furthermore, cooperative interference management methods are studied to address the cross-link interference (CLI) in a coordinated multipoint ISAC (CoMP-ISAC) system. Finally, future trends are revealed. This article may provide a reference for the study of interference management in ISAC systems.</p></p class="citation"></blockquote><h2 id=physicsao-ph-1>physics.ao-ph (1)</h2><h3 id=11--128146-interpretable-machine-learning-for-weather-and-climate-prediction-a-survey-ruyi-yang-et-al-2024>(1/1 | 128/146) Interpretable Machine Learning for Weather and Climate Prediction: A Survey (Ruyi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruyi Yang, Jingyu Hu, Zihao Li, Jianli Mu, Tingzhao Yu, Jiangjiang Xia, Xuhong Li, Aritra Dasgupta, Haoyi Xiong. (2024)<br><strong>Interpretable Machine Learning for Weather and Climate Prediction: A Survey</strong><br><button class=copy-to-clipboard title="Interpretable Machine Learning for Weather and Climate Prediction: A Survey" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.ao-ph<br>Categories: cs-AI, cs-LG, physics-ao-ph, physics.ao-ph<br>Keyword Score: 23<br>Keywords: Benchmarking, Foundation Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18864v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18864v1.pdf filename=2403.18864v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advanced machine learning models have recently achieved high predictive accuracy for weather and climate prediction. However, these complex models often lack inherent transparency and interpretability, acting as &ldquo;black boxes&rdquo; that impede user trust and hinder further model improvements. As such, interpretable machine learning techniques have become crucial in enhancing the credibility and utility of weather and climate modeling. In this survey, we review current interpretable machine learning approaches applied to meteorological predictions. We categorize methods into two major paradigms: 1) Post-hoc interpretability techniques that explain pre-trained models, such as perturbation-based, game theory based, and gradient-based attribution methods. 2) Designing inherently interpretable models from scratch using architectures like tree ensembles and explainable neural networks. We <b>summarize</b> how each technique provides insights into the predictions, uncovering novel meteorological relationships captured by machine learning. Lastly, we discuss research challenges around achieving deeper mechanistic interpretations aligned with physical principles, developing standardized evaluation <b>benchmarks,</b> integrating interpretability into iterative model development workflows, and providing explainability for large <b>foundation</b> <b>models.</b></p></p class="citation"></blockquote><h2 id=cshc-1>cs.HC (1)</h2><h3 id=11--129146-designing-child-centric-ai-learning-environments-insights-from-llm-enhanced-creative-project-based-learning-siyu-zha-et-al-2024>(1/1 | 129/146) Designing Child-Centric AI Learning Environments: Insights from LLM-Enhanced Creative Project-Based Learning (Siyu Zha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyu Zha, Yuehan Qiao, Qingyu Hu, Zhongsheng Li, Jiangtao Gong, Yingqing Xu. (2024)<br><strong>Designing Child-Centric AI Learning Environments: Insights from LLM-Enhanced Creative Project-Based Learning</strong><br><button class=copy-to-clipboard title="Designing Child-Centric AI Learning Environments: Insights from LLM-Enhanced Creative Project-Based Learning" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16159v1.pdf filename=2403.16159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Project-based learning (PBL) is an instructional method that is very helpful in nurturing students&rsquo; creativity, but it requires significant time and energy from both students and teachers. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been proven to assist in creative tasks, yet much controversy exists regarding their role in fostering creativity. This paper explores the potential of <b>LLMs</b> in PBL settings, with a special focus on fostering creativity. We began with an exploratory study involving 12 middle school students and identified five design considerations for <b>LLM</b> applications in PBL. Building on this, we developed an <b>LLM-empowered,</b> 48-hour PBL program and conducted an instructional experiment with 31 middle school students. Our results indicated that <b>LLMs</b> can enhance every stage of PBL. Additionally, we also discovered ambivalent perspectives among students and mentors toward <b>LLM</b> usage. Furthermore, we explored the challenge and design implications of integrating <b>LLMs</b> into PBL and reflected on the program. By bridging AI advancements into educational practice, our work aims to inspire further discourse and investigation into harnessing AI&rsquo;s potential in child-centric educational settings.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--130146-the-interplay-of-learning-analytics-and-artificial-intelligence-in-education-mutlu-cukurova-2024>(1/1 | 130/146) The Interplay of Learning, Analytics, and Artificial Intelligence in Education (Mutlu Cukurova, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mutlu Cukurova. (2024)<br><strong>The Interplay of Learning, Analytics, and Artificial Intelligence in Education</strong><br><button class=copy-to-clipboard title="The Interplay of Learning, Analytics, and Artificial Intelligence in Education" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Generative AI, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16081v1.pdf filename=2403.16081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a multi dimensional view of AI&rsquo;s role in learning and education, emphasizing the intricate interplay between AI, analytics, and the learning processes. Here, I challenge the prevalent narrow conceptualization of AI as stochastic tools, as exemplified in <b>generative</b> <b>AI,</b> and argue for the importance of alternative conceptualisations of AI. I highlight the differences between human intelligence and artificial information processing, the cognitive diversity inherent in AI algorithms, and posit that AI can also serve as an instrument for understanding human learning. Early learning sciences and AI in Education research, which saw AI as an analogy for human intelligence, have diverged from this perspective, <b>prompting</b> a need to rekindle this connection. The paper presents three unique conceptualizations of AI in education: the externalization of human cognition, the internalization of AI models to influence human thought processes, and the extension of human cognition via tightly integrated human-AI systems. Examples from current research and practice are examined as instances of the three conceptualisations, highlighting the potential value and limitations of each conceptualisation for education, as well as the perils of overemphasis on externalising human cognition as exemplified in today&rsquo;s hype surrounding <b>generative</b> <b>AI</b> tools. The paper concludes with an advocacy for a broader educational approach that includes educating people about AI and innovating educational systems to remain relevant in an AI enabled world.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--131146-target-speech-extraction-with-pre-trained-av-hubert-and-mask-and-recover-strategy-wenxuan-wu-et-al-2024>(1/2 | 131/146) Target Speech Extraction with Pre-trained AV-HuBERT and Mask-And-Recover Strategy (Wenxuan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenxuan Wu, Xueyuan Chen, Xixin Wu, Haizhou Li, Helen Meng. (2024)<br><strong>Target Speech Extraction with Pre-trained AV-HuBERT and Mask-And-Recover Strategy</strong><br><button class=copy-to-clipboard title="Target Speech Extraction with Pre-trained AV-HuBERT and Mask-And-Recover Strategy" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16078v1.pdf filename=2403.16078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Audio-visual target speech extraction (AV-TSE) is one of the enabling technologies in robotics and many audio-visual applications. One of the challenges of AV-TSE is how to effectively utilize audio-visual synchronization information in the process. AV-HuBERT can be a useful pre-trained model for lip-reading, which has not been adopted by AV-TSE. In this paper, we would like to explore the way to integrate a pre-trained AV-HuBERT into our AV-TSE system. We have good reasons to expect an improved performance. To benefit from the inter and intra-modality correlations, we also propose a novel Mask-And-Recover (MAR) strategy for <b>self-supervised</b> <b>learning.</b> The experimental results on the VoxCeleb2 dataset show that our proposed model outperforms the baselines both in terms of subjective and objective metrics, suggesting that the pre-trained AV-HuBERT model provides more informative visual cues for target speech extraction. Furthermore, through a comparative study, we confirm that the proposed Mask-And-Recover strategy is significantly effective.</p></p class="citation"></blockquote><h3 id=22--132146-theoretical-analysis-of-quality-of-conventional-beamforming-for-phased-microphone-arrays-dheepak-khatri-et-al-2024>(2/2 | 132/146) Theoretical Analysis of Quality of Conventional Beamforming for Phased Microphone Arrays (Dheepak Khatri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dheepak Khatri, Kenneth Granlund. (2024)<br><strong>Theoretical Analysis of Quality of Conventional Beamforming for Phased Microphone Arrays</strong><br><button class=copy-to-clipboard title="Theoretical Analysis of Quality of Conventional Beamforming for Phased Microphone Arrays" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: 76Q05 (Primary), cs-SD, cs.SD, eess-AS<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17376v1.pdf filename=2403.17376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A theoretical study is performed to analyze the directional response of different types of microphone array designs. 1-D (linear) and 2-D (planar) microphone array types are considered, and the delay and sum beamforming and conventional beamforming techniques are employed to localize the sound source. A non-dimensional parameter, G, is characterized to simplify and standardize the rejection performance of both 1-D and 2-D microphone arrays as a function of array <b>geometry</b> and sound source parameters. This parameter G is then used to determine an improved design of a 2-D microphone array for far-field sound localization. One such design, termed the Equi-area array is introduced and analyzed in detail. The design is shown to have an advantageous rejection performance compared to other conventionally used 2-D planar microphone arrays.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--133146-near-optimal-differentially-private-low-rank-trace-regression-with-guaranteed-private-initialization-mengyue-zha-2024>(1/3 | 133/146) Near-Optimal differentially private low-rank trace regression with guaranteed private initialization (Mengyue Zha, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengyue Zha. (2024)<br><strong>Near-Optimal differentially private low-rank trace regression with guaranteed private initialization</strong><br><button class=copy-to-clipboard title="Near-Optimal differentially private low-rank trace regression with guaranteed private initialization" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CR, cs-LG, stat-ML, stat.ML<br>Keyword Score: 13<br>Keywords: Sample Size, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15999v1.pdf filename=2403.15999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study differentially private (DP) estimation of a rank-$r$ matrix $M \in \mathbb{R}^{d_1\times d_2}$ under the trace regression model with Gaussian measurement matrices. Theoretically, the sensitivity of non-private spectral initialization is precisely characterized, and the <b>differential-privacy-constrained</b> <b>minimax</b> lower bound for estimating $M$ under the Schatten-$q$ norm is established. Methodologically, the paper introduces a computationally efficient algorithm for DP-initialization with a <b>sample</b> <b>size</b> of $n \geq \widetilde O (r^2 (d_1\vee d_2))$. Under certain regularity conditions, the DP-initialization falls within a local ball surrounding $M$. We also propose a differentially private algorithm for estimating $M$ based on Riemannian optimization (DP-RGrad), which achieves a near-optimal convergence rate with the DP-initialization and <b>sample</b> <b>size</b> of $n \geq \widetilde O(r (d_1 + d_2))$. Finally, the paper discusses the non-trivial gap between the minimax lower bound and the upper bound of low-rank matrix estimation under the trace regression model. It is shown that the estimator given by DP-RGrad attains the optimal convergence rate in a weaker notion of <b>differential</b> <b>privacy.</b> Our powerful technique for analyzing the sensitivity of initialization requires no eigengap condition between $r$ non-zero singular values.</p></p class="citation"></blockquote><h3 id=23--134146-manifold-regularization-classification-model-based-on-improved-diffusion-map-hongfu-guo-et-al-2024>(2/3 | 134/146) Manifold Regularization Classification Model Based On Improved Diffusion Map (Hongfu Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongfu Guo, Wencheng Zou, Zeyu Zhang, Shuishan Zhang, Ruitong Wang, Jintao Zhang. (2024)<br><strong>Manifold Regularization Classification Model Based On Improved Diffusion Map</strong><br><button class=copy-to-clipboard title="Manifold Regularization Classification Model Based On Improved Diffusion Map" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-OC, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16059v1.pdf filename=2403.16059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Manifold regularization model is a <b>semi-supervised</b> <b>learning</b> model that leverages the geometric structure of a dataset, comprising a small number of labeled samples and a large number of unlabeled samples, to generate classifiers. However, the original manifold norm limits the performance of models to local regions. To address this limitation, this paper proposes an approach to improve manifold regularization based on a label propagation model. We initially enhance the probability transition matrix of the diffusion map algorithm, which can be used to estimate the Neumann heat kernel, enabling it to accurately depict the label propagation process on the manifold. Using this matrix, we establish a label propagation function on the dataset to describe the distribution of labels at different time steps. Subsequently, we extend the label propagation function to the entire data manifold. We prove that the extended label propagation function converges to a stable distribution after a sufficiently long time and can be considered as a classifier. Building upon this concept, we propose a viable improvement to the manifold regularization model and validate its superiority through experiments.</p></p class="citation"></blockquote><h3 id=33--135146-learning-directed-acyclic-graphs-from-partial-orderings-ali-shojaie-et-al-2024>(3/3 | 135/146) Learning Directed Acyclic Graphs from Partial Orderings (Ali Shojaie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Shojaie, Wenyu Chen. (2024)<br><strong>Learning Directed Acyclic Graphs from Partial Orderings</strong><br><button class=copy-to-clipboard title="Learning Directed Acyclic Graphs from Partial Orderings" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16031v1.pdf filename=2403.16031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Directed acyclic <b>graphs</b> (DAGs) are commonly used to model causal relationships among random variables. In general, learning the DAG structure is both computationally and statistically challenging. Moreover, without additional information, the direction of edges may not be estimable from observational data. In contrast, given a complete causal ordering of the variables, the problem can be solved efficiently, even in high dimensions. In this paper, we consider the intermediate problem of learning DAGs when a partial causal ordering of variables is available. We propose a general estimation framework for leveraging the partial ordering and present efficient estimation algorithms for low- and high-dimensional problems. The advantages of the proposed framework are illustrated via numerical studies.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--136146-convolution-and-knapsack-in-higher-dimensions-kilian-grage-et-al-2024>(1/2 | 136/146) Convolution and Knapsack in Higher Dimensions (Kilian Grage et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kilian Grage, Klaus Jansen. (2024)<br><strong>Convolution and Knapsack in Higher Dimensions</strong><br><button class=copy-to-clipboard title="Convolution and Knapsack in Higher Dimensions" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16117v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16117v1.pdf filename=2403.16117v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the Knapsack problem, one is given the task of packing a knapsack of a given size with items in order to gain a packing with a high profit value. In recent years, a connection to the $(\max,+)$-convolution problem has been established, where knapsack solutions can be combined by building the <b>convolution</b> of two sequences. This observation has been used to give conditional lower bounds but also parameterized algorithms. In this paper we want to carry these results into higher dimension. We consider Knapsack where items are characterized by multiple properties - given through a vector - and a knapsack that has a capacity vector. The packing must now not exceed any of the given capacity constraints. In order to show a similar sub-quadratic lower bound we introduce a multi-dimensional version of <b>convolution</b> as well. Instead of combining sequences, we will generalize this problem and combine higher dimensional matrices. We will establish a few variants of these problems and prove that they are all equivalent in terms of algorithms that allow for a running time sub-quadratic in the number of entries of the matrix. We further develop a parameterized algorithm to solve higher dimensional Knapsack. The techniques we apply are inspired by an algorithm introduced by Axiotis and Tzamos. In general, we manage not only to extend their result to higher dimension. We will show that even for higher dimensional Knapsack, we can reduce the problem to <b>convolution</b> on one-dimensional sequences, leading to an $\mathcal{O}(d(n + D \cdot \max{\Pi_{i=1}^d{t_i}, t_{\max}\log t_{\max}} ))$ algorithm, where $D$ is the number of different weight vectors, $t$ the capacity vector and $d$ is the dimension of the problem. Finally we also modify this algorithm to handle items with negative weights to cross the bridge from solving not only Knapsack but also Integer Linear Programs (ILPs) in general.</p></p class="citation"></blockquote><h3 id=22--137146-hashing-geographical-point-data-using-the-space-filling-h-curve-igor-v-netay-2024>(2/2 | 137/146) Hashing geographical point data using the space-filling H-curve (Igor V. Netay, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Igor V. Netay. (2024)<br><strong>Hashing geographical point data using the space-filling H-curve</strong><br><button class=copy-to-clipboard title="Hashing geographical point data using the space-filling H-curve" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CG, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16216v1.pdf filename=2403.16216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We construct geohashing procedure based on using of space-filling H-curve. This curve provides a way to construct geohash with less computations than the construction based on usage of Hilbert curve. At the same time, H-curve has better <b>clustering</b> properties.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--138146-mimo-with-analogue-1-bit-phase-shifters-a-quantum-annealing-perspective-ioannis-krikidis-2024>(1/2 | 138/146) MIMO with Analogue 1-bit Phase Shifters: A Quantum Annealing Perspective (Ioannis Krikidis, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ioannis Krikidis. (2024)<br><strong>MIMO with Analogue 1-bit Phase Shifters: A Quantum Annealing Perspective</strong><br><button class=copy-to-clipboard title="MIMO with Analogue 1-bit Phase Shifters: A Quantum Annealing Perspective" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16045v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16045v1.pdf filename=2403.16045v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this letter, we study the analogue pre/post-coding vector design for a point-to-point multiple-input multiple-output (MIMO) system with 1-bit phase shifters. Specifically, we focus on the signal-to-noise ratio (SNR) maximization problem which corresponds to a combinatorial NP-hard due to the binary phase resolution. Two classical computation heuristics are proposed i.e., i) an 1-bit real-valued approximation of the optimal digital designs, and ii) an alternating optimization where a Rayleigh quotient problem is solved at each iteration. An iterative quantum annealing <b>(QA)-based</b> heuristic is also investigated, which outperforms classical counterparts and achieves near-optimal performance while ensuring polynomial time complexity. Experimental results in a real-world D-WAVE <b>QA</b> device validate the efficiency of the proposed <b>QA</b> approach.</p></p class="citation"></blockquote><h3 id=22--139146-on-the-secrecy-enhancement-of-an-integrated-ground-aerial-network-with-a-hybrid-fsothz-feeder-link-elmehdi-illi-et-al-2024>(2/2 | 139/146) On the Secrecy Enhancement of an Integrated Ground-Aerial Network with a Hybrid FSO/THz Feeder Link (Elmehdi Illi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elmehdi Illi, Marwa Qaraqe. (2024)<br><strong>On the Secrecy Enhancement of an Integrated Ground-Aerial Network with a Hybrid FSO/THz Feeder Link</strong><br><button class=copy-to-clipboard title="On the Secrecy Enhancement of an Integrated Ground-Aerial Network with a Hybrid FSO/THz Feeder Link" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16072v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16072v1.pdf filename=2403.16072v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High altitude platforms (HAPs)-aided terrestrial-aerial communication technology based on free-space optical (FSO) and Terahertz (THz) feeder links has been attracting notable interest recently due to its great potential in reaching a higher data rate and connectivity. Nonetheless, the presence of harsh vertical propagation environments and potential aerial eavesdroppers are two of the main challenges limiting the reliability and security of such a technology. In this work, a secrecy-enhancing scheme for HAP-aided ground-aerial communication is proposed. The considered network consists of HAP-assisted communication between a ground station and a legitimate user under the threat of an aerial and ground eavesdropper. Thus, the proposed scheme leverages (i) HAP diversity by exploiting the presence of multiple flying HAPs and (ii) the use of a hybrid FSO/THz transmission scheme to offer better resilience against eavesdropping attacks. An analytical secrecy outage probability (SOP) expression is derived for the scheme in consideration. Results manifest the notable gain in security of the proposed scheme with respect to both (i) the single-HAP and (ii) THz feeder-based <b>benchmark</b> ones, where the proposed scheme&rsquo;s SOP is decreased by four orders of magnitude using $4$ HAPs with respect to the first <b>benchmark</b> scheme, while a $5$-dB secrecy gain is manifested with respect to the second <b>benchmark</b> one.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--140146-cbgt-net-a-neuromimetic-architecture-for-robust-classification-of-streaming-data-shreya-sharma-et-al-2024>(1/1 | 140/146) CBGT-Net: A Neuromimetic Architecture for Robust Classification of Streaming Data (Shreya Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shreya Sharma, Dana Hughes, Katia Sycara. (2024)<br><strong>CBGT-Net: A Neuromimetic Architecture for Robust Classification of Streaming Data</strong><br><button class=copy-to-clipboard title="CBGT-Net: A Neuromimetic Architecture for Robust Classification of Streaming Data" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-CV, cs-LG, cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15974v1.pdf filename=2403.15974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes CBGT-Net, a neural network model inspired by the cortico-basal ganglia-thalamic (CBGT) circuits found in mammalian brains. Unlike traditional neural network models, which either generate an output for each provided input, or an output after a fixed sequence of inputs, the CBGT-Net learns to produce an output after a sufficient criteria for evidence is achieved from a stream of observed data. For each observation, the CBGT-Net generates a vector that explicitly represents the amount of evidence the observation provides for each potential decision, accumulates the evidence over time, and generates a decision when the accumulated evidence exceeds a pre-defined threshold. We evaluate the proposed model on two image classification tasks, where models need to predict image categories based on a stream of small patches extracted from the image. We show that the CBGT-Net provides improved accuracy and robustness compared to models trained to classify from a single patch, and models leveraging an <b>LSTM</b> layer to classify from a fixed sequence length of patches.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--141146-maximum-polygon-packing-the-cgshop-challenge-2024-sándor-p-fekete-et-al-2024>(1/1 | 141/146) Maximum Polygon Packing: The CG:SHOP Challenge 2024 (Sándor P. Fekete et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sándor P. Fekete, Phillip Keldenich, Dominik Krupke, Stefan Schirra. (2024)<br><strong>Maximum Polygon Packing: The CG:SHOP Challenge 2024</strong><br><button class=copy-to-clipboard title="Maximum Polygon Packing: The CG:SHOP Challenge 2024" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: F-2-2, cs-CG, cs-DS, cs.CG<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16203v1.pdf filename=2403.16203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We give an overview of the 2024 Computational <b>Geometry</b> Challenge targeting the problem \textsc{Maximum Polygon Packing}: Given a convex region $P$ in the plane, and a collection of simple polygons $Q_1, \ldots, Q_n$, each $Q_i$ with a respective value $c_i$, find a subset $S \subseteq {1, \ldots,n}$ and a feasible packing within $P$ of the polygons $Q_i$ (without rotation) for $i \in S$, maximizing $\sum_{i \in S} c_i$. Geometric packing problems, such as this, present significant computational challenges and are of substantial practical importance.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--142146-on-reporting-durable-patterns-in-temporal-proximity-graphs-pankaj-k-agarwal-et-al-2024>(1/1 | 142/146) On Reporting Durable Patterns in Temporal Proximity Graphs (Pankaj K. Agarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pankaj K. Agarwal, Xiao Hu, Stavros Sintos, Jun Yang. (2024)<br><strong>On Reporting Durable Patterns in Temporal Proximity Graphs</strong><br><button class=copy-to-clipboard title="On Reporting Durable Patterns in Temporal Proximity Graphs" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16312v1.pdf filename=2403.16312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Finding patterns in <b>graphs</b> is a fundamental problem in databases and data mining. In many applications, <b>graphs</b> are temporal and evolve over time, so we are interested in finding durable patterns, such as triangles and paths, which persist over a long time. While there has been work on finding durable simple patterns, existing algorithms do not have provable guarantees and run in strictly super-linear time. The paper leverages the observation that many <b>graphs</b> arising in practice are naturally proximity <b>graphs</b> or can be approximated as such, where nodes are embedded as points in some high-dimensional space, and two nodes are connected by an edge if they are close to each other. We work with an implicit representation of the proximity <b>graph,</b> where nodes are additionally annotated by time intervals, and design near-linear-time algorithms for finding (approximately) durable patterns above a given durability threshold. We also consider an interactive setting where a client experiments with different durability thresholds in a sequence of queries; we show how to compute incremental changes to result patterns efficiently in time near-linear to the size of the changes.</p></p class="citation"></blockquote><h2 id=q-biope-1>q-bio.PE (1)</h2><h3 id=11--143146-an-information-theoretic-treatment-of-animal-movement-tracks-wayne-m-getz-2024>(1/1 | 143/146) An Information Theoretic Treatment of Animal Movement Tracks (Wayne M Getz, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wayne M Getz. (2024)<br><strong>An Information Theoretic Treatment of Animal Movement Tracks</strong><br><button class=copy-to-clipboard title="An Information Theoretic Treatment of Animal Movement Tracks" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.PE<br>Categories: cs-IT, math-IT, q-bio-PE, q-bio.PE<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16290v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16290v2.pdf filename=2403.16290v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The two-dimensional track of an animal on a landscape has progressed over the past three decades from hourly to second-by-second recordings of locations. Track segmentation methods for analyzing the behavioral information in such relocation data has lagged somewhat behind, with scales of analysis currently at the sub-hourly to minute level. A new approach is needed to bring segmentation analysis down to a second-by-second level. Here, such an approach is presented that rests heavily on concepts from Shannon&rsquo;s Information Theory. In this paper, we first briefly review and update concepts relating to movement path segmentation. We then discuss how cluster analysis can be used to organize the smallest viable statistical movement elements (StaMEs), which are $\mu$ steps long, and to code the next level of movement elements called ``words&rsquo;&rsquo; that are $m \mu$ steps long. Centroids of these word clusters are identified as canonical activity modes (CAMs). Unlike current segmentation schemes, the approach presented here allows us to provide entropy measures for movement paths, compute the coding efficiencies of derived StaMEs and CAMs, and assess error rates in the allocation of strings of $m$ StaMEs to CAM types. In addition our approach allows us to employ the Jensen-Shannon divergence measure to assess and compare the best choices for the various parameters (number of steps in a StaME, number of StaME types, number of StaMEs in a word, number of CAM types), as well as the best <b>clustering</b> methods for generating segments that can then be used to interpret and predict sequences of higher order segments. The theory presented here provides another tool in our toolbox for dealing with the effects of global change on the movement and redistribution of animals across altered landscapes</p></p class="citation"></blockquote><h2 id=mathna-1>math.NA (1)</h2><h3 id=11--144146-performance-evaluation-of-accelerated-complex-multiple-precision-lu-decomposition-tomonori-kouya-2024>(1/1 | 144/146) Performance evaluation of accelerated complex multiple-precision LU decomposition (Tomonori Kouya, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tomonori Kouya. (2024)<br><strong>Performance evaluation of accelerated complex multiple-precision LU decomposition</strong><br><button class=copy-to-clipboard title="Performance evaluation of accelerated complex multiple-precision LU decomposition" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, cs-PF, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16013v1.pdf filename=2403.16013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The direct method is one of the most important algorithms for solving linear systems of equations, with LU decomposition comprising a significant portion of its computation time. This study explores strategies to accelerate complex LU decomposition using multiple-precision floating-point arithmetic of the multiple-component type. Specifically, we explore the potential efficiency gains using a combination of SIMDization and the 3M method for complex matrix multiplication. Our <b>benchmark</b> tests compare this approach with the direct method implementation in MPLAPACK, focusing on computation time and numerical errors.</p></p class="citation"></blockquote><h2 id=mathct-1>math.CT (1)</h2><h3 id=11--145146-term-rewriting-on-nestohedra-pierre-louis-curien-et-al-2024>(1/1 | 145/146) Term rewriting on nestohedra (Pierre-Louis Curien et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre-Louis Curien, Guillaume Laplante-Anfossi. (2024)<br><strong>Term rewriting on nestohedra</strong><br><button class=copy-to-clipboard title="Term rewriting on nestohedra" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CT<br>Categories: 68Q42 (Primary) 18N20, 52B11 (Secondary), cs-LO, math-AT, math-CO, math-CT, math.CT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15987v1.pdf filename=2403.15987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We define term rewriting systems on the vertices and faces of nestohedra, and show that the former are confluent and terminating. While the associated poset on vertices generalizes Barnard&ndash;McConville&rsquo;s flip order for <b>graph-associahedra,</b> the preorder on faces likely generalizes the facial weak order for permutahedra. Moreover, we define and study contextual families of nestohedra, whose local confluence diagrams satisfy a certain uniformity condition. Among them are associahedra and operahedra, whose associated proofs of confluence for their rewriting systems reproduce proofs of categorical coherence theorems for monoidal categories and categorified operads.</p></p class="citation"></blockquote><h2 id=mathdg-1>math.DG (1)</h2><h3 id=11--146146-geometric-signals-tatyana-barron-2024>(1/1 | 146/146) Geometric signals (Tatyana Barron, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tatyana Barron. (2024)<br><strong>Geometric signals</strong><br><button class=copy-to-clipboard title="Geometric signals" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.DG<br>Categories: cs-IT, math-DG, math-IT, math.DG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15978v1.pdf filename=2403.15978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In signal processing, a signal is the <b>graph</b> of a function. We define a signal as a submanifold of a Riemannian manifold (with corners). We obtain inequalities that relate the energy of the signal and the energy of its Fourier transform. We quantify noise and filtering.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.25</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.27</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#csai-8>cs.AI (8)</a><ul><li><a href=#18--1146-can-language-models-pretend-solvers-logic-code-simulation-with-llms-minyu-chen-et-al-2024>(1/8 | 1/146) Can Language Models Pretend Solvers? Logic Code Simulation with LLMs (Minyu Chen et al., 2024)</a></li><li><a href=#28--2146-sshpool-the-separated-subgraph-based-hierarchical-pooling-zhuo-xu-et-al-2024>(2/8 | 2/146) SSHPool: The Separated Subgraph-based Hierarchical Pooling (Zhuo Xu et al., 2024)</a></li><li><a href=#38--3146-engineering-safety-requirements-for-autonomous-driving-with-large-language-models-ali-nouri-et-al-2024>(3/8 | 3/146) Engineering Safety Requirements for Autonomous Driving with Large Language Models (Ali Nouri et al., 2024)</a></li><li><a href=#48--4146-rumor-detection-with-a-novel-graph-neural-network-approach-tianrui-liu-et-al-2024>(4/8 | 4/146) Rumor Detection with a novel graph neural network approach (Tianrui Liu et al., 2024)</a></li><li><a href=#58--5146-a-temporal-graph-network-framework-for-dynamic-recommendation-yejin-kim-et-al-2024>(5/8 | 5/146) A Temporal Graph Network Framework for Dynamic Recommendation (Yejin Kim et al., 2024)</a></li><li><a href=#68--6146-cyber-security-knowledge-graph-generation-by-hierarchical-nonnegative-matrix-factorization-ryan-barron-et-al-2024>(6/8 | 6/146) Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix Factorization (Ryan Barron et al., 2024)</a></li><li><a href=#78--7146-evaluating-fairness-metrics-across-borders-from-human-perceptions-yuya-sasaki-et-al-2024>(7/8 | 7/146) Evaluating Fairness Metrics Across Borders from Human Perceptions (Yuya Sasaki et al., 2024)</a></li><li><a href=#88--8146-landmark-guided-cross-speaker-lip-reading-with-mutual-information-regularization-linzhi-wu-et-al-2024>(8/8 | 8/146) Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization (Linzhi Wu et al., 2024)</a></li></ul></li><li><a href=#csse-5>cs.SE (5)</a><ul><li><a href=#15--9146-combining-fine-tuning-and-llm-based-agents-for-intuitive-smart-contract-auditing-with-justifications-wei-ma-et-al-2024>(1/5 | 9/146) Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications (Wei Ma et al., 2024)</a></li><li><a href=#25--10146-llms-as-compiler-for-arabic-programming-language-serry-sibaee-et-al-2024>(2/5 | 10/146) LLMs as Compiler for Arabic Programming Language (Serry Sibaee et al., 2024)</a></li><li><a href=#35--11146-coupled-requirements-driven-testing-of-cps-from-simulation-to-reality-ankit-agrawal-et-al-2024>(3/5 | 11/146) Coupled Requirements-driven Testing of CPS: From Simulation To Reality (Ankit Agrawal et al., 2024)</a></li><li><a href=#45--12146-coverup-coverage-guided-llm-based-test-generation-juan-altmayer-pizzorno-et-al-2024>(4/5 | 12/146) CoverUp: Coverage-Guided LLM-Based Test Generation (Juan Altmayer Pizzorno et al., 2024)</a></li><li><a href=#55--13146-finewave-fine-grained-warning-verification-of-bugs-for-automated-static-analysis-tools-han-liu-et-al-2024>(5/5 | 13/146) FineWAVE: Fine-Grained Warning Verification of Bugs for Automated Static Analysis Tools (Han Liu et al., 2024)</a></li></ul></li><li><a href=#cslg-18>cs.LG (18)</a><ul><li><a href=#118--14146-node-classification-via-semantic-structural-attention-enhanced-graph-convolutional-networks-hongyin-zhu-2024>(1/18 | 14/146) Node Classification via Semantic-Structural Attention-Enhanced Graph Convolutional Networks (Hongyin Zhu, 2024)</a></li><li><a href=#218--15146-a-survey-on-self-supervised-pre-training-of-graph-foundation-models-a-knowledge-based-perspective-ziwen-zhao-et-al-2024>(2/18 | 15/146) A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective (Ziwen Zhao et al., 2024)</a></li><li><a href=#318--16146-vcr-graphormer-a-mini-batch-graph-transformer-via-virtual-connections-dongqi-fu-et-al-2024>(3/18 | 16/146) VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections (Dongqi Fu et al., 2024)</a></li><li><a href=#418--17146-a-federated-parameter-aggregation-method-for-node-classification-tasks-with-different-graph-network-structures-hao-song-et-al-2024>(4/18 | 17/146) A Federated Parameter Aggregation Method for Node Classification Tasks with Different Graph Network Structures (Hao Song et al., 2024)</a></li><li><a href=#518--18146-the-n-implementation-details-of-rlhf-with-ppo-a-case-study-on-tldr-summarization-shengyi-huang-et-al-2024>(5/18 | 18/146) The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization (Shengyi Huang et al., 2024)</a></li><li><a href=#618--19146-ibcb-efficient-inverse-batched-contextual-bandit-for-behavioral-evolution-history-yi-xu-et-al-2024>(6/18 | 19/146) IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral Evolution History (Yi Xu et al., 2024)</a></li><li><a href=#718--20146-akbr-learning-adaptive-kernel-based-representations-for-graph-classification-feifei-qian-et-al-2024>(7/18 | 20/146) AKBR: Learning Adaptive Kernel-based Representations for Graph Classification (Feifei Qian et al., 2024)</a></li><li><a href=#818--21146-one-masked-model-is-all-you-need-for-sensor-fault-detection-isolation-and-accommodation-yiwei-fu-et-al-2024>(8/18 | 21/146) One Masked Model is All You Need for Sensor Fault Detection, Isolation and Accommodation (Yiwei Fu et al., 2024)</a></li><li><a href=#918--22146-stochastic-parameter-reduced-order-model-based-on-hybrid-machine-learning-approaches-cheng-fang-et-al-2024>(9/18 | 22/146) Stochastic parameter reduced-order model based on hybrid machine learning approaches (Cheng Fang et al., 2024)</a></li><li><a href=#1018--23146-interpretable-modeling-of-deep-reinforcement-learning-driven-scheduling-boyang-li-et-al-2024>(10/18 | 23/146) Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling (Boyang Li et al., 2024)</a></li><li><a href=#1118--24146-from-discrete-to-continuous-deep-fair-clustering-with-transferable-representations-xiang-zhang-2024>(11/18 | 24/146) From Discrete to Continuous: Deep Fair Clustering With Transferable Representations (Xiang Zhang, 2024)</a></li><li><a href=#1218--25146-enhancing-demand-prediction-in-open-systems-by-cartogram-aided-deep-learning-sangjoon-park-et-al-2024>(12/18 | 25/146) Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep Learning (Sangjoon Park et al., 2024)</a></li><li><a href=#1318--26146-out-of-distribution-detection-via-deep-multi-comprehension-ensemble-chenhui-xu-et-al-2024>(13/18 | 26/146) Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble (Chenhui Xu et al., 2024)</a></li><li><a href=#1418--27146-partially-blinded-unlearning-class-unlearning-for-deep-networks-a-bayesian-perspective-subhodip-panda-et-al-2024>(14/18 | 27/146) Partially Blinded Unlearning: Class Unlearning for Deep Networks a Bayesian Perspective (Subhodip Panda et al., 2024)</a></li><li><a href=#1518--28146-systematic-construction-of-continuous-time-neural-networks-for-linear-dynamical-systems-chinmay-datar-et-al-2024>(15/18 | 28/146) Systematic construction of continuous-time neural networks for linear dynamical systems (Chinmay Datar et al., 2024)</a></li><li><a href=#1618--29146-subspace-defense-discarding-adversarial-perturbations-by-learning-a-subspace-for-clean-signals-rui-zheng-et-al-2024>(16/18 | 29/146) Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals (Rui Zheng et al., 2024)</a></li><li><a href=#1718--30146-a-transformer-approach-for-electricity-price-forecasting-oscar-llorente-gonzalez-et-al-2024>(17/18 | 30/146) A Transformer approach for Electricity Price Forecasting (Oscar Llorente Gonzalez et al., 2024)</a></li><li><a href=#1818--31146-a-unified-module-for-accelerating-stable-diffusion-lcm-lora-ayush-thakur-et-al-2024>(18/18 | 31/146) A Unified Module for Accelerating STABLE-DIFFUSION: LCM-LORA (Ayush Thakur et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-2>physics.flu-dyn (2)</a><ul><li><a href=#12--32146-predicting-energy-budgets-in-droplet-dynamics-a-recurrent-neural-network-approach-diego-a-de-aguiar-et-al-2024>(1/2 | 32/146) Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural Network Approach (Diego A. de Aguiar et al., 2024)</a></li><li><a href=#22--33146-explicit-form-of-simplified-grads-13-moments-distribution-function-based-moment-gas-kinetic-solver-with-unstructured-meshes-for-the-multiscale-rarefied-flow-w-liu-et-al-2024>(2/2 | 33/146) Explicit form of simplified Grad&rsquo;s 13 moments distribution function-based moment gas kinetic solver with unstructured meshes for the multiscale rarefied flow (W. Liu et al., 2024)</a></li></ul></li><li><a href=#cscv-45>cs.CV (45)</a><ul><li><a href=#145--34146-cross-domain-multi-modal-few-shot-object-detection-via-rich-text-zeyu-shangguan-et-al-2024>(1/45 | 34/146) Cross-domain Multi-modal Few-shot Object Detection via Rich Text (Zeyu Shangguan et al., 2024)</a></li><li><a href=#245--35146-robust-diffusion-models-for-adversarial-purification-guang-lin-et-al-2024>(2/45 | 35/146) Robust Diffusion Models for Adversarial Purification (Guang Lin et al., 2024)</a></li><li><a href=#345--36146-papr-training-free-one-step-patch-pruning-with-lightweight-convnets-for-faster-inference-tanvir-mahmud-et-al-2024>(3/45 | 36/146) PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for Faster Inference (Tanvir Mahmud et al., 2024)</a></li><li><a href=#445--37146-pose-guided-self-training-with-two-stage-clustering-for-unsupervised-landmark-discovery-siddharth-tourani-et-al-2024>(4/45 | 37/146) Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised Landmark Discovery (Siddharth Tourani et al., 2024)</a></li><li><a href=#545--38146-multi-scale-spatio-temporal-graph-convolutional-network-for-facial-expression-spotting-yicheng-deng-et-al-2024>(5/45 | 38/146) Multi-Scale Spatio-Temporal Graph Convolutional Network for Facial Expression Spotting (Yicheng Deng et al., 2024)</a></li><li><a href=#645--39146-sdstrack-self-distillation-symmetric-adapter-learning-for-multi-modal-visual-object-tracking-xiaojun-hou-et-al-2024>(6/45 | 39/146) SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal Visual Object Tracking (Xiaojun Hou et al., 2024)</a></li><li><a href=#745--40146-a-general-and-efficient-federated-split-learning-with-pre-trained-image-transformers-for-heterogeneous-data-yifan-shi-et-al-2024>(7/45 | 40/146) A General and Efficient Federated Split Learning with Pre-trained Image Transformers for Heterogeneous Data (Yifan Shi et al., 2024)</a></li><li><a href=#845--41146-bimcv-r-a-landmark-dataset-for-3d-ct-text-image-retrieval-yinda-chen-et-al-2024>(8/45 | 41/146) BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval (Yinda Chen et al., 2024)</a></li><li><a href=#945--42146-exploiting-semantic-reconstruction-to-mitigate-hallucinations-in-vision-language-models-minchan-kim-et-al-2024>(9/45 | 42/146) Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models (Minchan Kim et al., 2024)</a></li><li><a href=#1045--43146-enhancing-video-transformers-for-action-understanding-with-vlm-aided-training-hui-lu-et-al-2024>(10/45 | 43/146) Enhancing Video Transformers for Action Understanding with VLM-aided Training (Hui Lu et al., 2024)</a></li><li><a href=#1145--44146-enhancing-visual-continual-learning-with-language-guided-supervision-bolin-ni-et-al-2024>(11/45 | 44/146) Enhancing Visual Continual Learning with Language-Guided Supervision (Bolin Ni et al., 2024)</a></li><li><a href=#1245--45146-segment-anything-model-for-road-network-graph-extraction-congrui-hetang-et-al-2024>(12/45 | 45/146) Segment Anything Model for Road Network Graph Extraction (Congrui Hetang et al., 2024)</a></li><li><a href=#1345--46146-constricting-normal-latent-space-for-anomaly-detection-with-normal-only-training-data-marcella-astrid-et-al-2024>(13/45 | 46/146) Constricting Normal Latent Space for Anomaly Detection with Normal-only Training Data (Marcella Astrid et al., 2024)</a></li><li><a href=#1445--47146-adversarially-masked-video-consistency-for-unsupervised-domain-adaptation-xiaoyu-zhu-et-al-2024>(14/45 | 47/146) Adversarially Masked Video Consistency for Unsupervised Domain Adaptation (Xiaoyu Zhu et al., 2024)</a></li><li><a href=#1545--48146-knowledge-enhanced-dual-stream-zero-shot-composed-image-retrieval-yucheng-suo-et-al-2024>(15/45 | 48/146) Knowledge-Enhanced Dual-stream Zero-shot Composed Image Retrieval (Yucheng Suo et al., 2024)</a></li><li><a href=#1645--49146-l-mae-longitudinal-masked-auto-encoder-with-time-and-severity-aware-encoding-for-diabetic-retinopathy-progression-prediction-rachid-zeghlache-et-al-2024>(16/45 | 49/146) L-MAE: Longitudinal masked auto-encoder with time and severity-aware encoding for diabetic retinopathy progression prediction (Rachid Zeghlache et al., 2024)</a></li><li><a href=#1745--50146-towards-online-real-time-memory-based-video-inpainting-transformers-guillaume-thiry-et-al-2024>(17/45 | 50/146) Towards Online Real-Time Memory-based Video Inpainting Transformers (Guillaume Thiry et al., 2024)</a></li><li><a href=#1845--51146-eva-zero-shot-accurate-attributes-and-multi-object-video-editing-xiangpeng-yang-et-al-2024>(18/45 | 51/146) EVA: Zero-shot Accurate Attributes and Multi-Object Video Editing (Xiangpeng Yang et al., 2024)</a></li><li><a href=#1945--52146-are-nerfs-ready-for-autonomous-driving-towards-closing-the-real-to-simulation-gap-carl-lindström-et-al-2024>(19/45 | 52/146) Are NeRFs ready for autonomous driving? Towards closing the real-to-simulation gap (Carl Lindström et al., 2024)</a></li><li><a href=#2045--53146-autoinst-automatic-instance-based-segmentation-of-lidar-3d-scans-cedric-perauer-et-al-2024>(20/45 | 53/146) AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans (Cedric Perauer et al., 2024)</a></li><li><a href=#2145--54146-avicuna-audio-visual-llm-with-interleaver-and-context-boundary-alignment-for-temporal-referential-dialogue-yunlong-tang-et-al-2024>(21/45 | 54/146) AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue (Yunlong Tang et al., 2024)</a></li><li><a href=#2245--55146-unlearning-backdoor-threats-enhancing-backdoor-defense-in-multimodal-contrastive-learning-via-local-token-unlearning-siyuan-liang-et-al-2024>(22/45 | 55/146) Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning (Siyuan Liang et al., 2024)</a></li><li><a href=#2345--56146-opportunities-and-challenges-in-the-application-of-large-artificial-intelligence-models-in-radiology-liangrui-pan-et-al-2024>(23/45 | 56/146) Opportunities and challenges in the application of large artificial intelligence models in radiology (Liangrui Pan et al., 2024)</a></li><li><a href=#2445--57146-object-detectors-in-the-open-environment-challenges-solutions-and-outlook-siyuan-liang-et-al-2024>(24/45 | 57/146) Object Detectors in the Open Environment: Challenges, Solutions, and Outlook (Siyuan Liang et al., 2024)</a></li><li><a href=#2545--58146-emotion-recognition-from-the-perspective-of-activity-recognition-savinay-nagendra-et-al-2024>(25/45 | 58/146) Emotion Recognition from the perspective of Activity Recognition (Savinay Nagendra et al., 2024)</a></li><li><a href=#2645--59146-skull-to-face-anatomy-guided-3d-facial-reconstruction-and-editing-yongqing-liang-et-al-2024>(26/45 | 59/146) Skull-to-Face: Anatomy-Guided 3D Facial Reconstruction and Editing (Yongqing Liang et al., 2024)</a></li><li><a href=#2745--60146-fh-sstnet-forehead-creases-based-user-verification-using-spatio-spatial-temporal-network-geetanjali-sharma-et-al-2024>(27/45 | 60/146) FH-SSTNet: Forehead Creases based User Verification using Spatio-Spatial Temporal Network (Geetanjali Sharma et al., 2024)</a></li><li><a href=#2845--61146-salience-detr-enhancing-detection-transformer-with-hierarchical-salience-filtering-refinement-xiuquan-hou-et-al-2024>(28/45 | 61/146) Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement (Xiuquan Hou et al., 2024)</a></li><li><a href=#2945--62146-self-supervised-multi-frame-neural-scene-flow-dongrui-liu-et-al-2024>(29/45 | 62/146) Self-Supervised Multi-Frame Neural Scene Flow (Dongrui Liu et al., 2024)</a></li><li><a href=#3045--63146-mars-spectrometry-2-gas-chromatography----second-place-solution-dmitry-a-konovalov-2024>(30/45 | 63/146) Mars Spectrometry 2: Gas Chromatography &ndash; Second place solution (Dmitry A. Konovalov, 2024)</a></li><li><a href=#3145--64146-towards-two-stream-foveation-based-active-vision-learning-timur-ibrayev-et-al-2024>(31/45 | 64/146) Towards Two-Stream Foveation-based Active Vision Learning (Timur Ibrayev et al., 2024)</a></li><li><a href=#3245--65146-edit3k-universal-representation-learning-for-video-editing-components-xin-gu-et-al-2024>(32/45 | 65/146) Edit3K: Universal Representation Learning for Video Editing Components (Xin Gu et al., 2024)</a></li><li><a href=#3345--66146-blur2blur-blur-conversion-for-unsupervised-image-deblurring-on-unknown-domains-bang-dang-pham-et-al-2024>(33/45 | 66/146) Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown Domains (Bang-Dang Pham et al., 2024)</a></li><li><a href=#3445--67146-improving-scene-graph-generation-with-relation-words-debiasing-in-vision-language-models-yuxuan-wang-et-al-2024>(34/45 | 67/146) Improving Scene Graph Generation with Relation Words&rsquo; Debiasing in Vision-Language Models (Yuxuan Wang et al., 2024)</a></li><li><a href=#3545--68146-gaze-guided-hand-object-interaction-synthesis-benchmark-and-method-jie-tian-et-al-2024>(35/45 | 68/146) Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method (Jie Tian et al., 2024)</a></li><li><a href=#3645--69146-sm2c-boost-the-semi-supervised-segmentation-for-medical-image-by-using-meta-pseudo-labels-and-mixed-images-yifei-wang-et-al-2024>(36/45 | 69/146) SM2C: Boost the Semi-supervised Segmentation for Medical Image by using Meta Pseudo Labels and Mixed Images (Yifei Wang et al., 2024)</a></li><li><a href=#3745--70146-image-captioning-in-news-report-scenario-tianrui-liu-et-al-2024>(37/45 | 70/146) Image Captioning in news report scenario (Tianrui Liu et al., 2024)</a></li><li><a href=#3845--71146-diffusion-model-is-a-good-pose-estimator-from-3d-rf-vision-junqiao-fan-et-al-2024>(38/45 | 71/146) Diffusion Model is a Good Pose Estimator from 3D RF-Vision (Junqiao Fan et al., 2024)</a></li><li><a href=#3945--72146-semantic-is-enough-only-semantic-information-for-nerf-reconstruction-ruibo-wang-et-al-2024>(39/45 | 72/146) Semantic Is Enough: Only Semantic Information For NeRF Reconstruction (Ruibo Wang et al., 2024)</a></li><li><a href=#4045--73146-exploring-the-impact-of-dataset-bias-on-dataset-distillation-yao-lu-et-al-2024>(40/45 | 73/146) Exploring the Impact of Dataset Bias on Dataset Distillation (Yao Lu et al., 2024)</a></li><li><a href=#4145--74146-fill-in-the-____-a-diffusion-based-image-inpainting-pipeline-eyoel-gebre-et-al-2024>(41/45 | 74/146) Fill in the ____ (a Diffusion-based Image Inpainting Pipeline) (Eyoel Gebre et al., 2024)</a></li><li><a href=#4245--75146-egoexolearn-a-dataset-for-bridging-asynchronous-ego--and-exo-centric-view-of-procedural-activities-in-real-world-yifei-huang-et-al-2024>(42/45 | 75/146) EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World (Yifei Huang et al., 2024)</a></li><li><a href=#4345--76146-v2x-real-a-largs-scale-dataset-for-vehicle-to-everything-cooperative-perception-hao-xiang-et-al-2024>(43/45 | 76/146) V2X-Real: a Largs-Scale Dataset for Vehicle-to-Everything Cooperative Perception (Hao Xiang et al., 2024)</a></li><li><a href=#4445--77146-inverse-rendering-of-glossy-objects-via-the-neural-plenoptic-function-and-radiance-fields-haoyuan-wang-et-al-2024>(44/45 | 77/146) Inverse Rendering of Glossy Objects via the Neural Plenoptic Function and Radiance Fields (Haoyuan Wang et al., 2024)</a></li><li><a href=#4545--78146-pku-dymvhumans-a-multi-view-video-benchmark-for-high-fidelity-dynamic-human-modeling-xiaoyun-zheng-et-al-2024>(45/45 | 78/146) PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling (Xiaoyun Zheng et al., 2024)</a></li></ul></li><li><a href=#eessiv-5>eess.IV (5)</a><ul><li><a href=#15--79146-enhancing-mri-based-classification-of-alzheimers-disease-with-explainable-3d-hybrid-compact-convolutional-transformers-arindam-majee-et-al-2024>(1/5 | 79/146) Enhancing MRI-Based Classification of Alzheimer&rsquo;s Disease with Explainable 3D Hybrid Compact Convolutional Transformers (Arindam Majee et al., 2024)</a></li><li><a href=#25--80146-leveraging-deep-learning-and-xception-architecture-for-high-accuracy-mri-classification-in-alzheimer-diagnosis-shaojie-li-et-al-2024>(2/5 | 80/146) Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI Classification in Alzheimer Diagnosis (Shaojie Li et al., 2024)</a></li><li><a href=#35--81146-laplacian-guided-entropy-model-in-neural-codec-with-blur-dissipated-synthesis-atefeh-khoshkhahtinat-et-al-2024>(3/5 | 81/146) Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis (Atefeh Khoshkhahtinat et al., 2024)</a></li><li><a href=#45--82146-cfat-unleashing-triangularwindows-for-image-super-resolution-abhisek-ray-et-al-2024>(4/5 | 82/146) CFAT: Unleashing TriangularWindows for Image Super-resolution (Abhisek Ray et al., 2024)</a></li><li><a href=#55--83146-hemoset-the-first-blood-segmentation-dataset-for-automation-of-hemostasis-management-albert-j-miao-shan-lin-et-al-2024>(5/5 | 83/146) HemoSet: The First Blood Segmentation Dataset for Automation of Hemostasis Management (Albert J. Miao Shan Lin et al., 2024)</a></li></ul></li><li><a href=#eesssy-7>eess.SY (7)</a><ul><li><a href=#17--84146-ann-based-adaptive-nmpc-for-uranium-extraction-scrubbing-operation-in-spent-nuclear-fuel-treatment-process-duc-tri-vo-et-al-2024>(1/7 | 84/146) ANN-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in Spent Nuclear Fuel Treatment Process (Duc-Tri Vo et al., 2024)</a></li><li><a href=#27--85146-voltage-regulation-in-polymer-electrolyte-fuel-cell-systems-using-gaussian-process-model-predictive-control-xiufei-li-et-al-2024>(2/7 | 85/146) Voltage Regulation in Polymer Electrolyte Fuel Cell Systems Using Gaussian Process Model Predictive Control (Xiufei Li et al., 2024)</a></li><li><a href=#37--86146-data-driven-sliding-mode-control-for-partially-unknown-nonlinear-systems-jianglin-lan-et-al-2024>(3/7 | 86/146) Data-Driven Sliding Mode Control for Partially Unknown Nonlinear Systems (Jianglin Lan et al., 2024)</a></li><li><a href=#47--87146-bi-level-control-of-weaving-sections-in-mixed-traffic-environments-with-connected-and-automated-vehicles-longhao-yan-et-al-2024>(4/7 | 87/146) Bi-Level Control of Weaving Sections in Mixed Traffic Environments with Connected and Automated Vehicles (Longhao Yan et al., 2024)</a></li><li><a href=#57--88146-semi-automatic-line-system-provisioning-with-integrated-physical-parameter-aware-methodology-field-verification-and-operational-feasibility-hideki-nishizawa-et-al-2024>(5/7 | 88/146) Semi-Automatic Line-System Provisioning with Integrated Physical-Parameter-Aware Methodology: Field Verification and Operational Feasibility (Hideki Nishizawa et al., 2024)</a></li><li><a href=#67--89146-digital-control-of-negative-imaginary-systems-a-discrete-time-hybrid-integrator-gain-system-approach-kanghong-shi-et-al-2024>(6/7 | 89/146) Digital control of negative imaginary systems: a discrete-time hybrid integrator-gain system approach (Kanghong Shi et al., 2024)</a></li><li><a href=#77--90146-fisher-information-approach-for-masking-the-sensing-plan-applications-in-multifunction-radars-shashwat-jain-et-al-2024>(7/7 | 90/146) Fisher Information Approach for Masking the Sensing Plan: Applications in Multifunction Radars (Shashwat Jain et al., 2024)</a></li></ul></li><li><a href=#cscl-15>cs.CL (15)</a><ul><li><a href=#115--91146-sql-encoder-improving-nl2sql-in-context-learning-through-a-context-aware-encoder-mohammadreza-pourreza-et-al-2024>(1/15 | 91/146) SQL-Encoder: Improving NL2SQL In-Context Learning Through a Context-Aware Encoder (Mohammadreza Pourreza et al., 2024)</a></li><li><a href=#215--92146-a-little-leak-will-sink-a-great-ship-survey-of-transparency-for-large-language-models-from-start-to-finish-masahiro-kaneko-et-al-2024>(2/15 | 92/146) A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish (Masahiro Kaneko et al., 2024)</a></li><li><a href=#315--93146-qibo-a-large-language-model-for-traditional-chinese-medicine-heyi-zhang-et-al-2024>(3/15 | 93/146) Qibo: A Large Language Model for Traditional Chinese Medicine (Heyi Zhang et al., 2024)</a></li><li><a href=#415--94146-monotonic-paraphrasing-improves-generalization-of-language-model-prompting-qin-liu-et-al-2024>(4/15 | 94/146) Monotonic Paraphrasing Improves Generalization of Language Model Prompting (Qin Liu et al., 2024)</a></li><li><a href=#515--95146-cbt-llm-a-chinese-large-language-model-for-cognitive-behavioral-therapy-based-mental-health-question-answering-hongbin-na-2024>(5/15 | 95/146) CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering (Hongbin Na, 2024)</a></li><li><a href=#615--96146-connecting-the-dots-inferring-patent-phrase-similarity-with-retrieved-phrase-graphs-zhuoyi-peng-et-al-2024>(6/15 | 96/146) Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs (Zhuoyi Peng et al., 2024)</a></li><li><a href=#715--97146-large-language-models-offer-an-alternative-to-the-traditional-approach-of-topic-modelling-yida-mu-et-al-2024>(7/15 | 97/146) Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling (Yida Mu et al., 2024)</a></li><li><a href=#815--98146-argument-quality-assessment-in-the-age-of-instruction-following-large-language-models-henning-wachsmuth-et-al-2024>(8/15 | 98/146) Argument Quality Assessment in the Age of Instruction-Following Large Language Models (Henning Wachsmuth et al., 2024)</a></li><li><a href=#915--99146-a-survey-on-lexical-ambiguity-detection-and-word-sense-disambiguation-miuru-abeysiriwardana-et-al-2024>(9/15 | 99/146) A Survey on Lexical Ambiguity Detection and Word Sense Disambiguation (Miuru Abeysiriwardana et al., 2024)</a></li><li><a href=#1015--100146-lexdrafter-terminology-drafting-for-legislative-documents-using-retrieval-augmented-generation-ashish-chouhan-et-al-2024>(10/15 | 100/146) LexDrafter: Terminology Drafting for Legislative Documents using Retrieval Augmented Generation (Ashish Chouhan et al., 2024)</a></li><li><a href=#1115--101146-improving-sequence-to-sequence-models-for-abstractive-text-summarization-using-meta-heuristic-approaches-aditya-saxena-et-al-2024>(11/15 | 101/146) Improving Sequence-to-Sequence Models for Abstractive Text Summarization Using Meta Heuristic Approaches (Aditya Saxena et al., 2024)</a></li><li><a href=#1215--102146-alora-allocating-low-rank-adaptation-for-fine-tuning-large-language-models-zequan-liu-et-al-2024>(12/15 | 102/146) ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models (Zequan Liu et al., 2024)</a></li><li><a href=#1315--103146-korean-bio-medical-corpus-kbmc-for-medical-named-entity-recognition-sungjoo-byun-et-al-2024>(13/15 | 103/146) Korean Bio-Medical Corpus (KBMC) for Medical Named Entity Recognition (Sungjoo Byun et al., 2024)</a></li><li><a href=#1415--104146-wangchanlion-and-wangchanx-mrc-eval-wannaphong-phatthiyaphaibun-et-al-2024>(14/15 | 104/146) WangchanLion and WangchanX MRC Eval (Wannaphong Phatthiyaphaibun et al., 2024)</a></li><li><a href=#1515--105146-a-multi-label-dataset-of-french-fake-news-human-and-machine-insights-benjamin-icard-et-al-2024>(15/15 | 105/146) A Multi-Label Dataset of French Fake News: Human and Machine Insights (Benjamin Icard et al., 2024)</a></li></ul></li><li><a href=#cscr-3>cs.CR (3)</a><ul><li><a href=#13--106146-eg-conmix-an-intrusion-detection-method-based-on-graph-contrastive-learning-lijin-wu-et-al-2024>(1/3 | 106/146) EG-ConMix: An Intrusion Detection Method based on Graph Contrastive Learning (Lijin Wu et al., 2024)</a></li><li><a href=#23--107146-is-watermarking-llm-generated-code-robust-tarun-suresh-et-al-2024>(2/3 | 107/146) Is Watermarking LLM-Generated Code Robust? (Tarun Suresh et al., 2024)</a></li><li><a href=#33--108146-a-survey-on-consumer-iot-traffic-security-and-privacy-yan-jia-et-al-2024>(3/3 | 108/146) A Survey on Consumer IoT Traffic: Security and Privacy (Yan Jia et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--109146-modal-adaptive-knowledge-enhanced-graph-based-financial-prediction-from-monetary-policy-conference-calls-with-llm-kun-ouyang-et-al-2024>(1/1 | 109/146) Modal-adaptive Knowledge-enhanced Graph-based Financial Prediction from Monetary Policy Conference Calls with LLM (Kun Ouyang et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#12--110146-ultra-low-cost-two-stage-multimodal-system-for-non-normative-behavior-detection-albert-lu-et-al-2024>(1/2 | 110/146) Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior Detection (Albert Lu et al., 2024)</a></li><li><a href=#22--111146-social-deliberation-vs-social-contracts-in-self-governing-voluntary-organisations-matthew-scott-et-al-2024>(2/2 | 111/146) Social Deliberation vs. Social Contracts in Self-Governing Voluntary Organisations (Matthew Scott et al., 2024)</a></li></ul></li><li><a href=#csro-8>cs.RO (8)</a><ul><li><a href=#18--112146-mqe-unleashing-the-power-of-interaction-with-multi-agent-quadruped-environment-ziyan-xiong-et-al-2024>(1/8 | 112/146) MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment (Ziyan Xiong et al., 2024)</a></li><li><a href=#28--113146-guessing-human-intentions-to-avoid-dangerous-situations-in-caregiving-robots-noé-zapata-et-al-2024>(2/8 | 113/146) Guessing human intentions to avoid dangerous situations in caregiving robots (Noé Zapata et al., 2024)</a></li><li><a href=#38--114146-rpmart-towards-robust-perception-and-manipulation-for-articulated-objects-junbo-wang-et-al-2024>(3/8 | 114/146) RPMArt: Towards Robust Perception and Manipulation for Articulated Objects (Junbo Wang et al., 2024)</a></li><li><a href=#48--115146-robust-locomotion-by-logic-perturbation-resilient-bipedal-locomotion-via-signal-temporal-logic-guided-model-predictive-control-zhaoyuan-gu-et-al-2024>(4/8 | 115/146) Robust-Locomotion-by-Logic: Perturbation-Resilient Bipedal Locomotion via Signal Temporal Logic Guided Model Predictive Control (Zhaoyuan Gu et al., 2024)</a></li><li><a href=#58--116146-realtime-robust-shape-estimation-of-deformable-linear-object-jiaming-zhang-et-al-2024>(5/8 | 116/146) Realtime Robust Shape Estimation of Deformable Linear Object (Jiaming Zhang et al., 2024)</a></li><li><a href=#68--117146-ht-lip-model-based-robust-control-of-quadrupedal-robot-locomotion-under-unknown-vertical-ground-motion-amir-iqbal-et-al-2024>(6/8 | 117/146) HT-LIP Model based Robust Control of Quadrupedal Robot Locomotion under Unknown Vertical Ground Motion (Amir Iqbal et al., 2024)</a></li><li><a href=#78--118146-combined-task-and-motion-planning-via-sketch-decompositions-extended-version-with-supplementary-material-magí-dalmau-moreno-et-al-2024>(7/8 | 118/146) Combined Task and Motion Planning Via Sketch Decompositions (Extended Version with Supplementary Material) (Magí Dalmau-Moreno et al., 2024)</a></li><li><a href=#88--119146-kitchen-a-real-world-benchmark-and-dataset-for-6d-object-pose-estimation-in-kitchen-environments-abdelrahman-younes-et-al-2024>(8/8 | 119/146) KITchen: A Real-World Benchmark and Dataset for 6D Object Pose Estimation in Kitchen Environments (Abdelrahman Younes et al., 2024)</a></li></ul></li><li><a href=#csir-2>cs.IR (2)</a><ul><li><a href=#12--120146-knowledge-aware-dual-side-attribute-enhanced-recommendation-taotian-pang-et-al-2024>(1/2 | 120/146) Knowledge-aware Dual-side Attribute-enhanced Recommendation (Taotian Pang et al., 2024)</a></li><li><a href=#22--121146-complementary-recommendation-in-e-commerce-definition-approaches-and-future-directions-linyue-li-et-al-2024>(2/2 | 121/146) Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions (Linyue Li et al., 2024)</a></li></ul></li><li><a href=#csdl-1>cs.DL (1)</a><ul><li><a href=#11--122146-large-language-models-in-biomedical-and-health-informatics-a-bibliometric-review-huizi-yu-et-al-2024>(1/1 | 122/146) Large Language Models in Biomedical and Health Informatics: A Bibliometric Review (Huizi Yu et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--123146-mrsch-multi-resource-scheduling-for-hpc-boyang-li-et-al-2024>(1/1 | 123/146) MRSch: Multi-Resource Scheduling for HPC (Boyang Li et al., 2024)</a></li></ul></li><li><a href=#csni-4>cs.NI (4)</a><ul><li><a href=#14--124146-study-of-workload-interference-with-intelligent-routing-on-dragonfly-yao-kang-et-al-2024>(1/4 | 124/146) Study of Workload Interference with Intelligent Routing on Dragonfly (Yao Kang et al., 2024)</a></li><li><a href=#24--125146-digital-twin-assisted-intelligent-network-management-for-vehicular-applications-kaige-qu-et-al-2024>(2/4 | 125/146) Digital Twin Assisted Intelligent Network Management for Vehicular Applications (Kaige Qu et al., 2024)</a></li><li><a href=#34--126146-q-adaptive-a-multi-agent-reinforcement-learning-based-routing-on-dragonfly-network-yao-kang-et-al-2024>(3/4 | 126/146) Q-adaptive: A Multi-Agent Reinforcement Learning Based Routing on Dragonfly Network (Yao Kang et al., 2024)</a></li><li><a href=#44--127146-interference-management-for-integrated-sensing-and-communication-systems-a-survey-yangyang-niu-et-al-2024>(4/4 | 127/146) Interference Management for Integrated Sensing and Communication Systems: A Survey (Yangyang Niu et al., 2024)</a></li></ul></li><li><a href=#physicsao-ph-1>physics.ao-ph (1)</a><ul><li><a href=#11--128146-interpretable-machine-learning-for-weather-and-climate-prediction-a-survey-ruyi-yang-et-al-2024>(1/1 | 128/146) Interpretable Machine Learning for Weather and Climate Prediction: A Survey (Ruyi Yang et al., 2024)</a></li></ul></li><li><a href=#cshc-1>cs.HC (1)</a><ul><li><a href=#11--129146-designing-child-centric-ai-learning-environments-insights-from-llm-enhanced-creative-project-based-learning-siyu-zha-et-al-2024>(1/1 | 129/146) Designing Child-Centric AI Learning Environments: Insights from LLM-Enhanced Creative Project-Based Learning (Siyu Zha et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--130146-the-interplay-of-learning-analytics-and-artificial-intelligence-in-education-mutlu-cukurova-2024>(1/1 | 130/146) The Interplay of Learning, Analytics, and Artificial Intelligence in Education (Mutlu Cukurova, 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--131146-target-speech-extraction-with-pre-trained-av-hubert-and-mask-and-recover-strategy-wenxuan-wu-et-al-2024>(1/2 | 131/146) Target Speech Extraction with Pre-trained AV-HuBERT and Mask-And-Recover Strategy (Wenxuan Wu et al., 2024)</a></li><li><a href=#22--132146-theoretical-analysis-of-quality-of-conventional-beamforming-for-phased-microphone-arrays-dheepak-khatri-et-al-2024>(2/2 | 132/146) Theoretical Analysis of Quality of Conventional Beamforming for Phased Microphone Arrays (Dheepak Khatri et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--133146-near-optimal-differentially-private-low-rank-trace-regression-with-guaranteed-private-initialization-mengyue-zha-2024>(1/3 | 133/146) Near-Optimal differentially private low-rank trace regression with guaranteed private initialization (Mengyue Zha, 2024)</a></li><li><a href=#23--134146-manifold-regularization-classification-model-based-on-improved-diffusion-map-hongfu-guo-et-al-2024>(2/3 | 134/146) Manifold Regularization Classification Model Based On Improved Diffusion Map (Hongfu Guo et al., 2024)</a></li><li><a href=#33--135146-learning-directed-acyclic-graphs-from-partial-orderings-ali-shojaie-et-al-2024>(3/3 | 135/146) Learning Directed Acyclic Graphs from Partial Orderings (Ali Shojaie et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--136146-convolution-and-knapsack-in-higher-dimensions-kilian-grage-et-al-2024>(1/2 | 136/146) Convolution and Knapsack in Higher Dimensions (Kilian Grage et al., 2024)</a></li><li><a href=#22--137146-hashing-geographical-point-data-using-the-space-filling-h-curve-igor-v-netay-2024>(2/2 | 137/146) Hashing geographical point data using the space-filling H-curve (Igor V. Netay, 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--138146-mimo-with-analogue-1-bit-phase-shifters-a-quantum-annealing-perspective-ioannis-krikidis-2024>(1/2 | 138/146) MIMO with Analogue 1-bit Phase Shifters: A Quantum Annealing Perspective (Ioannis Krikidis, 2024)</a></li><li><a href=#22--139146-on-the-secrecy-enhancement-of-an-integrated-ground-aerial-network-with-a-hybrid-fsothz-feeder-link-elmehdi-illi-et-al-2024>(2/2 | 139/146) On the Secrecy Enhancement of an Integrated Ground-Aerial Network with a Hybrid FSO/THz Feeder Link (Elmehdi Illi et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--140146-cbgt-net-a-neuromimetic-architecture-for-robust-classification-of-streaming-data-shreya-sharma-et-al-2024>(1/1 | 140/146) CBGT-Net: A Neuromimetic Architecture for Robust Classification of Streaming Data (Shreya Sharma et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--141146-maximum-polygon-packing-the-cgshop-challenge-2024-sándor-p-fekete-et-al-2024>(1/1 | 141/146) Maximum Polygon Packing: The CG:SHOP Challenge 2024 (Sándor P. Fekete et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--142146-on-reporting-durable-patterns-in-temporal-proximity-graphs-pankaj-k-agarwal-et-al-2024>(1/1 | 142/146) On Reporting Durable Patterns in Temporal Proximity Graphs (Pankaj K. Agarwal et al., 2024)</a></li></ul></li><li><a href=#q-biope-1>q-bio.PE (1)</a><ul><li><a href=#11--143146-an-information-theoretic-treatment-of-animal-movement-tracks-wayne-m-getz-2024>(1/1 | 143/146) An Information Theoretic Treatment of Animal Movement Tracks (Wayne M Getz, 2024)</a></li></ul></li><li><a href=#mathna-1>math.NA (1)</a><ul><li><a href=#11--144146-performance-evaluation-of-accelerated-complex-multiple-precision-lu-decomposition-tomonori-kouya-2024>(1/1 | 144/146) Performance evaluation of accelerated complex multiple-precision LU decomposition (Tomonori Kouya, 2024)</a></li></ul></li><li><a href=#mathct-1>math.CT (1)</a><ul><li><a href=#11--145146-term-rewriting-on-nestohedra-pierre-louis-curien-et-al-2024>(1/1 | 145/146) Term rewriting on nestohedra (Pierre-Louis Curien et al., 2024)</a></li></ul></li><li><a href=#mathdg-1>math.DG (1)</a><ul><li><a href=#11--146146-geometric-signals-tatyana-barron-2024>(1/1 | 146/146) Geometric signals (Tatyana Barron, 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>