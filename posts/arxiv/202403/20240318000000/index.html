<!doctype html><html><head><title>arXiv @ 2024.03.18</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.18"><meta property="og:description" content="Primary Categories cs.AI (3) cs.AR (1) cs.CE (1) cs.CL (22) cs.CR (4) cs.CV (33) cs.DC (1) cs.DM (1) cs.DS (1) cs.GT (1) cs.HC (2) cs.IR (1) cs.IT (4) cs.LG (22) cs.MM (2) cs.NI (1) cs.RO (22) cs.SD (2) cs.SE (2) eess.AS (3) eess.IV (4) eess.SY (3) math.CO (1) math.NA (2) math.OC (1) physics.flu-dyn (1) physics.optics (1) physics.plasm-ph (1) physics.soc-ph (1) q-bio.GN (1) q-bio.MN (1) quant-ph (3) stat.ML (4) Keywords keyword cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240318000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-18T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-18T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.18"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240318000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Monday, Mar 18, 2024</p></div><div class=title><h1>arXiv @ 2024.03.18</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#csai-3>cs.AI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#cscl-22>cs.CL (22)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#cscr-4>cs.CR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#cscv-33>cs.CV (33)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#cshc-2>cs.HC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#csir-1>cs.IR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#csit-4>cs.IT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#cslg-22>cs.LG (22)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#csmm-2>cs.MM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#csro-22>cs.RO (22)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#csse-2>cs.SE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#eessas-3>eess.AS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#eessiv-4>eess.IV (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#eesssy-3>eess.SY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#physicsoptics-1>physics.optics (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#physicsplasm-ph-1>physics.plasm-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#q-biogn-1>q-bio.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#q-biomn-1>q-bio.MN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#quant-ph-3>quant-ph (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/#statml-4>stat.ML (4)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Autoencoder</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td></td><td>1</td><td></td></tr><tr><td>BLOOM</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Benchmarking</td><td>5</td><td>7</td><td>3</td><td>6</td></tr><tr><td>Black Box</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Chain-of-thought Prompt</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Continual Learning</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>ControlNet</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>4</td><td>2</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>6</td><td>2</td><td>1</td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>4</td><td></td><td>2</td></tr><tr><td>Distribution Shift</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Few-shot</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>2</td><td>5</td><td>3</td><td></td></tr><tr><td>Foundation Model</td><td></td><td>4</td><td></td><td></td></tr><tr><td>GPT</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>GPT-4</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Generative Adversarial Network</td><td>1</td><td>2</td><td>1</td><td></td></tr><tr><td>Geometry</td><td></td><td>1</td><td></td><td>4</td></tr><tr><td>Graph</td><td>3</td><td>1</td><td>5</td><td>1</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Convolutional Network</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td>2</td><td></td><td>4</td><td></td></tr><tr><td>High-Resource</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td>2</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>2</td><td>1</td><td>3</td><td></td></tr><tr><td>Information Retrieval</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Base Question Answering</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Based Question Answering</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>6</td><td>5</td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td>1</td><td></td><td></td></tr><tr><td>LLaMA</td><td>2</td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Label Smoothing</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>19</td><td>1</td><td>1</td><td>2</td></tr><tr><td>Low-Resource</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Masked Language Model</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>2</td><td>5</td><td></td><td>1</td></tr><tr><td>Multiple Instance Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Neural Machine Translation</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Open Information Extraction</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>PaLM</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Pre-trained Language Model</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Prompt</td><td>5</td><td>3</td><td></td><td>2</td></tr><tr><td>Pruning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Question Answering</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>2</td><td>2</td><td>1</td><td>1</td></tr><tr><td>Recommendation</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td></td><td>2</td><td>6</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td>2</td><td>1</td><td>2</td><td></td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td></td><td>11</td></tr><tr><td>Simulator</td><td></td><td></td><td></td><td>11</td></tr><tr><td>Stemming</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Style Transfer</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Summarization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>3</td><td>1</td><td>4</td><td></td></tr><tr><td>Tensor Decomposition</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Text Generation</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Text2image</td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Tokenization</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Transformer</td><td>2</td><td>4</td><td>3</td><td>2</td></tr><tr><td>Unsupervised Learning</td><td></td><td>2</td><td>4</td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>4</td><td></td><td>1</td></tr><tr><td>Zero-shot</td><td>1</td><td>4</td><td>1</td><td>1</td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-22>cs.CL (22)</h2><h3 id=122--1153-pointer-generator-networks-for-low-resource-machine-translation-dont-copy-that-niyati-bafna-et-al-2024>(1/22 | 1/153) Pointer-Generator Networks for Low-Resource Machine Translation: Don&rsquo;t Copy That! (Niyati Bafna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niyati Bafna, David Yarowsky. (2024)<br><strong>Pointer-Generator Networks for Low-Resource Machine Translation: Don&rsquo;t Copy That!</strong><br><button class=copy-to-clipboard title="Pointer-Generator Networks for Low-Resource Machine Translation: Don't Copy That!" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 85<br>Keywords: Black Box, High-Resource, Low-Resource, Transformer, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10963v1.pdf filename=2403.10963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Transformer-based</b> <b>neural</b> <b>machine</b> <b>translation</b> <b>(NMT)</b> is very effective in <b>high-resource</b> settings, many languages lack the necessary large parallel corpora to benefit from it. In the context of <b>low-resource</b> (LR) <b>MT</b> between two closely-related languages, a natural intuition is to seek benefits from structural &ldquo;shortcuts&rdquo;, such as copying subwords from the source to the target, given that such language pairs often share a considerable number of identical words, cognates, and borrowings. We test Pointer-Generator Networks for this purpose for six language pairs over a variety of resource ranges, and find weak improvements for most settings. However, analysis shows that the model does not show greater improvements for closely-related vs. more distant language pairs, or for lower resource ranges, and that the models do not exhibit the expected usage of the mechanism for shared subwords. Our discussion of the reasons for this behaviour highlights several general challenges for LR <b>NMT,</b> such as modern <b>tokenization</b> strategies, noisy real-world conditions, and linguistic complexities. We call for better scrutiny of linguistically motivated improvements to <b>NMT</b> given the blackbox nature of <b>Transformer</b> models, as well as for a focus on the above problems in the field.</p></p class="citation"></blockquote><h3 id=222--2153-benqa-a-question-answering-and-reasoning-benchmark-for-bengali-and-english-sheikh-shafayat-et-al-2024>(2/22 | 2/153) BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English (Sheikh Shafayat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sheikh Shafayat, H M Quamran Hasan, Minhajur Rahman Chowdhury Mahim, Rifki Afina Putri, James Thorne, Alice Oh. (2024)<br><strong>BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English</strong><br><button class=copy-to-clipboard title="BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Low-Resource, Question Answering, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10900v1.pdf filename=2403.10900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce BEnQA, a dataset comprising parallel Bengali and English exam <b>questions</b> <b>for</b> middle and high school levels in Bangladesh. Our dataset consists of approximately 5K <b>questions</b> <b>covering</b> several subjects in science with different types of <b>questions,</b> <b>including</b> factual, application, and <b>reasoning-based</b> <b>questions.</b> <b>We</b> <b>benchmark</b> several <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with our parallel dataset and observe a notable performance disparity between the models in Bengali and English. We also investigate some <b>prompting</b> methods, and find that <b>Chain-of-Thought</b> <b>prompting</b> is beneficial mostly on <b>reasoning</b> <b>questions,</b> <b>but</b> not so much on factual ones. We also find that appending English translation helps to answer <b>questions</b> <b>in</b> Bengali. Our findings point to promising future research directions for improving the performance of <b>LLMs</b> in Bengali and more generally in <b>low-resource</b> languages.</p></p class="citation"></blockquote><h3 id=322--3153-selfie-self-interpretation-of-large-language-model-embeddings-haozhe-chen-et-al-2024>(3/22 | 3/153) SelfIE: Self-Interpretation of Large Language Model Embeddings (Haozhe Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haozhe Chen, Carl Vondrick, Chengzhi Mao. (2024)<br><strong>SelfIE: Self-Interpretation of Large Language Model Embeddings</strong><br><button class=copy-to-clipboard title="SelfIE: Self-Interpretation of Large Language Model Embeddings" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Reinforcement Learning from Human Feedback, Supervised Learning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10949v1.pdf filename=2403.10949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How do <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> obtain their answers? The ability to explain and control an <b>LLM&rsquo;s</b> <b>reasoning</b> process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables <b>LLMs</b> to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals <b>LLM</b> internal <b>reasoning</b> in cases such as making ethical decisions, internalizing <b>prompt</b> injection, and recalling harmful knowledge. SelfIE&rsquo;s text descriptions on hidden embeddings also open up new avenues to control <b>LLM</b> <b>reasoning.</b> We propose <b>Supervised</b> Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend <b>RLHF</b> to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in <b>LLM</b> without supervision targets.</p></p class="citation"></blockquote><h3 id=422--4153-detecting-bias-in-large-language-models-fine-tuned-kcbert-j-k-lee-et-al-2024>(4/22 | 4/153) Detecting Bias in Large Language Models: Fine-tuned KcBERT (J. K. Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>J. K. Lee, T. M. Chung. (2024)<br><strong>Detecting Bias in Large Language Models: Fine-tuned KcBERT</strong><br><button class=copy-to-clipboard title="Detecting Bias in Large Language Models: Fine-tuned KcBERT" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Transformer, Large Language Model, Large Language Model, Masked Language Model, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10774v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10774v1.pdf filename=2403.10774v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has enabled natural language processing capabilities similar to those of humans, and <b>LLMs</b> are being widely utilized across various societal domains such as education and healthcare. While the versatility of these models has increased, they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language. In this paper, we define such harm as societal bias and assess ethnic, gender, and racial biases in a model <b>fine-tuned</b> with Korean comments using Bidirectional Encoder Representations from <b>Transformers</b> (KcBERT) and KOLD data through template-based <b>Masked</b> <b>Language</b> <b>Modeling</b> <b>(MLM).</b> To quantitatively evaluate biases, we employ LPBS and CBS metrics. Compared to KcBERT, the <b>fine-tuned</b> model shows a reduction in ethnic bias but demonstrates significant changes in gender and racial biases. Based on these results, we propose two methods to mitigate societal bias. Firstly, a data balancing approach during the pre-training phase adjusts the uniformity of data by aligning the distribution of the occurrences of specific words and converting surrounding harmful words into non-harmful words. Secondly, during the in-training phase, we apply Debiasing Regularization by adjusting dropout and regularization, confirming a decrease in training loss. Our contribution lies in demonstrating that societal bias exists in Korean language models due to language-dependent characteristics.</p></p class="citation"></blockquote><h3 id=522--5153-do-large-language-models-understand-medical-codes-simon-a-lee-et-al-2024>(5/22 | 5/153) Do Large Language Models understand Medical Codes? (Simon A. Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon A. Lee, Timothy Lindsey. (2024)<br><strong>Do Large Language Models understand Medical Codes?</strong><br><button class=copy-to-clipboard title="Do Large Language Models understand Medical Codes?" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10822v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10822v1.pdf filename=2403.10822v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The overarching goal of recent AI research has been to make steady progress towards achieving Artificial General Intelligence (AGI), <b>prompting</b> the evaluation of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> across a variety of tasks and domains. One such domain is healthcare, where <b>LLMs</b> can greatly benefit clinical practice by assisting with a wide range of tasks. However, these models are also prone to producing &ldquo;hallucinations&rdquo; or incorrect responses when faced with queries they cannot adequately address, raising concerns and skepticism, especially within the healthcare community. Therefore, in this work, we investigate whether <b>LLMs</b> understand the inherent meaning of medical codes, which are widely used in healthcare practice. We evaluate various off-the-shelf <b>LLMs</b> (e.g., <b>GPT,</b> <b>LLaMA,</b> etc.) and <b>LLMs</b> specifically designed for biomedical applications to assess their awareness and understanding of these domain-specific terminologies. Our results indicate that these models do not comprehend the meaning of the medical codes, highlighting the need for better representation of these alphanumeric codes extensively used in healthcare. We call for improved strategies to effectively capture and represent the nuances of medical codes and terminologies within <b>LLMs,</b> enabling them to become more reliable and trustworthy tools for healthcare professionals.</p></p class="citation"></blockquote><h3 id=622--6153-efficient-pruning-of-large-language-model-with-adaptive-estimation-fusion-jun-liu-et-al-2024>(6/22 | 6/153) Efficient Pruning of Large Language Model with Adaptive Estimation Fusion (Jun Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Liu, Chao Wu, Changdi Yang, Hao Tang, Haoye Dong, Zhenglun Kong, Geng Yuan, Wei Niu, Dong Huang, Yanzhi Wang. (2024)<br><strong>Efficient Pruning of Large Language Model with Adaptive Estimation Fusion</strong><br><button class=copy-to-clipboard title="Efficient Pruning of Large Language Model with Adaptive Estimation Fusion" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Pruning, BLOOM, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10799v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10799v1.pdf filename=2403.10799v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured <b>pruning</b> is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for <b>pruning.</b> These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end <b>pruning</b> framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate average accuracy improvements of 1.1%, 1.02%, 2.0%, and 1.2% for <b>LLaMa-7B,Vicuna-7B,</b> Baichuan-7B, and <b>Bloom-7b1,</b> respectively.</p></p class="citation"></blockquote><h3 id=722--7153-from-words-to-routes-applying-large-language-models-to-vehicle-routing-zhehui-huang-et-al-2024>(7/22 | 7/153) From Words to Routes: Applying Large Language Models to Vehicle Routing (Zhehui Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhehui Huang, Guangyao Shi, Gaurav S. Sukhatme. (2024)<br><strong>From Words to Routes: Applying Large Language Models to Vehicle Routing</strong><br><button class=copy-to-clipboard title="From Words to Routes: Applying Large Language Models to Vehicle Routing" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs-RO, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10795v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10795v1.pdf filename=2403.10795v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLMs</b> have shown impressive progress in robotics (e.g., manipulation and navigation) with natural language task descriptions. The success of <b>LLMs</b> in these tasks leads us to wonder: What is the ability of <b>LLMs</b> to solve vehicle routing problems (VRPs) with natural language task descriptions? In this work, we study this question in three steps. First, we construct a dataset with 21 types of single- or multi-vehicle routing problems. Second, we evaluate the performance of <b>LLMs</b> across four basic <b>prompt</b> paradigms of text-to-code generation, each involving different types of text input. We find that the basic <b>prompt</b> paradigm, which generates code directly from natural language task descriptions, performs the best for <b>GPT-4,</b> achieving 56% feasibility, 40% optimality, and 53% efficiency. Third, based on the observation that <b>LLMs</b> may not be able to provide correct solutions at the initial attempt, we propose a framework that enables <b>LLMs</b> to refine solutions through self-reflection, including self-debugging and self-verification. With <b>GPT-4,</b> our proposed framework achieves a 16% increase in feasibility, a 7% increase in optimality, and a 15% increase in efficiency. Moreover, we examine the sensitivity of <b>GPT-4</b> to task descriptions, specifically focusing on how its performance changes when certain details are omitted from the task descriptions, yet the core meaning is preserved. Our findings reveal that such omissions lead to a notable decrease in performance: 4% in feasibility, 4% in optimality, and 5% in efficiency. Website: <a href=https://sites.google.com/view/words-to-routes/>https://sites.google.com/view/words-to-routes/</a></p></p class="citation"></blockquote><h3 id=822--8153-ecrc-emotion-causality-recognition-in-korean-conversation-for-gcn-j-k-lee-et-al-2024>(8/22 | 8/153) ECRC: Emotion-Causality Recognition in Korean Conversation for GCN (J. K. Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>J. K. Lee, T. M. Chung. (2024)<br><strong>ECRC: Emotion-Causality Recognition in Korean Conversation for GCN</strong><br><button class=copy-to-clipboard title="ECRC: Emotion-Causality Recognition in Korean Conversation for GCN" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Graph Convolutional Network, Graph, Graph Neural Network, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10764v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10764v1.pdf filename=2403.10764v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this multi-task learning study on simultaneous analysis of emotions and their underlying causes in conversational contexts, deep neural network methods were employed to effectively process and train large labeled datasets. However, these approaches are typically limited to conducting context analyses across the entire corpus because they rely on one of the two methods: word- or sentence-level embedding. The former struggles with polysemy and homonyms, whereas the latter causes information loss when processing <b>long</b> <b>sentences.</b> <b>In</b> <b>this</b> study, we overcome the limitations of previous embeddings by utilizing both word- and sentence-level embeddings. Furthermore, we propose the emotion-causality recognition in conversation (ECRC) model, which is based on a novel <b>graph</b> <b>structure,</b> <b>thereby</b> leveraging the strengths of both embedding methods. This model uniquely integrates the bidirectional <b>long</b> <b>short-term</b> <b>memory</b> <b>(Bi-LSTM)</b> and <b>graph</b> <b>neural</b> <b>network</b> <b>(GCN)</b> models for Korean conversation analysis. Compared with models that rely solely on one embedding method, the proposed model effectively structures abstract concepts, such as language features and relationships, thereby minimizing information loss. To assess model performance, we compared the multi-task learning results of three deep neural network models with varying <b>graph</b> <b>structures.</b> <b>Additionally,</b> we evaluated the proposed model using Korean and English datasets. The experimental results show that the proposed model performs better in emotion and causality multi-task learning (74.62% and 75.30%, respectively) when node and edge characteristics are incorporated into the <b>graph</b> <b>structure.</b> <b>Similar</b> results were recorded for the Korean ECC and Wellness datasets (74.62% and 73.44%, respectively) with 71.35% on the IEMOCAP English dataset.</p></p class="citation"></blockquote><h3 id=922--9153-optimizing-language-augmentation-for-multilingual-large-language-models-a-case-study-on-korean-changsu-choi-et-al-2024>(9/22 | 9/153) Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean (ChangSu Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>ChangSu Choi, Yongbin Jeong, Seoyoon Park, InHo Won, HyeonSeok Lim, SangMin Kim, Yejee Kang, Chanhyuk Yoon, Jaewan Park, Yiseul Lee, HyeJin Lee, Younggyun Hahm, Hansaem Kim, KyungTae Lim. (2024)<br><strong>Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean</strong><br><button class=copy-to-clipboard title="Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: GPT-4, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10882v1.pdf filename=2403.10882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> use pretraining to predict the subsequent word; however, their expansion requires significant computing resources. Numerous big tech companies and research institutes have developed multilingual <b>LLMs</b> (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs). This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness. Second, bilingual data were used for pretraining to align the high- and less-resourced languages. Third, a high-quality small-scale <b>instruction</b> <b>dataset</b> was constructed and <b>instruction-tuning</b> <b>was</b> performed to augment the LRL. The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed <b>LLMs</b> across eight tasks. Furthermore, a qualitative assessment was performed based on human evaluation and <b>GPT4.</b> Experimental results showed that our proposed Bllossom model exhibited superior performance in qualitative analyses compared to previously proposed Korean monolingual models.</p></p class="citation"></blockquote><h3 id=1022--10153-depression-detection-on-social-media-with-large-language-models-xiaochong-lan-et-al-2024>(10/22 | 10/153) Depression Detection on Social Media with Large Language Models (Xiaochong Lan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaochong Lan, Yiming Cheng, Li Sheng, Chen Gao, Yong Li. (2024)<br><strong>Depression Detection on Social Media with Large Language Models</strong><br><button class=copy-to-clipboard title="Depression Detection on Social Media with Large Language Models" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10750v1.pdf filename=2403.10750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Depression harms. However, due to a lack of mental health awareness and fear of stigma, many patients do not actively seek diagnosis and treatment, leading to detrimental outcomes. Depression detection aims to determine whether an individual suffers from depression by analyzing their history of posts on social media, which can significantly aid in early detection and intervention. It mainly faces two key challenges: 1) it requires professional medical knowledge, and 2) it necessitates both high accuracy and explainability. To address it, we propose a novel depression detection system called DORIS, combining medical knowledge and the recent advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Specifically, to tackle the first challenge, we proposed an <b>LLM-based</b> solution to first annotate whether high-risk texts meet medical diagnostic criteria. Further, we retrieve texts with high emotional intensity and <b>summarize</b> critical information from the historical mood records of users, so-called mood courses. To tackle the second challenge, we combine <b>LLM</b> and traditional classifiers to integrate medical knowledge-guided features, for which the model can also explain its prediction results, achieving both high accuracy and explainability. Extensive experimental results on <b>benchmarking</b> datasets show that, compared to the current best baseline, our approach improves by 0.036 in AUPRC, which can be considered significant, demonstrating the effectiveness of our approach and its high value as an NLP application.</p></p class="citation"></blockquote><h3 id=1122--11153-two-step-automated-cybercrime-coded-word-detection-using-multi-level-representation-learning-yongyeon-kim-et-al-2024>(11/22 | 11/153) Two-step Automated Cybercrime Coded Word Detection using Multi-level Representation Learning (Yongyeon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongyeon Kim, Byung-Won On, Ingyu Lee. (2024)<br><strong>Two-step Automated Cybercrime Coded Word Detection using Multi-level Representation Learning</strong><br><button class=copy-to-clipboard title="Two-step Automated Cybercrime Coded Word Detection using Multi-level Representation Learning" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 35<br>Keywords: Autoencoder, Representation Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10838v1.pdf filename=2403.10838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In social network service platforms, crime suspects are likely to use cybercrime coded words for communication by adding criminal meanings to existing words or replacing them with similar words. For instance, the word &lsquo;ice&rsquo; is often used to mean methamphetamine in drug crimes. To analyze the nature of cybercrime and the behavior of criminals, quickly detecting such words and further understanding their meaning are critical. In the automated cybercrime coded word detection problem, it is difficult to collect a sufficient amount of training data for <b>supervised</b> <b>learning</b> and to directly apply language models that utilize context information to better understand natural language. To overcome these limitations, we propose a new two-step approach, in which a mean latent vector is constructed for each cybercrime through one of five different <b>AutoEncoder</b> models in the first step, and cybercrime coded words are detected based on multi-level latent <b>representations</b> <b>in</b> the second step. Moreover, to deeply understand cybercrime coded words detected through the two-step approach, we propose three novel methods: (1) Detection of new words recently coined, (2) Detection of words frequently appeared in both drug and sex crimes, and (3) Automatic generation of word taxonomy. According to our experimental results, among various <b>AutoEncoder</b> models, the stacked <b>AutoEncoder</b> model shows the best performance. Additionally, the F1-score of the two-step approach is 0.991, which is higher than 0.987 and 0.903 of the existing dark-GloVe and dark-BERT models. By analyzing the experimental results of the three proposed methods, we can gain a deeper understanding of drug and sex crimes.</p></p class="citation"></blockquote><h3 id=1222--12153-pre-trained-language-models-represent-some-geographic-populations-better-than-others-jonathan-dunn-et-al-2024>(12/22 | 12/153) Pre-Trained Language Models Represent Some Geographic Populations Better Than Others (Jonathan Dunn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Dunn, Benjamin Adams, Harish Tayyar Madabushi. (2024)<br><strong>Pre-Trained Language Models Represent Some Geographic Populations Better Than Others</strong><br><button class=copy-to-clipboard title="Pre-Trained Language Models Represent Some Geographic Populations Better Than Others" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: BLOOM, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11025v1.pdf filename=2403.11025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper measures the skew in how well two families of <b>LLMs</b> represent diverse geographic populations. A spatial probing task is used with geo-referenced corpora to measure the degree to which <b>pre-trained</b> <b>language</b> <b>models</b> from the OPT and <b>BLOOM</b> series represent diverse populations around the world. Results show that these models perform much better for some populations than others. In particular, populations across the US and the UK are represented quite well while those in South and Southeast Asia are poorly represented. Analysis shows that both families of models largely share the same skew across populations. At the same time, this skew cannot be fully explained by sociolinguistic factors, economic factors, or geographic factors. The basic conclusion from this analysis is that <b>pre-trained</b> <b>models</b> <b>do</b> not equally represent the world&rsquo;s population: there is a strong skew towards specific geographic populations. This finding challenges the idea that a single model can be used for all populations.</p></p class="citation"></blockquote><h3 id=1322--13153-zero-shot-generative-linguistic-steganography-ke-lin-et-al-2024>(13/22 | 13/153) Zero-shot Generative Linguistic Steganography (Ke Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ke Lin, Yiyang Luo, Zijian Zhang, Ping Luo. (2024)<br><strong>Zero-shot Generative Linguistic Steganography</strong><br><button class=copy-to-clipboard title="Zero-shot Generative Linguistic Steganography" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs.CL<br>Keyword Score: 30<br>Keywords: Zero-shot, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10856v1.pdf filename=2403.10856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative linguistic steganography attempts to hide secret messages into covertext. Previous studies have generally focused on the statistical differences between the covertext and stegotext, however, ill-formed stegotext can readily be identified by humans. In this paper, we propose a novel <b>zero-shot</b> approach based on <b>in-context</b> <b>learning</b> for linguistic steganography to achieve better perceptual and statistical imperceptibility. We also design several new metrics and reproducible language evaluations to measure the imperceptibility of the stegotext. Our experimental results indicate that our method produces $1.926\times$ more innocent and intelligible stegotext than any other method.</p></p class="citation"></blockquote><h3 id=1422--14153-retinaqa--a-knowledge-base-question-answering-model-robust-to-both-answerable-and-unanswerable-questions-prayushi-faldu-et-al-2024>(14/22 | 14/153) RETINAQA : A Knowledge Base Question Answering Model Robust to both Answerable and Unanswerable Questions (Prayushi Faldu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prayushi Faldu, Indrajit Bhattacharya, Mausam. (2024)<br><strong>RETINAQA : A Knowledge Base Question Answering Model Robust to both Answerable and Unanswerable Questions</strong><br><button class=copy-to-clipboard title="RETINAQA : A Knowledge Base Question Answering Model Robust to both Answerable and Unanswerable Questions" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Knowledge Base Question Answering, Knowledge Based Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10849v1.pdf filename=2403.10849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art <b>KBQA</b> models assume answerability of <b>questions.</b> <b>Recent</b> research has shown that while these can be adapted to detect unaswerability with suitable training and thresholding, this comes at the expense of accuracy for answerable <b>questions,</b> <b>and</b> no single model is able to handle all categories of unanswerability. We propose a new model for <b>KBQA</b> named RetinaQA that is robust against unaswerability. It complements KB-traversal based logical form retrieval with sketch-filling based logical form construction. This helps with <b>questions</b> <b>that</b> have valid logical forms but no data paths in the KB leading to an answer. Additionally, it uses discrimination instead of generation to better identify <b>questions</b> <b>that</b> do not have valid logical forms. We demonstrate that RetinaQA significantly outperforms adaptations of state-of-the-art <b>KBQA</b> models across answerable and unanswerable <b>questions,</b> <b>while</b> showing robustness across unanswerability categories. Remarkably, it also establishes a new state-of-the art for answerable <b>KBQA</b> by surpassing existing models</p></p class="citation"></blockquote><h3 id=1522--15153-exploring-chinese-humor-generation-a-study-on-two-part-allegorical-sayings-rongwu-xu-2024>(15/22 | 15/153) Exploring Chinese Humor Generation: A Study on Two-Part Allegorical Sayings (Rongwu Xu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rongwu Xu. (2024)<br><strong>Exploring Chinese Humor Generation: A Study on Two-Part Allegorical Sayings</strong><br><button class=copy-to-clipboard title="Exploring Chinese Humor Generation: A Study on Two-Part Allegorical Sayings" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10781v1.pdf filename=2403.10781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humor, a culturally nuanced aspect of human language, poses challenges for computational understanding and generation, especially in Chinese humor, which remains relatively unexplored in the NLP community. This paper investigates the capability of state-of-the-art language models to comprehend and generate Chinese humor, specifically focusing on training them to create allegorical sayings. We employ two prominent training methods: <b>fine-tuning</b> a medium-sized language model and <b>prompting</b> a large one. Our novel <b>fine-tuning</b> approach incorporates fused Pinyin embeddings to consider homophones and employs <b>contrastive</b> <b>learning</b> with synthetic hard negatives to distinguish humor elements. Human-annotated results show that these models can generate humorous allegorical sayings, with <b>prompting</b> proving to be a practical and effective method. However, there is still room for improvement in generating allegorical sayings that match human creativity.</p></p class="citation"></blockquote><h3 id=1622--16153-llm-based-conversational-ai-therapist-for-daily-functioning-screening-and-psychotherapeutic-intervention-via-everyday-smart-devices-jingping-nie-et-al-2024>(16/22 | 16/153) LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices (Jingping Nie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingping Nie, Hanya Shao, Yuang Fan, Qijia Shao, Haoxuan You, Matthias Preindl, Xiaofan Jiang. (2024)<br><strong>LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices</strong><br><button class=copy-to-clipboard title="LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10779v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10779v1.pdf filename=2403.10779v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the global mental health crisis, access to screenings, professionals, and treatments remains high. In collaboration with licensed psychotherapists, we propose a Conversational AI Therapist with psychotherapeutic Interventions (CaiTI), a platform that leverages <b>large</b> <b>language</b> <b>models</b> <b>(LLM)s</b> and smart devices to enable better mental health self-care. CaiTI can screen the day-to-day functioning using natural and psychotherapeutic conversations. CaiTI leverages <b>reinforcement</b> <b>learning</b> to provide personalized conversation flow. CaiTI can accurately understand and interpret user responses. When the user needs further attention during the conversation, CaiTI can provide conversational psychotherapeutic interventions, including cognitive behavioral therapy (CBT) and motivational interviewing (MI). Leveraging the datasets prepared by the licensed psychotherapists, we experiment and microbenchmark various <b>LLMs&rsquo;</b> performance in tasks along CaiTI&rsquo;s conversation flow and discuss their strengths and weaknesses. With the psychotherapists, we implement CaiTI and conduct 14-day and 24-week studies. The study results, validated by therapists, demonstrate that CaiTI can converse with users naturally, accurately understand and interpret user responses, and provide psychotherapeutic interventions appropriately and effectively. We showcase the potential of CaiTI <b>LLMs</b> to assist the mental therapy diagnosis and treatment and improve day-to-day functioning screening and precautionary psychotherapeutic intervention systems.</p></p class="citation"></blockquote><h3 id=1722--17153-deciphering-hate-identifying-hateful-memes-and-their-targets-eftekhar-hossain-et-al-2024>(17/22 | 17/153) Deciphering Hate: Identifying Hateful Memes and Their Targets (Eftekhar Hossain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eftekhar Hossain, Omar Sharif, Mohammed Moshiul Hoque, Sarah M. Preum. (2024)<br><strong>Deciphering Hate: Identifying Hateful Memes and Their Targets</strong><br><button class=copy-to-clipboard title="Deciphering Hate: Identifying Hateful Memes and Their Targets" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 26<br>Keywords: High-Resource, Low-Resource, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10829v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10829v1.pdf filename=2403.10829v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Internet memes have become a powerful means for individuals to express emotions, thoughts, and perspectives on social media. While often considered as a source of humor and entertainment, memes can also disseminate hateful content targeting individuals or communities. Most existing research focuses on the negative aspects of memes in <b>high-resource</b> languages, overlooking the distinctive challenges associated with <b>low-resource</b> languages like Bengali (also known as Bangla). Furthermore, while previous work on Bengali memes has focused on detecting hateful memes, there has been no work on detecting their targeted entities. To bridge this gap and facilitate research in this arena, we introduce a novel <b>multimodal</b> dataset for Bengali, BHM (Bengali Hateful Memes). The dataset consists of 7,148 memes with Bengali as well as code-mixed captions, tailored for two tasks: (i) detecting hateful memes, and (ii) detecting the social entities they target (i.e., Individual, Organization, Community, and Society). To solve these tasks, we propose DORA (Dual cO attention fRAmework), a <b>multimodal</b> deep neural network that systematically extracts the significant modality features from the memes and jointly evaluates them with the modality-specific features to understand the context better. Our experiments show that DORA is generalizable on other <b>low-resource</b> hateful meme datasets and outperforms several state-of-the-art rivaling baselines.</p></p class="citation"></blockquote><h3 id=1822--18153-towards-robustness-and-diversity-continual-learning-in-dialog-generation-with-text-mixup-and-batch-nuclear-norm-maximization-zihan-wang-et-al-2024>(18/22 | 18/153) Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization (Zihan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Wang, Jiayu Xiao, Mengxiang Li, Zhongjiang He, Yongxiang Li, Chao Wang, Shuangyong Song. (2024)<br><strong>Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization</strong><br><button class=copy-to-clipboard title="Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Continual Learning, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10894v1.pdf filename=2403.10894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In our dynamic world where <b>data</b> <b>arrives</b> in a continuous stream, <b>continual</b> <b>learning</b> enables us to incrementally add new tasks/domains without the need to retrain from scratch. A major challenge in <b>continual</b> <b>learning</b> of language model is catastrophic forgetting, the tendency of models to forget knowledge from previously trained tasks/domains when training on new ones. This paper studies dialog generation under the <b>continual</b> <b>learning</b> setting. We propose a novel method that 1) uses \textit{Text-Mixup} as <b>data</b> <b>augmentation</b> to avoid model overfitting on replay memory and 2) leverages Batch-Nuclear Norm Maximization (BNNM) to alleviate the problem of mode collapse. Experiments on a $37$-domain task-oriented dialog dataset and DailyDialog (a $10$-domain chitchat dataset) demonstrate that our proposed approach outperforms the state-of-the-art in <b>continual</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=1922--19153-rules-still-work-for-open-information-extraction-jialin-hua-et-al-2024>(19/22 | 19/153) Rules still work for Open Information Extraction (Jialin Hua et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialin Hua, Liangqing Luo, Weiying Ping, Yan Liao, Chunhai Tao, Xuewen Lub. (2024)<br><strong>Rules still work for Open Information Extraction</strong><br><button class=copy-to-clipboard title="Rules still work for Open Information Extraction" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Information Retrieval, Open Information Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10758v1.pdf filename=2403.10758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Open</b> <b>information</b> <b>extraction</b> (OIE) aims to extract surface relations and their corresponding arguments from natural language text, irrespective of domain. This paper presents an innovative OIE model, APRCOIE, tailored for Chinese text. Diverging from previous models, our model generates extraction patterns autonomously. The model defines a new pattern form for Chinese OIE and proposes an automated pattern generation methodology. In that way, the model can handle a wide array of complex and diverse Chinese grammatical phenomena. We design a preliminary filter based on tensor computing to conduct the extraction procedure efficiently. To train the model, we manually annotated a large-scale Chinese OIE dataset. In the comparative evaluation, we demonstrate that APRCOIE outperforms state-of-the-art Chinese OIE models and significantly expands the boundaries of achievable OIE performance. The code of APRCOIE and the annotated dataset are released on GitHub (<a href=https://github.com/jialin666/APRCOIE_v1>https://github.com/jialin666/APRCOIE_v1</a>)</p></p class="citation"></blockquote><h3 id=2022--20153-entity-alignment-with-unlabeled-dangling-cases-hang-yin-et-al-2024>(20/22 | 20/153) Entity Alignment with Unlabeled Dangling Cases (Hang Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Yin, Dong Ding, Liyao Xiang, Yuheng He, Yihan Wu, Xinbing Wang, Chenghu Zhou. (2024)<br><strong>Entity Alignment with Unlabeled Dangling Cases</strong><br><button class=copy-to-clipboard title="Entity Alignment with Unlabeled Dangling Cases" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-4; H-3-3, cs-CL, cs-IR, cs.CL<br>Keyword Score: 18<br>Keywords: Graph, Graph Neural Network, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10978v1.pdf filename=2403.10978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the entity alignment problem with unlabeled dangling cases, meaning that there are entities in the source or target <b>graph</b> having no counterparts in the other, and those entities remain unlabeled. The problem arises when the source and target <b>graphs</b> are of different scales, and it is much cheaper to label the matchable pairs than the dangling entities. To solve the issue, we propose a novel <b>GNN-based</b> dangling detection and entity alignment framework. While the two tasks share the same <b>GNN</b> and are trained together, the detected dangling entities are removed in the alignment. Our framework is featured by a designed entity and relation attention mechanism for selective neighborhood aggregation in <b>representation</b> <b>learning,</b> as well as a positive-unlabeled learning loss for an unbiased estimation of dangling entities. Experimental results have shown that each component of our design contributes to the overall alignment performance which is comparable or superior to baselines, even if the baselines additionally have 30% of the dangling entities labeled as training data.</p></p class="citation"></blockquote><h3 id=2122--21153-multi-party-response-generation-with-relation-disentanglement-tianhao-dai-et-al-2024>(21/22 | 21/153) Multi-party Response Generation with Relation Disentanglement (Tianhao Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianhao Dai, Chengyu Huang, Lizi Liao. (2024)<br><strong>Multi-party Response Generation with Relation Disentanglement</strong><br><button class=copy-to-clipboard title="Multi-party Response Generation with Relation Disentanglement" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 16<br>Keywords: Graph, Benchmarking, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10827v1.pdf filename=2403.10827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing neural response generation models have achieved impressive improvements for two-party conversations, which assume that utterances are sequentially organized. However, many real-world dialogues involve multiple interlocutors and the structure of conversational context is much more complex, e.g. utterances from different interlocutors can occur &ldquo;in parallel&rdquo;. Facing this challenge, there are works trying to model the relations among utterances or interlocutors to facilitate response generation with clearer context. Nonetheless, these methods rely heavily on such relations and all assume that these are given beforehand, which is impractical and hinders the generality of such methods. In this work, we propose to automatically infer the relations via relational thinking on subtle clues inside the conversation context without any human label, and leverage these relations to guide the neural response generation. Specifically, we first apply a deep <b>graph</b> random process to fully consider all possible relations among utterances in the conversational context. Then the inferred relation <b>graphs</b> are integrated with a variational auto-encoder framework to train a <b>GAN</b> for structure-aware response generation. Experimental results on the Ubuntu Internet Relay Chat (IRC) channel <b>benchmark</b> and the most recent Movie Dialogues show that our method outperforms various baseline models for multi-party response generation.</p></p class="citation"></blockquote><h3 id=2222--22153-dialectbench-a-nlp-benchmark-for-dialects-varieties-and-closely-related-languages-fahim-faisal-et-al-2024>(22/22 | 22/153) DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages (Fahim Faisal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fahim Faisal, Orevaoghene Ahia, Aarohi Srivastava, Kabir Ahuja, David Chiang, Yulia Tsvetkov, Antonios Anastasopoulos. (2024)<br><strong>DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages</strong><br><button class=copy-to-clipboard title="DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11009v1.pdf filename=2403.11009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language technologies should be judged on their usefulness in real-world use cases. An often overlooked aspect in natural language processing (NLP) research and evaluation is language variation in the form of non-standard dialects or language varieties (hereafter, varieties). Most NLP <b>benchmarks</b> are limited to standard language varieties. To fill this gap, we propose DIALECTBENCH, the first-ever large-scale <b>benchmark</b> for NLP on varieties, which aggregates an extensive set of task-varied variety datasets (10 text-level tasks covering 281 varieties). This allows for a comprehensive evaluation of NLP system performance on different language varieties. We provide substantial evidence of performance disparities between standard and non-standard language varieties, and we also identify language clusters with large performance divergence across tasks. We believe DIALECTBENCH provides a comprehensive view of the current state of NLP for language varieties and one step towards advancing it further. Code/data: <a href=https://github.com/ffaisal93/DialectBench>https://github.com/ffaisal93/DialectBench</a></p></p class="citation"></blockquote><h2 id=cslg-22>cs.LG (22)</h2><h3 id=122--23153-time-series-representation-learning-with-supervised-contrastive-temporal-transformer-yuansan-liu-et-al-2024>(1/22 | 23/153) Time Series Representation Learning with Supervised Contrastive Temporal Transformer (Yuansan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuansan Liu, Sudanthi Wijewickrema, Christofer Bester, Stephen O&rsquo;Leary, James Bailey. (2024)<br><strong>Time Series Representation Learning with Supervised Contrastive Temporal Transformer</strong><br><button class=copy-to-clipboard title="Time Series Representation Learning with Supervised Contrastive Temporal Transformer" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 75<br>Keywords: Convolution, Convolutional Neural Network, Representation Learning, Self-supervised Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10787v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10787v1.pdf filename=2403.10787v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Finding effective <b>representations</b> <b>for</b> time series data is a useful but challenging task. Several works utilize <b>self-supervised</b> or <b>unsupervised</b> <b>learning</b> methods to address this. However, there still remains the open question of how to leverage available label information for better <b>representations.</b> <b>To</b> answer this question, we exploit pre-existing techniques in time series and <b>representation</b> <b>learning</b> domains and develop a simple, yet novel fusion model, called: \textbf{S}upervised \textbf{CO}ntrastive \textbf{T}emporal \textbf{T}ransformer (SCOTT). We first investigate suitable augmentation methods for various types of time series data to assist with learning change-invariant <b>representations.</b> <b>Secondly,</b> we combine <b>Transformer</b> and Temporal <b>Convolutional</b> <b>Networks</b> in a simple way to efficiently learn both global and local features. Finally, we simplify <b>Supervised</b> Contrastive Loss for <b>representation</b> <b>learning</b> of labelled time series data. We preliminarily evaluate SCOTT on a downstream task, Time Series Classification, using 45 datasets from the UCR archive. The results show that with the <b>representations</b> <b>learnt</b> by SCOTT, even a weak classifier can perform similar to or better than existing state-of-the-art models (best performance on 23/45 datasets and highest rank against 9 baseline models). Afterwards, we investigate SCOTT&rsquo;s ability to address a real-world task, online Change Point Detection (CPD), on two datasets: a human activity dataset and a surgical patient dataset. We show that the model performs with high reliability and efficiency on the online CPD problem ($\sim$98% and $\sim$97% area under precision-recall curve respectively). Furthermore, we demonstrate the model&rsquo;s potential in tackling early detection and show it performs best compared to other candidates.</p></p class="citation"></blockquote><h3 id=222--24153-flykd-graph-knowledge-distillation-on-the-fly-with-curriculum-learning-eugene-ku-2024>(2/22 | 24/153) FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning (Eugene Ku, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eugene Ku. (2024)<br><strong>FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning</strong><br><button class=copy-to-clipboard title="FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 73<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Curriculum Learning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10807v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10807v1.pdf filename=2403.10807v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> aims to transfer a more capable teacher model&rsquo;s <b>knowledge</b> <b>to</b> a lighter student model in order to improve the efficiency of the model, making it faster and more deployable. However, the student model&rsquo;s optimization process over the noisy pseudo labels (generated by the teacher model) is tricky and the amount of pseudo labels one can generate is limited due to Out of Memory (OOM) error. In this paper, we propose FlyKD <b>(Knowledge</b> <b>Distillation</b> on the Fly) which enables the generation of virtually unlimited number of pseudo labels, coupled with <b>Curriculum</b> <b>Learning</b> that greatly alleviates the optimization process over the noisy pseudo labels. Empirically, we observe that FlyKD outperforms vanilla <b>KD</b> and the renown Local Structure Preserving <b>Graph</b> <b>Convolutional</b> <b>Network</b> (LSPGCN). Lastly, with the success of <b>Curriculum</b> <b>Learning,</b> we shed light on a new research direction of improving optimization over noisy pseudo labels.</p></p class="citation"></blockquote><h3 id=322--25153-energy-based-models-with-applications-to-speech-and-language-processing-zhijian-ou-2024>(3/22 | 25/153) Energy-Based Models with Applications to Speech and Language Processing (Zhijian Ou, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhijian Ou. (2024)<br><strong>Energy-Based Models with Applications to Speech and Language Processing</strong><br><button class=copy-to-clipboard title="Energy-Based Models with Applications to Speech and Language Processing" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs-SD, cs.LG, eess-AS<br>Keyword Score: 60<br>Keywords: Generative Adversarial Network, Probabilistic Model, Semi-Supervised Learning, Automatic Speech Recognition, Natural Language Understanding, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10961v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10961v1.pdf filename=2403.10961v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Energy-Based Models (EBMs) are an important class of <b>probabilistic</b> <b>models,</b> also known as random fields and undirected graphical models. EBMs are un-normalized and thus radically different from other popular self-normalized <b>probabilistic</b> <b>models</b> such as hidden Markov models (HMMs), autoregressive models, generative adversarial nets <b>(GANs)</b> and variational auto-encoders (VAEs). Over the past years, EBMs have attracted increasing interest not only from the core machine learning community, but also from application domains such as <b>speech,</b> <b>vision,</b> <b>natural</b> <b>language</b> <b>processing</b> (NLP) and so on, due to significant theoretical and algorithmic progress. The sequential nature of <b>speech</b> <b>and</b> language also presents special challenges and needs a different treatment from processing fix-dimensional data (e.g., images). Therefore, the purpose of this monograph is to present a systematic introduction to energy-based models, including both algorithmic progress and applications in <b>speech</b> <b>and</b> language processing. First, the basics of EBMs are introduced, including classic models, recent models parameterized by neural networks, sampling methods, and various learning methods from the classic learning algorithms to the most advanced ones. Then, the application of EBMs in three different scenarios is presented, i.e., for modeling marginal, conditional and joint distributions, respectively. 1) EBMs for sequential data with applications in language modeling, where the main focus is on the marginal distribution of a sequence itself; 2) EBMs for modeling conditional distributions of target sequences given observation sequences, with applications in <b>speech</b> <b>recognition,</b> sequence labeling and <b>text</b> <b>generation;</b> 3) EBMs for modeling joint distributions of both sequences of observations and targets, and their applications in <b>semi-supervised</b> <b>learning</b> and calibrated <b>natural</b> <b>language</b> <b>understanding.</b></p></p class="citation"></blockquote><h3 id=422--26153-forward-learning-of-graph-neural-networks-namyong-park-et-al-2024>(4/22 | 26/153) Forward Learning of Graph Neural Networks (Namyong Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Namyong Park, Xing Wang, Antoine Simoulin, Shuai Yang, Grey Yang, Ryan Rossi, Puja Trivedi, Nesreen Ahmed. (2024)<br><strong>Forward Learning of Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Forward Learning of Graph Neural Networks" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Recommendation, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11004v1.pdf filename=2403.11004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have achieved remarkable success across a wide range of applications, such as <b>recommendation,</b> drug discovery, and <b>question</b> <b>answering.</b> Behind the success of <b>GNNs</b> lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks (NNs). However, despite its effectiveness, BP imposes several constraints, which are not only biologically implausible, but also limit the scalability, parallelism, and flexibility in learning NNs. Examples of such constraints include storage of neural activities computed in the forward pass for use in the subsequent backward pass, and the dependence of parameter updates on non-local signals. To address these limitations, the forward-forward algorithm (FF) was recently proposed as an alternative to BP in the image classification domain, which trains NNs by performing two forward passes over positive and negative data. Inspired by this advance, we propose ForwardGNN in this work, a new forward learning procedure for <b>GNNs,</b> which avoids the constraints imposed by BP via an effective layer-wise local forward training. ForwardGNN extends the original FF to deal with <b>graph</b> <b>data</b> <b>and</b> <b>GNNs,</b> and makes it possible to operate without generating negative inputs (hence no longer forward-forward). Further, ForwardGNN enables each layer to learn from both the bottom-up and top-down signals without relying on the backpropagation of errors. Extensive experiments on real-world datasets show the effectiveness and generality of the proposed forward <b>graph</b> <b>learning</b> <b>framework.</b> We release our code at <a href=https://github.com/facebookresearch/forwardgnn>https://github.com/facebookresearch/forwardgnn</a>.</p></p class="citation"></blockquote><h3 id=522--27153-edge-private-graph-neural-networks-with-singular-value-perturbation-tingting-tang-et-al-2024>(5/22 | 27/153) Edge Private Graph Neural Networks with Singular Value Perturbation (Tingting Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tingting Tang, Yue Niu, Salman Avestimehr, Murali Annavaram. (2024)<br><strong>Edge Private Graph Neural Networks with Singular Value Perturbation</strong><br><button class=copy-to-clipboard title="Edge Private Graph Neural Networks with Singular Value Perturbation" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs-SI, cs.LG<br>Keyword Score: 36<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10995v1.pdf filename=2403.10995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> play a key role in learning representations from <b>graph-structured</b> <b>data</b> <b>and</b> are demonstrated to be useful in many applications. However, the <b>GNN</b> training pipeline has been shown to be vulnerable to node feature leakage and edge extraction attacks. This paper investigates a scenario where an attacker aims to recover private edge information from a trained <b>GNN</b> model. Previous studies have employed <b>differential</b> <b>privacy</b> (DP) to add noise directly to the adjacency matrix or a compact <b>graph</b> <b>representation.</b> <b>The</b> added perturbations cause the <b>graph</b> <b>structure</b> <b>to</b> be substantially morphed, reducing the model utility. We propose a new privacy-preserving <b>GNN</b> training algorithm, Eclipse, that maintains good model utility while providing strong privacy protection on edges. Eclipse is based on two key observations. First, adjacency matrices in <b>graph</b> <b>structures</b> <b>exhibit</b> low-rank behavior. Thus, Eclipse trains <b>GNNs</b> with a low-rank format of the <b>graph</b> <b>via</b> <b>singular</b> values decomposition (SVD), rather than the original <b>graph.</b> <b>Using</b> <b>the</b> low-rank format, Eclipse preserves the primary <b>graph</b> <b>topology</b> <b>and</b> removes the remaining residual edges. Eclipse adds noise to the low-rank singular values instead of the entire <b>graph,</b> <b>thereby</b> <b>preserving</b> the <b>graph</b> <b>privacy</b> <b>while</b> still maintaining enough of the <b>graph</b> <b>structure</b> <b>to</b> maintain model utility. We theoretically show Eclipse provide formal DP guarantee on edges. Experiments on <b>benchmark</b> <b>graph</b> <b>datasets</b> <b>show</b> that Eclipse achieves significantly better privacy-utility tradeoff compared to existing privacy-preserving <b>GNN</b> training methods. In particular, under strong privacy constraints ($\epsilon$ &lt; 4), Eclipse shows significant gains in the model utility by up to 46%. We further demonstrate that Eclipse also has better resilience against common edge attacks (e.g., LPA), lowering the attack AUC by up to 5% compared to other state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=622--28153-just-say-the-name-online-continual-learning-with-category-names-only-via-data-generation-minhyuk-seo-et-al-2024>(6/22 | 28/153) Just Say the Name: Online Continual Learning with Category Names Only via Data Generation (Minhyuk Seo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minhyuk Seo, Diganta Misra, Seongwon Cho, Minjae Lee, Jonghyun Choi. (2024)<br><strong>Just Say the Name: Online Continual Learning with Category Names Only via Data Generation</strong><br><button class=copy-to-clipboard title="Just Say the Name: Online Continual Learning with Category Names Only via Data Generation" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Continual Learning, Out-of-distribution, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10853v1.pdf filename=2403.10853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real-world scenarios, extensive manual annotation for <b>continual</b> <b>learning</b> is impractical due to prohibitive costs. Although prior arts, influenced by large-scale webly <b>supervised</b> training, suggest leveraging web-scraped data in <b>continual</b> <b>learning,</b> this poses challenges such as data imbalance, usage restrictions, and privacy concerns. Addressing the risks of <b>continual</b> <b>webly</b> <b>supervised</b> training, we present an online <b>continual</b> <b>learning</b> framework - Generative Name only <b>Continual</b> <b>Learning</b> (G-NoCL). The proposed G-NoCL uses a set of generators G along with the learner. When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data. Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL <b>benchmarks,</b> covering both In-Distribution (ID) and <b>Out-of-Distribution</b> (OOD) generalization evaluations, compared to naive generator-ensembling, web-supervised, and manually annotated data.</p></p class="citation"></blockquote><h3 id=722--29153-dreaming-of-many-worlds-learning-contextual-world-models-aids-zero-shot-generalization-sai-prasanna-et-al-2024>(7/22 | 29/153) Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization (Sai Prasanna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sai Prasanna, Karim Farid, Raghu Rajan, André Biedenkapp. (2024)<br><strong>Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization</strong><br><button class=copy-to-clipboard title="Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Zero-shot, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10967v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10967v1.pdf filename=2403.10967v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-shot</b> generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual <b>reinforcement</b> <b>learning</b> (cRL), assuming observability of the context values that parameterize the variation in the system&rsquo;s dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of the Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our experiments show that such systematic incorporation of the context improves the ZSG of the policies trained on the ``dreams&rsquo;&rsquo; of the world model. We further find qualitatively that our approach allows Dreamer to disentangle the latent state from context, allowing it to extrapolate its dreams to the many worlds of unseen contexts. The code for all our experiments is available at \url{https://github.com/sai-prasanna/dreaming_of_many_worlds}.</p></p class="citation"></blockquote><h3 id=822--30153-interpretable-machine-learning-for-tabpfn-david-rundel-et-al-2024>(8/22 | 30/153) Interpretable Machine Learning for TabPFN (David Rundel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Rundel, Julius Kobialka, Constantin von Crailsheim, Matthias Feurer, Thomas Nagler, David Rügamer. (2024)<br><strong>Interpretable Machine Learning for TabPFN</strong><br><button class=copy-to-clipboard title="Interpretable Machine Learning for TabPFN" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-CO, stat-ML<br>Keyword Score: 30<br>Keywords: Transformer, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10923v1.pdf filename=2403.10923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recently developed Prior-Data Fitted Networks (PFNs) have shown very promising results for applications in low-data regimes. The TabPFN model, a special case of PFNs for tabular data, is able to achieve state-of-the-art performance on a variety of classification tasks while producing posterior predictive distributions in mere seconds by <b>in-context</b> <b>learning</b> without the need for learning parameters or hyperparameter tuning. This makes TabPFN a very attractive option for a wide range of domain applications. However, a major drawback of the method is its lack of interpretability. Therefore, we propose several adaptations of popular interpretability methods that we specifically design for TabPFN. By taking advantage of the unique properties of the model, our adaptations allow for more efficient computations than existing implementations. In particular, we show how <b>in-context</b> <b>learning</b> facilitates the estimation of Shapley values by avoiding approximate retraining and enables the use of Leave-One-Covariate-Out (LOCO) even when working with large-scale <b>Transformers.</b> In addition, we demonstrate how data valuation methods can be used to address scalability challenges of TabPFN. Our proposed methods are implemented in a package tabpfn_iml and made available at <a href=https://github.com/david-rundel/tabpfn_iml>https://github.com/david-rundel/tabpfn_iml</a>.</p></p class="citation"></blockquote><h3 id=922--31153-lookalike-human-mimicry-based-collaborative-decision-making-rabimba-karanjai-et-al-2024>(9/22 | 31/153) LookALike: Human Mimicry based collaborative decision making (Rabimba Karanjai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rabimba Karanjai, Weidong Shi. (2024)<br><strong>LookALike: Human Mimicry based collaborative decision making</strong><br><button class=copy-to-clipboard title="LookALike: Human Mimicry based collaborative decision making" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-HC, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10824v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10824v1.pdf filename=2403.10824v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial General Intelligence falls short when communicating role specific nuances to other systems. This is more pronounced when building autonomous <b>LLM</b> agents capable and designed to communicate with each other for real world problem solving. Humans can communicate context and domain specific nuances along with <b>knowledge,</b> <b>and</b> that has led to refinement of skills. In this work we propose and evaluate a novel method that leads to <b>knowledge</b> <b>distillation</b> among <b>LLM</b> agents leading to realtime human role play preserving unique contexts without relying on any stored data or pretraining. We also evaluate how our system performs better in simulated real world tasks compared to state of the art.</p></p class="citation"></blockquote><h3 id=1022--32153-model-reprogramming-outperforms-fine-tuning-on-out-of-distribution-data-in-text-image-encoders-andrew-geng-et-al-2024>(10/22 | 32/153) Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders (Andrew Geng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Geng, Pin-Yu Chen. (2024)<br><strong>Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders</strong><br><button class=copy-to-clipboard title="Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Out-of-distribution, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10800v1.pdf filename=2403.10800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When evaluating the performance of a pre-trained model transferred to a downstream task, it is imperative to assess not only the in-distribution (ID) accuracy of the downstream model but also its capacity to generalize and identify <b>out-of-distribution</b> (OOD) samples. In this paper, we unveil the hidden costs associated with intrusive <b>fine-tuning</b> techniques. Specifically, we demonstrate that commonly used <b>fine-tuning</b> methods not only distort the representations necessary for generalizing to covariate-shifted OOD samples (OOD generalization) but also distort the representations necessary for detecting semantically-shifted OOD samples (OOD detection). To address these challenges, we introduce a new model reprogramming approach for <b>fine-tuning,</b> which we name Reprogrammer. Reprogrammer aims to improve the holistic performance of the downstream model across ID, OOD generalization, and OOD detection tasks. Our empirical evidence reveals that Reprogrammer is less intrusive and yields superior downstream models. Furthermore, we demonstrate that by appending an additional representation residual connection to Reprogrammer, we can further preserve pre-training representations, resulting in an even more safe and robust downstream model capable of excelling in many ID classification, OOD generalization, and OOD detection settings.</p></p class="citation"></blockquote><h3 id=1122--33153-graph-regularized-nmf-with-l20-norm-for-unsupervised-feature-learning-zhen-wang-et-al-2024>(11/22 | 33/153) Graph Regularized NMF with L20-norm for Unsupervised Feature Learning (Zhen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Wang, Wenwen Min. (2024)<br><strong>Graph Regularized NMF with L20-norm for Unsupervised Feature Learning</strong><br><button class=copy-to-clipboard title="Graph Regularized NMF with L20-norm for Unsupervised Feature Learning" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Graph, Clustering, Unsupervised Learning, PaLM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10910v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10910v1.pdf filename=2403.10910v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nonnegative Matrix Factorization (NMF) is a widely applied technique in the fields of machine learning and data mining. <b>Graph</b> Regularized Non-negative Matrix Factorization (GNMF) is an extension of NMF that incorporates <b>graph</b> regularization constraints. GNMF has demonstrated exceptional performance in <b>clustering</b> and dimensionality reduction, effectively discovering inherent low-dimensional structures embedded within high-dimensional spaces. However, the sensitivity of GNMF to noise limits its stability and robustness in practical applications. In order to enhance feature sparsity and mitigate the impact of noise while mining row sparsity patterns in the data for effective feature selection, we introduce the $\ell_{2,0}$-norm constraint as the sparsity constraints for GNMF. We propose an <b>unsupervised</b> feature learning framework based on GNMF_$\ell_{20}$ and devise an algorithm based on <b>PALM</b> and its accelerated version to address this problem. Additionally, we establish the convergence of the proposed algorithms and validate the efficacy and superiority of our approach through experiments conducted on both simulated and real image data.</p></p class="citation"></blockquote><h3 id=1222--34153-probabilistic-world-modeling-with-asymmetric-distance-measure-meng-song-2024>(12/22 | 34/153) Probabilistic World Modeling with Asymmetric Distance Measure (Meng Song, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng Song. (2024)<br><strong>Probabilistic World Modeling with Asymmetric Distance Measure</strong><br><button class=copy-to-clipboard title="Probabilistic World Modeling with Asymmetric Distance Measure" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Contrastive Learning, Representation Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10875v1.pdf filename=2403.10875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Representation</b> <b>learning</b> is a fundamental task in machine learning, aiming at uncovering structures from data to facilitate subsequent tasks. However, what is a good <b>representation</b> <b>for</b> planning and <b>reasoning</b> in a stochastic world remains an open problem. In this work, we posit that learning a distance function is essential to allow planning and <b>reasoning</b> in the <b>representation</b> <b>space.</b> We show that a geometric abstraction of the probabilistic world dynamics can be embedded into the <b>representation</b> <b>space</b> through asymmetric <b>contrastive</b> <b>learning.</b> Unlike previous approaches that focus on learning mutual similarity or compatibility measures, we instead learn an asymmetric similarity function that reflects the state reachability and allows multi-way probabilistic inference. Moreover, by conditioning on a common reference state (e.g. the observer&rsquo;s current state), the learned <b>representation</b> <b>space</b> allows us to discover the geometrically salient states that only a handful of paths can lead through. These states can naturally serve as subgoals to break down long-horizon planning tasks. We evaluate our method in gridworld environments with various layouts and demonstrate its effectiveness in discovering the subgoals.</p></p class="citation"></blockquote><h3 id=1322--35153-reinforcement-learning-with-options-ayoub-ghriss-et-al-2024>(13/22 | 35/153) Reinforcement Learning with Options (Ayoub Ghriss et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayoub Ghriss, Masashi Sugiyama, Alessandro Lazaric. (2024)<br><strong>Reinforcement Learning with Options</strong><br><button class=copy-to-clipboard title="Reinforcement Learning with Options" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10855v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10855v1.pdf filename=2403.10855v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The current thesis aims to explore the <b>reinforcement</b> <b>learning</b> field and build on existing methods to produce improved ones to tackle the problem of learning in high-dimensional and complex environments. It addresses such goals by decomposing learning tasks in a hierarchical fashion known as Hierarchical <b>Reinforcement</b> <b>Learning.</b> We start in the first chapter by getting familiar with the <b>Markov</b> <b>Decision</b> <b>Process</b> framework and presenting some of its recent techniques that the following chapters use. We then proceed to build our Hierarchical Policy learning as an answer to the limitations of a single primitive policy. The hierarchy is composed of a manager agent at the top and employee agents at the lower level. In the last chapter, which is the core of this thesis, we attempt to learn lower-level elements of the hierarchy independently of the manager level in what is known as the &ldquo;Eigenoption&rdquo;. Based on the <b>graph</b> structure of the environment, Eigenoptions allow us to build agents that are aware of the geometric and dynamic properties of the environment. Their decision-making has a special property: it is invariant to symmetric transformations of the environment, allowing as a consequence to greatly reduce the complexity of the learning task.</p></p class="citation"></blockquote><h3 id=1422--36153-twin-transformer-using-gated-dynamic-learnable-attention-mechanism-for-fault-detection-and-diagnosis-in-the-tennessee-eastman-process-mohammad-ali-labbaf-khaniki-et-al-2024>(14/22 | 36/153) Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process (Mohammad Ali Labbaf-Khaniki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Ali Labbaf-Khaniki, Mohammad Manthouri, Hanieh Ajami. (2024)<br><strong>Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process</strong><br><button class=copy-to-clipboard title="Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph Attention Networks, Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10842v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10842v1.pdf filename=2403.10842v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety and efficiency of industrial processes. We propose a novel FDD methodology for the Tennessee Eastman Process (TEP), a widely used <b>benchmark</b> for chemical process control. The model employs two separate <b>Transformer</b> branches, enabling independent processing of input data and potential extraction of diverse information. A novel attention mechanism, <b>Gated</b> Dynamic Learnable Attention (GDLAttention), is introduced which integrates a <b>gating</b> mechanism and dynamic learning capabilities. The <b>gating</b> mechanism modulates the attention weights, allowing the model to focus on the most relevant parts of the input. The dynamic learning approach adapts the attention strategy during training, potentially leading to improved performance. The attention mechanism uses a bilinear similarity function, providing greater flexibility in capturing complex relationships between query and key vectors. In order to assess the effectiveness of our approach, we tested it against 21 and 18 distinct fault scenarios in TEP, and compared its performance with several established FDD techniques. The outcomes indicate that the method outperforms others in terms of accuracy, false alarm rate, and misclassification rate. This underscores the robustness and efficacy of the approach for FDD in intricate industrial processes.</p></p class="citation"></blockquote><h3 id=1522--37153-enhancing-out-of-distribution-detection-with-multitesting-based-layer-wise-feature-fusion-jiawei-li-et-al-2024>(15/22 | 37/153) Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion (Jiawei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Li, Sitong Li, Shanshan Wang, Yicheng Zeng, Falong Tan, Chuanlong Xie. (2024)<br><strong>Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion</strong><br><button class=copy-to-clipboard title="Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10803v1.pdf filename=2403.10803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deploying machine learning in open environments presents the challenge of encountering diverse test inputs that differ significantly from the training data. These <b>out-of-distribution</b> samples may exhibit shifts in local or global features compared to the training distribution. The machine learning (ML) community has responded with a number of methods aimed at distinguishing anomalous inputs from original training data. However, the majority of previous studies have primarily focused on the output layer or penultimate layer of pre-trained deep neural networks. In this paper, we propose a novel framework, Multitesting-based Layer-wise <b>Out-of-Distribution</b> (OOD) Detection (MLOD), to identify distributional shifts in test samples at different levels of features through rigorous multiple testing procedure. Our approach distinguishes itself from existing methods as it does not require modifying the structure or <b>fine-tuning</b> of the pre-trained classifier. Through extensive experiments, we demonstrate that our proposed framework can seamlessly integrate with any existing distance-based inspection method while efficiently utilizing feature extractors of varying depths. Our scheme effectively enhances the performance of <b>out-of-distribution</b> detection when compared to baseline methods. In particular, MLOD-Fisher achieves superior performance in general. When trained using KNN on CIFAR10, MLOD-Fisher significantly lowers the false positive rate (FPR) from 24.09% to 7.47% on average compared to merely utilizing the features of the last layer.</p></p class="citation"></blockquote><h3 id=1622--38153-anomaly-detection-based-on-isolation-mechanisms-a-survey-yang-cao-et-al-2024>(16/22 | 38/153) Anomaly Detection Based on Isolation Mechanisms: A Survey (Yang Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Cao, Haolong Xiang, Hang Zhang, Ye Zhu, Kai Ming Ting. (2024)<br><strong>Anomaly Detection Based on Isolation Mechanisms: A Survey</strong><br><button class=copy-to-clipboard title="Anomaly Detection Based on Isolation Mechanisms: A Survey" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10802v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10802v1.pdf filename=2403.10802v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Anomaly</b> <b>detection</b> is a longstanding and active research area that has many applications in domains such as finance, security, and manufacturing. However, the efficiency and performance of <b>anomaly</b> <b>detection</b> algorithms are challenged by the large-scale, high-dimensional, and heterogeneous data that are prevalent in the era of big data. Isolation-based <b>unsupervised</b> <b>anomaly</b> <b>detection</b> is a novel and effective approach for identifying anomalies in data. It relies on the idea that anomalies are few and different from normal instances, and thus can be easily isolated by random partitioning. Isolation-based methods have several advantages over existing methods, such as low computational complexity, low memory usage, high scalability, robustness to noise and irrelevant features, and no need for prior knowledge or heavy parameter tuning. In this survey, we review the state-of-the-art isolation-based <b>anomaly</b> <b>detection</b> methods, including their data partitioning strategies, <b>anomaly</b> <b>score</b> functions, and algorithmic details. We also discuss some extensions and applications of isolation-based methods in different scenarios, such as detecting anomalies in streaming data, time series, trajectory, and image datasets. Finally, we identify some open challenges and future directions for isolation-based <b>anomaly</b> <b>detection</b> research.</p></p class="citation"></blockquote><h3 id=1722--39153-a-probabilistic-approach-for-alignment-with-human-comparisons-junyu-cao-et-al-2024>(17/22 | 39/153) A Probabilistic Approach for Alignment with Human Comparisons (Junyu Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyu Cao, Mohsen Bayati. (2024)<br><strong>A Probabilistic Approach for Alignment with Human Comparisons</strong><br><button class=copy-to-clipboard title="A Probabilistic Approach for Alignment with Human Comparisons" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Fine-tuning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10771v1.pdf filename=2403.10771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A growing trend involves integrating human knowledge into learning frameworks, leveraging subtle human feedback to refine AI models. Despite these advances, no comprehensive theoretical framework describing the specific conditions under which human comparisons improve the traditional <b>supervised</b> <b>fine-tuning</b> process has been developed. To bridge this gap, this paper studies the effective use of human comparisons to address limitations arising from noisy data and high-dimensional models. We propose a two-stage <b>&ldquo;Supervised</b> Fine Tuning+Human Comparison&rdquo; (SFT+HC) framework connecting machine learning with human feedback through a probabilistic bisection approach. The two-stage framework first learns low-dimensional representations from noisy-labeled data via an SFT procedure, and then uses human comparisons to improve the model alignment. To examine the efficacy of the alignment phase, we introduce a novel concept termed the &ldquo;label-noise-to-comparison-accuracy&rdquo; (LNCA) ratio. This paper theoretically identifies the conditions under which the &ldquo;SFT+HC&rdquo; framework outperforms pure SFT approach, leveraging this ratio to highlight the advantage of incorporating human evaluators in reducing sample complexity. We validate that the proposed conditions for the LNCA ratio are met in a case study conducted via an Amazon Mechanical Turk experiment.</p></p class="citation"></blockquote><h3 id=1822--40153-fagh-accelerating-federated-learning-with-approximated-global-hessian-mrinmay-sen-et-al-2024>(18/22 | 40/153) FAGH: Accelerating Federated Learning with Approximated Global Hessian (Mrinmay Sen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mrinmay Sen, A. K. Qin, Krishna Mohan C. (2024)<br><strong>FAGH: Accelerating Federated Learning with Approximated Global Hessian</strong><br><button class=copy-to-clipboard title="FAGH: Accelerating Federated Learning with Approximated Global Hessian" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11041v1.pdf filename=2403.11041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>federated</b> <b>learning</b> (FL), the significant communication overhead due to the slow convergence speed of training the global model poses a great challenge. Specifically, a large number of communication rounds are required to achieve the convergence in FL. One potential solution is to employ the Newton-based optimization method for training, known for its quadratic convergence rate. However, the existing Newton-based FL training methods suffer from either memory inefficiency or high computational costs for local clients or the server. To address this issue, we propose an FL with approximated global Hessian (FAGH) method to accelerate FL training. FAGH leverages the first moment of the approximated global Hessian and the first moment of the global gradient to train the global model. By harnessing the approximated global Hessian curvature, FAGH accelerates the convergence of global model training, leading to the reduced number of communication rounds and thus the shortened training time. Experimental results verify FAGH&rsquo;s effectiveness in decreasing the number of communication rounds and the time required to achieve the pre-specified objectives of the global model performance in terms of training and test losses as well as test accuracy. Notably, FAGH outperforms several state-of-the-art FL training methods.</p></p class="citation"></blockquote><h3 id=1922--41153-iotco2-assessing-the-end-to-end-carbon-footprint-of-internet-of-things-enabled-deep-learning-ahmad-faiz-et-al-2024>(19/22 | 41/153) IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning (Ahmad Faiz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmad Faiz, Shahzeen Attari, Gayle Buck, Fan Chen, Lei Jiang. (2024)<br><strong>IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning</strong><br><button class=copy-to-clipboard title="IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10984v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10984v1.pdf filename=2403.10984v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To improve privacy and ensure quality-of-service (QoS), deep learning (DL) models are increasingly deployed on Internet of Things (IoT) devices for data processing, significantly increasing the carbon footprint associated with DL on IoT, covering both operational and embodied aspects. Existing operational energy predictors often overlook <b>quantized</b> DL models and emerging neural processing units (NPUs), while embodied carbon footprint modeling tools neglect non-computing hardware components common in IoT devices, creating a gap in accurate carbon footprint modeling tools for IoT-enabled DL. This paper introduces \textit{\carb}, an end-to-end modeling tool for precise carbon footprint estimation in IoT-enabled DL, demonstrating a maximum $\pm21%$ deviation in carbon footprint values compared to actual measurements across various DL models. Additionally, practical applications of \carb are showcased through multiple user case studies.</p></p class="citation"></blockquote><h3 id=2022--42153-dtor-decision-tree-outlier-regressor-to-explain-anomalies-riccardo-crupi-et-al-2024>(20/22 | 42/153) DTOR: Decision Tree Outlier Regressor to explain anomalies (Riccardo Crupi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Riccardo Crupi, Alessandro Damiano Sabatino, Immacolata Marano, Massimiliano Brinis, Luca Albertazzi, Andrea Cirillo, Andrea Claudio Cosentini. (2024)<br><strong>DTOR: Decision Tree Outlier Regressor to explain anomalies</strong><br><button class=copy-to-clipboard title="DTOR: Decision Tree Outlier Regressor to explain anomalies" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10903v1.pdf filename=2403.10903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating <b>anomaly</b> <b>scores</b> generated by an <b>anomaly</b> <b>detection</b> model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approaches, the generated rules are consistently satisfied by the points to be explained. Furthermore, our evaluation metrics indicate comparable performance to Anchors in outlier explanation tasks, with reduced execution time.</p></p class="citation"></blockquote><h3 id=2122--43153-list-sample-compression-and-uniform-convergence-steve-hanneke-et-al-2024>(21/22 | 43/153) List Sample Compression and Uniform Convergence (Steve Hanneke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Steve Hanneke, Shay Moran, Tom Waknine. (2024)<br><strong>List Sample Compression and Uniform Convergence</strong><br><button class=copy-to-clipboard title="List Sample Compression and Uniform Convergence" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10889v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10889v1.pdf filename=2403.10889v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>List learning is a variant of <b>supervised</b> classification where the learner outputs multiple plausible labels for each instance rather than just one. We investigate classical principles related to generalization within the context of list learning. Our primary goal is to determine whether classical principles in the PAC setting retain their applicability in the domain of list PAC learning. We focus on uniform convergence (which is the basis of Empirical Risk Minimization) and on sample compression (which is a powerful manifestation of Occam&rsquo;s Razor). In classical PAC learning, both uniform convergence and sample compression satisfy a form of `completeness&rsquo;: whenever a class is learnable, it can also be learned by a learning rule that adheres to these principles. We ask whether the same completeness holds true in the list learning setting. We show that uniform convergence remains equivalent to learnability in the list PAC learning setting. In contrast, our findings reveal surprising results regarding sample compression: we prove that when the label space is $Y={0,1,2}$, then there are 2-list-learnable classes that cannot be compressed. This refutes the list version of the sample compression conjecture by Littlestone and Warmuth (1986). We prove an even stronger impossibility result, showing that there are $2$-list-learnable classes that cannot be compressed even when the reconstructed function can work with lists of arbitrarily large size. We prove a similar result for (1-list) PAC learnable classes when the label space is unbounded. This generalizes a recent result by arXiv:2308.06424.</p></p class="citation"></blockquote><h3 id=2222--44153-incentivized-exploration-of-non-stationary-stochastic-bandits-sourav-chakraborty-et-al-2024>(22/22 | 44/153) Incentivized Exploration of Non-Stationary Stochastic Bandits (Sourav Chakraborty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sourav Chakraborty, Lijun Chen. (2024)<br><strong>Incentivized Exploration of Non-Stationary Stochastic Bandits</strong><br><button class=copy-to-clipboard title="Incentivized Exploration of Non-Stationary Stochastic Bandits" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10819v1.pdf filename=2403.10819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study incentivized exploration for the multi-armed <b>bandit</b> (MAB) problem with non-stationary reward distributions, where players receive compensation for exploring arms other than the greedy choice and may provide biased feedback on the reward. We consider two different non-stationary environments: abruptly-changing and continuously-changing, and propose respective incentivized exploration algorithms. We show that the proposed algorithms achieve sublinear regret and compensation over time, thus effectively incentivizing exploration despite the nonstationarity and the biased or drifted feedback.</p></p class="citation"></blockquote><h2 id=cscv-33>cs.CV (33)</h2><h3 id=133--45153-a-comprehensive-study-of-multimodal-large-language-models-for-image-quality-assessment-tianhe-wu-et-al-2024>(1/33 | 45/153) A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment (Tianhe Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianhe Wu, Kede Ma, Jie Liang, Yujiu Yang, Lei Zhang. (2024)<br><strong>A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment</strong><br><button class=copy-to-clipboard title="A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Multi-modal, Multi-modal, GPT, Reasoning, Chain-of-thought Prompt, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10854v1.pdf filename=2403.10854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have experienced significant advancement on visual understanding and <b>reasoning,</b> their potentials to serve as powerful, flexible, interpretable, and text-driven models for Image Quality Assessment (IQA) remains largely unexplored. In this paper, we conduct a comprehensive and systematic study of <b>prompting</b> MLLMs for IQA. Specifically, we first investigate nine <b>prompting</b> systems for MLLMs as the combinations of three standardized testing procedures in psychophysics (i.e., the single-stimulus, double-stimulus, and multiple-stimulus methods) and three popular <b>prompting</b> strategies in natural language processing (i.e., the standard, <b>in-context,</b> and <b>chain-of-thought</b> <b>prompting).</b> We then present a difficult sample selection procedure, taking into account sample diversity and uncertainty, to further challenge MLLMs equipped with the respective optimal <b>prompting</b> systems. We assess three open-source and one close-source MLLMs on several visual attributes of image quality (e.g., structural and textural distortions, color differences, and geometric transformations) in both full-reference and no-reference scenarios. Experimental results show that only the close-source <b>GPT-4V</b> provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly.</p></p class="citation"></blockquote><h3 id=233--46153-efficientmorph-parameter-efficient-transformer-based-architecture-for-3d-image-registration-abu-zahid-bin-aziz-et-al-2024>(2/33 | 46/153) EfficientMorph: Parameter-Efficient Transformer-Based Architecture for 3D Image Registration (Abu Zahid Bin Aziz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abu Zahid Bin Aziz, Mokshagna Sai Teja Karanam, Tushar Kataria, Shireen Y. Elhabian. (2024)<br><strong>EfficientMorph: Parameter-Efficient Transformer-Based Architecture for 3D Image Registration</strong><br><button class=copy-to-clipboard title="EfficientMorph: Parameter-Efficient Transformer-Based Architecture for 3D Image Registration" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Unsupervised Learning, Transformer, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11026v1.pdf filename=2403.11026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have emerged as the state-of-the-art architecture in medical image registration, outperforming <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> by addressing their limited receptive fields and overcoming gradient instability in deeper models. Despite their success, <b>transformer-based</b> models require substantial resources for training, including data, memory, and computational power, which may restrict their applicability for end users with limited resources. In particular, existing <b>transformer-based</b> 3D image registration architectures face three critical gaps that challenge their efficiency and effectiveness. Firstly, while mitigating the quadratic complexity of full attention by focusing on local regions, window-based attention mechanisms often fail to adequately integrate local and global information. Secondly, feature similarities across attention heads that were recently found in multi-head attention architectures indicate a significant computational redundancy, suggesting that the capacity of the network could be better utilized to enhance performance. Lastly, the granularity of <b>tokenization,</b> a key factor in registration accuracy, presents a trade-off; smaller tokens improve detail capture at the cost of higher computational complexity, increased memory demands, and a risk of overfitting. Here, we propose EfficientMorph, a <b>transformer-based</b> architecture for <b>unsupervised</b> 3D image registration. It optimizes the balance between local and global attention through a plane-based attention mechanism, reduces computational redundancy via cascaded group attention, and captures fine details without compromising computational efficiency, thanks to a Hi-Res <b>tokenization</b> strategy complemented by merging operations. Notably, EfficientMorph sets a new <b>benchmark</b> for performance on the OASIS dataset with 16-27x fewer parameters.</p></p class="citation"></blockquote><h3 id=333--47153-efficient-diffusion-driven-corruption-editor-for-test-time-adaptation-yeongtak-oh-et-al-2024>(3/33 | 47/153) Efficient Diffusion-Driven Corruption Editor for Test-Time Adaptation (Yeongtak Oh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeongtak Oh, Jonghyun Lee, Jooyoung Choi, Dahuin Jung, Uiwon Hwang, Sungroh Yoon. (2024)<br><strong>Efficient Diffusion-Driven Corruption Editor for Test-Time Adaptation</strong><br><button class=copy-to-clipboard title="Efficient Diffusion-Driven Corruption Editor for Test-Time Adaptation" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Data Augmentation, Distribution Shift, Distribution Shift, Fine-tuning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10911v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10911v1.pdf filename=2403.10911v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Test-time adaptation (TTA) addresses the unforeseen <b>distribution</b> <b>shifts</b> occurring during test time. In TTA, both performance and, memory and time consumption serve as crucial considerations. A recent <b>diffusion-based</b> <b>TTA</b> approach for restoring corrupted images involves image-level updates. However, using pixel space <b>diffusion</b> <b>significantly</b> increases resource requirements compared to conventional model updating TTA approaches, revealing limitations as a TTA method. To address this, we propose a novel TTA method by leveraging a latent <b>diffusion</b> <b>model</b> (LDM) based image editing model and <b>fine-tuning</b> it with our newly introduced corruption modeling scheme. This scheme enhances the robustness of the <b>diffusion</b> <b>model</b> against <b>distribution</b> <b>shifts</b> by creating (clean, corrupted) image pairs and <b>fine-tuning</b> the model to edit corrupted images into clean ones. Moreover, we introduce a <b>distilled</b> variant to accelerate the model for corruption editing using only 4 network function evaluations (NFEs). We extensively validated our method across various architectures and datasets including image and video domains. Our model achieves the best performance with a 100 times faster runtime than that of a <b>diffusion-based</b> <b>baseline.</b> Furthermore, it outpaces the speed of the model updating TTA method based on <b>data</b> <b>augmentation</b> threefold, rendering an image-level updating approach more practical.</p></p class="citation"></blockquote><h3 id=433--48153-neuro-symbolic-video-search-minkyu-choi-et-al-2024>(4/33 | 48/153) Neuro-Symbolic Video Search (Minkyu Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minkyu Choi, Harsh Goel, Mohammad Omama, Yunhao Yang, Sahil Shah, Sandeep Chinchali. (2024)<br><strong>Neuro-Symbolic Video Search</strong><br><button class=copy-to-clipboard title="Neuro-Symbolic Video Search" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Foundation Model, GPT-4, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11021v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11021v1.pdf filename=2403.11021v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The unprecedented surge in video data production in recent years necessitates efficient tools to extract meaningful frames from videos for downstream tasks. Long-term temporal <b>reasoning</b> is a key desideratum for frame retrieval systems. While state-of-the-art <b>foundation</b> <b>models,</b> like VideoLLaMA and ViCLIP, are proficient in short-term semantic understanding, they surprisingly fail at long-term <b>reasoning</b> across frames. A key reason for this failure is that they intertwine per-frame perception and temporal <b>reasoning</b> into a single deep network. Hence, decoupling but co-designing semantic understanding and temporal <b>reasoning</b> is essential for efficient scene identification. We propose a system that leverages <b>vision-language</b> models for semantic understanding of individual frames but effectively reasons about the long-term evolution of events using state machines and temporal logic (TL) formulae that inherently capture memory. Our TL-based <b>reasoning</b> improves the F1 score of complex event identification by 9-15% compared to <b>benchmarks</b> that use <b>GPT4</b> for <b>reasoning</b> on state-of-the-art self-driving datasets such as Waymo and NuScenes.</p></p class="citation"></blockquote><h3 id=533--49153-task-aware-low-rank-adaptation-of-segment-anything-model-xuehao-wang-et-al-2024>(5/33 | 49/153) Task-Aware Low-Rank Adaptation of Segment Anything Model (Xuehao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuehao Wang, Feiyang Ye, Yu Zhang. (2024)<br><strong>Task-Aware Low-Rank Adaptation of Segment Anything Model</strong><br><button class=copy-to-clipboard title="Task-Aware Low-Rank Adaptation of Segment Anything Model" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Foundation Model, Tensor Decomposition, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10971v1.pdf filename=2403.10971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Segment Anything Model (SAM), with its remarkable <b>zero-shot</b> capability, has been proven to be a powerful <b>foundation</b> <b>model</b> for image segmentation tasks, which is an important task in computer vision. However, the transfer of its rich semantic information to multiple different downstream tasks remains unexplored. In this paper, we propose the Task-Aware Low-Rank Adaptation (TA-LoRA) method, which enables SAM to work as a <b>foundation</b> <b>model</b> for multi-task learning. Specifically, TA-LoRA injects an update parameter <b>tensor</b> <b>into</b> each layer of the encoder in SAM and leverages a low-rank <b>tensor</b> <b>decomposition</b> method to incorporate both task-shared and task-specific information. Furthermore, we introduce modified SAM (mSAM) for multi-task learning where we remove the <b>prompt</b> encoder of SAM and use task-specific no mask embeddings and mask decoder for each task. Extensive experiments conducted on <b>benchmark</b> datasets substantiate the efficacy of TA-LoRA in enhancing the performance of mSAM across multiple downstream tasks.</p></p class="citation"></blockquote><h3 id=633--50153-reward-guided-latent-consistency-distillation-jiachen-li-et-al-2024>(6/33 | 50/153) Reward Guided Latent Consistency Distillation (Jiachen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiachen Li, Weixi Feng, Wenhu Chen, William Yang Wang. (2024)<br><strong>Reward Guided Latent Consistency Distillation</strong><br><button class=copy-to-clipboard title="Reward Guided Latent Consistency Distillation" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Knowledge Distillation, Knowledge Distillation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11027v1.pdf filename=2403.11027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Latent Consistency <b>Distillation</b> (LCD) has emerged as a promising paradigm for efficient <b>text-to-image</b> synthesis. By <b>distilling</b> a latent consistency model (LCM) from a pre-trained teacher latent <b>diffusion</b> <b>model</b> (LDM), LCD facilitates the generation of high-fidelity images within merely 2 to 4 inference steps. However, the LCM&rsquo;s efficient inference is obtained at the cost of the sample quality. In this paper, we propose compensating the quality loss by aligning LCM&rsquo;s output with human preference during training. Specifically, we introduce Reward Guided LCD (RG-LCD), which integrates feedback from a reward model (RM) into the LCD process by augmenting the original LCD loss with the objective of maximizing the reward associated with LCM&rsquo;s single-step generation. As validated through human evaluation, when trained with the feedback of a good RM, the 2-step generations from our RG-LCM are favored by humans over the 50-step DDIM samples from the teacher LDM, representing a 25 times inference acceleration without quality loss. As directly optimizing towards differentiable RMs can suffer from over-optimization, we overcome this difficulty by proposing the use of a latent proxy RM (LRM). This novel component serves as an intermediary, connecting our LCM with the RM. Empirically, we demonstrate that incorporating the LRM into our RG-LCD successfully avoids high-frequency noise in the generated images, contributing to both improved FID on MS-COCO and a higher HPSv2.1 score on HPSv2&rsquo;s test set, surpassing those achieved by the baseline LCM.</p></p class="citation"></blockquote><h3 id=733--51153-automatic-location-detection-based-on-deep-learning-anjali-karangiya-et-al-2024>(7/33 | 51/153) Automatic location detection based on deep learning (Anjali Karangiya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anjali Karangiya, Anirudh Sharma, Divax Shah, Kartavya Badgujar, Dr. Chintan Thacker, Dainik Dave. (2024)<br><strong>Automatic location detection based on deep learning</strong><br><button class=copy-to-clipboard title="Automatic location detection based on deep learning" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10912v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10912v1.pdf filename=2403.10912v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of digital images and the advancements in deep learning have paved the way for innovative solutions in various domains, especially in the field of image classification. Our project presents an in-depth study and implementation of an image classification system specifically tailored to identify and classify images of Indian cities. Drawing from an extensive dataset, our model classifies images into five major Indian cities: Ahmedabad, Delhi, Kerala, Kolkata, and Mumbai to recognize the distinct features and characteristics of each city/state. To achieve high precision and recall rates, we adopted two approaches. The first, a vanilla <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> and then we explored the power of <b>transfer</b> <b>learning</b> by leveraging the VGG16 model. The vanilla <b>CNN</b> achieved commendable accuracy and the VGG16 model achieved a test accuracy of 63.6%. Evaluations highlighted the strengths and potential areas of improvement, positioning our model as not only competitive but also scalable for broader applications. With an emphasis on open-source ethos, our work aims to contribute to the community, encouraging further development and diverse applications. Our findings demonstrate the potential applications in tourism, urban planning, and even real-time location identification systems, among others.</p></p class="citation"></blockquote><h3 id=833--52153-luojiahog-a-hierarchy-oriented-geo-aware-image-caption-dataset-for-remote-sensing-image-text-retrival-yuanxin-zhao-et-al-2024>(8/33 | 52/153) LuoJiaHOG: A Hierarchy Oriented Geo-aware Image Caption Dataset for Remote Sensing Image-Text Retrival (Yuanxin Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanxin Zhao, Mi Zhang, Bingnan Yang, Zhan Zhang, Jiaju Kang, Jianya Gong. (2024)<br><strong>LuoJiaHOG: A Hierarchy Oriented Geo-aware Image Caption Dataset for Remote Sensing Image-Text Retrival</strong><br><button class=copy-to-clipboard title="LuoJiaHOG: A Hierarchy Oriented Geo-aware Image Caption Dataset for Remote Sensing Image-Text Retrival" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Knowledge Transfer, Image2text, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10887v1.pdf filename=2403.10887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Image-text</b> retrieval (ITR) plays a significant role in making informed decisions for various remote sensing (RS) applications. Nonetheless, creating ITR datasets containing vision and language modalities not only requires significant geo-spatial sampling area but also varing categories and detailed descriptions. To this end, we introduce an image caption dataset LuojiaHOG, which is geospatial-aware, label-extension-friendly and comprehensive-captioned. LuojiaHOG involves the hierarchical spatial sampling, extensible classification system to Open Geospatial Consortium (OGC) standards, and detailed caption generation. In addition, we propose a CLIP-based Image Semantic Enhancement Network (CISEN) to promote sophisticated ITR. CISEN consists of two components, namely dual-path <b>knowledge</b> <b>transfer</b> and progressive cross-modal feature fusion. Comprehensive statistics on LuojiaHOG reveal the richness in sampling diversity, labels quantity and descriptions granularity. The evaluation on LuojiaHOG is conducted across various state-of-the-art ITR models, including ALBEF, ALIGN, CLIP, FILIP, Wukong, GeoRSCLIP and CISEN. We use second- and third-level labels to evaluate these <b>vision-language</b> models through adapter-tuning and CISEN demonstrates superior performance. For instance, it achieves the highest scores with WMAP@5 of 88.47% and 87.28% on third-level ITR tasks, respectively. In particular, CISEN exhibits an improvement of approximately 1.3% and 0.9% in terms of WMAP@5 compared to its baseline. These findings highlight CISEN advancements accurately retrieving pertinent information across image and text. LuojiaHOG and CISEN can serve as a foundational resource for future RS <b>image-text</b> alignment research, facilitating a wide range of <b>vision-language</b> applications.</p></p class="citation"></blockquote><h3 id=933--53153-regularizing-cnns-using-confusion-penalty-based-label-smoothing-for-histopathology-images-somenath-kuiry-et-al-2024>(9/33 | 53/153) Regularizing CNNs using Confusion Penalty Based Label Smoothing for Histopathology Images (Somenath Kuiry et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Somenath Kuiry, Alaka Das, Mita Nasipuri, Nibaran Das. (2024)<br><strong>Regularizing CNNs using Confusion Penalty Based Label Smoothing for Histopathology Images</strong><br><button class=copy-to-clipboard title="Regularizing CNNs using Confusion Penalty Based Label Smoothing for Histopathology Images" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Label Smoothing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10881v1.pdf filename=2403.10881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Learning, particularly <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNN),</b> has been successful in computer vision tasks and medical image analysis. However, modern <b>CNNs</b> can be overconfident, making them difficult to deploy in real-world scenarios. Researchers propose regularizing techniques, such as <b>Label</b> <b>Smoothing</b> (LS), which introduces soft <b>labels</b> <b>for</b> training data, making the classifier more regularized. LS captures disagreements or lack of confidence in the training phase, making the classifier more regularized. Although LS is quite simple and effective, traditional LS techniques utilize a weighted average between target distribution and a uniform distribution across the classes, which limits the objective of LS as well as the performance. This paper introduces a novel LS technique based on the confusion penalty, which treats model confusion for each class with more importance than others. We have performed extensive experiments with well-known <b>CNN</b> architectures with this technique on publicly available Colorectal Histology datasets and got satisfactory results. Also, we have compared our findings with the State-of-the-art and shown our method&rsquo;s efficacy with Reliability diagrams and t-distributed Stochastic Neighbor Embedding (t-SNE) plots of feature space.</p></p class="citation"></blockquote><h3 id=1033--54153-stablegarment-garment-centric-generation-via-stable-diffusion-rui-wang-et-al-2024>(10/33 | 54/153) StableGarment: Garment-Centric Generation via Stable Diffusion (Rui Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Wang, Hailong Guo, Jiaming Liu, Huaxia Li, Haibo Zhao, Xu Tang, Yao Hu, Hao Tang, Peipei Li. (2024)<br><strong>StableGarment: Garment-Centric Generation via Stable Diffusion</strong><br><button class=copy-to-clipboard title="StableGarment: Garment-Centric Generation via Stable Diffusion" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: ControlNet, Text2image, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10783v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10783v1.pdf filename=2403.10783v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce StableGarment, a unified framework to tackle garment-centric(GC) generation tasks, including GC <b>text-to-image,</b> controllable GC <b>text-to-image,</b> stylized GC <b>text-to-image,</b> and robust virtual try-on. The main challenge lies in retaining the intricate textures of the garment while maintaining the flexibility of pre-trained Stable Diffusion. Our solution involves the development of a garment encoder, a trainable copy of the denoising UNet equipped with additive <b>self-attention</b> (ASA) layers. These ASA layers are specifically devised to transfer detailed garment textures, also facilitating the integration of stylized base models for the creation of stylized images. Furthermore, the incorporation of a dedicated try-on <b>ControlNet</b> enables StableGarment to execute virtual try-on tasks with precision. We also build a novel data engine that produces high-quality synthesized data to preserve the model&rsquo;s ability to follow <b>prompts.</b> Extensive experiments demonstrate that our approach delivers state-of-the-art (SOTA) results among existing virtual try-on methods and exhibits high flexibility with broad potential applications in various garment-centric image generation.</p></p class="citation"></blockquote><h3 id=1133--55153-improving-adversarial-transferability-of-visual-language-pre-training-models-through-collaborative-multimodal-interaction-jiyuan-fu-et-al-2024>(11/33 | 55/153) Improving Adversarial Transferability of Visual-Language Pre-training Models through Collaborative Multimodal Interaction (Jiyuan Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiyuan Fu, Zhaoyu Chen, Kaixun Jiang, Haijing Guo, Jiafeng Wang, Shuyong Gao, Wenqiang Zhang. (2024)<br><strong>Improving Adversarial Transferability of Visual-Language Pre-training Models through Collaborative Multimodal Interaction</strong><br><button class=copy-to-clipboard title="Improving Adversarial Transferability of Visual-Language Pre-training Models through Collaborative Multimodal Interaction" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs-MM, cs.CV<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Image2text, Vision-and-Language, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10883v1.pdf filename=2403.10883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the substantial advancements in <b>Vision-Language</b> Pre-training (VLP) models, their susceptibility to <b>adversarial</b> <b>attacks</b> poses a significant challenge. Existing work rarely studies the transferability of attacks on VLP models, resulting in a substantial performance gap from white-box attacks. We observe that prior work overlooks the interaction mechanisms between modalities, which plays a crucial role in understanding the intricacies of VLP models. In response, we propose a novel attack, called Collaborative <b>Multimodal</b> Interaction Attack (CMI-Attack), leveraging modality interaction through embedding guidance and interaction enhancement. Specifically, attacking text at the embedding level while preserving semantics, as well as utilizing interaction image gradients to enhance constraints on perturbations of texts and images. Significantly, in the <b>image-text</b> retrieval task on Flickr30K dataset, CMI-Attack raises the transfer success rates from ALBEF to TCL, $\text{CLIP}<em>{\text{ViT}}$ and $\text{CLIP}</em>{\text{CNN}}$ by 8.11%-16.75% over state-of-the-art methods. Moreover, CMI-Attack also demonstrates superior performance in cross-task generalization scenarios. Our work addresses the underexplored realm of transfer attacks on VLP models, shedding light on the importance of modality interaction for enhanced <b>adversarial</b> <b>robustness.</b></p></p class="citation"></blockquote><h3 id=1233--56153-understanding-robustness-of-visual-state-space-models-for-image-classification-chengbin-du-et-al-2024>(12/33 | 56/153) Understanding Robustness of Visual State Space Models for Image Classification (Chengbin Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengbin Du, Yanxi Li, Chang Xu. (2024)<br><strong>Understanding Robustness of Visual State Space Models for Image Classification</strong><br><button class=copy-to-clipboard title="Understanding Robustness of Visual State Space Models for Image Classification" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Transformer, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10935v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10935v1.pdf filename=2403.10935v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual State Space Model (VMamba) has recently emerged as a promising architecture, exhibiting remarkable performance in various computer vision tasks. However, its robustness has not yet been thoroughly studied. In this paper, we delve into the robustness of this architecture through comprehensive investigations from multiple perspectives. Firstly, we investigate its robustness to <b>adversarial</b> <b>attacks,</b> employing both whole-image and patch-specific <b>adversarial</b> <b>attacks.</b> Results demonstrate superior <b>adversarial</b> <b>robustness</b> compared to <b>Transformer</b> architectures while revealing scalability weaknesses. Secondly, the general robustness of VMamba is assessed against diverse scenarios, including natural <b>adversarial</b> <b>examples,</b> <b>out-of-distribution</b> data, and common corruptions. VMamba exhibits exceptional generalizability with <b>out-of-distribution</b> data but shows scalability weaknesses against natural <b>adversarial</b> <b>examples</b> and common corruptions. Additionally, we explore VMamba&rsquo;s gradients and back-propagation during white-box attacks, uncovering unique vulnerabilities and defensive capabilities of its novel components. Lastly, the sensitivity of VMamba to image structure variations is examined, highlighting vulnerabilities associated with the distribution of disturbance areas and spatial information, with increased susceptibility closer to the image center. Through these comprehensive studies, we contribute to a deeper understanding of VMamba&rsquo;s robustness, providing valuable insights for refining and advancing the capabilities of deep neural networks in computer vision applications.</p></p class="citation"></blockquote><h3 id=1333--57153-securely-fine-tuning-pre-trained-encoders-against-adversarial-examples-ziqi-zhou-et-al-2024>(13/33 | 57/153) Securely Fine-tuning Pre-trained Encoders Against Adversarial Examples (Ziqi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqi Zhou, Minghui Li, Wei Liu, Shengshan Hu, Yechao Zhang, Wei Wan, Lulu Xue, Leo Yu Zhang, Dezhong Yang, Hai Jin. (2024)<br><strong>Securely Fine-tuning Pre-trained Encoders Against Adversarial Examples</strong><br><button class=copy-to-clipboard title="Securely Fine-tuning Pre-trained Encoders Against Adversarial Examples" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10801v1.pdf filename=2403.10801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the evolution of <b>self-supervised</b> <b>learning,</b> the pre-training paradigm has emerged as a predominant solution within the deep learning landscape. Model providers furnish pre-trained encoders designed to function as versatile feature extractors, enabling downstream users to harness the benefits of expansive models with minimal effort through <b>fine-tuning.</b> Nevertheless, recent works have exposed a vulnerability in pre-trained encoders, highlighting their susceptibility to downstream-agnostic adversarial examples (DAEs) meticulously crafted by attackers. The lingering question pertains to the feasibility of fortifying the robustness of downstream models against DAEs, particularly in scenarios where the pre-trained encoders are publicly accessible to the attackers. In this paper, we initially delve into existing defensive mechanisms against adversarial examples within the pre-training paradigm. Our findings reveal that the failure of current defenses stems from the domain shift between pre-training data and downstream tasks, as well as the sensitivity of encoder parameters. In response to these challenges, we propose Genetic Evolution-Nurtured Adversarial <b>Fine-tuning</b> (Gen-AF), a two-stage adversarial <b>fine-tuning</b> approach aimed at enhancing the robustness of downstream models. Our extensive experiments, conducted across ten <b>self-supervised</b> <b>training</b> methods and six datasets, demonstrate that Gen-AF attains high testing accuracy and robust testing accuracy against state-of-the-art DAEs.</p></p class="citation"></blockquote><h3 id=1433--58153-sfda2-source-free-domain-adaptation-through-the-lens-of-data-augmentation-uiwon-hwang-et-al-2024>(14/33 | 58/153) SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation (Uiwon Hwang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Uiwon Hwang, Jonghyun Lee, Juhyeon Shin, Sungroh Yoon. (2024)<br><strong>SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation</strong><br><button class=copy-to-clipboard title="SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 26<br>Keywords: Graph, Clustering, Data Augmentation, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10834v1.pdf filename=2403.10834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the face of the deep learning model&rsquo;s vulnerability to <b>domain</b> <b>shift,</b> source-free <b>domain</b> <b>adaptation</b> (SFDA) methods have been proposed to adapt models to new, unseen target <b>domains</b> <b>without</b> requiring access to source <b>domain</b> <b>data.</b> <b>Although</b> the potential benefits of applying <b>data</b> <b>augmentation</b> to SFDA are attractive, several challenges arise such as the dependence on prior knowledge of class-preserving transformations and the increase in memory and computational requirements. In this paper, we propose Source-free <b>Domain</b> <b>Adaptation</b> Through the Lens of <b>Data</b> <b>Augmentation</b> (SF(DA)$^2$), a novel approach that leverages the benefits of <b>data</b> <b>augmentation</b> without suffering from these challenges. We construct an augmentation <b>graph</b> in the feature space of the pretrained model using the neighbor relationships between target features and propose spectral neighborhood <b>clustering</b> to identify partitions in the prediction space. Furthermore, we propose implicit feature augmentation and feature disentanglement as regularization loss functions that effectively utilize class semantic information within the feature space. These regularizers simulate the inclusion of an unlimited number of augmented target features into the augmentation <b>graph</b> while minimizing computational and memory demands. Our method shows superior adaptation performance in SFDA scenarios, including 2D image and 3D point cloud datasets and a highly imbalanced dataset.</p></p class="citation"></blockquote><h3 id=1533--59153-affective-behaviour-analysis-via-integrating-multi-modal-knowledge-wei-zhang-et-al-2024>(15/33 | 59/153) Affective Behaviour Analysis via Integrating Multi-Modal Knowledge (Wei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Zhang, Feng Qiu, Chen Liu, Lincheng Li, Heming Du, Tiancheng Guo, Xin Yu. (2024)<br><strong>Affective Behaviour Analysis via Integrating Multi-Modal Knowledge</strong><br><button class=copy-to-clipboard title="Affective Behaviour Analysis via Integrating Multi-Modal Knowledge" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Fine-tuning, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10825v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10825v1.pdf filename=2403.10825v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Affective Behavior Analysis aims to facilitate technology emotionally smart, creating a world where devices can understand and react to our emotions as humans do. To comprehensively evaluate the authenticity and applicability of emotional behavior analysis techniques in natural environments, the 6th competition on Affective Behavior Analysis in-the-wild (ABAW) utilizes the Aff-Wild2, Hume-Vidmimic2, and C-EXPR-DB datasets to set up five competitive tracks, i.e., Valence-Arousal (VA) Estimation, Expression (EXPR) Recognition, Action Unit (AU) Detection, Compound Expression (CE) Recognition, and Emotional Mimicry Intensity (EMI) Estimation. In this paper, we present our method designs for the five tasks. Specifically, our design mainly includes three aspects: 1) Utilizing a <b>transformer-based</b> feature fusion module to fully integrate emotional information provided by audio signals, visual images, and transcripts, offering high-quality expression features for the downstream tasks. 2) To achieve high-quality facial feature representations, we employ Masked-Auto Encoder as the visual features extraction model and <b>fine-tune</b> it with our facial dataset. 3) Considering the complexity of the video collection scenes, we conduct a more detailed dataset division based on scene characteristics and train the classifier for each scene. Extensive experiments demonstrate the superiority of our designs.</p></p class="citation"></blockquote><h3 id=1633--60153-n2f2-hierarchical-scene-understanding-with-nested-neural-feature-fields-yash-bhalgat-et-al-2024>(16/33 | 60/153) N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields (Yash Bhalgat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yash Bhalgat, Iro Laina, João F. Henriques, Andrew Zisserman, Andrea Vedaldi. (2024)<br><strong>N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields</strong><br><button class=copy-to-clipboard title="N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10997v1.pdf filename=2403.10997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding complex scenes at multiple levels of abstraction remains a formidable challenge in computer vision. To address this, we introduce Nested Neural Feature Fields (N2F2), a novel approach that employs hierarchical supervision to learn a single feature field, wherein different dimensions within the same high-dimensional feature encode scene properties at varying granularities. Our method allows for a flexible definition of hierarchies, tailored to either the physical dimensions or semantics or both, thereby enabling a comprehensive and nuanced understanding of scenes. We leverage a 2D class-agnostic segmentation model to provide semantically meaningful pixel groupings at arbitrary scales in the image space, and query the CLIP vision-encoder to obtain language-aligned embeddings for each of these segments. Our proposed hierarchical supervision method then assigns different nested dimensions of the feature field to <b>distill</b> the CLIP embeddings using deferred volumetric rendering at varying physical scales, creating a coarse-to-fine representation. Extensive experiments show that our approach outperforms the state-of-the-art feature field <b>distillation</b> methods on tasks such as open-vocabulary 3D segmentation and localization, demonstrating the effectiveness of the learned nested feature field.</p></p class="citation"></blockquote><h3 id=1733--61153-omg-occlusion-friendly-personalized-multi-concept-generation-in-diffusion-models-zhe-kong-et-al-2024>(17/33 | 61/153) OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models (Zhe Kong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhe Kong, Yong Zhang, Tianyu Yang, Tao Wang, Kaihao Zhang, Bizhu Wu, Guanying Chen, Wei Liu, Wenhan Luo. (2024)<br><strong>OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models</strong><br><button class=copy-to-clipboard title="OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10983v1.pdf filename=2403.10983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personalization is an important topic in <b>text-to-image</b> generation, especially the challenging multi-concept personalization. Current multi-concept methods are struggling with identity preservation, occlusion, and the harmony between foreground and background. In this work, we propose OMG, an occlusion-friendly personalized generation framework designed to seamlessly integrate multiple concepts within a single image. We propose a novel two-stage sampling solution. The first stage takes charge of layout generation and visual comprehension information collection for handling occlusions. The second one utilizes the acquired visual comprehension information and the designed noise blending to integrate multiple concepts while considering occlusions. We also observe that the initiation denoising timestep for noise blending is the key to identity preservation and layout. Moreover, our method can be combined with various single-concept models, such as LoRA and InstantID without additional tuning. Especially, LoRA models on civitai.com can be exploited directly. Extensive experiments demonstrate that OMG exhibits superior performance in multi-concept personalization.</p></p class="citation"></blockquote><h3 id=1833--62153-exploiting-topological-prior-for-boosting-point-cloud-generation-baiyuan-chen-2024>(18/33 | 62/153) Exploiting Topological Prior for Boosting Point Cloud Generation (Baiyuan Chen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baiyuan Chen. (2024)<br><strong>Exploiting Topological Prior for Boosting Point Cloud Generation</strong><br><button class=copy-to-clipboard title="Exploiting Topological Prior for Boosting Point Cloud Generation" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10962v1.pdf filename=2403.10962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an innovative enhancement to the Sphere as Prior <b>Generative</b> <b>Adversarial</b> <b>Network</b> (SP-GAN) model, a state-of-the-art <b>GAN</b> designed for point cloud generation. A novel method is introduced for point cloud generation that elevates the structural integrity and overall quality of the generated point clouds by incorporating topological priors into the training process of the generator. Specifically, this work utilizes the K-means algorithm to segment a point cloud from the repository into clusters and extract centroids, which are then used as priors in the generation process of the SP-GAN. Furthermore, the discriminator component of the SP-GAN utilizes the identical point cloud that contributed the centroids, ensuring a coherent and consistent learning environment. This strategic use of centroids as intuitive guides not only boosts the efficiency of global feature learning but also substantially improves the structural coherence and fidelity of the generated point clouds. By applying the K-means algorithm to generate centroids as the prior, the work intuitively and experimentally demonstrates that such a prior enhances the quality of generated point clouds.</p></p class="citation"></blockquote><h3 id=1933--63153-ctrl123-consistent-novel-view-synthesis-via-closed-loop-transcription-hongxiang-zhao-et-al-2024>(19/33 | 63/153) Ctrl123: Consistent Novel View Synthesis via Closed-Loop Transcription (Hongxiang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongxiang Zhao, Xili Dai, Jianan Wang, Shengbang Tong, Jingyuan Zhang, Weida Wang, Lei Zhang, Yi Ma. (2024)<br><strong>Ctrl123: Consistent Novel View Synthesis via Closed-Loop Transcription</strong><br><button class=copy-to-clipboard title="Ctrl123: Consistent Novel View Synthesis via Closed-Loop Transcription" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10953v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10953v1.pdf filename=2403.10953v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large image <b>diffusion</b> <b>models</b> have demonstrated <b>zero-shot</b> capability in novel view synthesis (NVS). However, existing <b>diffusion-based</b> <b>NVS</b> methods struggle to generate novel views that are accurately consistent with the corresponding ground truth poses and appearances, even on the training set. This consequently limits the performance of downstream tasks, such as image-to-multiview generation and 3D reconstruction. We realize that such inconsistency is largely due to the fact that it is difficult to enforce accurate pose and appearance alignment directly in the <b>diffusion</b> <b>training,</b> as mostly done by existing methods such as Zero123. To remedy this problem, we propose Ctrl123, a closed-loop transcription-based NVS <b>diffusion</b> <b>method</b> that enforces alignment between the generated view and ground truth in a pose-sensitive feature space. Our extensive experiments demonstrate the effectiveness of Ctrl123 on the tasks of NVS and 3D reconstruction, achieving significant improvements in both multiview-consistency and pose-consistency over existing methods.</p></p class="citation"></blockquote><h3 id=2033--64153-efficient-domain-adaptation-for-endoscopic-visual-odometry-junyang-wu-et-al-2024>(20/33 | 64/153) Efficient Domain Adaptation for Endoscopic Visual Odometry (Junyang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyang Wu, Yun Gu, Guang-Zhong Yang. (2024)<br><strong>Efficient Domain Adaptation for Endoscopic Visual Odometry</strong><br><button class=copy-to-clipboard title="Efficient Domain Adaptation for Endoscopic Visual Odometry" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Style Transfer, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10860v1.pdf filename=2403.10860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual odometry plays a crucial role in endoscopic imaging, yet the scarcity of realistic images with ground truth poses poses a significant challenge. Therefore, <b>domain</b> <b>adaptation</b> offers a promising approach to bridge the pre-operative planning <b>domain</b> <b>with</b> the intra-operative real <b>domain</b> <b>for</b> learning odometry information. However, existing methodologies suffer from inefficiencies in the training time. In this work, an efficient neural <b>style</b> <b>transfer</b> framework for endoscopic visual odometry is proposed, which compresses the time from pre-operative planning to testing phase to less than five minutes. For efficient traing, this work focuses on training modules with only a limited number of real images and we exploit pre-operative prior information to dramatically reduce training duration. Moreover, during the testing phase, we propose a novel Test Time Adaptation (TTA) method to mitigate the gap in lighting conditions between training and testing datasets. Experimental evaluations conducted on two public endoscope datasets showcase that our method achieves state-of-the-art accuracy in visual odometry tasks while boasting the fastest training speeds. These results demonstrate significant promise for intra-operative surgery applications.</p></p class="citation"></blockquote><h3 id=2133--65153-retmil-retentive-multiple-instance-learning-for-histopathological-whole-slide-image-classification-hongbo-chu-et-al-2024>(21/33 | 65/153) RetMIL: Retentive Multiple Instance Learning for Histopathological Whole Slide Image Classification (Hongbo Chu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongbo Chu, Qiehe Sun, Jiawen Li, Yuxuan Chen, Lizhong Zhang, Tian Guan, Anjia Han, Yonghong He. (2024)<br><strong>RetMIL: Retentive Multiple Instance Learning for Histopathological Whole Slide Image Classification</strong><br><button class=copy-to-clipboard title="RetMIL: Retentive Multiple Instance Learning for Histopathological Whole Slide Image Classification" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Multiple Instance Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10858v1.pdf filename=2403.10858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Histopathological whole slide image (WSI) analysis with deep learning has become a research focus in computational pathology. The current paradigm is mainly based on <b>multiple</b> <b>instance</b> <b>learning</b> (MIL), in which approaches with <b>Transformer</b> as the backbone are well discussed. These methods convert WSI tasks into sequence tasks by representing patches as tokens in the WSI sequence. However, the feature complexity brought by high heterogeneity and the ultra-long sequences brought by gigapixel size makes <b>Transformer-based</b> MIL suffer from the challenges of high memory consumption, slow inference speed, and lack of performance. To this end, we propose a retentive MIL method called RetMIL, which processes WSI sequences through hierarchical feature propagation structure. At the local level, the WSI sequence is divided into <b>multiple</b> <b>subsequences.</b> <b>Tokens</b> of each subsequence are updated through a parallel linear retention mechanism and aggregated utilizing an attention layer. At the global level, subsequences are fused into a global sequence, then updated through a serial retention mechanism, and finally the slide-level representation is obtained through a global attention pooling. We conduct experiments on two public CAMELYON and BRACS datasets and an public-internal LUNG dataset, confirming that RetMIL not only achieves state-of-the-art performance but also significantly reduces computational overhead. Our code will be accessed shortly.</p></p class="citation"></blockquote><h3 id=2233--66153-visionclip-an-med-aigc-based-ethical-language-image-foundation-model-for-generalizable-retina-image-analysis-hao-wei-et-al-2024>(22/33 | 66/153) VisionCLIP: An Med-AIGC based Ethical Language-Image Foundation Model for Generalizable Retina Image Analysis (Hao Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wei, Bowen Liu, Minqing Zhang, Peilun Shi, Wu Yuan. (2024)<br><strong>VisionCLIP: An Med-AIGC based Ethical Language-Image Foundation Model for Generalizable Retina Image Analysis</strong><br><button class=copy-to-clipboard title="VisionCLIP: An Med-AIGC based Ethical Language-Image Foundation Model for Generalizable Retina Image Analysis" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Foundation Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10823v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10823v1.pdf filename=2403.10823v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generalist <b>foundation</b> <b>model</b> has ushered in newfound capabilities in medical domain. However, the contradiction between the growing demand for high-quality annotated data with patient privacy continues to intensify. The utilization of medical artificial intelligence generated content (Med-AIGC) as an inexhaustible resource repository arises as a potential solution to address the aforementioned challenge. Here we harness 1 million open-source synthetic fundus images paired with natural language descriptions, to curate an ethical language-image <b>foundation</b> <b>model</b> for retina image analysis named VisionCLIP. VisionCLIP achieves competitive performance on three external datasets compared with the existing method pre-trained on real-world data in a <b>zero-shot</b> fashion. The employment of artificially synthetic images alongside corresponding textual data for training enables the medical <b>foundation</b> <b>model</b> to successfully assimilate knowledge of disease symptomatology, thereby circumventing potential breaches of patient confidentiality.</p></p class="citation"></blockquote><h3 id=2333--67153-active-label-correction-for-semantic-segmentation-with-foundation-models-hoyoung-kim-et-al-2024>(23/33 | 67/153) Active Label Correction for Semantic Segmentation with Foundation Models (Hoyoung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hoyoung Kim, Sehyun Hwang, Suha Kwak, Jungseul Ok. (2024)<br><strong>Active Label Correction for Semantic Segmentation with Foundation Models</strong><br><button class=copy-to-clipboard title="Active Label Correction for Semantic Segmentation with Foundation Models" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Foundation Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10820v1.pdf filename=2403.10820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training and validating models for semantic segmentation require datasets with pixel-wise annotations, which are notoriously labor-intensive. Although useful priors such as <b>foundation</b> <b>models</b> or crowdsourced datasets are available, they are error-prone. We hence propose an effective framework of active label correction (ALC) based on a design of correction query to rectify pseudo labels of pixels, which in turn is more annotator-friendly than the standard one inquiring to classify a pixel directly according to our theoretical analysis and user study. Specifically, leveraging <b>foundation</b> <b>models</b> providing useful <b>zero-shot</b> predictions on pseudo labels and superpixels, our method comprises two key techniques: (i) an annotator-friendly design of correction query with the pseudo labels, and (ii) an acquisition function looking ahead label expansions based on the superpixels. Experimental results on PASCAL, Cityscapes, and Kvasir-SEG datasets demonstrate the effectiveness of our ALC framework, outperforming prior methods for active semantic segmentation and label correction. Notably, utilizing our method, we obtained a revised dataset of PASCAL by rectifying errors in 2.6 million pixels in PASCAL dataset.</p></p class="citation"></blockquote><h3 id=2433--68153-hcf-net-hierarchical-context-fusion-network-for-infrared-small-object-detection-shibiao-xu-et-al-2024>(24/33 | 68/153) HCF-Net: Hierarchical Context Fusion Network for Infrared Small Object Detection (Shibiao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shibiao Xu, ShuChen Zheng, Wenhao Xu, Rongtao Xu, Changwei Wang, Jiguang Zhang, Xiaoqiang Teng, Ao Li, Li Guo. (2024)<br><strong>HCF-Net: Hierarchical Context Fusion Network for Infrared Small Object Detection</strong><br><button class=copy-to-clipboard title="HCF-Net: Hierarchical Context Fusion Network for Infrared Small Object Detection" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10778v1.pdf filename=2403.10778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Infrared small <b>object</b> <b>detection</b> is an important computer vision task involving the recognition and localization of tiny <b>objects</b> <b>in</b> infrared images, which usually contain only a few pixels. However, it encounters difficulties due to the diminutive size of the <b>objects</b> <b>and</b> the generally complex backgrounds in infrared images. In this paper, we propose a deep learning method, HCF-Net, that significantly improves infrared small <b>object</b> <b>detection</b> performance through multiple practical modules. Specifically, it includes the parallelized patch-aware attention (PPA) module, dimension-aware selective integration (DASI) module, and multi-dilated channel refiner (MDCR) module. The PPA module uses a multi-branch feature extraction strategy to capture feature information at different scales and levels. The DASI module enables adaptive channel selection and fusion. The MDCR module captures spatial features of different receptive field ranges through multiple depth-separable <b>convolutional</b> layers. Extensive experimental results on the SIRST infrared single-frame image dataset show that the proposed HCF-Net performs well, surpassing other traditional and deep learning models. Code is available at <a href=https://github.com/zhengshuchen/HCFNet>https://github.com/zhengshuchen/HCFNet</a>.</p></p class="citation"></blockquote><h3 id=2533--69153-fast-sparse-view-guided-nerf-update-for-object-reconfigurations-ziqi-lu-et-al-2024>(25/33 | 69/153) Fast Sparse View Guided NeRF Update for Object Reconfigurations (Ziqi Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqi Lu, Jianbo Ye, Xiaohan Fei, Xiaolong Li, Jiawei Mo, Ashwin Swaminathan, Stefano Soatto. (2024)<br><strong>Fast Sparse View Guided NeRF Update for Object Reconfigurations</strong><br><button class=copy-to-clipboard title="Fast Sparse View Guided NeRF Update for Object Reconfigurations" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Fine-tuning, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11024v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11024v1.pdf filename=2403.11024v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Field (NeRF), as an implicit 3D scene representation, lacks inherent ability to accommodate changes made to the initial static scene. If objects are reconfigured, it is difficult to update the NeRF to reflect the new state of the scene without time-consuming data re-capturing and NeRF re-training. To address this limitation, we develop the first update method for NeRFs to physical changes. Our method takes only sparse new images (e.g. 4) of the altered scene as extra inputs and update the pre-trained NeRF in around 1 to 2 minutes. Particularly, we develop a pipeline to identify scene changes and update the NeRF accordingly. Our core idea is the use of a second helper NeRF to learn the local <b>geometry</b> and appearance changes, which sidesteps the optimization difficulties in direct NeRF <b>fine-tuning.</b> The interpolation power of the helper NeRF is the key to accurately reconstruct the un-occluded objects regions under sparse view supervision. Our method imposes no constraints on NeRF pre-training, and requires no extra user input or explicit semantic priors. It is an order of magnitude faster than re-training NeRF from scratch while maintaining on-par and even superior performance.</p></p class="citation"></blockquote><h3 id=2633--70153-rethinking-multi-view-representation-learning-via-distilled-disentangling-guanzhou-ke-et-al-2024>(26/33 | 70/153) Rethinking Multi-view Representation Learning via Distilled Disentangling (Guanzhou Ke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guanzhou Ke, Bo Wang, Xiaoli Wang, Shengfeng He. (2024)<br><strong>Rethinking Multi-view Representation Learning via Distilled Disentangling</strong><br><button class=copy-to-clipboard title="Rethinking Multi-view Representation Learning via Distilled Disentangling" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 15<br>Keywords: Knowledge Distillation, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10897v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10897v1.pdf filename=2403.10897v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-view <b>representation</b> <b>learning</b> aims to derive robust <b>representations</b> <b>that</b> are both view-consistent and view-specific from diverse data sources. This paper presents an in-depth analysis of existing approaches in this domain, highlighting a commonly overlooked aspect: the redundancy between view-consistent and view-specific <b>representations.</b> <b>To</b> this end, we propose an innovative framework for multi-view <b>representation</b> <b>learning,</b> which incorporates a technique we term <b>&lsquo;distilled</b> disentangling&rsquo;. Our method introduces the concept of masked cross-view prediction, enabling the extraction of compact, high-quality view-consistent <b>representations</b> <b>from</b> various sources without incurring extra computational overhead. Additionally, we develop a <b>distilled</b> disentangling module that efficiently filters out consistency-related information from multi-view <b>representations,</b> <b>resulting</b> in purer view-specific <b>representations.</b> <b>This</b> approach significantly reduces redundancy between view-consistent and view-specific <b>representations,</b> <b>enhancing</b> the overall efficiency of the learning process. Our empirical evaluations reveal that higher mask ratios substantially improve the quality of view-consistent <b>representations.</b> <b>Moreover,</b> we find that reducing the dimensionality of view-consistent <b>representations</b> <b>relative</b> to that of view-specific <b>representations</b> <b>further</b> refines the quality of the combined <b>representations.</b> <b>Our</b> code is accessible at: <a href=https://github.com/Guanzhou-Ke/MRDD>https://github.com/Guanzhou-Ke/MRDD</a>.</p></p class="citation"></blockquote><h3 id=2733--71153-hourglassnerf-casting-an-hourglass-as-a-bundle-of-rays-for-few-shot-neural-rendering-seunghyeon-seo-et-al-2024>(27/33 | 71/153) HourglassNeRF: Casting an Hourglass as a Bundle of Rays for Few-shot Neural Rendering (Seunghyeon Seo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seunghyeon Seo, Yeonjin Chang, Jayeon Yoo, Seungwoo Lee, Hojun Lee, Nojun Kwak. (2024)<br><strong>HourglassNeRF: Casting an Hourglass as a Bundle of Rays for Few-shot Neural Rendering</strong><br><button class=copy-to-clipboard title="HourglassNeRF: Casting an Hourglass as a Bundle of Rays for Few-shot Neural Rendering" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10906v1.pdf filename=2403.10906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in the Neural Radiance Field (NeRF) have bolstered its capabilities for novel view synthesis, yet its reliance on dense multi-view training images poses a practical challenge. Addressing this, we propose HourglassNeRF, an effective regularization-based approach with a novel hourglass casting strategy. Our proposed hourglass is conceptualized as a bundle of additional rays within the area between the original input ray and its corresponding reflection ray, by featurizing the conical frustum via Integrated Positional Encoding (IPE). This design expands the coverage of unseen views and enables an adaptive high-frequency regularization based on target pixel photo-consistency. Furthermore, we propose luminance consistency regularization based on the Lambertian assumption, which is known to be effective for training a set of augmented rays under the <b>few-shot</b> setting. Leveraging the inherent property of a Lambertian surface, which retains consistent luminance irrespective of the viewing angle, we assume our proposed hourglass as a collection of flipped diffuse reflection rays and enhance the luminance consistency between the original input ray and its corresponding hourglass, resulting in more physically grounded training framework and performance improvement. Our HourglassNeRF outperforms its baseline and achieves competitive results on multiple <b>benchmarks</b> with sharply rendered fine details. The code will be available.</p></p class="citation"></blockquote><h3 id=2833--72153-unsupervised-collaborative-metric-learning-with-mixed-scale-groups-for-general-object-retrieval-shichao-kan-et-al-2024>(28/33 | 72/153) Unsupervised Collaborative Metric Learning with Mixed-Scale Groups for General Object Retrieval (Shichao Kan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shichao Kan, Yuhai Deng, Yixiong Liang, Lihui Cen, Zhe Qu, Yigang Cen, Zhihai He. (2024)<br><strong>Unsupervised Collaborative Metric Learning with Mixed-Scale Groups for General Object Retrieval</strong><br><button class=copy-to-clipboard title="Unsupervised Collaborative Metric Learning with Mixed-Scale Groups for General Object Retrieval" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10798v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10798v1.pdf filename=2403.10798v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of searching for visual objects in a large image dataset is difficult because it requires efficient matching and accurate localization of objects that can vary in size. Although the segment anything model (SAM) offers a potential solution for extracting object spatial context, learning embeddings for local objects remains a challenging problem. This paper presents a novel <b>unsupervised</b> deep metric learning approach, termed <b>unsupervised</b> collaborative metric learning with mixed-scale groups (MS-UGCML), devised to learn embeddings for objects of varying scales. Following this, a <b>benchmark</b> of challenges is assembled by utilizing COCO 2017 and VOC 2007 datasets to facilitate the training and evaluation of general object retrieval models. Finally, we conduct comprehensive ablation studies and discuss the complexities faced within the domain of general object retrieval. Our object retrieval evaluations span a range of datasets, including BelgaLogos, Visual Genome, LVIS, in addition to a challenging evaluation set that we have individually assembled for open-vocabulary evaluation. These comprehensive evaluations effectively highlight the robustness of our <b>unsupervised</b> MS-UGCML approach, with an object level and image level mAPs improvement of up to 6.69% and 10.03%, respectively. The code is publicly available at <a href=https://github.com/dengyuhai/MS-UGCML>https://github.com/dengyuhai/MS-UGCML</a>.</p></p class="citation"></blockquote><h3 id=2933--73153-fishnet-deep-neural-networks-for-low-cost-fish-stock-estimation-moseli-motsoehli-et-al-2024>(29/33 | 73/153) FishNet: Deep Neural Networks for Low-Cost Fish Stock Estimation (Moseli Mots&rsquo;oehli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Moseli Mots&rsquo;oehli, Anton Nikolaev, Wawan B. IGede, John Lynham, Peter J. Mous, Peter Sadowski. (2024)<br><strong>FishNet: Deep Neural Networks for Low-Cost Fish Stock Estimation</strong><br><button class=copy-to-clipboard title="FishNet: Deep Neural Networks for Low-Cost Fish Stock Estimation" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, econ-GN, q-fin-EC<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10916v1.pdf filename=2403.10916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fish stock assessment often involves manual fish counting by taxonomy specialists, which is both time-consuming and costly. We propose an automated computer vision system that performs both taxonomic classification and fish size estimation from images taken with a low-cost digital camera. The system first performs <b>object</b> <b>detection</b> and segmentation using a Mask R-CNN to identify individual fish from images containing multiple fish, possibly consisting of different species. Then each fish species is classified and the predicted length using separate machine learning models. These models are trained on a dataset of 50,000 hand-annotated images containing 163 different fish species, ranging in length from 10cm to 250cm. Evaluated on held-out test data, our system achieves a $92%$ intersection over union on the fish segmentation task, a $89%$ top-1 classification accuracy on single fish species classification, and a $2.3$~cm mean error on the fish length estimation task.</p></p class="citation"></blockquote><h3 id=3033--74153-fuzzy-rank-based-late-fusion-technique-for-cytology-image-segmentation-soumyajyoti-dey-et-al-2024>(30/33 | 74/153) Fuzzy Rank-based Late Fusion Technique for Cytology image Segmentation (Soumyajyoti Dey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soumyajyoti Dey, Sukanta Chakraborty, Utso Guha Roy, Nibaran Das. (2024)<br><strong>Fuzzy Rank-based Late Fusion Technique for Cytology image Segmentation</strong><br><button class=copy-to-clipboard title="Fuzzy Rank-based Late Fusion Technique for Cytology image Segmentation" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10884v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10884v1.pdf filename=2403.10884v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cytology image segmentation is quite challenging due to its complex cellular structure and multiple overlapping regions. On the other hand, for <b>supervised</b> machine learning techniques, we need a large amount of annotated data, which is costly. In recent years, late fusion techniques have given some promising performances in the field of image classification. In this paper, we have explored a fuzzy-based late fusion techniques for cytology image segmentation. This fusion rule integrates three traditional semantic segmentation models UNet, SegNet, and PSPNet. The technique is applied on two cytology image datasets, i.e., cervical cytology(HErlev) and breast cytology(JUCYT-v1) image datasets. We have achieved maximum MeanIoU score 84.27% and 83.79% on the HErlev dataset and JUCYT-v1 dataset after the proposed late fusion technique, respectively which are better than that of the traditional fusion rules such as average probability, geometric mean, Borda Count, etc. The codes of the proposed model are available on GitHub.</p></p class="citation"></blockquote><h3 id=3133--75153-segment-any-object-model-saom-real-to-simulation-fine-tuning-strategy-for-multi-class-multi-instance-segmentation-mariia-khan-et-al-2024>(31/33 | 75/153) Segment Any Object Model (SAOM): Real-to-Simulation Fine-Tuning Strategy for Multi-Class Multi-Instance Segmentation (Mariia Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mariia Khan, Yue Qiu, Yuren Cong, Jumana Abu-Khalaf, David Suter, Bodo Rosenhahn. (2024)<br><strong>Segment Any Object Model (SAOM): Real-to-Simulation Fine-Tuning Strategy for Multi-Class Multi-Instance Segmentation</strong><br><button class=copy-to-clipboard title="Segment Any Object Model (SAOM): Real-to-Simulation Fine-Tuning Strategy for Multi-Class Multi-Instance Segmentation" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10780v1.pdf filename=2403.10780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-class multi-instance segmentation is the task of identifying masks for multiple object classes and multiple instances of the same class within an image. The foundational Segment Anything Model (SAM) is designed for promptable multi-class multi-instance segmentation but tends to output part or sub-part masks in the &ldquo;everything&rdquo; mode for various real-world applications. Whole object segmentation masks play a crucial role for indoor scene understanding, especially in robotics applications. We propose a new domain invariant Real-to-Simulation (Real-Sim) <b>fine-tuning</b> strategy for SAM. We use object images and ground truth data collected from Ai2Thor simulator during <b>fine-tuning</b> (real-to-sim). To allow our Segment Any Object Model (SAOM) to work in the &ldquo;everything&rdquo; mode, we propose the novel nearest neighbour assignment method, updating point embeddings for each ground-truth mask. SAOM is evaluated on our own dataset collected from Ai2Thor simulator. SAOM significantly improves on SAM, with a 28% increase in mIoU and a 25% increase in mAcc for 54 frequently-seen indoor object classes. Moreover, our Real-to-Simulation <b>fine-tuning</b> strategy demonstrates promising generalization performance in real environments without being trained on the real-world data (sim-to-real). The dataset and the code will be released after publication.</p></p class="citation"></blockquote><h3 id=3233--76153-learning-dual-level-deformable-implicit-representation-for-real-world-scale-arbitrary-super-resolution-zhiheng-li-et-al-2024>(32/33 | 76/153) Learning Dual-Level Deformable Implicit Representation for Real-World Scale Arbitrary Super-Resolution (Zhiheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiheng Li, Muheng Li, Jixuan Fan, Lei Chen, Yansong Tang, Jie Zhou, Jiwen Lu. (2024)<br><strong>Learning Dual-Level Deformable Implicit Representation for Real-World Scale Arbitrary Super-Resolution</strong><br><button class=copy-to-clipboard title="Learning Dual-Level Deformable Implicit Representation for Real-World Scale Arbitrary Super-Resolution" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10925v1.pdf filename=2403.10925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scale arbitrary super-resolution based on implicit image function gains increasing popularity since it can better represent the visual world in a continuous manner. However, existing scale arbitrary works are trained and evaluated on simulated datasets, where low-resolution images are generated from their ground truths by the simplest bicubic downsampling. These models exhibit limited generalization to real-world scenarios due to the greater complexity of real-world degradations. To address this issue, we build a RealArbiSR dataset, a new real-world super-resolution <b>benchmark</b> with both integer and non-integer scaling factors for the training and evaluation of real-world scale arbitrary super-resolution. Moreover, we propose a Dual-level Deformable Implicit Representation (DDIR) to solve real-world scale arbitrary super-resolution. Specifically, we design the appearance embedding and deformation field to handle both image-level and pixel-level deformations caused by real-world degradations. The appearance embedding models the characteristics of low-resolution inputs to deal with photometric variations at different scales, and the pixel-based deformation field learns RGB differences which result from the deviations between the real-world and simulated degradations at arbitrary coordinates. Extensive experiments show our trained model achieves state-of-the-art performance on the RealArbiSR and RealSR <b>benchmarks</b> for real-world scale arbitrary super-resolution. Our dataset as well as source code will be publicly available.</p></p class="citation"></blockquote><h3 id=3333--77153-match-stereo-videos-bidirectional-alignment-for-consistent-dynamic-stereo-matching-junpeng-jing-et-al-2024>(33/33 | 77/153) Match-Stereo-Videos: Bidirectional Alignment for Consistent Dynamic Stereo Matching (Junpeng Jing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junpeng Jing, Ye Mao, Krystian Mikolajczyk. (2024)<br><strong>Match-Stereo-Videos: Bidirectional Alignment for Consistent Dynamic Stereo Matching</strong><br><button class=copy-to-clipboard title="Match-Stereo-Videos: Bidirectional Alignment for Consistent Dynamic Stereo Matching" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10755v1.pdf filename=2403.10755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic stereo matching is the task of estimating consistent disparities from stereo videos with dynamic objects. Recent learning-based methods prioritize optimal performance on a single stereo pair, resulting in temporal inconsistencies. Existing video methods apply per-frame matching and window-based cost aggregation across the time dimension, leading to low-frequency oscillations at the scale of the window size. Towards this challenge, we develop a bidirectional alignment mechanism for adjacent frames as a fundamental operation. We further propose a novel framework, BiDAStereo, that achieves consistent dynamic stereo matching. Unlike the existing methods, we model this task as local matching and global aggregation. Locally, we consider correlation in a triple-frame manner to pool information from adjacent frames and improve the temporal consistency. Globally, to exploit the entire sequence&rsquo;s consistency and extract dynamic scene cues for aggregation, we develop a motion-propagation recurrent unit. Extensive experiments demonstrate the performance of our method, showcasing improvements in prediction quality and achieving state-of-the-art results on various commonly used <b>benchmarks.</b></p></p class="citation"></blockquote><h2 id=csro-22>cs.RO (22)</h2><h3 id=122--78153-visarl-visual-reinforcement-learning-guided-by-human-saliency-anthony-liang-et-al-2024>(1/22 | 78/153) ViSaRL: Visual Reinforcement Learning Guided by Human Saliency (Anthony Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anthony Liang, Jesse Thomason, Erdem Bıyık. (2024)<br><strong>ViSaRL: Visual Reinforcement Learning Guided by Human Saliency</strong><br><button class=copy-to-clipboard title="ViSaRL: Visual Reinforcement Learning Guided by Human Saliency" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 53<br>Keywords: Benchmarking, Convolutional Neural Network, Reinforcement Learning, Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10940v1.pdf filename=2403.10940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training robots to perform complex control tasks from high-dimensional pixel input using <b>reinforcement</b> <b>learning</b> (RL) is sample-inefficient, because image observations are comprised primarily of task-irrelevant information. By contrast, humans are able to visually attend to task-relevant objects and areas. Based on this insight, we introduce Visual Saliency-Guided <b>Reinforcement</b> <b>Learning</b> (ViSaRL). Using ViSaRL to learn visual representations significantly improves the success rate, sample efficiency, and generalization of an RL agent on diverse tasks including DeepMind Control <b>benchmark,</b> robot manipulation in <b>simulation</b> and on a real robot. We present approaches for incorporating saliency into both <b>CNN</b> and <b>Transformer-based</b> encoders. We show that visual representations learned using ViSaRL are robust to various sources of visual perturbations including perceptual noise and scene variations. ViSaRL nearly doubles success rate on the real-robot tasks compared to the baseline which does not use saliency.</p></p class="citation"></blockquote><h3 id=222--79153-narrate-versatile-language-architecture-for-optimal-control-in-robotics-seif-ismail-et-al-2024>(2/22 | 79/153) NARRATE: Versatile Language Architecture for Optimal Control in Robotics (Seif Ismail et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seif Ismail, Antonio Arbues, Ryan Cotterell, René Zurbrügg, Carmen Amo Alonso. (2024)<br><strong>NARRATE: Versatile Language Architecture for Optimal Control in Robotics</strong><br><button class=copy-to-clipboard title="NARRATE: Versatile Language Architecture for Optimal Control in Robotics" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 43<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10762v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10762v1.pdf filename=2403.10762v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The impressive capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have led to various efforts to enable robots to be controlled through natural language instructions, opening exciting possibilities for human-robot interaction The goal is for the motor-control task to be performed accurately, efficiently and safely while also enjoying the flexibility imparted by <b>LLMs</b> to specify and adjust the task through natural language. In this work, we demonstrate how a careful layering of an <b>LLM</b> in combination with a Model Predictive Control (MPC) formulation allows for accurate and flexible robotic control via natural language while taking into consideration safety constraints. In particular, we rely on the <b>LLM</b> to effectively frame constraints and objective functions as mathematical expressions, which are later used in the motor-control module via MPC. The transparency of the optimization formulation allows for interpretability of the task and enables adjustments through human feedback. We demonstrate the validity of our method through extensive experiments on long-horizon <b>reasoning,</b> contact-rich, and multi-object interaction tasks. Our evaluations show that NARRATE outperforms current existing methods on these <b>benchmarks</b> and effectively transfers to the real world on two different embodiments. Videos, Code and <b>Prompts</b> at narrate-mpc.github.io</p></p class="citation"></blockquote><h3 id=322--80153-deep-reinforcement-learning-based-large-scale-robot-exploration-yuhong-cao-et-al-2024>(3/22 | 80/153) Deep Reinforcement Learning-based Large-scale Robot Exploration (Yuhong Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhong Cao, Rui Zhao, Yizhuo Wang, Bairan Xiang, Guillaume Sartoretti. (2024)<br><strong>Deep Reinforcement Learning-based Large-scale Robot Exploration</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning-based Large-scale Robot Exploration" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 36<br>Keywords: Graph, Benchmarking, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10833v1.pdf filename=2403.10833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose a deep <b>reinforcement</b> <b>learning</b> (DRL) based reactive planner to solve large-scale Lidar-based autonomous robot exploration problems in 2D action space. Our DRL-based planner allows the agent to reactively plan its exploration path by making implicit predictions about unknown areas, based on a learned estimation of the underlying transition model of the environment. To this end, our approach relies on learned attention mechanisms for their powerful ability to capture long-term dependencies at different spatial scales to reason about the robot&rsquo;s entire belief over known areas. Our approach relies on ground truth information (i.e., privileged learning) to guide the environment estimation during training, as well as on a <b>graph</b> rarefaction algorithm, which allows models trained in small-scale environments to scale to large-scale ones. <b>Simulation</b> results show that our model exhibits better exploration efficiency (12% in path length, 6% in makespan) and lower planning time (60%) than the state-of-the-art planners in a 130m x 100m <b>benchmark</b> scenario. We also validate our learned model on hardware.</p></p class="citation"></blockquote><h3 id=422--81153-corn-contact-based-object-representation-for-nonprehensile-manipulation-of-general-unseen-objects-yoonyoung-cho-et-al-2024>(4/22 | 81/153) CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects (Yoonyoung Cho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoonyoung Cho, Junhyek Han, Yoontae Cho, Beomjoon Kim. (2024)<br><strong>CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects</strong><br><button class=copy-to-clipboard title="CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 35<br>Keywords: Geometry, Reinforcement Learning, Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10760v1.pdf filename=2403.10760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nonprehensile manipulation is essential for manipulating objects that are too thin, large, or otherwise ungraspable in the wild. To sidestep the difficulty of contact modeling in conventional modeling-based approaches, <b>reinforcement</b> <b>learning</b> (RL) has recently emerged as a promising alternative. However, previous RL approaches either lack the ability to generalize over diverse object shapes, or use simple action primitives that limit the diversity of robot motions. Furthermore, using RL over diverse object <b>geometry</b> is challenging due to the high cost of training a policy that takes in high-dimensional sensory inputs. We propose a novel contact-based object representation and pretraining pipeline to tackle this. To enable massively parallel training, we leverage a lightweight patch-based <b>transformer</b> architecture for our encoder that processes point clouds, thus scaling our training across thousands of environments. Compared to learning from scratch, or other shape representation baselines, our representation facilitates both time- and data-efficient learning. We validate the efficacy of our overall system by <b>zero-shot</b> transferring the trained policy to novel real-world objects. Code and videos are available at <a href=https://sites.google.com/view/contact-non-prehensile>https://sites.google.com/view/contact-non-prehensile</a>.</p></p class="citation"></blockquote><h3 id=522--82153-quantifying-the-sim2real-gap-for-gps-and-imu-sensors-ishaan-mahajan-et-al-2024>(5/22 | 82/153) Quantifying the Sim2real Gap for GPS and IMU Sensors (Ishaan Mahajan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ishaan Mahajan, Huzaifa Unjhawala, Harry Zhang, Zhenhao Zhou, Aaron Young, Alexis Ruiz, Stefan Caldararu, Nevindu Batagoda, Sriram Ashokkumar, Dan Negrut. (2024)<br><strong>Quantifying the Sim2real Gap for GPS and IMU Sensors</strong><br><button class=copy-to-clipboard title="Quantifying the Sim2real Gap for GPS and IMU Sensors" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11000v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11000v1.pdf filename=2403.11000v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Simulation</b> can and should play a critical role in the development and testing of algorithms for autonomous agents. What might reduce its impact is the <code>sim2real'' gap -- the algorithm response differs between operation in simulated versus real-world environments. This paper introduces an approach to evaluate this gap, focusing on the accuracy of sensor &lt;b>simulation&lt;/b> -- specifically IMU and GPS -- in velocity estimation tasks for autonomous agents. Using a scaled autonomous vehicle, we conduct 40 real-world experiments across diverse environments then replicate the experiments in &lt;b>simulation&lt;/b> with five distinct sensor noise models. We note that direct comparison of raw &lt;b>simulation&lt;/b> and real sensor data fails to quantify the sim2real gap for robotics applications. We demonstrate that by using a state of the art state-estimation package as a </code>judge&rsquo;&rsquo;, and by evaluating the performance of this state-estimator in both real and simulated scenarios, we can isolate the sim2real discrepancies <b>stemming</b> from sensor <b>simulations</b> alone. The dataset generated is open-source and publicly available for unfettered use.</p></p class="citation"></blockquote><h3 id=622--83153-a-scalable-and-parallelizable-digital-twin-framework-for-sustainable-sim2real-transition-of-multi-agent-reinforcement-learning-systems-chinmay-vilas-samak-et-al-2024>(6/22 | 83/153) A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems (Chinmay Vilas Samak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chinmay Vilas Samak, Tanmay Vilas Samak, Venkat Krovi. (2024)<br><strong>A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems</strong><br><button class=copy-to-clipboard title="A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-MA, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10996v1.pdf filename=2403.10996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents a sustainable multi-agent deep <b>reinforcement</b> <b>learning</b> framework capable of selectively scaling parallelized training workloads on-demand, and transferring the trained policies from <b>simulation</b> to reality using minimal hardware resources. We introduce AutoDRIVE Ecosystem as an enabling digital twin framework to train, deploy, and transfer cooperative as well as competitive multi-agent <b>reinforcement</b> <b>learning</b> policies from <b>simulation</b> to reality. Particularly, we first investigate an intersection traversal problem of 4 cooperative vehicles (Nigel) that share limited state information in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial autonomous racing problem of 2 vehicles (F1TENTH) using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the policies in stochastic environments. The agents were provided with realistically sparse observation spaces, and were restricted to sample control actions that implicitly satisfied the imposed kinodynamic and safety constraints. The experimental results for both problem statements are reported in terms of quantitative metrics and qualitative remarks for training as well as deployment phases. We also discuss agent and environment parallelization techniques adopted to efficiently accelerate MARL training, while analyzing their computational performance. Finally, we demonstrate a resource-aware transition of the trained policies from <b>simulation</b> to reality using the proposed digital twin framework.</p></p class="citation"></blockquote><h3 id=722--84153-inverse-submodular-maximization-with-application-to-human-in-the-loop-multi-robot-multi-objective-coverage-control-guangyao-shi-et-al-2024>(7/22 | 84/153) Inverse Submodular Maximization with Application to Human-in-the-Loop Multi-Robot Multi-Objective Coverage Control (Guangyao Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyao Shi, Gaurav S. Sukhatme. (2024)<br><strong>Inverse Submodular Maximization with Application to Human-in-the-Loop Multi-Robot Multi-Objective Coverage Control</strong><br><button class=copy-to-clipboard title="Inverse Submodular Maximization with Application to Human-in-the-Loop Multi-Robot Multi-Objective Coverage Control" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10991v1.pdf filename=2403.10991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a new type of inverse combinatorial optimization, Inverse Submodular Maximization (ISM), for <b>human-in-the-loop</b> multi-robot coordination. Forward combinatorial optimization, defined as the process of solving a combinatorial problem given the reward (cost)-related parameters, is widely used in multi-robot coordination. In the standard pipeline, the reward (cost)-related parameters are designed offline by domain experts first and then these parameters are utilized for coordinating robots online. What if we need to change these parameters by non-expert human supervisors who watch over the robots during tasks to adapt to some new requirements? We are interested in the case where human supervisors can suggest what actions to take, and the robots need to change the internal parameters based on such suggestions. We study such problems from the perspective of inverse combinatorial optimization, i.e., the process of finding parameters given solutions to the problem. Specifically, we propose a new formulation for ISM, in which we aim to find a new set of parameters that minimally deviate from the current parameters and can make the greedy algorithm output actions the same as those suggested by humans. We show that such problems can be formulated as a Mixed Integer Quadratic Program (MIQP). However, MIQP involves exponentially many binary variables, making it intractable for the existing solver when the problem size is large. We propose a new algorithm under the Branch $&$ Bound paradigm to solve such problems. In numerical <b>simulations,</b> we demonstrate how to use ISM in multi-robot multi-objective coverage control, and we show that the proposed algorithm achieves significant advantages in running time and peak memory usage compared to directly using an existing solver.</p></p class="citation"></blockquote><h3 id=822--85153-learning-based-design-of-off-policy-gaussian-controllers-integrating-model-predictive-control-and-gaussian-process-regression-shiva-kumar-tekumatla-et-al-2024>(8/22 | 85/153) Learning-Based Design of Off-Policy Gaussian Controllers: Integrating Model Predictive Control and Gaussian Process Regression (Shiva Kumar Tekumatla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiva Kumar Tekumatla, Varun Gampa, Siavash Farzan. (2024)<br><strong>Learning-Based Design of Off-Policy Gaussian Controllers: Integrating Model Predictive Control and Gaussian Process Regression</strong><br><button class=copy-to-clipboard title="Learning-Based Design of Off-Policy Gaussian Controllers: Integrating Model Predictive Control and Gaussian Process Regression" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10932v1.pdf filename=2403.10932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an off-policy <b>Gaussian</b> <b>Predictive</b> Control (GPC) framework aimed at solving optimal control problems with a smaller computational footprint, thereby facilitating real-time applicability while ensuring critical safety considerations. The proposed controller imitates classical control methodologies by modeling the optimization process through a <b>Gaussian</b> <b>process</b> and employs <b>Gaussian</b> <b>Process</b> Regression to learn from the Model Predictive Control (MPC) algorithm. Notably, the <b>Gaussian</b> <b>Process</b> setup does not incorporate a built-in model, enhancing its applicability to a broad range of control problems. We applied this framework experimentally to a differential drive mobile robot, tasking it with trajectory tracking and obstacle avoidance. Leveraging the off-policy aspect, the controller demonstrated adaptability to diverse trajectories and obstacle behaviors. <b>Simulation</b> experiments confirmed the effectiveness of the proposed GPC method, emphasizing its ability to learn the dynamics of optimal control strategies. Consequently, our findings highlight the significant potential of off-policy <b>Gaussian</b> <b>Predictive</b> Control in achieving real-time optimal control for handling of robotic systems in safety-critical scenarios.</p></p class="citation"></blockquote><h3 id=922--86153-efficient-trajectory-forecasting-and-generation-with-conditional-flow-matching-sean-ye-et-al-2024>(9/22 | 86/153) Efficient Trajectory Forecasting and Generation with Conditional Flow Matching (Sean Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sean Ye, Matthew Gombolay. (2024)<br><strong>Efficient Trajectory Forecasting and Generation with Conditional Flow Matching</strong><br><button class=copy-to-clipboard title="Efficient Trajectory Forecasting and Generation with Conditional Flow Matching" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Diffusion Model, Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10809v1.pdf filename=2403.10809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trajectory prediction and generation are vital for autonomous robots navigating dynamic environments. While prior research has typically focused on either prediction or generation, our approach unifies these tasks to provide a versatile framework and achieve state-of-the-art performance. <b>Diffusion</b> <b>models,</b> which are currently state-of-the-art for learned trajectory generation in long-horizon planning and <b>offline</b> <b>reinforcement</b> <b>learning</b> tasks, rely on a computationally intensive iterative sampling process. This slow process impedes the dynamic capabilities of robotic systems. In contrast, we introduce Trajectory Conditional Flow Matching (T-CFM), a novel data-driven approach that utilizes flow matching techniques to learn a solver time-varying vector field for efficient and fast trajectory generation. We demonstrate the effectiveness of T-CFM on three separate tasks: adversarial tracking, real-world aircraft trajectory forecasting, and long-horizon planning. Our model outperforms state-of-the-art baselines with an increase of 35% in predictive accuracy and 142% increase in planning performance. Notably, T-CFM achieves up to 100$\times$ speed-up compared to <b>diffusion-based</b> <b>models</b> without sacrificing accuracy, which is crucial for real-time decision making in robotics.</p></p class="citation"></blockquote><h3 id=1022--87153-robotic-task-success-evaluation-under-multi-modal-non-parametric-object-pose-uncertainty-lakshadeep-naik-et-al-2024>(10/22 | 87/153) Robotic Task Success Evaluation Under Multi-modal Non-Parametric Object Pose Uncertainty (Lakshadeep Naik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lakshadeep Naik, Thorbjørn Mosekjær Iversen, Aljaz Kramberger, Norbert Krüger. (2024)<br><strong>Robotic Task Success Evaluation Under Multi-modal Non-Parametric Object Pose Uncertainty</strong><br><button class=copy-to-clipboard title="Robotic Task Success Evaluation Under Multi-modal Non-Parametric Object Pose Uncertainty" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Multi-modal, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10874v1.pdf filename=2403.10874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate 6D object pose estimation is essential for various robotic tasks. Uncertain pose estimates can lead to task failures; however, a certain degree of error in the pose estimates is often acceptable. Hence, by quantifying errors in the object pose estimate and acceptable errors for task success, robots can make informed decisions. This is a challenging problem as both the object pose uncertainty and acceptable error for the robotic task are often <b>multi-modal</b> and cannot be parameterized with commonly used uni-modal distributions. In this paper, we introduce a framework for evaluating robotic task success under object pose uncertainty, representing both the estimated error space of the object pose and the acceptable error space for task success using <b>multi-modal</b> non-parametric probability distributions. The proposed framework pre-computes the acceptable error space for task success using dynamic <b>simulations</b> and subsequently integrates the pre-computed acceptable error space over the estimated error space of the object pose to predict the likelihood of the task success. We evaluated the proposed framework on two mobile manipulation tasks. Our results show that by representing the estimated and the acceptable error space using <b>multi-modal</b> non-parametric distributions, we achieve higher task success rates and fewer failures.</p></p class="citation"></blockquote><h3 id=1122--88153-fully-distributed-cooperative-multi-agent-underwater-obstacle-avoidance-under-dog-walking-paradigm-kanzhong-yao-et-al-2024>(11/22 | 88/153) Fully Distributed Cooperative Multi-agent Underwater Obstacle Avoidance Under Dog Walking Paradigm (Kanzhong Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kanzhong Yao, Ognjen Marjanovic, Simon Watson. (2024)<br><strong>Fully Distributed Cooperative Multi-agent Underwater Obstacle Avoidance Under Dog Walking Paradigm</strong><br><button class=copy-to-clipboard title="Fully Distributed Cooperative Multi-agent Underwater Obstacle Avoidance Under Dog Walking Paradigm" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10759v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10759v1.pdf filename=2403.10759v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Navigation in cluttered underwater environments is challenging, especially when there are constraints on communication and self-localisation. Part of the fully distributed underwater navigation problem has been resolved by introducing multi-agent robot teams, however when the environment becomes cluttered, the problem remains unresolved. In this paper, we first studied the connection between everyday activity of dog walking and the cooperative underwater obstacle avoidance problem. Inspired by this analogy, we propose a novel dog walking paradigm and implement it in a multi-agent underwater system. <b>Simulations</b> were conducted across various scenarios, with performance <b>benchmarked</b> against traditional methods utilising Image-Based Visual Servoing in a multi-agent setup. Results indicate that our dog walking-inspired paradigm significantly enhances cooperative behavior among agents and outperforms the existing approach in navigating through obstacles.</p></p class="citation"></blockquote><h3 id=1222--89153-robust-co-design-of-canonical-underactuated-systems-for-increased-certifiable-stability-federico-girlanda-et-al-2024>(12/22 | 89/153) Robust Co-Design of Canonical Underactuated Systems for Increased Certifiable Stability (Federico Girlanda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Federico Girlanda, Lasse Shala, Shivesh Kumar, Frank Kirchner. (2024)<br><strong>Robust Co-Design of Canonical Underactuated Systems for Increased Certifiable Stability</strong><br><button class=copy-to-clipboard title="Robust Co-Design of Canonical Underactuated Systems for Increased Certifiable Stability" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10966v1.pdf filename=2403.10966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimal behaviours of a system to perform a specific task can be achieved by leveraging the coupling between trajectory optimization, stabilization, and design optimization. This approach is particularly advantageous for underactuated systems, which are systems that have fewer actuators than degrees of freedom and thus require for more elaborate control systems. This paper proposes a novel co-design algorithm, namely Robust Trajectory Control with Design optimization (RTC-D). An inner optimization layer (RTC) simultaneously performs direct transcription (DIRTRAN) to find a nominal trajectory while computing optimal hyperparameters for a stabilizing time-varying linear quadratic regulator (TVLQR). RTC-D augments RTC with a design optimization layer, maximizing the system&rsquo;s robustness through a time-varying Lyapunov-based region of attraction (ROA) analysis. This analysis provides a formal guarantee of stability for a set of off-nominal states. The proposed algorithm has been tested on two different underactuated systems: the torque-limited simple pendulum and the cart-pole. Extensive <b>simulations</b> of off-nominal initial conditions demonstrate improved robustness, while real-system experiments show increased insensitivity to torque disturbances.</p></p class="citation"></blockquote><h3 id=1322--90153-real-to-sim-adaptation-via-high-fidelity-simulation-to-control-a-wheeled-humanoid-robot-with-unknown-dynamics-donghoon-baek-et-al-2024>(13/22 | 90/153) Real-to-Sim Adaptation via High-Fidelity Simulation to Control a Wheeled-Humanoid Robot with Unknown Dynamics (Donghoon Baek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donghoon Baek, Youngwoo Sim, Amartya Purushottam, Saurabh Gupta, Joao Ramos. (2024)<br><strong>Real-to-Sim Adaptation via High-Fidelity Simulation to Control a Wheeled-Humanoid Robot with Unknown Dynamics</strong><br><button class=copy-to-clipboard title="Real-to-Sim Adaptation via High-Fidelity Simulation to Control a Wheeled-Humanoid Robot with Unknown Dynamics" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10948v1.pdf filename=2403.10948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model-based controllers using a linearized model around the system&rsquo;s equilibrium point is a common approach in the control of a wheeled humanoid due to their less computational load and ease of stability analysis. However, controlling a wheeled humanoid robot while it lifts an unknown object presents significant challenges, primarily due to the lack of knowledge in object dynamics. This paper presents a framework designed for predicting the new equilibrium point explicitly to control a wheeled-legged robot with unknown dynamics. We estimated the total mass and center of mass of the system from its response to initially unknown dynamics, then calculated the new equilibrium point accordingly. To avoid using additional sensors (e.g., force torque sensor) and reduce the effort of obtaining expensive real data, a data-driven approach is utilized with a novel real-to-sim adaptation. A more accurate nonlinear dynamics model, offering a closer representation of real-world physics, is injected into a rigid-body <b>simulation</b> for real-to-sim adaptation. The nonlinear dynamics model parameters were optimized using Particle Swarm Optimization. The efficacy of this framework was validated on a physical wheeled inverted pendulum, a simplified model of a wheeled-legged robot. The experimental results indicate that employing a more precise analytical model with optimized parameters significantly reduces the gap between <b>simulation</b> and reality, thus improving the efficiency of a model-based controller in controlling a wheeled robot with unknown dynamics.</p></p class="citation"></blockquote><h3 id=1422--91153-gagent-an-adaptive-rigid-soft-gripping-agent-with-vision-language-models-for-complex-lighting-environments-zhuowei-li-et-al-2024>(14/22 | 91/153) GAgent: An Adaptive Rigid-Soft Gripping Agent with Vision Language Models for Complex Lighting Environments (Zhuowei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuowei Li, Miao Zhang, Xiaotian Lin, Meng Yin, Shuai Lu, Xueqian Wang. (2024)<br><strong>GAgent: An Adaptive Rigid-Soft Gripping Agent with Vision Language Models for Complex Lighting Environments</strong><br><button class=copy-to-clipboard title="GAgent: An Adaptive Rigid-Soft Gripping Agent with Vision Language Models for Complex Lighting Environments" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10850v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10850v1.pdf filename=2403.10850v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces GAgent: an Gripping Agent designed for open-world environments that provides advanced cognitive abilities via VLM agents and flexible grasping abilities with variable stiffness soft grippers. GAgent comprises three primary components - <b>Prompt</b> Engineer module, Visual-Language Model (VLM) core and Workflow module. These three modules enhance gripper success rates by recognizing objects and materials and accurately estimating grasp area even under challenging lighting conditions. As part of creativity, researchers also created a bionic hybrid soft gripper with variable stiffness capable of gripping heavy loads while still gently engaging objects. This intelligent agent, featuring VLM-based cognitive processing with bionic design, shows promise as it could potentially benefit UAVs in various scenarios.</p></p class="citation"></blockquote><h3 id=1522--92153-task-driven-manipulation-with-reconfigurable-parallel-robots-daniel-morton-et-al-2024>(15/22 | 92/153) Task-Driven Manipulation with Reconfigurable Parallel Robots (Daniel Morton et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Morton, Mark Cutkosky, Marco Pavone. (2024)<br><strong>Task-Driven Manipulation with Reconfigurable Parallel Robots</strong><br><button class=copy-to-clipboard title="Task-Driven Manipulation with Reconfigurable Parallel Robots" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10768v1.pdf filename=2403.10768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>ReachBot, a proposed robotic platform, employs extendable booms as limbs for mobility in challenging environments, such as martian caves. When attached to the environment, ReachBot acts as a parallel robot, with reconfiguration driven by the ability to detach and re-place the booms. This ability enables manipulation-focused scientific objectives: for instance, through operating tools, or handling and transporting samples. To achieve these capabilities, we develop a two-part solution, optimizing for robustness against task uncertainty and stochastic failure modes. First, we present a mixed-integer stance planner to determine the positioning of ReachBot&rsquo;s booms to maximize the task wrench space about the nominal point(s). Second, we present a convex tension planner to determine boom tensions for the desired task wrenches, accounting for the probabilistic nature of microspine grasping. We demonstrate improvements in key robustness metrics from the field of dexterous manipulation, and show a large increase in the volume of the manipulation workspace. Finally, we employ Monte-Carlo <b>simulation</b> to validate the robustness of these methods, demonstrating good performance across a range of randomized tasks and environments, and generalization to cable-driven morphologies. We make our code available at our project webpage, <a href=https://stanfordasl.github.io/reachbot_manipulation/>https://stanfordasl.github.io/reachbot_manipulation/</a></p></p class="citation"></blockquote><h3 id=1622--93153-identifying-optimal-launch-sites-of-high-altitude-latex-balloons-using-bayesian-optimisation-for-the-task-of-station-keeping-jack-saunders-et-al-2024>(16/22 | 93/153) Identifying Optimal Launch Sites of High-Altitude Latex-Balloons using Bayesian Optimisation for the Task of Station-Keeping (Jack Saunders et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jack Saunders, Sajad Saeedi, Adam Hartshorne, Binbin Xu, Özgur Şimşek, Alan Hunter, Wenbin Li. (2024)<br><strong>Identifying Optimal Launch Sites of High-Altitude Latex-Balloons using Bayesian Optimisation for the Task of Station-Keeping</strong><br><button class=copy-to-clipboard title="Identifying Optimal Launch Sites of High-Altitude Latex-Balloons using Bayesian Optimisation for the Task of Station-Keeping" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 15<br>Keywords: Geometry, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10784v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10784v1.pdf filename=2403.10784v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Station-keeping tasks for high-altitude balloons show promise in areas such as ecological surveys, atmospheric analysis, and communication relays. However, identifying the optimal time and position to launch a latex high-altitude balloon is still a challenging and multifaceted problem. For example, tasks such as forest fire tracking place geometric constraints on the launch location of the balloon. Furthermore, identifying the most optimal location also heavily depends on atmospheric conditions. We first illustrate how <b>reinforcement</b> <b>learning-based</b> controllers, frequently used for station-keeping tasks, can exploit the environment. This exploitation can degrade performance on unseen weather patterns and affect station-keeping performance when identifying an optimal launch configuration. Valuing all states equally in the region, the agent exploits the region&rsquo;s <b>geometry</b> by flying near the edge, leading to risky behaviours. We propose a modification which compensates for this exploitation and finds this leads to, on average, higher steps within the target region on unseen data. Then, we illustrate how Bayesian Optimisation (BO) can identify the optimal launch location to perform station-keeping tasks, maximising the expected undiscounted return from a given rollout. We show BO can find this launch location in fewer steps compared to other optimisation methods. Results indicate that, surprisingly, the most optimal location to launch from is not commonly within the target region. Please find further information about our project at <a href=https://sites.google.com/view/bo-lauch-balloon/>https://sites.google.com/view/bo-lauch-balloon/</a>.</p></p class="citation"></blockquote><h3 id=1722--94153-diffusion-reinforcement-learning-hierarchical-motion-planning-in-adversarial-multi-agent-games-zixuan-wu-et-al-2024>(17/22 | 94/153) Diffusion-Reinforcement Learning Hierarchical Motion Planning in Adversarial Multi-agent Games (Zixuan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zixuan Wu, Sean Ye, Manisha Natarajan, Matthew C. Gombolay. (2024)<br><strong>Diffusion-Reinforcement Learning Hierarchical Motion Planning in Adversarial Multi-agent Games</strong><br><button class=copy-to-clipboard title="Diffusion-Reinforcement Learning Hierarchical Motion Planning in Adversarial Multi-agent Games" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-MA, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10794v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10794v1.pdf filename=2403.10794v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reinforcement Learning- (RL-)based motion planning has recently shown the potential to outperform traditional approaches from autonomous navigation to robot manipulation. In this work, we focus on a motion planning task for an evasive target in a partially observable multi-agent adversarial pursuit-evasion games (PEG). These pursuit-evasion problems are relevant to various applications, such as search and rescue operations and surveillance robots, where robots must effectively plan their actions to gather intelligence or accomplish mission tasks while avoiding detection or capture themselves. We propose a hierarchical architecture that integrates a high-level <b>diffusion</b> <b>model</b> to plan global paths responsive to environment data while a low-level RL algorithm reasons about evasive versus global path-following behavior. Our approach outperforms baselines by 51.2% by leveraging the <b>diffusion</b> <b>model</b> to guide the RL algorithm for more efficient exploration and improves the explanability and predictability.</p></p class="citation"></blockquote><h3 id=1822--95153-dppe-dense-pose-estimation-in-a-plenoxels-environment-using-gradient-approximation-christopher-kolios-et-al-2024>(18/22 | 95/153) DPPE: Dense Pose Estimation in a Plenoxels Environment using Gradient Approximation (Christopher Kolios et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christopher Kolios, Yeganeh Bahoo, Sajad Saeedi. (2024)<br><strong>DPPE: Dense Pose Estimation in a Plenoxels Environment using Gradient Approximation</strong><br><button class=copy-to-clipboard title="DPPE: Dense Pose Estimation in a Plenoxels Environment using Gradient Approximation" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10773v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10773v1.pdf filename=2403.10773v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present DPPE, a dense pose estimation algorithm that functions over a Plenoxels environment. Recent advances in neural radiance field techniques have shown that it is a powerful tool for environment representation. More recent neural rendering algorithms have significantly improved both training duration and rendering speed. Plenoxels introduced a fully-differentiable radiance field technique that uses Plenoptic volume elements contained in voxels for rendering, offering reduced training times and better rendering accuracy, while also eliminating the neural net component. In this work, we introduce a 6-DoF monocular RGB-only pose estimation procedure for Plenoxels, which seeks to recover the ground truth camera pose after a perturbation. We employ a variation on classical template matching techniques, using <b>stochastic</b> <b>gradient</b> <b>descent</b> to optimize the pose by minimizing errors in re-rendering. In particular, we examine an approach that takes advantage of the rapid rendering speed of Plenoxels to numerically approximate part of the pose gradient, using a central differencing technique. We show that such methods are effective in pose estimation. Finally, we perform ablations over key components of the problem space, with a particular focus on image subsampling and Plenoxel grid resolution. Project website: <a href=https://sites.google.com/view/dppe>https://sites.google.com/view/dppe</a></p></p class="citation"></blockquote><h3 id=1922--96153-msi-nerf-linking-omni-depth-with-view-synthesis-through-multi-sphere-image-aided-generalizable-neural-radiance-field-dongyu-yan-et-al-2024>(19/22 | 96/153) MSI-NeRF: Linking Omni-Depth with View Synthesis through Multi-Sphere Image aided Generalizable Neural Radiance Field (Dongyu Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongyu Yan, Guanyu Huang, Fengyu Quan, Haoyao Chen. (2024)<br><strong>MSI-NeRF: Linking Omni-Depth with View Synthesis through Multi-Sphere Image aided Generalizable Neural Radiance Field</strong><br><button class=copy-to-clipboard title="MSI-NeRF: Linking Omni-Depth with View Synthesis through Multi-Sphere Image aided Generalizable Neural Radiance Field" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10840v1.pdf filename=2403.10840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Panoramic observation using fisheye cameras is significant in robot perception, reconstruction, and remote operation. However, panoramic images synthesized by traditional methods lack depth information and can only provide three degrees-of-freedom (3DoF) rotation rendering in virtual reality applications. To fully preserve and exploit the parallax information within the original fisheye cameras, we introduce MSI-NeRF, which combines deep learning omnidirectional depth estimation and novel view rendering. We first construct a multi-sphere image as a cost volume through feature extraction and warping of the input images. It is then processed by <b>geometry</b> and appearance decoders, respectively. Unlike methods that regress depth maps directly, we further build an implicit radiance field using spatial points and interpolated 3D feature vectors as input. In this way, we can simultaneously realize omnidirectional depth estimation and 6DoF view synthesis. Our method is trained in a semi-self-supervised manner. It does not require target view images and only uses depth data for supervision. Our network has the generalization ability to reconstruct unknown scenes efficiently using only four images. Experimental results show that our method outperforms existing methods in depth estimation and novel view synthesis tasks.</p></p class="citation"></blockquote><h3 id=2022--97153-h3-mapping-quasi-heterogeneous-feature-grids-for-real-time-dense-mapping-using-hierarchical-hybrid-representation-chenxing-jiang-et-al-2024>(20/22 | 97/153) H3-Mapping: Quasi-Heterogeneous Feature Grids for Real-time Dense Mapping Using Hierarchical Hybrid Representation (Chenxing Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenxing Jiang, Yiming Luo, Boyu Zhou, Shaojie Shen. (2024)<br><strong>H3-Mapping: Quasi-Heterogeneous Feature Grids for Real-time Dense Mapping Using Hierarchical Hybrid Representation</strong><br><button class=copy-to-clipboard title="H3-Mapping: Quasi-Heterogeneous Feature Grids for Real-time Dense Mapping Using Hierarchical Hybrid Representation" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10821v1.pdf filename=2403.10821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, implicit online dense mapping methods have achieved high-quality reconstruction results, showcasing great potential in robotics, AR/VR, and digital twins applications. However, existing methods struggle with slow texture modeling which limits their real-time performance. To address these limitations, we propose a NeRF-based dense mapping method that enables faster and higher-quality reconstruction. To improve texture modeling, we introduce quasi-heterogeneous feature grids, which inherit the fast querying ability of uniform feature grids while adapting to varying levels of texture complexity. Besides, we present a gradient-aided coverage-maximizing strategy for keyframe selection that enables the selected keyframes to exhibit a closer focus on rich-textured regions and a broader scope for weak-textured areas. Experimental results demonstrate that our method surpasses existing NeRF-based approaches in texture fidelity, <b>geometry</b> accuracy, and time consumption. The code for our method will be available at: <a href=https://github.com/SYSU-STAR/H3-Mapping>https://github.com/SYSU-STAR/H3-Mapping</a>.</p></p class="citation"></blockquote><h3 id=2122--98153-quaternion-based-sliding-mode-control-for-six-degrees-of-freedom-flight-control-of-quadrotors-amin-yazdanshenas-et-al-2024>(21/22 | 98/153) Quaternion-Based Sliding Mode Control for Six Degrees of Freedom Flight Control of Quadrotors (Amin Yazdanshenas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amin Yazdanshenas, Reza Faieghi. (2024)<br><strong>Quaternion-Based Sliding Mode Control for Six Degrees of Freedom Flight Control of Quadrotors</strong><br><button class=copy-to-clipboard title="Quaternion-Based Sliding Mode Control for Six Degrees of Freedom Flight Control of Quadrotors" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10934v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10934v1.pdf filename=2403.10934v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite extensive research on sliding mode control (SMC) design for quadrotors, the existing approaches suffer from certain limitations. Euler angle-based SMC formulations suffer from poor performance in high-pitch or -roll maneuvers. Quaternion-based SMC approaches have unwinding issues and complex architecture. Coordinate-free methods are slow and only almost globally stable. This paper presents a new six degrees of freedom SMC flight controller to address the above limitations. We use a cascaded architecture with a position controller in the outer loop and a quaternion-based attitude controller in the inner loop. The position controller generates the desired trajectory for the attitude controller using a coordinate-free approach. The quaternion-based attitude controller uses the natural characteristics of the quaternion hypersphere, featuring a simple structure while providing global stability and avoiding unwinding issues. We compare our controller with three other common control methods conducting challenging maneuvers like flip-over and high-speed trajectory tracking in the presence of model uncertainties and disturbances. Our controller consistently outperforms the <b>benchmark</b> approaches with less control effort and actuator saturation, offering highly effective and efficient flight control.</p></p class="citation"></blockquote><h3 id=2222--99153-idb-rrt-sampling-based-kinodynamic-motion-planning-with-motion-primitives-and-trajectory-optimization-joaquim-ortiz-haro-et-al-2024>(22/22 | 99/153) iDb-RRT: Sampling-based Kinodynamic Motion Planning with Motion Primitives and Trajectory Optimization (Joaquim Ortiz-Haro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joaquim Ortiz-Haro, Wolfgang Hönig, Valentin N. Hartmann, Marc Toussaint, Ludovic Righetti. (2024)<br><strong>iDb-RRT: Sampling-based Kinodynamic Motion Planning with Motion Primitives and Trajectory Optimization</strong><br><button class=copy-to-clipboard title="iDb-RRT: Sampling-based Kinodynamic Motion Planning with Motion Primitives and Trajectory Optimization" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10745v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10745v1.pdf filename=2403.10745v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rapidly-exploring Random Trees (RRT) and its variations have emerged as a robust and efficient tool for finding collision-free paths in robotic systems. However, adding dynamic constraints makes the motion planning problem significantly harder, as it requires solving two-value boundary problems (computationally expensive) or propagating random control inputs (uninformative). Alternatively, Iterative Discontinuity Bounded A* (iDb-A*), introduced in our previous study, combines search and optimization iteratively. The search step connects short trajectories (motion primitives) while allowing a bounded discontinuity between the motion primitives, which is later repaired in the trajectory optimization step. Building upon these foundations, in this paper, we present iDb-RRT, a sampling-based kinodynamic motion planning algorithm that combines motion primitives and trajectory optimization within the RRT framework. iDb-RRT is probabilistically complete and can be implemented in forward or bidirectional mode. We have tested our algorithm across a <b>benchmark</b> suite comprising 30 problems, spanning 8 different systems, and shown that iDb-RRT can find solutions up to 10x faster than previous methods, especially in complex scenarios that require long trajectories or involve navigating through narrow passages.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--100153-transformer-based-wireless-traffic-prediction-and-network-optimization-in-o-ran-md-arafat-habib-et-al-2024>(1/1 | 100/153) Transformer-Based Wireless Traffic Prediction and Network Optimization in O-RAN (Md Arafat Habib et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Arafat Habib, Pedro Enrique Iturria-Rivera, Yigit Ozcan, Medhat Elsayed, Majid Bavand, Raimundus Gaigalas, Melike Erol-Kantarci. (2024)<br><strong>Transformer-Based Wireless Traffic Prediction and Network Optimization in O-RAN</strong><br><button class=copy-to-clipboard title="Transformer-Based Wireless Traffic Prediction and Network Optimization in O-RAN" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 50<br>Keywords: Generative AI, Reinforcement Learning, Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10808v1.pdf filename=2403.10808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces an innovative method for predicting wireless network traffic in concise temporal intervals for Open Radio Access Networks (O-RAN) using a <b>transformer</b> architecture, which is the machine learning model behind <b>generative</b> <b>AI</b> tools. Depending on the anticipated traffic, the system either launches a <b>reinforcement</b> <b>learning-based</b> traffic steering xApp or a cell sleeping rApp to enhance performance metrics like throughput or energy efficiency. Our <b>simulation</b> results demonstrate that the proposed traffic prediction-based network optimization mechanism matches the performance of standalone RAN applications (rApps/ xApps) that are always on during the whole <b>simulation</b> time while offering on-demand activation. This feature is particularly advantageous during instances of abrupt fluctuations in traffic volume. Rather than persistently operating specific applications irrespective of the actual incoming traffic conditions, the proposed prediction-based method increases the average energy efficiency by 39.7% compared to the &ldquo;Always on Traffic Steering xApp&rdquo; and achieves 10.1% increase in throughput compared to the &ldquo;Always on Cell Sleeping rApp&rdquo;. The <b>simulation</b> has been conducted over 24 hours, emulating a whole day traffic pattern for a dense urban area.</p></p class="citation"></blockquote><h2 id=eessiv-4>eess.IV (4)</h2><h3 id=14--101153-uncertainty-aware-adapter-adapting-segment-anything-model-sam-for-ambiguous-medical-image-segmentation-mingzhou-jiang-et-al-2024>(1/4 | 101/153) Uncertainty-Aware Adapter: Adapting Segment Anything Model (SAM) for Ambiguous Medical Image Segmentation (Mingzhou Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingzhou Jiang, Jiaying Zhou, Junde Wu, Tianyang Wang, Yueming Jin, Min Xu. (2024)<br><strong>Uncertainty-Aware Adapter: Adapting Segment Anything Model (SAM) for Ambiguous Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Uncertainty-Aware Adapter: Adapting Segment Anything Model (SAM) for Ambiguous Medical Image Segmentation" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 43<br>Keywords: Autoencoder, Benchmarking, Fine-tuning, Fine-tuning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10931v1.pdf filename=2403.10931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Segment Anything Model (SAM) gained significant success in natural image segmentation, and many methods have tried to <b>fine-tune</b> it to medical image segmentation. An efficient way to do so is by using Adapters, specialized modules that learn just a few parameters to tailor SAM specifically for medical images. However, unlike natural images, many tissues and lesions in medical images have blurry boundaries and may be ambiguous. Previous efforts to adapt SAM ignore this challenge and can only predict distinct segmentation.It may mislead clinicians or cause misdiagnosis, especially when encountering rare variants or situations with low model confidence. In this work, we propose a novel module called the Uncertainty-aware Adapter, which efficiently <b>fine-tuning</b> SAM for uncertainty-aware medical image segmentation. Utilizing a conditional <b>variational</b> <b>autoencoder,</b> we encoded stochastic samples to effectively represent the inherent uncertainty in medical imaging. We designed a new module on a standard adapter that utilizes a condition-based strategy to interact with samples to help SAM integrate uncertainty. We evaluated our method on two multi-annotated datasets with different modalities: LIDC-IDRI (lung abnormalities segmentation) and REFUGE2 (optic-cup segmentation). The experimental results show that the proposed model outperforms all the previous methods and achieves the new state-of-the-art (SOTA) on both <b>benchmarks.</b> We also demonstrated that our method can generate diverse segmentation hypotheses that are more realistic as well as heterogeneous.</p></p class="citation"></blockquote><h3 id=24--102153-could-we-generate-cytology-images-from-histopathology-images-an-empirical-study-soumyajyoti-dey-et-al-2024>(2/4 | 102/153) Could We Generate Cytology Images from Histopathology Images? An Empirical Study (Soumyajyoti Dey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soumyajyoti Dey, Sukanta Chakraborty, Utso Guha Roy, Nibaran Das. (2024)<br><strong>Could We Generate Cytology Images from Histopathology Images? An Empirical Study</strong><br><button class=copy-to-clipboard title="Could We Generate Cytology Images from Histopathology Images? An Empirical Study" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Data Augmentation, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10885v1.pdf filename=2403.10885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automation in medical imaging is quite challenging due to the unavailability of annotated datasets and the scarcity of domain experts. In recent years, deep learning techniques have solved some complex medical imaging tasks like disease classification, important object localization, segmentation, etc. However, most of the task requires a large amount of annotated <b>data</b> <b>for</b> their successful implementation. To mitigate the shortage of <b>data,</b> <b>different</b> generative models are proposed for <b>data</b> <b>augmentation</b> purposes which can boost the classification performances. For this, different synthetic medical image <b>data</b> <b>generation</b> models are developed to increase the dataset. Unpaired image-to-image translation models here shift the source domain to the target domain. In the breast malignancy identification domain, FNAC is one of the low-cost low-invasive modalities normally used by medical practitioners. But availability of public datasets in this domain is very poor. Whereas, for automation of cytology images, we need a large amount of annotated <b>data.</b> <b>Therefore</b> synthetic cytology images are generated by translating breast histopathology samples which are publicly available. In this study, we have explored traditional image-to-image transfer models like CycleGAN, and Neural <b>Style</b> <b>Transfer.</b> Further, it is observed that the generated cytology images are quite similar to real breast cytology samples by measuring FID and KID scores.</p></p class="citation"></blockquote><h3 id=34--103153-microdiffusion-implicit-representation-guided-diffusion-for-3d-reconstruction-from-limited-2d-microscopy-projections-mude-hui-et-al-2024>(3/4 | 103/153) MicroDiffusion: Implicit Representation-Guided Diffusion for 3D Reconstruction from Limited 2D Microscopy Projections (Mude Hui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mude Hui, Zihao Wei, Hongru Zhu, Fei Xia, Yuyin Zhou. (2024)<br><strong>MicroDiffusion: Implicit Representation-Guided Diffusion for 3D Reconstruction from Limited 2D Microscopy Projections</strong><br><button class=copy-to-clipboard title="MicroDiffusion: Implicit Representation-Guided Diffusion for 3D Reconstruction from Limited 2D Microscopy Projections" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10815v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10815v1.pdf filename=2403.10815v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Volumetric optical microscopy using non-diffracting beams enables rapid imaging of 3D volumes by projecting them axially to 2D images but lacks crucial depth information. Addressing this, we introduce MicroDiffusion, a pioneering tool facilitating high-quality, depth-resolved 3D volume reconstruction from limited 2D projections. While existing Implicit Neural Representation (INR) models often yield incomplete outputs and Denoising <b>Diffusion</b> <b>Probabilistic</b> <b>Models</b> (DDPM) excel at capturing details, our method integrates INR&rsquo;s structural coherence with DDPM&rsquo;s fine-detail enhancement capabilities. We pretrain an INR model to transform 2D axially-projected images into a preliminary 3D volume. This pretrained INR acts as a global prior guiding DDPM&rsquo;s generative process through a linear interpolation between INR outputs and noise inputs. This strategy enriches the <b>diffusion</b> <b>process</b> with structured 3D information, enhancing detail and reducing noise in localized 2D images. By conditioning the <b>diffusion</b> <b>model</b> on the closest 2D projection, MicroDiffusion substantially enhances fidelity in resulting 3D reconstructions, surpassing INR and standard DDPM outputs with unparalleled image quality and structural fidelity. Our code and dataset are available at <a href=https://github.com/UCSC-VLAA/MicroDiffusion>https://github.com/UCSC-VLAA/MicroDiffusion</a>.</p></p class="citation"></blockquote><h3 id=44--104153-contourdiff-unpaired-image-translation-with-contour-guided-diffusion-models-yuwen-chen-et-al-2024>(4/4 | 104/153) ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion Models (Yuwen Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuwen Chen, Nicholas Konz, Hanxue Gu, Haoyu Dong, Yaqian Chen, Lin Li, Jisoo Lee, Maciej A. Mazurowski. (2024)<br><strong>ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion Models</strong><br><button class=copy-to-clipboard title="ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion Models" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10786v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10786v1.pdf filename=2403.10786v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurately translating medical <b>images</b> <b>across</b> different modalities (e.g., CT to MRI) has numerous downstream clinical and machine learning applications. While several methods have been proposed to achieve this, they often prioritize perceptual quality with respect to output domain features over preserving anatomical fidelity. However, maintaining anatomy during translation is essential for many tasks, e.g., when leveraging masks from the input domain to develop a segmentation model with <b>images</b> <b>translated</b> to the output domain. To address these challenges, we propose ContourDiff, a novel framework that leverages domain-invariant anatomical contour representations of <b>images.</b> <b>These</b> representations are simple to extract from <b>images,</b> <b>yet</b> form precise spatial constraints on their anatomical content. We introduce a <b>diffusion</b> <b>model</b> that converts contour representations of <b>images</b> <b>from</b> arbitrary input domains into <b>images</b> <b>in</b> the output domain of interest. By applying the contour as a constraint at every <b>diffusion</b> <b>sampling</b> step, we ensure the preservation of anatomical content. We evaluate our method by training a segmentation model on <b>images</b> <b>translated</b> from CT to MRI with their original CT masks and testing its performance on real MRIs. Our method outperforms other unpaired <b>image</b> <b>translation</b> methods by a significant margin, furthermore without the need to access any input domain information during training.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--105153-urban-sound-propagation-a-benchmark-for-1-step-generative-modeling-of-complex-physical-systems-martin-spitznagel-et-al-2024>(1/2 | 105/153) Urban Sound Propagation: a Benchmark for 1-Step Generative Modeling of Complex Physical Systems (Martin Spitznagel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Spitznagel, Janis Keuper. (2024)<br><strong>Urban Sound Propagation: a Benchmark for 1-Step Generative Modeling of Complex Physical Systems</strong><br><button class=copy-to-clipboard title="Urban Sound Propagation: a Benchmark for 1-Step Generative Modeling of Complex Physical Systems" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CV, cs-SD, cs.SD, eess-AS<br>Keyword Score: 43<br>Keywords: Diffusion Model, Benchmarking, Generative Adversarial Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10904v1.pdf filename=2403.10904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data-driven modeling of complex physical systems is receiving a growing amount of attention in the <b>simulation</b> and machine learning communities. Since most physical <b>simulations</b> are based on compute-intensive, iterative implementations of differential equation systems, a (partial) replacement with learned, 1-step inference models has the potential for significant speedups in a wide range of application areas. In this context, we present a novel <b>benchmark</b> for the evaluation of 1-step generative learning models in terms of speed and physical correctness. Our Urban Sound Propagation <b>benchmark</b> is based on the physically complex and practically relevant, yet intuitively easy to grasp task of modeling the 2d propagation of waves from a sound source in an urban environment. We provide a dataset with 100k samples, where each sample consists of pairs of real 2d building maps drawn from OpenStreetmap, a parameterized sound source, and a simulated ground truth sound propagation for the given scene. The dataset provides four different <b>simulation</b> tasks with increasing complexity regarding reflection, diffraction and source variance. A first baseline evaluation of common generative U-Net, <b>GAN</b> and <b>Diffusion</b> <b>models</b> shows, that while these models are very well capable of modeling sound propagations in simple cases, the approximation of sub-systems represented by higher order equations systematically fails. Information about the dataset, download instructions and source codes are provided on our anonymous website: <a href=https://www.urban-sound-data.org>https://www.urban-sound-data.org</a>.</p></p class="citation"></blockquote><h3 id=22--106153-speech-driven-personalized-gesture-synthetics-harnessing-automatic-fuzzy-feature-inference-fan-zhang-et-al-2024>(2/2 | 106/153) Speech-driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference (Fan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Zhang, Zhaohan Wang, Xin Lyu, Siyuan Zhao, Mengjian Li, Weidong Geng, Naye Ji, Hui Du, Fuxing Gao, Hao Wu, Shunman Li. (2024)<br><strong>Speech-driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference</strong><br><button class=copy-to-clipboard title="Speech-driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-CV, cs-GR, cs-HC, cs-SD, cs.SD, eess-AS<br>Keyword Score: 29<br>Keywords: Diffusion Model, Benchmarking, Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10805v1.pdf filename=2403.10805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speech-driven gesture generation is an emerging field within virtual human creation. However, a significant challenge lies in accurately determining and processing the multitude of input features (such as acoustic, semantic, emotional, personality, and even subtle unknown features). Traditional approaches, reliant on various explicit feature inputs and complex <b>multimodal</b> processing, constrain the expressiveness of resulting gestures and limit their applicability. To address these challenges, we present Persona-Gestor, a novel end-to-end generative model designed to generate highly personalized 3D full-body gestures solely relying on raw speech audio. The model combines a fuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization (AdaLN) <b>transformer</b> <b>diffusion</b> <b>architecture.</b> The fuzzy feature extractor harnesses a fuzzy inference strategy that automatically infers implicit, continuous fuzzy features. These fuzzy features, represented as a unified latent feature, are fed into the AdaLN <b>transformer.</b> The AdaLN <b>transformer</b> introduces a conditional mechanism that applies a uniform function across all tokens, thereby effectively modeling the correlation between the fuzzy features and the gesture sequence. This module ensures a high level of gesture-speech synchronization while preserving naturalness. Finally, we employ the <b>diffusion</b> <b>model</b> to train and infer various gestures. Extensive subjective and objective evaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model&rsquo;s superior performance to the current state-of-the-art approaches. Persona-Gestor improves the system&rsquo;s usability and generalization capabilities, setting a new <b>benchmark</b> in speech-driven gesture synthesis and broadening the horizon for virtual human technology. Supplementary videos and code can be accessed at <a href=https://zf223669.github.io/Diffmotion-v2-website/>https://zf223669.github.io/Diffmotion-v2-website/</a></p></p class="citation"></blockquote><h2 id=cscr-4>cs.CR (4)</h2><h3 id=14--107153-adversarial-knapsack-and-secondary-effects-of-common-information-for-cyber-operations-jon-goohs-et-al-2024>(1/4 | 107/153) Adversarial Knapsack and Secondary Effects of Common Information for Cyber Operations (Jon Goohs et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jon Goohs, Georgel Savin, Lucas Starks, Josiah Dykstra, William Casey. (2024)<br><strong>Adversarial Knapsack and Secondary Effects of Common Information for Cyber Operations</strong><br><button class=copy-to-clipboard title="Adversarial Knapsack and Secondary Effects of Common Information for Cyber Operations" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Reasoning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10789v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10789v1.pdf filename=2403.10789v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Variations of the Flip-It game have been applied to model network cyber operations. While Flip-It can accurately express uncertainty and loss of control, it imposes no essential resource constraints for operations. Capture the flag (CTF) style competitive games, such as Flip-It , entail uncertainties and loss of control, but also impose realistic constraints on resource use. As such, they bear a closer resemblance to actual cyber operations. We formalize a dynamical network control game for CTF competitions and detail the static game for each time step. The static game can be reformulated as instances of a novel optimization problem called Adversarial Knapsack (AK) or Dueling Knapsack (DK) when there are only two players. We define the Adversarial Knapsack optimization problems as a system of interacting Weighted Knapsack problems, and illustrate its applications to general scenarios involving multiple agents with conflicting optimization goals, e.g., cyber operations and CTF games in particular. Common awareness of the scenario, rewards, and costs will set the stage for a non-cooperative game. Critically, rational players may second guess that their AK solution &ndash; with a better response and higher reward &ndash; is possible if opponents predictably play their AK optimal solutions. Thus, secondary <b>reasoning</b> which such as belief modeling of opponents play can be anticipated for rational players and will introduce a type of non-stability where players maneuver for slight reward differentials. To analyze this, we provide the best-response algorithms and <b>simulation</b> software to consider how rational agents may heuristically search for maneuvers. We further <b>summarize</b> insights offered by the game model by predicting that metrics such as Common Vulnerability Scoring System (CVSS) may intensify the secondary <b>reasoning</b> in cyber operations.</p></p class="citation"></blockquote><h3 id=24--108153-enhancing-iot-security-against-ddos-attacks-through-federated-learning-ghazaleh-shirvani-et-al-2024>(2/4 | 108/153) Enhancing IoT Security Against DDoS Attacks through Federated Learning (Ghazaleh Shirvani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ghazaleh Shirvani, Saeid Ghasemshirazi, Mohammad Ali Alipour. (2024)<br><strong>Enhancing IoT Security Against DDoS Attacks through Federated Learning</strong><br><button class=copy-to-clipboard title="Enhancing IoT Security Against DDoS Attacks through Federated Learning" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Autoencoder, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10968v1.pdf filename=2403.10968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid proliferation of the Internet of Things (IoT) has ushered in transformative connectivity between physical devices and the digital realm. Nonetheless, the escalating threat of Distributed Denial of Service (DDoS) attacks jeopardizes the integrity and reliability of IoT networks. Conventional DDoS mitigation approaches are ill-equipped to handle the intricacies of IoT ecosystems, potentially compromising data privacy. This paper introduces an innovative strategy to bolster the security of IoT networks against DDoS attacks by harnessing the power of <b>Federated</b> <b>Learning</b> that allows multiple IoT devices or edge nodes to collaboratively build a global model while preserving data privacy and minimizing communication overhead. The research aims to investigate <b>Federated</b> <b>Learning&rsquo;s</b> effectiveness in detecting and mitigating DDoS attacks in IoT. Our proposed framework leverages IoT devices&rsquo; collective intelligence for real-time attack detection without compromising sensitive data. This study proposes innovative deep <b>autoencoder</b> approaches for data dimensionality reduction, retraining, and partial selection to enhance the performance and stability of the proposed model. Additionally, two renowned aggregation algorithms, FedAvg and FedAvgM, are employed in this research. Various metrics, including true positive rate, false positive rate, and F1-score, are employed to evaluate the model. The dataset utilized in this research, N-BaIoT, exhibits non-IID data distribution, where data categories are distributed quite differently. The negative impact of these distribution disparities is managed by employing retraining and partial selection techniques, enhancing the final model&rsquo;s stability. Furthermore, evaluation results demonstrate that the FedAvgM aggregation algorithm outperforms FedAvg, indicating that in non-IID datasets, FedAvgM provides better stability and performance.</p></p class="citation"></blockquote><h3 id=34--109153-batch-oriented-element-wise-approximate-activation-for-privacy-preserving-neural-networks-peng-zhang-et-al-2024>(3/4 | 109/153) Batch-oriented Element-wise Approximate Activation for Privacy-Preserving Neural Networks (Peng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Zhang, Ao Duan, Xianglu Zou, Yuhong Liu. (2024)<br><strong>Batch-oriented Element-wise Approximate Activation for Privacy-Preserving Neural Networks</strong><br><button class=copy-to-clipboard title="Batch-oriented Element-wise Approximate Activation for Privacy-Preserving Neural Networks" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10920v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10920v1.pdf filename=2403.10920v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Privacy-Preserving Neural Networks (PPNN) are advanced to perform inference without breaching user privacy, which can serve as an essential tool for medical diagnosis to simultaneously achieve big data utility and privacy protection. As one of the key techniques to enable PPNN, Fully Homomorphic Encryption (FHE) is facing a great challenge that homomorphic operations cannot be easily adapted for non-linear activation calculations. In this paper, batch-oriented element-wise data packing and approximate activation are proposed, which train linear low-degree polynomials to approximate the non-linear activation function - ReLU. Compared with other approximate activation methods, the proposed fine-grained, trainable approximation scheme can effectively reduce the accuracy loss caused by approximation errors. Meanwhile, due to element-wise data packing, a large batch of images can be packed and inferred concurrently, leading to a much higher utility ratio of ciphertext slots. Therefore, although the total inference time increases sharply, the amortized time for each image actually decreases, especially when the batch size increases. Furthermore, <b>knowledge</b> <b>distillation</b> is adopted in the training process to further enhance the inference accuracy. Experiment results show that when ciphertext inference is performed on 4096 input images, compared with the current most efficient channel-wise method, the inference accuracy is improved by 1.65%, and the amortized inference time is reduced by 99.5%.</p></p class="citation"></blockquote><h3 id=44--110153-a-watermark-conditioned-diffusion-model-for-ip-protection-rui-min-et-al-2024>(4/4 | 110/153) A Watermark-Conditioned Diffusion Model for IP Protection (Rui Min et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Min, Sen Li, Hongyang Chen, Minhao Cheng. (2024)<br><strong>A Watermark-Conditioned Diffusion Model for IP Protection</strong><br><button class=copy-to-clipboard title="A Watermark-Conditioned Diffusion Model for IP Protection" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 15<br>Keywords: Diffusion Model, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10893v1.pdf filename=2403.10893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ethical need to protect AI-generated content has been a significant concern in recent years. While existing watermarking strategies have demonstrated success in detecting synthetic content (detection), there has been limited exploration in identifying the users responsible for generating these outputs from a single model (owner identification). In this paper, we focus on both practical scenarios and propose a unified watermarking framework for content copyright protection within the context of <b>diffusion</b> <b>models.</b> Specifically, we consider two parties: the model provider, who grants public access to a <b>diffusion</b> <b>model</b> via an API, and the users, who can solely query the model API and generate images in a <b>black-box</b> <b>manner.</b> Our task is to embed hidden information into the generated contents, which facilitates further detection and owner identification. To tackle this challenge, we propose a Watermark-conditioned <b>Diffusion</b> <b>model</b> called WaDiff, which manipulates the watermark as a conditioned input and incorporates fingerprinting into the generation process. All the generative outputs from our WaDiff carry user-specific information, which can be recovered by an image extractor and further facilitate forensic identification. Extensive experiments are conducted on two popular <b>diffusion</b> <b>models,</b> and we demonstrate that our method is effective and robust in both the detection and owner identification tasks. Meanwhile, our watermarking framework only exerts a negligible impact on the original generation and is more stealthy and efficient in comparison to existing watermarking strategies.</p></p class="citation"></blockquote><h2 id=csmm-2>cs.MM (2)</h2><h3 id=12--111153-mintrec20-a-large-scale-benchmark-dataset-for-multimodal-intent-recognition-and-out-of-scope-detection-in-conversations-hanlei-zhang-et-al-2024>(1/2 | 111/153) MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations (Hanlei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanlei Zhang, Xin Wang, Hua Xu, Qianrui Zhou, Kai Gao, Jianhua Su, jinyue Zhao, Wenrui Li, Yanting Chen. (2024)<br><strong>MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations</strong><br><button class=copy-to-clipboard title="MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-CL, cs-MM, cs.MM<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, ChatGPT, Intent Detection, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10943v1.pdf filename=2403.10943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>intent</b> <b>recognition</b> poses significant challenges, requiring the incorporation of non-verbal modalities from real-world contexts to enhance the comprehension of human intentions. Existing <b>benchmark</b> datasets are limited in scale and suffer from difficulties in handling out-of-scope samples that arise in multi-turn conversational interactions. We introduce MIntRec2.0, a <b>large-scale</b> <b>benchmark</b> <b>dataset</b> for <b>multimodal</b> <b>intent</b> <b>recognition</b> in multi-party conversations. It contains 1,245 dialogues with 15,040 samples, each annotated within a new <b>intent</b> <b>taxonomy</b> of 30 fine-grained classes. Besides 9,304 in-scope samples, it also includes 5,736 out-of-scope samples appearing in multi-turn contexts, which naturally occur in real-world scenarios. Furthermore, we provide comprehensive information on the speakers in each utterance, enriching its utility for multi-party conversational research. We establish a general framework supporting the organization of single-turn and multi-turn dialogue data, modality feature extraction, <b>multimodal</b> fusion, as well as in-scope classification and out-of-scope detection. Evaluation <b>benchmarks</b> are built using classic <b>multimodal</b> fusion methods, <b>ChatGPT,</b> and human evaluators. While existing methods incorporating nonverbal information yield improvements, effectively leveraging context information and detecting out-of-scope samples remains a substantial challenge. Notably, <b>large</b> <b>language</b> <b>models</b> exhibit a significant performance gap compared to humans, highlighting the limitations of machine learning methods in the cognitive <b>intent</b> <b>understanding</b> task. We believe that MIntRec2.0 will serve as a valuable resource, providing a pioneering foundation for research in human-machine conversational interactions, and significantly facilitating related applications. The full dataset and codes are available at <a href=https://github.com/thuiar/MIntRec2.0>https://github.com/thuiar/MIntRec2.0</a>.</p></p class="citation"></blockquote><h3 id=22--112153-quality-aware-dynamic-resolution-adaptation-framework-for-adaptive-video-streaming-amritha-premkumar-et-al-2024>(2/2 | 112/153) Quality-Aware Dynamic Resolution Adaptation Framework for Adaptive Video Streaming (Amritha Premkumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amritha Premkumar, Prajit T Rajendran, Vignesh V Menon, Adam Wieckowski, Benjamin Bross, Detlev Marpe. (2024)<br><strong>Quality-Aware Dynamic Resolution Adaptation Framework for Adaptive Video Streaming</strong><br><button class=copy-to-clipboard title="Quality-Aware Dynamic Resolution Adaptation Framework for Adaptive Video Streaming" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-MM, cs.MM<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10976v1.pdf filename=2403.10976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional per-title encoding schemes aim to optimize encoding resolutions to deliver the highest perceptual quality for each representation. XPSNR is observed to correlate better with the subjective quality of VVC-coded bitstreams. Towards this realization, we predict the average XPSNR of VVC-coded bitstreams using spatiotemporal complexity features of the video and the target encoding configuration using an XGBoost-based model. Based on the predicted XPSNR scores, we introduce a Quality-A ware Dynamic Resolution Adaptation (QADRA) framework for adaptive video streaming applications, where we determine the convex-hull online. Furthermore, keeping the encoding and decoding times within an acceptable threshold is mandatory for smooth and energy-efficient streaming. Hence, QADRA determines the encoding resolution and <b>quantization</b> parameter (QP) for each target bitrate by maximizing XPSNR while constraining the maximum encoding and/ or decoding time below a threshold. QADRA implements a JND-based representation elimination algorithm to remove perceptually redundant representations from the bitrate ladder. QADRA is an open-source Python-based framework published under the GNU GPLv3 license. Github: <a href=https://github.com/PhoenixVideo/QADRA>https://github.com/PhoenixVideo/QADRA</a> Online documentation: <a href=https://phoenixvideo.github.io/QADRA/>https://phoenixvideo.github.io/QADRA/</a></p></p class="citation"></blockquote><h2 id=csit-4>cs.IT (4)</h2><h3 id=14--113153-csi-transfer-from-sub-6g-to-mmwave-reduced-overhead-multi-user-hybrid-beamforming-weicao-deng-et-al-2024>(1/4 | 113/153) CSI Transfer From Sub-6G to mmWave: Reduced-Overhead Multi-User Hybrid Beamforming (Weicao Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weicao Deng, Min Li, Ming-Min Zhao, Min-Jian Zhao, Osvaldo Simeone. (2024)<br><strong>CSI Transfer From Sub-6G to mmWave: Reduced-Overhead Multi-User Hybrid Beamforming</strong><br><button class=copy-to-clipboard title="CSI Transfer From Sub-6G to mmWave: Reduced-Overhead Multi-User Hybrid Beamforming" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 36<br>Keywords: Graph, Graph Neural Network, Benchmarking, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10873v1.pdf filename=2403.10873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hybrid beamforming is vital in modern wireless systems, especially for massive MIMO and millimeter-wave deployments, offering efficient directional transmission with reduced hardware complexity. However, effective beamforming in multi-user scenarios relies heavily on accurate channel state information, the acquisition of which often incurs excessive pilot overhead, degrading system performance. To address this and inspired by the spatial congruence between sub-6GHz (sub-6G) and mmWave channels, we propose a Sub-6G information Aided Multi-User Hybrid Beamforming (SA-MUHBF) framework, avoiding excessive use of pilots. SA-MUHBF employs a <b>convolutional</b> <b>neural</b> <b>network</b> to predict mmWave beamspace from sub-6G channel estimate, followed by a novel multi-layer <b>graph</b> <b>neural</b> <b>network</b> for analog beam selection and a linear minimum mean-square error algorithm for digital beamforming. Numerical results demonstrate that SA-MUHBF efficiently predicts the mmWave beamspace representation and achieves superior spectrum efficiency over state-of-the-art <b>benchmarks.</b> Moreover, SA-MUHBF demonstrates robust performance across varied sub-6G system configurations and exhibits strong generalization to unseen scenarios.</p></p class="citation"></blockquote><h3 id=24--114153-distributed-multi-objective-dynamic-offloading-scheduling-for-air-ground-cooperative-mec-yang-huang-et-al-2024>(2/4 | 114/153) Distributed Multi-Objective Dynamic Offloading Scheduling for Air-Ground Cooperative MEC (Yang Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Huang, Miaomiao Dong, Yijie Mao, Wenqiang Liu, Zhen Gao. (2024)<br><strong>Distributed Multi-Objective Dynamic Offloading Scheduling for Air-Ground Cooperative MEC</strong><br><button class=copy-to-clipboard title="Distributed Multi-Objective Dynamic Offloading Scheduling for Air-Ground Cooperative MEC" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10927v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10927v1.pdf filename=2403.10927v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Utilizing unmanned aerial vehicles (UAVs) with edge server to assist terrestrial mobile edge computing (MEC) has attracted tremendous attention. Nevertheless, state-of-the-art schemes based on deterministic optimizations or single-objective <b>reinforcement</b> <b>learning</b> (RL) cannot reduce the backlog of task bits and simultaneously improve energy efficiency in highly dynamic network environments, where the design problem amounts to a sequential decision-making problem. In order to address the aforementioned problems, as well as the curses of dimensionality introduced by the growing number of terrestrial terrestrial users, this paper proposes a distributed multi-objective (MO) dynamic trajectory planning and offloading scheduling scheme, integrated with MORL and the kernel method. The design of n-step return is also applied to average fluctuations in the backlog. Numerical results reveal that the n-step return can benefit the proposed kernel-based approach, achieving significant improvement in the long-term average backlog performance, compared to the conventional 1-step return design. Due to such design and the kernel-based neural network, to which decision-making features can be continuously added, the kernel-based approach can outperform the approach based on fully-connected deep neural network, yielding improvement in energy consumption and the backlog performance, as well as a significant reduction in decision-making and online learning time.</p></p class="citation"></blockquote><h3 id=34--115153-simultaneously-transmitting-and-reflecting-reconfigurable-intelligent-surfaces-empowered-cooperative-rate-splitting-with-user-relaying-kangchun-zhao-et-al-2024>(3/4 | 115/153) Simultaneously Transmitting and Reflecting Reconfigurable Intelligent Surfaces Empowered Cooperative Rate Splitting with User Relaying (Kangchun Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kangchun Zhao, Yijie Mao, Yuanmin Shi. (2024)<br><strong>Simultaneously Transmitting and Reflecting Reconfigurable Intelligent Surfaces Empowered Cooperative Rate Splitting with User Relaying</strong><br><button class=copy-to-clipboard title="Simultaneously Transmitting and Reflecting Reconfigurable Intelligent Surfaces Empowered Cooperative Rate Splitting with User Relaying" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10921v1.pdf filename=2403.10921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we unveil the advantages of synergizing cooperative rate splitting (CRS) with user relaying and simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR RIS). Specifically, we propose a novel STAR RIS-assisted CRS transmission framework, featuring six unique transmission modes that leverage various combination of the relaying protocols (including full duplex-FD and half duplex-HD) and the STAR RIS configuration protocols (including energy splitting-ES, mode switching-MS, and time splitting-TS). With the objective of maximizing the minimum user rate, we then propose a unified successive convex approximation (SCA)-based alternative optimization (AO) algorithm to jointly optimize the transmit active beamforming, common rate allocation, STAR RIS passive beamforming, as well as time allocation (for HD or TS protocols) subject to the transmit power constraint at the base station (BS) and the law of energy conservation at the STAR RIS. To alleviate the computational burden, we further propose a low-complexity algorithm that incorporates a closed-form passive beamforming design. Numerical results show that our proposed framework significantly enhances user <b>fairness</b> compared with conventional CRS schemes without STAR RIS or other STAR RIS empowered multiple access schemes. Moreover, the proposed low-complexity algorithm dramatically reduces the computational complexity while achieving very close performance to the AO method.</p></p class="citation"></blockquote><h3 id=44--116153-bounding-the-graph-capacity-with-quantum-mechanics-and-finite-automata-alexander-meiburg-2024>(4/4 | 116/153) Bounding the Graph Capacity with Quantum Mechanics and Finite Automata (Alexander Meiburg, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Meiburg. (2024)<br><strong>Bounding the Graph Capacity with Quantum Mechanics and Finite Automata</strong><br><button class=copy-to-clipboard title="Bounding the Graph Capacity with Quantum Mechanics and Finite Automata" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10985v1.pdf filename=2403.10985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The zero-error capacity of a channel (or Shannon capacity of a <b>graph)</b> quantifies how much information can be transmitted with no risk of error. In contrast to the Shannon capacity of a channel, the zero-error capacity has not even been shown to be computable: we have no convergent upper bounds. In this work, we present a new quantity, the zero-error {\em unitary} capacity, and show that it can be succinctly represented as the tensor product value of a quantum game. By studying the structure of finite automata, we show that the unitary capacity is within a controllable factor of the zero-error capacity. This allows new upper bounds through the sum-of-squares hierarchy, which converges to the commuting operator value of the game. Under the conjecture that the commuting operator and tensor product value of this game are equal, this would yield an algorithm for computing the zero-error capacity.</p></p class="citation"></blockquote><h2 id=q-biogn-1>q-bio.GN (1)</h2><h3 id=11--117153-stmcdi-masked-conditional-diffusion-model-with-graph-neural-network-for-spatial-transcriptomics-data-imputation-xiaoyu-li-et-al-2024>(1/1 | 117/153) stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for Spatial Transcriptomics Data Imputation (Xiaoyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyu Li, Wenwen Min, Shunfang Wang, Changmiao Wang, Taosheng Xu. (2024)<br><strong>stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for Spatial Transcriptomics Data Imputation</strong><br><button class=copy-to-clipboard title="stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for Spatial Transcriptomics Data Imputation" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.GN<br>Categories: cs-AI, cs-LG, q-bio-GN, q-bio.GN<br>Keyword Score: 33<br>Keywords: Diffusion Model, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10863v1.pdf filename=2403.10863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spatially resolved transcriptomics represents a significant advancement in single-cell analysis by offering both gene expression data and their corresponding physical locations. However, this high degree of spatial resolution entails a drawback, as the resulting spatial transcriptomic data at the cellular level is notably plagued by a high incidence of missing values. Furthermore, most existing imputation methods either overlook the spatial information between spots or compromise the overall gene expression data distribution. To address these challenges, our primary focus is on effectively utilizing the spatial location information within spatial transcriptomic data to impute missing values, while preserving the overall data distribution. We introduce \textbf{stMCDI}, a novel conditional <b>diffusion</b> <b>model</b> for spatial transcriptomics data imputation, which employs a denoising network trained using randomly masked data portions as guidance, with the unmasked data serving as conditions. Additionally, it utilizes a <b>GNN</b> encoder to integrate the spatial position information, thereby enhancing model performance. The results obtained from spatial transcriptomics datasets elucidate the performance of our methods relative to existing approaches.</p></p class="citation"></blockquote><h2 id=cshc-2>cs.HC (2)</h2><h3 id=12--118153-human-centered-ai-for-indian-legal-text-analytics-sudipto-ghosh-et-al-2024>(1/2 | 118/153) Human Centered AI for Indian Legal Text Analytics (Sudipto Ghosh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sudipto Ghosh, Devanshu Verma, Balaji Ganesan, Purnima Bindal, Vikas Kumar, Vasudha Bhatnagar. (2024)<br><strong>Human Centered AI for Indian Legal Text Analytics</strong><br><button class=copy-to-clipboard title="Human Centered AI for Indian Legal Text Analytics" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10944v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10944v1.pdf filename=2403.10944v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Legal research is a crucial task in the practice of law. It requires intense human effort and intellectual prudence to research a legal case and prepare arguments. Recent boom in <b>generative</b> <b>AI</b> has not translated to proportionate rise in impactful legal applications, because of low trustworthiness and and the scarcity of specialized datasets for training <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> This position paper explores the potential of <b>LLMs</b> within Legal Text Analytics (LTA), highlighting specific areas where the integration of human expertise can significantly enhance their performance to match that of experts. We introduce a novel dataset and describe a human centered, compound AI system that principally incorporates human inputs for performing LTA tasks with <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=22--119153-from-melting-pots-to-misrepresentations-exploring-harms-in-generative-ai-sanjana-gautam-et-al-2024>(2/2 | 119/153) From Melting Pots to Misrepresentations: Exploring Harms in Generative AI (Sanjana Gautam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanjana Gautam, Pranav Narayanan Venkit, Sourojit Ghosh. (2024)<br><strong>From Melting Pots to Misrepresentations: Exploring Harms in Generative AI</strong><br><button class=copy-to-clipboard title="From Melting Pots to Misrepresentations: Exploring Harms in Generative AI" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CY, cs-HC, cs-LG, cs.HC<br>Keyword Score: 30<br>Keywords: Generative AI, GPT, Gemini<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10776v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10776v1.pdf filename=2403.10776v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the widespread adoption of advanced <b>generative</b> <b>models</b> such as <b>Gemini</b> and <b>GPT,</b> there has been a notable increase in the incorporation of such models into sociotechnical systems, categorized under AI-as-a-Service (AIaaS). Despite their versatility across diverse sectors, concerns persist regarding discriminatory tendencies within these models, particularly favoring selected `majority&rsquo; demographics across various sociodemographic dimensions. Despite widespread calls for diversification of media representations, marginalized racial and ethnic groups continue to face persistent distortion, stereotyping, and neglect within the AIaaS context. In this work, we provide a critical summary of the state of research in the context of social harms to lead the conversation to focus on their implications. We also present open-ended research questions, guided by our discussion, to help define future research pathways.</p></p class="citation"></blockquote><h2 id=csir-1>cs.IR (1)</h2><h3 id=11--120153-improving-the-robustness-of-dense-retrievers-against-typos-via-multi-positive-contrastive-learning-georgios-sidiropoulos-et-al-2024>(1/1 | 120/153) Improving the Robustness of Dense Retrievers Against Typos via Multi-Positive Contrastive Learning (Georgios Sidiropoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Georgios Sidiropoulos, Evangelos Kanoulas. (2024)<br><strong>Improving the Robustness of Dense Retrievers Against Typos via Multi-Positive Contrastive Learning</strong><br><button class=copy-to-clipboard title="Improving the Robustness of Dense Retrievers Against Typos via Multi-Positive Contrastive Learning" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Data Augmentation, Dense Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10939v1.pdf filename=2403.10939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Dense</b> <b>retrieval</b> has become the new paradigm in passage retrieval. Despite its effectiveness on typo-free queries, it is not robust when dealing with queries that contain typos. Current works on improving the typo-robustness of <b>dense</b> <b>retrievers</b> combine (i) <b>data</b> <b>augmentation</b> to obtain the typoed queries during training time with (ii) additional robustifying subtasks that aim to align the original, typo-free queries with their typoed variants. Even though multiple typoed variants are available as positive samples per query, some methods assume a single positive sample and a set of negative ones per anchor and tackle the robustifying subtask with <b>contrastive</b> <b>learning;</b> therefore, making insufficient use of the multiple positives (typoed queries). In contrast, in this work, we argue that all available positives can be used at the same time and employ <b>contrastive</b> <b>learning</b> that supports multiple positives (multi-positive). Experimental results on two datasets show that our proposed approach of leveraging all positives simultaneously and employing multi-positive <b>contrastive</b> <b>learning</b> on the robustifying subtask yields improvements in robustness against using <b>contrastive</b> <b>learning</b> with a single positive.</p></p class="citation"></blockquote><h2 id=eessas-3>eess.AS (3)</h2><h3 id=13--121153-initial-decoding-with-minimally-augmented-language-model-for-improved-lattice-rescoring-in-low-resource-asr-savitha-murthy-et-al-2024>(1/3 | 121/153) Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR (Savitha Murthy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Savitha Murthy, Dinkar Sitaram. (2024)<br><strong>Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR</strong><br><button class=copy-to-clipboard title="Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-LG, eess-AS, eess.AS<br>Keyword Score: 30<br>Keywords: Low-Resource, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10937v1.pdf filename=2403.10937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the problem of improving <b>speech</b> <b>recognition</b> accuracy with lattice rescoring in <b>low-resource</b> languages where the baseline language model is insufficient for generating inclusive lattices. We minimally augment the baseline language model with word unigram counts that are present in a larger text corpus of the target language but absent in the baseline. The lattices generated after decoding with such an augmented baseline language model are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada) relative word error reduction with our proposed method. This reduction in word error rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word error reduction obtained by decoding with full Wikipedia text augmented language mode while our approach consumes only 1/8th the memory. We demonstrate that our method is comparable with various text selection-based language model augmentation and also consistent for data sets of different sizes. Our approach is applicable for training <b>speech</b> <b>recognition</b> systems under low resource conditions where <b>speech</b> <b>data</b> and compute resources are insufficient, while there is a large text corpus that is available in the target language. Our research involves addressing the issue of out-of-vocabulary words of the baseline in general and does not focus on resolving the absence of named entities. Our proposed method is simple and yet computationally less expensive.</p></p class="citation"></blockquote><h3 id=23--122153-fine-grained-engine-fault-sound-event-detection-using-multimodal-signals-dennis-fedorishin-et-al-2024>(2/3 | 122/153) Fine-Grained Engine Fault Sound Event Detection Using Multimodal Signals (Dennis Fedorishin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dennis Fedorishin, Livio Forte III, Philip Schneider, Srirangaraj Setlur, Venu Govindaraju. (2024)<br><strong>Fine-Grained Engine Fault Sound Event Detection Using Multimodal Signals</strong><br><button class=copy-to-clipboard title="Fine-Grained Engine Fault Sound Event Detection Using Multimodal Signals" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11037v1.pdf filename=2403.11037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sound <b>event</b> <b>detection</b> (SED) is an active area of audio research that aims to detect the temporal occurrence of sounds. In this paper, we apply SED to engine fault detection by introducing a <b>multimodal</b> SED framework that detects fine-grained engine faults of automobile engines using audio and accelerometer-recorded vibration. We first introduce the problem of engine fault SED on a dataset collected from a large variety of vehicles with expertly-labeled engine fault sound <b>events.</b> <b>Next,</b> we propose a SED model to temporally detect ten fine-grained engine faults that occur within vehicle engines and further explore a pretraining strategy using a large-scale weakly-labeled engine fault dataset. Through multiple evaluations, we show our proposed framework is able to effectively detect engine fault sound <b>events.</b> <b>Finally,</b> we investigate the interaction and characteristics of each modality and show that fusing features from audio and vibration improves overall engine fault SED capabilities.</p></p class="citation"></blockquote><h3 id=33--123153-refining-knowledge-transfer-on-audio-image-temporal-agreement-for-audio-text-cross-retrieval-shunsuke-tsubaki-et-al-2024>(3/3 | 123/153) Refining Knowledge Transfer on Audio-Image Temporal Agreement for Audio-Text Cross Retrieval (Shunsuke Tsubaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shunsuke Tsubaki, Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, Keisuke Imoto. (2024)<br><strong>Refining Knowledge Transfer on Audio-Image Temporal Agreement for Audio-Text Cross Retrieval</strong><br><button class=copy-to-clipboard title="Refining Knowledge Transfer on Audio-Image Temporal Agreement for Audio-Text Cross Retrieval" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 10<br>Keywords: Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10756v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10756v1.pdf filename=2403.10756v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The aim of this research is to refine <b>knowledge</b> <b>transfer</b> on audio-image temporal agreement for audio-text cross retrieval. To address the limited availability of paired non-speech audio-text data, learning methods for transferring the <b>knowledge</b> <b>acquired</b> from a large amount of paired audio-image data to shared audio-text representation have been investigated, suggesting the importance of how audio-image co-occurrence is learned. Conventional approaches in audio-image learning assign a single image randomly selected from the corresponding video stream to the entire audio clip, assuming their co-occurrence. However, this method may not accurately capture the temporal agreement between the target audio and image because a single image can only represent a snapshot of a scene, though the target audio changes from moment to moment. To address this problem, we propose two methods for audio and image matching that effectively capture the temporal information: (i) Nearest Match wherein an image is selected from multiple time frames based on similarity with audio, and (ii) Multiframe Match wherein audio and image pairs of multiple time frames are used. Experimental results show that method (i) improves the audio-text retrieval performance by selecting the nearest image that aligns with the audio information and transferring the learned <b>knowledge.</b> <b>Conversely,</b> method (ii) improves the performance of audio-image retrieval while not showing significant improvements in audio-text retrieval performance. These results indicate that refining audio-image temporal agreement may contribute to better <b>knowledge</b> <b>transfer</b> to audio-text retrieval.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--124153-inverse-learning-of-black-box-aggregator-for-robust-nash-equilibrium-guanpu-chen-et-al-2024>(1/1 | 124/153) Inverse learning of black-box aggregator for robust Nash equilibrium (Guanpu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guanpu Chen, Gehui Xu, Fengxiang He, Dacheng Tao, Thomas Parisini, Karl Henrik Johansson. (2024)<br><strong>Inverse learning of black-box aggregator for robust Nash equilibrium</strong><br><button class=copy-to-clipboard title="Inverse learning of black-box aggregator for robust Nash equilibrium" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-SY, cs.GT, eess-SY, math-OC<br>Keyword Score: 25<br>Keywords: Black Box, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10980v1.pdf filename=2403.10980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this note, we investigate the robustness of Nash equilibria (NE) in multi-player aggregative games with coupling constraints. There are many algorithms for computing an NE of an aggregative game given a known aggregator. When the coupling parameters are affected by uncertainty, robust NE need to be computed. We consider a scenario where players&rsquo; weight in the aggregator is unknown, making the aggregator kind of &ldquo;a <b>black</b> <b>box&rdquo;.</b> We pursue a suitable learning approach to estimate the unknown aggregator by proposing an inverse variational inequality-based relationship. We then utilize the counterpart to reconstruct the game and obtain first-order conditions for robust NE in the worst case. Furthermore, we characterize the generalization property of the learning methodology via an upper bound on the violation probability. <b>Simulation</b> experiments show the effectiveness of the proposed inverse learning approach.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--125153-computational-seismic-fracture-synthesis-of-tidal-barrage-using-enhanced-isotropic-plasticity-damage-mechanics-and-coupled-lagrangian-eulerian-multiphase-interaction-sayan-chowdhury-et-al-2024>(1/1 | 125/153) Computational Seismic Fracture Synthesis of Tidal Barrage using Enhanced Isotropic Plasticity Damage Mechanics and Coupled Lagrangian-Eulerian Multiphase Interaction (Sayan Chowdhury et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayan Chowdhury, Satya Kiran Raju Alluri, Jayaprakash J, Fang Yenn Teo, Umashankar M. (2024)<br><strong>Computational Seismic Fracture Synthesis of Tidal Barrage using Enhanced Isotropic Plasticity Damage Mechanics and Coupled Lagrangian-Eulerian Multiphase Interaction</strong><br><button class=copy-to-clipboard title="Computational Seismic Fracture Synthesis of Tidal Barrage using Enhanced Isotropic Plasticity Damage Mechanics and Coupled Lagrangian-Eulerian Multiphase Interaction" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-NA, math-DS, math-NA, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10905v1.pdf filename=2403.10905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mega-engineered hydraulic structures like dams and barrages are critically sensitive to strong ground motion if constructed within the vicinity of triggered fault lines. Collapse post excessive deformation leads to severe environmental impact. In this study, fracture corresponding to the response of a concrete tidal barrage to strong ground motion is analyzed along with behavioral effects due to reservoir-barrage dynamic interaction. An enhanced version of the plasticity damage mechanical model, which includes effects due to degradation of elastic stiffness of concrete as well as restoration of fracture energy losses is assigned as material behavior. The fluid-structure interaction is solved using an idealized Lagrangian-Eulerian formulation. The proposed improvised numerical formulations are validated against <b>benchmark</b> <b>simulations</b> performed on the Koyna dam situated in Maharashtra, India and the results captured are upto 94% accurate. Finite element <b>simulation</b> of a tidal barrage is performed using a computationally stable mesh with global grid to length ratio of 4.2. The yield surface captured is elliptical in nature and fracture is observed to be propagating from bottom of gate housing covering upto four nodal integration points.</p></p class="citation"></blockquote><h2 id=eesssy-3>eess.SY (3)</h2><h3 id=13--126153-stochastic-lp-string-stability-analysis-in-predecessor-following-platoons-under-packet-losses-alejandro-i-maass-et-al-2024>(1/3 | 126/153) Stochastic Lp string stability analysis in predecessor-following platoons under packet losses (Alejandro I. Maass et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alejandro I. Maass, Francisco J. Vargas, Andres A. Peters, Juan I. Yuz. (2024)<br><strong>Stochastic Lp string stability analysis in predecessor-following platoons under packet losses</strong><br><button class=copy-to-clipboard title="Stochastic Lp string stability analysis in predecessor-following platoons under packet losses" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11043v1.pdf filename=2403.11043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study (homogeneous) predecessor-following platoons in which the vehicle-to-vehicle (V2V) communications are affected by random packet losses. We model the overall platoon as a stochastic hybrid system and analyse its string stability via a small-gain approach. For nonlinear platoons, we illustrate how the different elements of the platoon have an impact on string stability, such as platoon topology and vehicle scheduling. For linear time-invariant platoons, we provide an explicit string stability condition that illustrates the interplay between the channel success probability, transmission rate, and time headway constant. Lastly, we illustrate our results by numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=23--127153-mpc-for-tracking-applied-to-rendezvous-with-non-cooperative-tumbling-targets-ensuring-stability-and-feasibility-jose-antonio-rebollo-et-al-2024>(2/3 | 127/153) MPC for Tracking applied to rendezvous with non-cooperative tumbling targets ensuring stability and feasibility (Jose Antonio Rebollo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jose Antonio Rebollo, Rafael Vazquez, Ignacio Alvarado, Daniel Limon. (2024)<br><strong>MPC for Tracking applied to rendezvous with non-cooperative tumbling targets ensuring stability and feasibility</strong><br><button class=copy-to-clipboard title="MPC for Tracking applied to rendezvous with non-cooperative tumbling targets ensuring stability and feasibility" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10986v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10986v1.pdf filename=2403.10986v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A Model Predictive Controller for Tracking is introduced for rendezvous with non-cooperative tumbling targets in active debris removal applications. The target&rsquo;s three-dimensional non-periodic rotational dynamics as well as other state and control constraints are considered. The approach is based on applying an intermediate coordinate transformation that eliminates the time-dependency due to rotations in the constraints. The control law is then found as the solution to a QP problem with linear constraints and dynamics, as derived from the HCW equations, that provides feasibility and stability guarantees by means of a terminal LQR and dead-beat region. The proposed control algorithm performs well in a realistic <b>simulation</b> scenario, namely a near rendezvous with the Envisat spacecraft.</p></p class="citation"></blockquote><h3 id=33--128153-extended-kalman-filtering-for-recursive-online-discrete-time-inverse-optimal-control-tian-zhao-et-al-2024>(3/3 | 128/153) Extended Kalman Filtering for Recursive Online Discrete-Time Inverse Optimal Control (Tian Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Zhao, Timothy L. Molloy. (2024)<br><strong>Extended Kalman Filtering for Recursive Online Discrete-Time Inverse Optimal Control</strong><br><button class=copy-to-clipboard title="Extended Kalman Filtering for Recursive Online Discrete-Time Inverse Optimal Control" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10841v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10841v1.pdf filename=2403.10841v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We formulate the <b>discrete-time</b> <b>inverse</b> optimal control problem of inferring unknown parameters in the objective function of an optimal control problem from measurements of optimal states and controls as a nonlinear filtering problem. This formulation enables us to propose a novel extended Kalman filter (EKF) for solving inverse optimal control problems in a computationally efficient recursive online manner that requires only a single pass through the measurement data. Importantly, we show that the Jacobians required to implement our EKF can be computed efficiently by exploiting recent Pontryagin differentiable programming results, and that our consideration of an EKF enables the development of first-of-their-kind theoretical error guarantees for online inverse optimal control with noisy incomplete measurements. Our proposed EKF is shown to be significantly faster than an alternative unscented Kalman filter-based approach.</p></p class="citation"></blockquote><h2 id=physicsoptics-1>physics.optics (1)</h2><h3 id=11--129153-multiplane-quantitative-phase-imaging-using-a-wavelength-multiplexed-diffractive-optical-processor-che-yung-shen-et-al-2024>(1/1 | 129/153) Multiplane Quantitative Phase Imaging Using a Wavelength-Multiplexed Diffractive Optical Processor (Che-Yung Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Che-Yung Shen, Jingxi Li, Tianyi Gan, Yuhang Li, Langxing Bai, Mona Jarrahi, Aydogan Ozcan. (2024)<br><strong>Multiplane Quantitative Phase Imaging Using a Wavelength-Multiplexed Diffractive Optical Processor</strong><br><button class=copy-to-clipboard title="Multiplane Quantitative Phase Imaging Using a Wavelength-Multiplexed Diffractive Optical Processor" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.optics<br>Categories: cs-CV, cs-NE, physics-app-ph, physics-optics, physics.optics<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11035v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11035v1.pdf filename=2403.11035v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantitative phase imaging (QPI) is a label-free technique that provides optical path length information for transparent specimens, finding utility in biology, materials science, and engineering. Here, we present quantitative phase imaging of a 3D stack of phase-only objects using a wavelength-multiplexed diffractive optical processor. Utilizing multiple spatially engineered diffractive layers trained through deep learning, this diffractive processor can transform the phase distributions of multiple 2D objects at various axial positions into intensity patterns, each encoded at a unique wavelength channel. These wavelength-multiplexed patterns are projected onto a single field-of-view (FOV) at the output plane of the diffractive processor, enabling the capture of quantitative phase distributions of input objects located at different axial planes using an intensity-only image sensor. Based on numerical <b>simulations,</b> we show that our diffractive processor could simultaneously achieve all-optical quantitative phase imaging across several distinct axial planes at the input by scanning the illumination wavelength. A proof-of-concept experiment with a 3D-fabricated diffractive processor further validated our approach, showcasing successful imaging of two distinct phase objects at different axial positions by scanning the illumination wavelength in the terahertz spectrum. Diffractive network-based multiplane QPI designs can open up new avenues for compact on-chip phase imaging and sensing devices.</p></p class="citation"></blockquote><h2 id=physicsplasm-ph-1>physics.plasm-ph (1)</h2><h3 id=11--130153-stellarator-optimization-with-constraints-rory-conlin-et-al-2024>(1/1 | 130/153) Stellarator Optimization with Constraints (Rory Conlin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rory Conlin, Patrick Kim, Daniel W. Dudt, Dario Panici, Egemen Kolemen. (2024)<br><strong>Stellarator Optimization with Constraints</strong><br><button class=copy-to-clipboard title="Stellarator Optimization with Constraints" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.plasm-ph<br>Categories: cs-NA, math-NA, math-OC, physics-comp-ph, physics-plasm-ph, physics.plasm-ph<br>Keyword Score: 20<br>Keywords: Question Answering, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11033v1.pdf filename=2403.11033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work we consider the problem of optimizing a stellarator subject to hard constraints on the design variables and physics properties of the equilibrium. We survey current numerical methods for handling these constraints, and <b>summarize</b> a number of methods from the wider optimization community that have not been used extensively for stellarator optimization thus far. We demonstrate the utility of new methods of constrained optimization by optimizing a <b>QA</b> stellarator for favorable physics properties while preventing strong shaping of the plasma boundary which can be difficult to create with external current sources.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--131153-circle-packing-problem-using-nature-inspired-optimization-techniques-pulkit-mundra-et-al-2024>(1/1 | 131/153) Circle Packing Problem Using Nature-Inspired Optimization Techniques (Pulkit Mundra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pulkit Mundra, Veni Goyal, Kusum Deep. (2024)<br><strong>Circle Packing Problem Using Nature-Inspired Optimization Techniques</strong><br><button class=copy-to-clipboard title="Circle Packing Problem Using Nature-Inspired Optimization Techniques" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-CE, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10965v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10965v1.pdf filename=2403.10965v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper deals with the problem of circle packing, in which the largest radii circle is to be fit in a confined space filled with arbitrary circles of different radii and centers. A circle packing problem is one of a variety of cutting and packing problems. We suggest four different nature-inspired Meta-heuristic algorithms to solve this problem. Algorithms are based on the social behavior of other biology species such as birds, wolves, fireflies, and bats. Moreover, recent advancements in these algorithms are also considered for problem-solving. The circle packing problem is one of the NP-hard problems. It is challenging to solve NP-hard problems exactly, so the proposed algorithms provide an approximate solution within the allotted time. Standard statistical parameters are used for comparison, and <b>simulation</b> and results indicate that the problem is highly non-linear and sensitive.</p></p class="citation"></blockquote><h2 id=statml-4>stat.ML (4)</h2><h3 id=14--132153-the-fallacy-of-minimizing-local-regret-in-the-sequential-task-setting-ziping-xu-et-al-2024>(1/4 | 132/153) The Fallacy of Minimizing Local Regret in the Sequential Task Setting (Ziping Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziping Xu, Kelly W. Zhang, Susan A. Murphy. (2024)<br><strong>The Fallacy of Minimizing Local Regret in the Sequential Task Setting</strong><br><button class=copy-to-clipboard title="The Fallacy of Minimizing Local Regret in the Sequential Task Setting" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10946v1.pdf filename=2403.10946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of <b>Reinforcement</b> <b>Learning</b> (RL), online RL is often conceptualized as an optimization problem, where an algorithm interacts with an unknown environment to minimize cumulative regret. In a stationary setting, strong theoretical guarantees, like a sublinear ($\sqrt{T}$) regret bound, can be obtained, which typically implies the convergence to an optimal policy and the cessation of exploration. However, these theoretical setups often oversimplify the complexities encountered in real-world RL implementations, where tasks arrive sequentially with substantial changes between tasks and the algorithm may not be allowed to adaptively learn within certain tasks. We study the changes beyond the outcome distributions, encompassing changes in the reward designs (mappings from outcomes to rewards) and the permissible policy spaces. Our results reveal the fallacy of myopically minimizing regret within each task: obtaining optimal regret rates in the early tasks may lead to worse rates in the subsequent ones, even when the outcome distributions stay the same. To realize the optimal cumulative regret bound across all the tasks, the algorithm has to overly explore in the earlier tasks. This theoretical insight is practically significant, suggesting that due to unanticipated changes (e.g., rapid technological development or <b>human-in-the-loop</b> involvement) between tasks, the algorithm needs to explore more than it would in the usual stationary setting within each task. Such implication resonates with the common practice of using clipped policies in mobile health clinical trials and maintaining a fixed rate of $\epsilon$-greedy exploration in robotic learning.</p></p class="citation"></blockquote><h3 id=24--133153-function-space-parameterization-of-neural-networks-for-sequential-learning-aidan-scannell-et-al-2024>(2/4 | 133/153) Function-space Parameterization of Neural Networks for Sequential Learning (Aidan Scannell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aidan Scannell, Riccardo Mereu, Paul Chang, Ella Tamir, Joni Pajarinen, Arno Solin. (2024)<br><strong>Function-space Parameterization of Neural Networks for Sequential Learning</strong><br><button class=copy-to-clipboard title="Function-space Parameterization of Neural Networks for Sequential Learning" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10929v1.pdf filename=2403.10929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequential learning paradigms pose challenges for gradient-based deep learning due to difficulties incorporating new data and retaining prior knowledge. While Gaussian processes elegantly tackle these problems, they struggle with scalability and handling rich inputs, such as images. To address these issues, we introduce a technique that converts neural networks from weight space to function space, through a dual parameterization. Our parameterization offers: (i) a way to scale function-space methods to large data sets via sparsification, (ii) retention of prior knowledge when access to past data is limited, and (iii) a mechanism to incorporate new data without retraining. Our experiments demonstrate that we can retain knowledge in <b>continual</b> <b>learning</b> and incorporate new data efficiently. We further show its strengths in uncertainty quantification and guiding exploration in model-based RL. Further information and code is available on the project website.</p></p class="citation"></blockquote><h3 id=34--134153-neural-kernel-conditional-mean-embeddings-eiki-shimizu-et-al-2024>(3/4 | 134/153) Neural-Kernel Conditional Mean Embeddings (Eiki Shimizu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eiki Shimizu, Kenji Fukumizu, Dino Sejdinovic. (2024)<br><strong>Neural-Kernel Conditional Mean Embeddings</strong><br><button class=copy-to-clipboard title="Neural-Kernel Conditional Mean Embeddings" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10859v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10859v1.pdf filename=2403.10859v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Kernel conditional mean embeddings (CMEs) offer a powerful framework for representing conditional distribution, but they often face scalability and expressiveness challenges. In this work, we propose a new method that effectively combines the strengths of deep learning with CMEs in order to address these challenges. Specifically, our approach leverages the end-to-end neural network (NN) optimization framework using a kernel-based objective. This design circumvents the computationally expensive Gram matrix inversion required by current CME methods. To further enhance performance, we provide efficient strategies to optimize the remaining kernel hyperparameters. In conditional density estimation tasks, our NN-CME hybrid achieves competitive performance and often surpasses existing deep learning-based methods. Lastly, we showcase its remarkable versatility by seamlessly integrating it into <b>reinforcement</b> <b>learning</b> (RL) contexts. Building on Q-learning, our approach naturally leads to a new variant of distributional RL methods, which demonstrates consistent effectiveness across different environments.</p></p class="citation"></blockquote><h3 id=44--135153-a-primal-dual-algorithm-for-faster-distributionally-robust-optimization-ronak-mehta-et-al-2024>(4/4 | 135/153) A Primal-Dual Algorithm for Faster Distributionally Robust Optimization (Ronak Mehta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ronak Mehta, Jelena Diakonikolas, Zaid Harchaoui. (2024)<br><strong>A Primal-Dual Algorithm for Faster Distributionally Robust Optimization</strong><br><button class=copy-to-clipboard title="A Primal-Dual Algorithm for Faster Distributionally Robust Optimization" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-OC, stat-ML, stat.ML<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10763v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10763v1.pdf filename=2403.10763v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the penalized distributionally robust optimization (DRO) problem with a closed, convex uncertainty set, a setting that encompasses the $f$-DRO, Wasserstein-DRO, and spectral/$L$-risk formulations used in practice. We present Drago, a stochastic primal-dual algorithm that achieves a state-of-the-art linear convergence rate on strongly convex-strongly concave DRO problems. The method combines both randomized and cyclic components with mini-batching, which effectively handles the unique asymmetric nature of the primal and dual problems in DRO. We support our theoretical results with numerical <b>benchmarks</b> in classification and regression.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--136153-modelling-co-evolution-of-resource-feedback-and-social-network-dynamics-in-human-environmental-systems-meghdad-saeedian-et-al-2024>(1/1 | 136/153) Modelling co-evolution of resource feedback and social network dynamics in human-environmental systems (Meghdad Saeedian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meghdad Saeedian, Chengyi Tu, Fabio Menegazzo, Paolo D&rsquo;Odorico, Sandro Azaele, Samir Suweis. (2024)<br><strong>Modelling co-evolution of resource feedback and social network dynamics in human-environmental systems</strong><br><button class=copy-to-clipboard title="Modelling co-evolution of resource feedback and social network dynamics in human-environmental systems" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-IT, math-IT, nlin-AO, physics-soc-ph, physics.soc-ph, stat-AP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10938v1.pdf filename=2403.10938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Games with environmental feedback have become a crucial area of study across various scientific domains, modelling the dynamic interplay between human decisions and environmental changes, and highlighting the consequences of our choices on natural resources and biodiversity. In this work, we propose a co-evolutionary model for human-environment systems that incorporates the effects of knowledge feedback and social interaction on the sustainability of common pool resources. The model represents consumers as agents who adjust their resource extraction based on the resource&rsquo;s state. These agents are connected through social networks, where links symbolize either affinity or aversion among them. The interplay between social dynamics and resource dynamics is explored, with the system&rsquo;s evolution analyzed across various network topologies and initial conditions. We find that knowledge feedback can independently sustain common pool resources. However, the impact of social interactions on sustainability is dual-faceted: it can either support or impede sustainability, influenced by the network&rsquo;s connectivity and heterogeneity. A notable finding is the identification of a critical network mean degree, beyond which a depletion/repletion transition parallels an absorbing/active state transition in social dynamics, i.e., individual agents and their connections are/are not prone to being frozen in their social states. Furthermore, the study examines the evolution of the social network, revealing the emergence of two polarized groups where agents within each community have the same affinity. Comparative analyses using Monte-Carlo <b>simulations</b> and rate equations are employed, along with analytical arguments, to reinforce the study&rsquo;s findings. The model successfully captures how information spread and social dynamics may impact the sustanebility of common pool resource.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--137153-a-new-coupled-electro-thermo-fluid-radiofrequency-model-of-cardiac-tissue-mathematical-modeling-analysis-and-numerical-simulation-mostafa-bendahmane-et-al-2024>(1/2 | 137/153) A new coupled electro-thermo-fluid radiofrequency model of cardiac tissue: Mathematical modeling, analysis and numerical simulation (Mostafa Bendahmane et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mostafa Bendahmane, Youssef Ouakrim, Yassine Ouzrour, Mohamed Zagour. (2024)<br><strong>A new coupled electro-thermo-fluid radiofrequency model of cardiac tissue: Mathematical modeling, analysis and numerical simulation</strong><br><button class=copy-to-clipboard title="A new coupled electro-thermo-fluid radiofrequency model of cardiac tissue: Mathematical modeling, analysis and numerical simulation" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-AP, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10892v1.pdf filename=2403.10892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a nonlinear reaction-diffusion-fluid system that simulates radiofrequency ablation within cardiac tissue. The model conveys the dynamic evolution of temperature and electric potential in both the fluid and solid regions, along with the evolution of velocity within the solid region. By formulating the system that describes the phenomena across the entire domain, encompassing both solid and fluid phases, we proceed to an analysis of well-posedness, considering a broad class of right-hand side terms. The system involves parameters such as heat conductivity, kinematic viscosity, and electrical conductivity, all of which exhibit nonlinearity contingent upon the temperature variable. The mathematical analysis extends to establishing the existence of a global solution, employing the Faedo-Galerkin method in a three-dimensional space. To enhance the practical applicability of our theoretical results, we complement our study with a series of numerical experiments. We implement the discrete system using the finite element method for spatial discretization and an Euler scheme for temporal discretization. Nonlinear parameters are linearized through decoupling systems, as introduced in our continuous analysis. These experiments are conducted to demonstrate and validate the theoretical findings we have established.</p></p class="citation"></blockquote><h3 id=22--138153-high-order-well-balanced-arbitrary-lagrangian-eulerian-ader-discontinuous-galerkin-schemes-on-general-polygonal-moving-meshes-elena-gaburro-et-al-2024>(2/2 | 138/153) High order Well-Balanced Arbitrary-Lagrangian-Eulerian ADER discontinuous Galerkin schemes on general polygonal moving meshes (Elena Gaburro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elena Gaburro, Simone Chiocchetti. (2024)<br><strong>High order Well-Balanced Arbitrary-Lagrangian-Eulerian ADER discontinuous Galerkin schemes on general polygonal moving meshes</strong><br><button class=copy-to-clipboard title="High order Well-Balanced Arbitrary-Lagrangian-Eulerian ADER discontinuous Galerkin schemes on general polygonal moving meshes" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Graph Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10917v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10917v1.pdf filename=2403.10917v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present a novel family of high order accurate numerical schemes for the solution of hyperbolic partial differential equations (PDEs) which combines several geometrical and physical structure preserving properties. First, we settle our methods in the Lagrangian framework, where each element of the mesh evolves following as close as possible the local fluid flow, so to reduce the numerical dissipation at contact waves and moving interfaces and to satisfy the Galilean and rotational invariance properties of the studied PDEs system. In particular, we choose the direct Arbitrary-Lagrangian-Eulerian (ALE) approach which, in order to always guarantee the high quality of the moving mesh, allows to combine the Lagrangian motion with mesh optimization techniques. The employed polygonal tessellation is thus regenerated at each time step, the previous one is connected with the new one by space-time control volumes, including hole-like sliver elements in correspondence of topology changes, over which we integrate a space-time divergence form of the original PDEs through a high order accurate ADER discontinuous Galerkin (DG) scheme. Mass conservation and adherence to the <b>GCL</b> condition are guaranteed by construction thanks to the integration over closed control volumes, and robustness over shock discontinuities is ensured by the use of an a posteriori subcell finite volume (FV) limiting technique.</p></p class="citation"></blockquote><h2 id=csai-3>cs.AI (3)</h2><h3 id=13--139153-game-and-reference-policy-combination-synthesis-for-epidemic-prevention-and-control-zhiyi-tan-et-al-2024>(1/3 | 139/153) Game and Reference: Policy Combination Synthesis for Epidemic Prevention and Control (Zhiyi Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyi Tan, Bingkun Bao. (2024)<br><strong>Game and Reference: Policy Combination Synthesis for Epidemic Prevention and Control</strong><br><button class=copy-to-clipboard title="Game and Reference: Policy Combination Synthesis for Epidemic Prevention and Control" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10744v1.pdf filename=2403.10744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, epidemic policy-making models are increasingly being used to provide reference for governors on prevention and control policies against catastrophic epidemics such as SARS, H1N1 and COVID-19. Existing studies are currently constrained by two issues: First, previous methods develop policies based on effect evaluation, since few of factors in real-world decision-making can be modeled, the output policies will then easily become extreme. Second, the subjectivity and cognitive limitation of human make the historical policies not always optimal for the training of decision models. To these ends, we present a novel Policy Combination Synthesis (PCS) model for epidemic policy-making. Specially, to prevent extreme decisions, we introduce <b>adversarial</b> <b>learning</b> between the model-made policies and the real policies to force the output policies to be more human-liked. On the other hand, to minimize the impact of sub-optimal historical policies, we employ <b>contrastive</b> <b>learning</b> to let the model draw on experience from the best historical policies under similar scenarios. Both <b>adversarial</b> <b>and</b> <b>contrastive</b> <b>learning</b> are adaptive based on the comprehensive effects of real policies to ensure the model always learns useful information. Extensive experiments on real-world data prove the effectiveness of the proposed model.</p></p class="citation"></blockquote><h3 id=23--140153-scheduling-drone-and-mobile-charger-via-hybrid-action-deep-reinforcement-learning-jizhe-dou-et-al-2024>(2/3 | 140/153) Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning (Jizhe Dou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jizhe Dou, Haotian Zhang, Guodong Sun. (2024)<br><strong>Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-RO, cs.AI<br>Keyword Score: 15<br>Keywords: Reinforcement Learning, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10761v1.pdf filename=2403.10761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently there has been a growing interest in industry and academia, regarding the use of wireless chargers to prolong the operational longevity of unmanned aerial vehicles (commonly knowns as drones). In this paper we consider a charger-assisted drone application: a drone is deployed to observe a set points of interest, while a charger can move to recharge the drone&rsquo;s battery. We focus on the route and charging schedule of the drone and the mobile charger, to obtain high observation utility with the shortest possible time, while ensuring the drone remains operational during task execution. Essentially, this proposed drone-charger scheduling problem is a multi-stage decision-making process, in which the drone and the mobile charger act as two agents who cooperate to finish a task. The discrete-continuous hybrid action space of the two agents poses a significant challenge in our problem. To address this issue, we present a hybrid-action deep <b>reinforcement</b> <b>learning</b> framework, called HaDMC, which uses a standard policy learning algorithm to generate latent continuous actions. Motivated by <b>representation</b> <b>learning,</b> we specifically design and train an action decoder. It involves two pipelines to convert the latent continuous actions into original discrete and continuous actions, by which the drone and the charger can directly interact with environment. We embed a mutual learning scheme in model training, emphasizing the collaborative rather than individual actions. We conduct extensive numerical experiments to evaluate HaDMC and compare it with state-of-the-art deep <b>reinforcement</b> <b>learning</b> approaches. The experimental results show the effectiveness and efficiency of our solution.</p></p class="citation"></blockquote><h3 id=33--141153-inducing-individual-students-learning-strategies-through-homomorphic-pomdps-huifan-gao-et-al-2024>(3/3 | 141/153) Inducing Individual Students&rsquo; Learning Strategies through Homomorphic POMDPs (Huifan Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huifan Gao, Yifeng Zeng, Yinghui Pan. (2024)<br><strong>Inducing Individual Students&rsquo; Learning Strategies through Homomorphic POMDPs</strong><br><button class=copy-to-clipboard title="Inducing Individual Students' Learning Strategies through Homomorphic POMDPs" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10930v1.pdf filename=2403.10930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimizing students&rsquo; learning strategies is a crucial component in intelligent tutoring systems. Previous research has demonstrated the effectiveness of devising personalized learning strategies for students by modelling their learning processes through partially observable <b>Markov</b> <b>decision</b> <b>process</b> (POMDP). However, the research holds the assumption that the student population adheres to a uniform cognitive pattern. While this assumption simplifies the POMDP modelling process, it evidently deviates from a real-world scenario, thus reducing the precision of inducing individual students&rsquo; learning strategies. In this article, we propose the homomorphic POMDP (H-POMDP) model to accommodate multiple cognitive patterns and present the parameter learning approach to automatically construct the H-POMDP model. Based on the H-POMDP model, we are able to represent different cognitive patterns from the data and induce more personalized learning strategies for individual students. We conduct experiments to show that, in comparison to the general POMDP approach, the H-POMDP model demonstrates better precision when modelling mixed data from multiple cognitive patterns. Moreover, the learning strategies derived from H-POMDPs exhibit better personalization in the performance evaluation.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--142153-defa-efficient-deformable-attention-acceleration-via-pruning-assisted-grid-sampling-and-multi-scale-parallel-processing-yansong-xu-et-al-2024>(1/1 | 142/153) DEFA: Efficient Deformable Attention Acceleration via Pruning-Assisted Grid-Sampling and Multi-Scale Parallel Processing (Yansong Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yansong Xu, Dongxu Lyu, Zhenyu Li, Zilong Wang, Yuzhou Chen, Gang Wang, Zhican Wang, Haomin Li, Guanghui He. (2024)<br><strong>DEFA: Efficient Deformable Attention Acceleration via Pruning-Assisted Grid-Sampling and Multi-Scale Parallel Processing</strong><br><button class=copy-to-clipboard title="DEFA: Efficient Deformable Attention Acceleration via Pruning-Assisted Grid-Sampling and Multi-Scale Parallel Processing" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 13<br>Keywords: Benchmarking, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10913v1.pdf filename=2403.10913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-scale deformable attention (MSDeformAttn) has emerged as a key mechanism in various vision tasks, demonstrating explicit superiority attributed to multi-scale grid-sampling. However, this newly introduced operator incurs irregular data access and enormous memory requirement, leading to severe PE underutilization. Meanwhile, existing approaches for attention acceleration cannot be directly applied to MSDeformAttn due to lack of support for this distinct procedure. Therefore, we propose a dedicated algorithm-architecture co-design dubbed DEFA, the first-of-its-kind method for MSDeformAttn acceleration. At the algorithm level, DEFA adopts frequency-weighted <b>pruning</b> and probability-aware <b>pruning</b> for feature maps and sampling points respectively, alleviating the memory footprint by over 80%. At the architecture level, it explores the multi-scale parallelism to boost the throughput significantly and further reduces the memory access via fine-grained layer fusion and feature map reusing. Extensively evaluated on representative <b>benchmarks,</b> DEFA achieves 10.1-31.9x speedup and 20.3-37.7x energy efficiency boost compared to powerful GPUs. It also rivals the related accelerators by 2.2-3.7x energy efficiency improvement while providing pioneering support for MSDeformAttn.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--143153-an-open-source-experimentation-framework-for-the-edge-cloud-continuum-georgios-koukis-et-al-2024>(1/1 | 143/153) An Open-Source Experimentation Framework for the Edge Cloud Continuum (Georgios Koukis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Georgios Koukis, Sotiris Skaperas, Ioanna Angeliki Kapetanidou, Vassilis Tsaoussidis, Lefteris Mamatas. (2024)<br><strong>An Open-Source Experimentation Framework for the Edge Cloud Continuum</strong><br><button class=copy-to-clipboard title="An Open-Source Experimentation Framework for the Edge Cloud Continuum" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-NI, cs.DC<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10977v1.pdf filename=2403.10977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The CODECO Experimentation Framework is an open-source solution designed for the rapid experimentation of Kubernetes-based edge cloud deployments. It adopts a microservice-based architecture and introduces innovative abstractions for (i) the holistic deployment of Kubernetes clusters and associated applications, starting from the VM allocation level; (ii) declarative cross-layer experiment configuration; and (iii) automation features covering the entire experimental process, from the configuration up to the results visualization. We present proof-of-concept results that demonstrate the above capabilities in three distinct contexts: (i) a comparative evaluation of various network fabrics across different edge-oriented Kubernetes distributions; (ii) the automated deployment of EdgeNet, which is a complex edge cloud orchestration system; and (iii) an assessment of <b>anomaly</b> <b>detection</b> (AD) workflows tailored for edge environments.</p></p class="citation"></blockquote><h2 id=csse-2>cs.SE (2)</h2><h3 id=12--144153-a-hypergraph-based-formalization-of-hierarchical-reactive-modules-and-a-compositional-verification-method-daisuke-ishii-2024>(1/2 | 144/153) A Hypergraph-based Formalization of Hierarchical Reactive Modules and a Compositional Verification Method (Daisuke Ishii, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daisuke Ishii. (2024)<br><strong>A Hypergraph-based Formalization of Hierarchical Reactive Modules and a Compositional Verification Method</strong><br><button class=copy-to-clipboard title="A Hypergraph-based Formalization of Hierarchical Reactive Modules and a Compositional Verification Method" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10919v1.pdf filename=2403.10919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The compositional approach is important for <b>reasoning</b> about large and complex systems. In this work, we address synchronous systems with hierarchical structures, which are often used to model cyber-physical systems. We revisit the theory of reactive modules and reformulate it based on hypergraphs to clarify the parallel composition and the hierarchical description of modules. Then, we propose an automatic verification method for hierarchical systems. Given a system description annotated with assume-guarantee contracts, the proposed method divides the system into modules and verifies them separately to show that the top-level system satisfies its contract. Our method allows an input to be a circular system in which submodules mutually depend on each other. Experimental result shows our method can be effectively implemented using an SMT-based model checker.</p></p class="citation"></blockquote><h3 id=22--145153-ipsynth-interprocedural-program-synthesis-for-software-security-implementation-ali-shokri-et-al-2024>(2/2 | 145/153) IPSynth: Interprocedural Program Synthesis for Software Security Implementation (Ali Shokri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Shokri, Ibrahim Jameel Mujhid, Mehdi Mirakhorli. (2024)<br><strong>IPSynth: Interprocedural Program Synthesis for Software Security Implementation</strong><br><button class=copy-to-clipboard title="IPSynth: Interprocedural Program Synthesis for Software Security Implementation" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10836v1.pdf filename=2403.10836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To implement important quality attributes of software such as architectural security tactics, developers incorporate API of software frameworks, as building blocks, to avoid re-inventing the wheel and improve their productivity. However, this is a challenging and error-prone task, especially for novice programmers. Despite the advances in the field of API-based program synthesis, the state-of-the-art suffers from a twofold shortcoming when it comes to architectural tactic implementation tasks. First, the specification of the desired tactic must be explicitly expressed, which is out of the knowledge of such programmers. Second, these approaches synthesize a block of code and leave the task of breaking it down into smaller pieces, adding each piece to the proper location in the code, and establishing correct dependencies between each piece and its surrounding environment as well as the other pieces, to the programmer. To mitigate these challenges, we introduce IPSynth, a novel inter-procedural program synthesis approach that automatically learns the specification of the tactic, synthesizes the tactic as inter-related code snippets, and adds them to an existing code base. We extend our first-place award-winning extended abstract recognized at the 36th IEEE/ACM International Conference on Automated Software Engineering (ASE'21) research competition track. In this paper, we provide the details of the approach, present the results of the experimental evaluation of IPSynth, and analyses and insights for a more comprehensive exploration of the research topic. Moreover, we compare the results of our approach to one of the most powerful code generator tools, <b>ChatGPT.</b> Our results show that our approach can accurately locate corresponding spots in the program, synthesize needed code snippets, add them to the program, and outperform <b>ChatGPT</b> in inter-procedural tactic synthesis tasks.</p></p class="citation"></blockquote><h2 id=quant-ph-3>quant-ph (3)</h2><h3 id=13--146153-fedqnn-federated-learning-using-quantum-neural-networks-nouhaila-innan-et-al-2024>(1/3 | 146/153) FedQNN: Federated Learning using Quantum Neural Networks (Nouhaila Innan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nouhaila Innan, Muhammad Al-Zafar Khan, Alberto Marchisio, Muhammad Shafique, Mohamed Bennai. (2024)<br><strong>FedQNN: Federated Learning using Quantum Neural Networks</strong><br><button class=copy-to-clipboard title="FedQNN: Federated Learning using Quantum Neural Networks" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-ET, cs-LG, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10861v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10861v1.pdf filename=2403.10861v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we explore the innovative domain of Quantum <b>Federated</b> <b>Learning</b> (QFL) as a framework for training Quantum Machine Learning (QML) models via distributed networks. Conventional machine learning models frequently grapple with issues about data privacy and the exposure of sensitive information. Our proposed <b>Federated</b> <b>Quantum</b> Neural Network (FedQNN) framework emerges as a cutting-edge solution, integrating the singular characteristics of QML with the principles of classical <b>federated</b> <b>learning.</b> This work thoroughly investigates QFL, underscoring its capability to secure data handling in a distributed environment and facilitate cooperative learning without direct data sharing. Our research corroborates the concept through experiments across varied datasets, including genomics and healthcare, thereby validating the versatility and efficacy of our FedQNN framework. The results consistently exceed 86% accuracy across three distinct datasets, proving its suitability for conducting various QML tasks. Our research not only identifies the limitations of classical paradigms but also presents a novel framework to propel the field of QML into a new era of secure and collaborative innovation.</p></p class="citation"></blockquote><h3 id=23--147153-quantumleak-stealing-quantum-neural-networks-from-cloud-based-nisq-machines-zhenxiao-fu-et-al-2024>(2/3 | 147/153) QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines (Zhenxiao Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenxiao Fu, Min Yang, Cheng Chu, Yilun Xu, Gang Huang, Fan Chen. (2024)<br><strong>QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines</strong><br><button class=copy-to-clipboard title="QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CR, cs-LG, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Model Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10790v1.pdf filename=2403.10790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Variational quantum circuits (VQCs) have become a powerful tool for implementing Quantum Neural Networks (QNNs), addressing a wide range of complex problems. Well-trained VQCs serve as valuable intellectual assets hosted on cloud-based Noisy Intermediate Scale Quantum (NISQ) computers, making them susceptible to malicious VQC stealing attacks. However, traditional <b>model</b> <b>extraction</b> techniques designed for classical machine learning <b>models</b> <b>encounter</b> challenges when applied to NISQ computers due to significant noise in current devices. In this paper, we introduce QuantumLeak, an effective and accurate QNN <b>model</b> <b>extraction</b> technique from cloud-based NISQ machines. Compared to existing classical <b>model</b> <b>stealing</b> techniques, QuantumLeak improves local VQC accuracy by 4.99%$\sim$7.35% across diverse datasets and VQC architectures.</p></p class="citation"></blockquote><h3 id=33--148153-multi-controlled-phase-gate-synthesis-with-zx-calculus-applied-to-neutral-atom-hardware-korbinian-staudacher-et-al-2024>(3/3 | 148/153) Multi-controlled Phase Gate Synthesis with ZX-calculus applied to Neutral Atom Hardware (Korbinian Staudacher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Korbinian Staudacher, Ludwig Schmid, Johannes Zeiher, Robert Wille, Dieter Kranzlmüller. (2024)<br><strong>Multi-controlled Phase Gate Synthesis with ZX-calculus applied to Neutral Atom Hardware</strong><br><button class=copy-to-clipboard title="Multi-controlled Phase Gate Synthesis with ZX-calculus applied to Neutral Atom Hardware" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-ET, quant-ph, quant-ph<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10864v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10864v1.pdf filename=2403.10864v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum circuit synthesis describes the process of converting arbitrary unitary operations into a gate sequence of a fixed universal gate set, usually defined by the operations native to a given hardware platform. Most current synthesis algorithms are designed to synthesize towards a set of single qubit rotations and an additional entangling two qubit gate, such as CX, CZ, or the Molmer Sorensen gate. However, with the emergence of neutral atom based hardware and their native support for gates with more than two qubits, synthesis approaches tailored to these new gate sets become necessary. In this work, we present an approach to synthesize multi controlled phase gates using ZX calculus. By representing quantum circuits as <b>graph</b> like ZX diagrams, one can utilize the distinct <b>graph</b> structure of diagonal gates to identify multi controlled phase gates inherently present in some quantum circuits even if none were explicitly defined in the original circuit. We evaluate the approach on a wide range of <b>benchmark</b> circuits and compare them to the standard Qiskit synthesis regarding its circuit execution time for neutral atom based hardware with native support of multi controlled gates. Our results show possible advantages for current state of the art hardware and represent the first exact synthesis algorithm supporting arbitrary sized multi controlled phase gates.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--149153-a-comprehensive-review-of-latent-space-dynamics-identification-algorithms-for-intrusive-and-non-intrusive-reduced-order-modeling-christophe-bonneville-et-al-2024>(1/1 | 149/153) A Comprehensive Review of Latent Space Dynamics Identification Algorithms for Intrusive and Non-Intrusive Reduced-Order-Modeling (Christophe Bonneville et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christophe Bonneville, Xiaolong He, April Tran, Jun Sur Park, William Fries, Daniel A. Messenger, Siu Wun Cheung, Yeonjong Shin, David M. Bortz, Debojyoti Ghosh, Jiun-Shyan Chen, Jonathan Belof, Youngsoo Choi. (2024)<br><strong>A Comprehensive Review of Latent Space Dynamics Identification Algorithms for Intrusive and Non-Intrusive Reduced-Order-Modeling</strong><br><button class=copy-to-clipboard title="A Comprehensive Review of Latent Space Dynamics Identification Algorithms for Intrusive and Non-Intrusive Reduced-Order-Modeling" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs-LG, cs-MS, cs-NA, cs.CE, math-NA<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10748v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10748v1.pdf filename=2403.10748v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Numerical solvers of partial differential equations (PDEs) have been widely employed for simulating physical systems. However, the computational cost remains a major bottleneck in various scientific and engineering applications, which has motivated the development of reduced-order models (ROMs). Recently, machine-learning-based ROMs have gained significant popularity and are promising for addressing some limitations of traditional ROM methods, especially for advection dominated systems. In this chapter, we focus on a particular framework known as Latent Space Dynamics Identification (LaSDI), which transforms the high-fidelity data, governed by a PDE, to simpler and low-dimensional latent-space data, governed by ordinary differential equations (ODEs). These ODEs can be learned and subsequently interpolated to make ROM predictions. Each building block of LaSDI can be easily modulated depending on the application, which makes the LaSDI framework highly flexible. In particular, we present strategies to enforce the laws of thermodynamics into LaSDI models (tLaSDI), enhance robustness in the presence of noise through the weak form (WLaSDI), select high-fidelity training data efficiently through <b>active</b> <b>learning</b> (gLaSDI, GPLaSDI), and quantify the ROM prediction uncertainty through Gaussian processes (GPLaSDI). We demonstrate the performance of different LaSDI approaches on Burgers equation, a non-linear heat conduction problem, and a plasma physics problem, showing that LaSDI algorithms can achieve relative errors of less than a few percent and up to thousands of times speed-ups.</p></p class="citation"></blockquote><h2 id=q-biomn-1>q-bio.MN (1)</h2><h3 id=11--150153-identifying-the-attractors-of-gene-regulatory-networks-from-expression-data-under-uncertainty-an-interpretable-approach-alireza-rowhanimanesh-2024>(1/1 | 150/153) Identifying the Attractors of Gene Regulatory Networks from Expression Data under Uncertainty: An Interpretable Approach (Alireza Rowhanimanesh, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alireza Rowhanimanesh. (2024)<br><strong>Identifying the Attractors of Gene Regulatory Networks from Expression Data under Uncertainty: An Interpretable Approach</strong><br><button class=copy-to-clipboard title="Identifying the Attractors of Gene Regulatory Networks from Expression Data under Uncertainty: An Interpretable Approach" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.MN<br>Categories: cs-AI, cs-SY, eess-SY, q-bio-MN, q-bio.MN<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11015v1.pdf filename=2403.11015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In systems biology, attractor landscape analysis of gene regulatory networks is recognized as a powerful computational tool for studying various cellular states from proliferation and differentiation to senescence and apoptosis. Therefore, accurate identification of attractors plays a critical role in determination of the cell fates. On the other hand, in a real biological circuit, genetic/epigenetic alterations as well as varying environmental factors drastically take effect on the location, characteristics, and even the number of attractors. The central question is: Given a temporal gene expression profile of a real gene regulatory network, how can the attractors be robustly identified in the presence of huge amount of uncertainty? This paper addresses this question using a novel approach based on Zadeh Computing with Words. The proposed scheme could effectively identify the attractors from temporal gene expression data in terms of both fuzzy logic-based and linguistic descriptions which are simply interpretable by human experts. Therefore, this method can be considered as an effective step towards interpretable artificial intelligence. Without loss of generality, genetic toggle switch is considered as the case study. The nonlinear dynamics of this <b>benchmark</b> gene regulatory network is computationally modeled by the notion of uncertain stochastic differential equations. The results of in-silico study demonstrate the efficiency and robustness of the proposed method.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--151153-on-extended-perfect-codes-konstantin-vorobev-2024>(1/1 | 151/153) On extended perfect codes (Konstantin Vorob&rsquo;ev, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Konstantin Vorob&rsquo;ev. (2024)<br><strong>On extended perfect codes</strong><br><button class=copy-to-clipboard title="On extended perfect codes" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05B30, 05A05, 94B25, cs-IT, math-CO, math-IT, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10992v1.pdf filename=2403.10992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider extended $1$-perfect codes in Hamming <b>graphs</b> $H(n,q)$. Such nontrivial codes are known only when $n=2^k$, $k\geq 1$, $q=2$, or $n=q+2$, $q=2^m$, $m\geq 1$. Recently, Bespalov proved nonexistence of extended $1$-perfect codes for $q=3$, $4$, $n>q+2$. In this work, we characterize all positive integers $n$, $r$ and prime $p$, for which there exist such a code in $H(n,p^r)$.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--152153-solving-the-multiobjective-quasi-clique-problem-daniela-scherer-dos-santos-et-al-2024>(1/1 | 152/153) Solving the Multiobjective Quasi-Clique Problem (Daniela Scherer dos Santos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniela Scherer dos Santos, Kathrin Klamroth, Pedro Martins, Luís Paquete. (2024)<br><strong>Solving the Multiobjective Quasi-Clique Problem</strong><br><button class=copy-to-clipboard title="Solving the Multiobjective Quasi-Clique Problem" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs-DS, cs.DM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10896v1.pdf filename=2403.10896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a simple undirected <b>graph</b> $G$, a quasi-clique is a subgraph of $G$ whose density is at least $\gamma$ $(0 &lt; \gamma \leq 1)$. Finding a maximum quasi-clique has been addressed from two different perspectives: $i)$ maximizing vertex cardinality for a given edge density; and $ii)$ maximizing edge density for a given vertex cardinality. However, when no a priori preference information about cardinality and density is available, a more natural approach is to consider the problem from a multiobjective perspective. We introduce the Multiobjective Quasi-clique Problem (MOQC), which aims to find a quasi-clique by simultaneously maximizing both vertex cardinality and edge density. To efficiently address this problem, we explore the relationship among MOQC, its single-objective counterpart problems, and a biobjective optimization problem, along with several properties of the MOQC problem and quasi-cliques. We propose a baseline approach using $\varepsilon$-constraint scalarization and introduce a Two-phase strategy, which applies a dichotomic search based on weighted sum scalarization in the first phase and an $\varepsilon$-constraint methodology in the second phase. Additionally, we present a Three-phase strategy that combines the dichotomic search used in Two-phase with a vertex-degree-based local search employing novel sufficient conditions to assess quasi-clique efficiency, followed by an $\varepsilon$-constraint in a final stage. Experimental results on real-world sparse <b>graphs</b> indicate that the integrated use of dichotomic search and local search, together with mechanisms to assess quasi-clique efficiency, makes the Three-phase strategy an effective approach for solving the MOQC problem in terms of running time and ability to produce new efficient quasi-cliques.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=11--153153-approximation-ratio-of-the-min-degree-greedy-algorithm-for-maximum-independent-set-on-interval-and-chordal-graphs-steven-chaplick-et-al-2024>(1/1 | 153/153) Approximation Ratio of the Min-Degree Greedy Algorithm for Maximum Independent Set on Interval and Chordal Graphs (Steven Chaplick et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Steven Chaplick, Martin Frohn, Steven Kelk, Johann Lottermoser, Matus Mihalak. (2024)<br><strong>Approximation Ratio of the Min-Degree Greedy Algorithm for Maximum Independent Set on Interval and Chordal Graphs</strong><br><button class=copy-to-clipboard title="Approximation Ratio of the Min-Degree Greedy Algorithm for Maximum Independent Set on Interval and Chordal Graphs" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10868v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10868v1.pdf filename=2403.10868v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this article we prove that the minimum-degree greedy algorithm, with adversarial tie-breaking, is a $(2/3)$-approximation for the Maximum Independent Set problem on interval <b>graphs.</b> We show that this is tight, even on unit interval <b>graphs</b> of maximum degree 3. We show that on chordal <b>graphs,</b> the greedy algorithm is a $(1/2)$-approximation and that this is again tight. These results contrast with the known (tight) approximation ratio of $\frac{3}{\Delta+2}$ of the greedy algorithm for general <b>graphs</b> of maximum degree $\Delta$.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.17</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.19</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-22>cs.CL (22)</a><ul><li><a href=#122--1153-pointer-generator-networks-for-low-resource-machine-translation-dont-copy-that-niyati-bafna-et-al-2024>(1/22 | 1/153) Pointer-Generator Networks for Low-Resource Machine Translation: Don&rsquo;t Copy That! (Niyati Bafna et al., 2024)</a></li><li><a href=#222--2153-benqa-a-question-answering-and-reasoning-benchmark-for-bengali-and-english-sheikh-shafayat-et-al-2024>(2/22 | 2/153) BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English (Sheikh Shafayat et al., 2024)</a></li><li><a href=#322--3153-selfie-self-interpretation-of-large-language-model-embeddings-haozhe-chen-et-al-2024>(3/22 | 3/153) SelfIE: Self-Interpretation of Large Language Model Embeddings (Haozhe Chen et al., 2024)</a></li><li><a href=#422--4153-detecting-bias-in-large-language-models-fine-tuned-kcbert-j-k-lee-et-al-2024>(4/22 | 4/153) Detecting Bias in Large Language Models: Fine-tuned KcBERT (J. K. Lee et al., 2024)</a></li><li><a href=#522--5153-do-large-language-models-understand-medical-codes-simon-a-lee-et-al-2024>(5/22 | 5/153) Do Large Language Models understand Medical Codes? (Simon A. Lee et al., 2024)</a></li><li><a href=#622--6153-efficient-pruning-of-large-language-model-with-adaptive-estimation-fusion-jun-liu-et-al-2024>(6/22 | 6/153) Efficient Pruning of Large Language Model with Adaptive Estimation Fusion (Jun Liu et al., 2024)</a></li><li><a href=#722--7153-from-words-to-routes-applying-large-language-models-to-vehicle-routing-zhehui-huang-et-al-2024>(7/22 | 7/153) From Words to Routes: Applying Large Language Models to Vehicle Routing (Zhehui Huang et al., 2024)</a></li><li><a href=#822--8153-ecrc-emotion-causality-recognition-in-korean-conversation-for-gcn-j-k-lee-et-al-2024>(8/22 | 8/153) ECRC: Emotion-Causality Recognition in Korean Conversation for GCN (J. K. Lee et al., 2024)</a></li><li><a href=#922--9153-optimizing-language-augmentation-for-multilingual-large-language-models-a-case-study-on-korean-changsu-choi-et-al-2024>(9/22 | 9/153) Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean (ChangSu Choi et al., 2024)</a></li><li><a href=#1022--10153-depression-detection-on-social-media-with-large-language-models-xiaochong-lan-et-al-2024>(10/22 | 10/153) Depression Detection on Social Media with Large Language Models (Xiaochong Lan et al., 2024)</a></li><li><a href=#1122--11153-two-step-automated-cybercrime-coded-word-detection-using-multi-level-representation-learning-yongyeon-kim-et-al-2024>(11/22 | 11/153) Two-step Automated Cybercrime Coded Word Detection using Multi-level Representation Learning (Yongyeon Kim et al., 2024)</a></li><li><a href=#1222--12153-pre-trained-language-models-represent-some-geographic-populations-better-than-others-jonathan-dunn-et-al-2024>(12/22 | 12/153) Pre-Trained Language Models Represent Some Geographic Populations Better Than Others (Jonathan Dunn et al., 2024)</a></li><li><a href=#1322--13153-zero-shot-generative-linguistic-steganography-ke-lin-et-al-2024>(13/22 | 13/153) Zero-shot Generative Linguistic Steganography (Ke Lin et al., 2024)</a></li><li><a href=#1422--14153-retinaqa--a-knowledge-base-question-answering-model-robust-to-both-answerable-and-unanswerable-questions-prayushi-faldu-et-al-2024>(14/22 | 14/153) RETINAQA : A Knowledge Base Question Answering Model Robust to both Answerable and Unanswerable Questions (Prayushi Faldu et al., 2024)</a></li><li><a href=#1522--15153-exploring-chinese-humor-generation-a-study-on-two-part-allegorical-sayings-rongwu-xu-2024>(15/22 | 15/153) Exploring Chinese Humor Generation: A Study on Two-Part Allegorical Sayings (Rongwu Xu, 2024)</a></li><li><a href=#1622--16153-llm-based-conversational-ai-therapist-for-daily-functioning-screening-and-psychotherapeutic-intervention-via-everyday-smart-devices-jingping-nie-et-al-2024>(16/22 | 16/153) LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices (Jingping Nie et al., 2024)</a></li><li><a href=#1722--17153-deciphering-hate-identifying-hateful-memes-and-their-targets-eftekhar-hossain-et-al-2024>(17/22 | 17/153) Deciphering Hate: Identifying Hateful Memes and Their Targets (Eftekhar Hossain et al., 2024)</a></li><li><a href=#1822--18153-towards-robustness-and-diversity-continual-learning-in-dialog-generation-with-text-mixup-and-batch-nuclear-norm-maximization-zihan-wang-et-al-2024>(18/22 | 18/153) Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization (Zihan Wang et al., 2024)</a></li><li><a href=#1922--19153-rules-still-work-for-open-information-extraction-jialin-hua-et-al-2024>(19/22 | 19/153) Rules still work for Open Information Extraction (Jialin Hua et al., 2024)</a></li><li><a href=#2022--20153-entity-alignment-with-unlabeled-dangling-cases-hang-yin-et-al-2024>(20/22 | 20/153) Entity Alignment with Unlabeled Dangling Cases (Hang Yin et al., 2024)</a></li><li><a href=#2122--21153-multi-party-response-generation-with-relation-disentanglement-tianhao-dai-et-al-2024>(21/22 | 21/153) Multi-party Response Generation with Relation Disentanglement (Tianhao Dai et al., 2024)</a></li><li><a href=#2222--22153-dialectbench-a-nlp-benchmark-for-dialects-varieties-and-closely-related-languages-fahim-faisal-et-al-2024>(22/22 | 22/153) DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages (Fahim Faisal et al., 2024)</a></li></ul></li><li><a href=#cslg-22>cs.LG (22)</a><ul><li><a href=#122--23153-time-series-representation-learning-with-supervised-contrastive-temporal-transformer-yuansan-liu-et-al-2024>(1/22 | 23/153) Time Series Representation Learning with Supervised Contrastive Temporal Transformer (Yuansan Liu et al., 2024)</a></li><li><a href=#222--24153-flykd-graph-knowledge-distillation-on-the-fly-with-curriculum-learning-eugene-ku-2024>(2/22 | 24/153) FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning (Eugene Ku, 2024)</a></li><li><a href=#322--25153-energy-based-models-with-applications-to-speech-and-language-processing-zhijian-ou-2024>(3/22 | 25/153) Energy-Based Models with Applications to Speech and Language Processing (Zhijian Ou, 2024)</a></li><li><a href=#422--26153-forward-learning-of-graph-neural-networks-namyong-park-et-al-2024>(4/22 | 26/153) Forward Learning of Graph Neural Networks (Namyong Park et al., 2024)</a></li><li><a href=#522--27153-edge-private-graph-neural-networks-with-singular-value-perturbation-tingting-tang-et-al-2024>(5/22 | 27/153) Edge Private Graph Neural Networks with Singular Value Perturbation (Tingting Tang et al., 2024)</a></li><li><a href=#622--28153-just-say-the-name-online-continual-learning-with-category-names-only-via-data-generation-minhyuk-seo-et-al-2024>(6/22 | 28/153) Just Say the Name: Online Continual Learning with Category Names Only via Data Generation (Minhyuk Seo et al., 2024)</a></li><li><a href=#722--29153-dreaming-of-many-worlds-learning-contextual-world-models-aids-zero-shot-generalization-sai-prasanna-et-al-2024>(7/22 | 29/153) Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization (Sai Prasanna et al., 2024)</a></li><li><a href=#822--30153-interpretable-machine-learning-for-tabpfn-david-rundel-et-al-2024>(8/22 | 30/153) Interpretable Machine Learning for TabPFN (David Rundel et al., 2024)</a></li><li><a href=#922--31153-lookalike-human-mimicry-based-collaborative-decision-making-rabimba-karanjai-et-al-2024>(9/22 | 31/153) LookALike: Human Mimicry based collaborative decision making (Rabimba Karanjai et al., 2024)</a></li><li><a href=#1022--32153-model-reprogramming-outperforms-fine-tuning-on-out-of-distribution-data-in-text-image-encoders-andrew-geng-et-al-2024>(10/22 | 32/153) Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders (Andrew Geng et al., 2024)</a></li><li><a href=#1122--33153-graph-regularized-nmf-with-l20-norm-for-unsupervised-feature-learning-zhen-wang-et-al-2024>(11/22 | 33/153) Graph Regularized NMF with L20-norm for Unsupervised Feature Learning (Zhen Wang et al., 2024)</a></li><li><a href=#1222--34153-probabilistic-world-modeling-with-asymmetric-distance-measure-meng-song-2024>(12/22 | 34/153) Probabilistic World Modeling with Asymmetric Distance Measure (Meng Song, 2024)</a></li><li><a href=#1322--35153-reinforcement-learning-with-options-ayoub-ghriss-et-al-2024>(13/22 | 35/153) Reinforcement Learning with Options (Ayoub Ghriss et al., 2024)</a></li><li><a href=#1422--36153-twin-transformer-using-gated-dynamic-learnable-attention-mechanism-for-fault-detection-and-diagnosis-in-the-tennessee-eastman-process-mohammad-ali-labbaf-khaniki-et-al-2024>(14/22 | 36/153) Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process (Mohammad Ali Labbaf-Khaniki et al., 2024)</a></li><li><a href=#1522--37153-enhancing-out-of-distribution-detection-with-multitesting-based-layer-wise-feature-fusion-jiawei-li-et-al-2024>(15/22 | 37/153) Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion (Jiawei Li et al., 2024)</a></li><li><a href=#1622--38153-anomaly-detection-based-on-isolation-mechanisms-a-survey-yang-cao-et-al-2024>(16/22 | 38/153) Anomaly Detection Based on Isolation Mechanisms: A Survey (Yang Cao et al., 2024)</a></li><li><a href=#1722--39153-a-probabilistic-approach-for-alignment-with-human-comparisons-junyu-cao-et-al-2024>(17/22 | 39/153) A Probabilistic Approach for Alignment with Human Comparisons (Junyu Cao et al., 2024)</a></li><li><a href=#1822--40153-fagh-accelerating-federated-learning-with-approximated-global-hessian-mrinmay-sen-et-al-2024>(18/22 | 40/153) FAGH: Accelerating Federated Learning with Approximated Global Hessian (Mrinmay Sen et al., 2024)</a></li><li><a href=#1922--41153-iotco2-assessing-the-end-to-end-carbon-footprint-of-internet-of-things-enabled-deep-learning-ahmad-faiz-et-al-2024>(19/22 | 41/153) IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning (Ahmad Faiz et al., 2024)</a></li><li><a href=#2022--42153-dtor-decision-tree-outlier-regressor-to-explain-anomalies-riccardo-crupi-et-al-2024>(20/22 | 42/153) DTOR: Decision Tree Outlier Regressor to explain anomalies (Riccardo Crupi et al., 2024)</a></li><li><a href=#2122--43153-list-sample-compression-and-uniform-convergence-steve-hanneke-et-al-2024>(21/22 | 43/153) List Sample Compression and Uniform Convergence (Steve Hanneke et al., 2024)</a></li><li><a href=#2222--44153-incentivized-exploration-of-non-stationary-stochastic-bandits-sourav-chakraborty-et-al-2024>(22/22 | 44/153) Incentivized Exploration of Non-Stationary Stochastic Bandits (Sourav Chakraborty et al., 2024)</a></li></ul></li><li><a href=#cscv-33>cs.CV (33)</a><ul><li><a href=#133--45153-a-comprehensive-study-of-multimodal-large-language-models-for-image-quality-assessment-tianhe-wu-et-al-2024>(1/33 | 45/153) A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment (Tianhe Wu et al., 2024)</a></li><li><a href=#233--46153-efficientmorph-parameter-efficient-transformer-based-architecture-for-3d-image-registration-abu-zahid-bin-aziz-et-al-2024>(2/33 | 46/153) EfficientMorph: Parameter-Efficient Transformer-Based Architecture for 3D Image Registration (Abu Zahid Bin Aziz et al., 2024)</a></li><li><a href=#333--47153-efficient-diffusion-driven-corruption-editor-for-test-time-adaptation-yeongtak-oh-et-al-2024>(3/33 | 47/153) Efficient Diffusion-Driven Corruption Editor for Test-Time Adaptation (Yeongtak Oh et al., 2024)</a></li><li><a href=#433--48153-neuro-symbolic-video-search-minkyu-choi-et-al-2024>(4/33 | 48/153) Neuro-Symbolic Video Search (Minkyu Choi et al., 2024)</a></li><li><a href=#533--49153-task-aware-low-rank-adaptation-of-segment-anything-model-xuehao-wang-et-al-2024>(5/33 | 49/153) Task-Aware Low-Rank Adaptation of Segment Anything Model (Xuehao Wang et al., 2024)</a></li><li><a href=#633--50153-reward-guided-latent-consistency-distillation-jiachen-li-et-al-2024>(6/33 | 50/153) Reward Guided Latent Consistency Distillation (Jiachen Li et al., 2024)</a></li><li><a href=#733--51153-automatic-location-detection-based-on-deep-learning-anjali-karangiya-et-al-2024>(7/33 | 51/153) Automatic location detection based on deep learning (Anjali Karangiya et al., 2024)</a></li><li><a href=#833--52153-luojiahog-a-hierarchy-oriented-geo-aware-image-caption-dataset-for-remote-sensing-image-text-retrival-yuanxin-zhao-et-al-2024>(8/33 | 52/153) LuoJiaHOG: A Hierarchy Oriented Geo-aware Image Caption Dataset for Remote Sensing Image-Text Retrival (Yuanxin Zhao et al., 2024)</a></li><li><a href=#933--53153-regularizing-cnns-using-confusion-penalty-based-label-smoothing-for-histopathology-images-somenath-kuiry-et-al-2024>(9/33 | 53/153) Regularizing CNNs using Confusion Penalty Based Label Smoothing for Histopathology Images (Somenath Kuiry et al., 2024)</a></li><li><a href=#1033--54153-stablegarment-garment-centric-generation-via-stable-diffusion-rui-wang-et-al-2024>(10/33 | 54/153) StableGarment: Garment-Centric Generation via Stable Diffusion (Rui Wang et al., 2024)</a></li><li><a href=#1133--55153-improving-adversarial-transferability-of-visual-language-pre-training-models-through-collaborative-multimodal-interaction-jiyuan-fu-et-al-2024>(11/33 | 55/153) Improving Adversarial Transferability of Visual-Language Pre-training Models through Collaborative Multimodal Interaction (Jiyuan Fu et al., 2024)</a></li><li><a href=#1233--56153-understanding-robustness-of-visual-state-space-models-for-image-classification-chengbin-du-et-al-2024>(12/33 | 56/153) Understanding Robustness of Visual State Space Models for Image Classification (Chengbin Du et al., 2024)</a></li><li><a href=#1333--57153-securely-fine-tuning-pre-trained-encoders-against-adversarial-examples-ziqi-zhou-et-al-2024>(13/33 | 57/153) Securely Fine-tuning Pre-trained Encoders Against Adversarial Examples (Ziqi Zhou et al., 2024)</a></li><li><a href=#1433--58153-sfda2-source-free-domain-adaptation-through-the-lens-of-data-augmentation-uiwon-hwang-et-al-2024>(14/33 | 58/153) SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation (Uiwon Hwang et al., 2024)</a></li><li><a href=#1533--59153-affective-behaviour-analysis-via-integrating-multi-modal-knowledge-wei-zhang-et-al-2024>(15/33 | 59/153) Affective Behaviour Analysis via Integrating Multi-Modal Knowledge (Wei Zhang et al., 2024)</a></li><li><a href=#1633--60153-n2f2-hierarchical-scene-understanding-with-nested-neural-feature-fields-yash-bhalgat-et-al-2024>(16/33 | 60/153) N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields (Yash Bhalgat et al., 2024)</a></li><li><a href=#1733--61153-omg-occlusion-friendly-personalized-multi-concept-generation-in-diffusion-models-zhe-kong-et-al-2024>(17/33 | 61/153) OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models (Zhe Kong et al., 2024)</a></li><li><a href=#1833--62153-exploiting-topological-prior-for-boosting-point-cloud-generation-baiyuan-chen-2024>(18/33 | 62/153) Exploiting Topological Prior for Boosting Point Cloud Generation (Baiyuan Chen, 2024)</a></li><li><a href=#1933--63153-ctrl123-consistent-novel-view-synthesis-via-closed-loop-transcription-hongxiang-zhao-et-al-2024>(19/33 | 63/153) Ctrl123: Consistent Novel View Synthesis via Closed-Loop Transcription (Hongxiang Zhao et al., 2024)</a></li><li><a href=#2033--64153-efficient-domain-adaptation-for-endoscopic-visual-odometry-junyang-wu-et-al-2024>(20/33 | 64/153) Efficient Domain Adaptation for Endoscopic Visual Odometry (Junyang Wu et al., 2024)</a></li><li><a href=#2133--65153-retmil-retentive-multiple-instance-learning-for-histopathological-whole-slide-image-classification-hongbo-chu-et-al-2024>(21/33 | 65/153) RetMIL: Retentive Multiple Instance Learning for Histopathological Whole Slide Image Classification (Hongbo Chu et al., 2024)</a></li><li><a href=#2233--66153-visionclip-an-med-aigc-based-ethical-language-image-foundation-model-for-generalizable-retina-image-analysis-hao-wei-et-al-2024>(22/33 | 66/153) VisionCLIP: An Med-AIGC based Ethical Language-Image Foundation Model for Generalizable Retina Image Analysis (Hao Wei et al., 2024)</a></li><li><a href=#2333--67153-active-label-correction-for-semantic-segmentation-with-foundation-models-hoyoung-kim-et-al-2024>(23/33 | 67/153) Active Label Correction for Semantic Segmentation with Foundation Models (Hoyoung Kim et al., 2024)</a></li><li><a href=#2433--68153-hcf-net-hierarchical-context-fusion-network-for-infrared-small-object-detection-shibiao-xu-et-al-2024>(24/33 | 68/153) HCF-Net: Hierarchical Context Fusion Network for Infrared Small Object Detection (Shibiao Xu et al., 2024)</a></li><li><a href=#2533--69153-fast-sparse-view-guided-nerf-update-for-object-reconfigurations-ziqi-lu-et-al-2024>(25/33 | 69/153) Fast Sparse View Guided NeRF Update for Object Reconfigurations (Ziqi Lu et al., 2024)</a></li><li><a href=#2633--70153-rethinking-multi-view-representation-learning-via-distilled-disentangling-guanzhou-ke-et-al-2024>(26/33 | 70/153) Rethinking Multi-view Representation Learning via Distilled Disentangling (Guanzhou Ke et al., 2024)</a></li><li><a href=#2733--71153-hourglassnerf-casting-an-hourglass-as-a-bundle-of-rays-for-few-shot-neural-rendering-seunghyeon-seo-et-al-2024>(27/33 | 71/153) HourglassNeRF: Casting an Hourglass as a Bundle of Rays for Few-shot Neural Rendering (Seunghyeon Seo et al., 2024)</a></li><li><a href=#2833--72153-unsupervised-collaborative-metric-learning-with-mixed-scale-groups-for-general-object-retrieval-shichao-kan-et-al-2024>(28/33 | 72/153) Unsupervised Collaborative Metric Learning with Mixed-Scale Groups for General Object Retrieval (Shichao Kan et al., 2024)</a></li><li><a href=#2933--73153-fishnet-deep-neural-networks-for-low-cost-fish-stock-estimation-moseli-motsoehli-et-al-2024>(29/33 | 73/153) FishNet: Deep Neural Networks for Low-Cost Fish Stock Estimation (Moseli Mots&rsquo;oehli et al., 2024)</a></li><li><a href=#3033--74153-fuzzy-rank-based-late-fusion-technique-for-cytology-image-segmentation-soumyajyoti-dey-et-al-2024>(30/33 | 74/153) Fuzzy Rank-based Late Fusion Technique for Cytology image Segmentation (Soumyajyoti Dey et al., 2024)</a></li><li><a href=#3133--75153-segment-any-object-model-saom-real-to-simulation-fine-tuning-strategy-for-multi-class-multi-instance-segmentation-mariia-khan-et-al-2024>(31/33 | 75/153) Segment Any Object Model (SAOM): Real-to-Simulation Fine-Tuning Strategy for Multi-Class Multi-Instance Segmentation (Mariia Khan et al., 2024)</a></li><li><a href=#3233--76153-learning-dual-level-deformable-implicit-representation-for-real-world-scale-arbitrary-super-resolution-zhiheng-li-et-al-2024>(32/33 | 76/153) Learning Dual-Level Deformable Implicit Representation for Real-World Scale Arbitrary Super-Resolution (Zhiheng Li et al., 2024)</a></li><li><a href=#3333--77153-match-stereo-videos-bidirectional-alignment-for-consistent-dynamic-stereo-matching-junpeng-jing-et-al-2024>(33/33 | 77/153) Match-Stereo-Videos: Bidirectional Alignment for Consistent Dynamic Stereo Matching (Junpeng Jing et al., 2024)</a></li></ul></li><li><a href=#csro-22>cs.RO (22)</a><ul><li><a href=#122--78153-visarl-visual-reinforcement-learning-guided-by-human-saliency-anthony-liang-et-al-2024>(1/22 | 78/153) ViSaRL: Visual Reinforcement Learning Guided by Human Saliency (Anthony Liang et al., 2024)</a></li><li><a href=#222--79153-narrate-versatile-language-architecture-for-optimal-control-in-robotics-seif-ismail-et-al-2024>(2/22 | 79/153) NARRATE: Versatile Language Architecture for Optimal Control in Robotics (Seif Ismail et al., 2024)</a></li><li><a href=#322--80153-deep-reinforcement-learning-based-large-scale-robot-exploration-yuhong-cao-et-al-2024>(3/22 | 80/153) Deep Reinforcement Learning-based Large-scale Robot Exploration (Yuhong Cao et al., 2024)</a></li><li><a href=#422--81153-corn-contact-based-object-representation-for-nonprehensile-manipulation-of-general-unseen-objects-yoonyoung-cho-et-al-2024>(4/22 | 81/153) CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects (Yoonyoung Cho et al., 2024)</a></li><li><a href=#522--82153-quantifying-the-sim2real-gap-for-gps-and-imu-sensors-ishaan-mahajan-et-al-2024>(5/22 | 82/153) Quantifying the Sim2real Gap for GPS and IMU Sensors (Ishaan Mahajan et al., 2024)</a></li><li><a href=#622--83153-a-scalable-and-parallelizable-digital-twin-framework-for-sustainable-sim2real-transition-of-multi-agent-reinforcement-learning-systems-chinmay-vilas-samak-et-al-2024>(6/22 | 83/153) A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems (Chinmay Vilas Samak et al., 2024)</a></li><li><a href=#722--84153-inverse-submodular-maximization-with-application-to-human-in-the-loop-multi-robot-multi-objective-coverage-control-guangyao-shi-et-al-2024>(7/22 | 84/153) Inverse Submodular Maximization with Application to Human-in-the-Loop Multi-Robot Multi-Objective Coverage Control (Guangyao Shi et al., 2024)</a></li><li><a href=#822--85153-learning-based-design-of-off-policy-gaussian-controllers-integrating-model-predictive-control-and-gaussian-process-regression-shiva-kumar-tekumatla-et-al-2024>(8/22 | 85/153) Learning-Based Design of Off-Policy Gaussian Controllers: Integrating Model Predictive Control and Gaussian Process Regression (Shiva Kumar Tekumatla et al., 2024)</a></li><li><a href=#922--86153-efficient-trajectory-forecasting-and-generation-with-conditional-flow-matching-sean-ye-et-al-2024>(9/22 | 86/153) Efficient Trajectory Forecasting and Generation with Conditional Flow Matching (Sean Ye et al., 2024)</a></li><li><a href=#1022--87153-robotic-task-success-evaluation-under-multi-modal-non-parametric-object-pose-uncertainty-lakshadeep-naik-et-al-2024>(10/22 | 87/153) Robotic Task Success Evaluation Under Multi-modal Non-Parametric Object Pose Uncertainty (Lakshadeep Naik et al., 2024)</a></li><li><a href=#1122--88153-fully-distributed-cooperative-multi-agent-underwater-obstacle-avoidance-under-dog-walking-paradigm-kanzhong-yao-et-al-2024>(11/22 | 88/153) Fully Distributed Cooperative Multi-agent Underwater Obstacle Avoidance Under Dog Walking Paradigm (Kanzhong Yao et al., 2024)</a></li><li><a href=#1222--89153-robust-co-design-of-canonical-underactuated-systems-for-increased-certifiable-stability-federico-girlanda-et-al-2024>(12/22 | 89/153) Robust Co-Design of Canonical Underactuated Systems for Increased Certifiable Stability (Federico Girlanda et al., 2024)</a></li><li><a href=#1322--90153-real-to-sim-adaptation-via-high-fidelity-simulation-to-control-a-wheeled-humanoid-robot-with-unknown-dynamics-donghoon-baek-et-al-2024>(13/22 | 90/153) Real-to-Sim Adaptation via High-Fidelity Simulation to Control a Wheeled-Humanoid Robot with Unknown Dynamics (Donghoon Baek et al., 2024)</a></li><li><a href=#1422--91153-gagent-an-adaptive-rigid-soft-gripping-agent-with-vision-language-models-for-complex-lighting-environments-zhuowei-li-et-al-2024>(14/22 | 91/153) GAgent: An Adaptive Rigid-Soft Gripping Agent with Vision Language Models for Complex Lighting Environments (Zhuowei Li et al., 2024)</a></li><li><a href=#1522--92153-task-driven-manipulation-with-reconfigurable-parallel-robots-daniel-morton-et-al-2024>(15/22 | 92/153) Task-Driven Manipulation with Reconfigurable Parallel Robots (Daniel Morton et al., 2024)</a></li><li><a href=#1622--93153-identifying-optimal-launch-sites-of-high-altitude-latex-balloons-using-bayesian-optimisation-for-the-task-of-station-keeping-jack-saunders-et-al-2024>(16/22 | 93/153) Identifying Optimal Launch Sites of High-Altitude Latex-Balloons using Bayesian Optimisation for the Task of Station-Keeping (Jack Saunders et al., 2024)</a></li><li><a href=#1722--94153-diffusion-reinforcement-learning-hierarchical-motion-planning-in-adversarial-multi-agent-games-zixuan-wu-et-al-2024>(17/22 | 94/153) Diffusion-Reinforcement Learning Hierarchical Motion Planning in Adversarial Multi-agent Games (Zixuan Wu et al., 2024)</a></li><li><a href=#1822--95153-dppe-dense-pose-estimation-in-a-plenoxels-environment-using-gradient-approximation-christopher-kolios-et-al-2024>(18/22 | 95/153) DPPE: Dense Pose Estimation in a Plenoxels Environment using Gradient Approximation (Christopher Kolios et al., 2024)</a></li><li><a href=#1922--96153-msi-nerf-linking-omni-depth-with-view-synthesis-through-multi-sphere-image-aided-generalizable-neural-radiance-field-dongyu-yan-et-al-2024>(19/22 | 96/153) MSI-NeRF: Linking Omni-Depth with View Synthesis through Multi-Sphere Image aided Generalizable Neural Radiance Field (Dongyu Yan et al., 2024)</a></li><li><a href=#2022--97153-h3-mapping-quasi-heterogeneous-feature-grids-for-real-time-dense-mapping-using-hierarchical-hybrid-representation-chenxing-jiang-et-al-2024>(20/22 | 97/153) H3-Mapping: Quasi-Heterogeneous Feature Grids for Real-time Dense Mapping Using Hierarchical Hybrid Representation (Chenxing Jiang et al., 2024)</a></li><li><a href=#2122--98153-quaternion-based-sliding-mode-control-for-six-degrees-of-freedom-flight-control-of-quadrotors-amin-yazdanshenas-et-al-2024>(21/22 | 98/153) Quaternion-Based Sliding Mode Control for Six Degrees of Freedom Flight Control of Quadrotors (Amin Yazdanshenas et al., 2024)</a></li><li><a href=#2222--99153-idb-rrt-sampling-based-kinodynamic-motion-planning-with-motion-primitives-and-trajectory-optimization-joaquim-ortiz-haro-et-al-2024>(22/22 | 99/153) iDb-RRT: Sampling-based Kinodynamic Motion Planning with Motion Primitives and Trajectory Optimization (Joaquim Ortiz-Haro et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--100153-transformer-based-wireless-traffic-prediction-and-network-optimization-in-o-ran-md-arafat-habib-et-al-2024>(1/1 | 100/153) Transformer-Based Wireless Traffic Prediction and Network Optimization in O-RAN (Md Arafat Habib et al., 2024)</a></li></ul></li><li><a href=#eessiv-4>eess.IV (4)</a><ul><li><a href=#14--101153-uncertainty-aware-adapter-adapting-segment-anything-model-sam-for-ambiguous-medical-image-segmentation-mingzhou-jiang-et-al-2024>(1/4 | 101/153) Uncertainty-Aware Adapter: Adapting Segment Anything Model (SAM) for Ambiguous Medical Image Segmentation (Mingzhou Jiang et al., 2024)</a></li><li><a href=#24--102153-could-we-generate-cytology-images-from-histopathology-images-an-empirical-study-soumyajyoti-dey-et-al-2024>(2/4 | 102/153) Could We Generate Cytology Images from Histopathology Images? An Empirical Study (Soumyajyoti Dey et al., 2024)</a></li><li><a href=#34--103153-microdiffusion-implicit-representation-guided-diffusion-for-3d-reconstruction-from-limited-2d-microscopy-projections-mude-hui-et-al-2024>(3/4 | 103/153) MicroDiffusion: Implicit Representation-Guided Diffusion for 3D Reconstruction from Limited 2D Microscopy Projections (Mude Hui et al., 2024)</a></li><li><a href=#44--104153-contourdiff-unpaired-image-translation-with-contour-guided-diffusion-models-yuwen-chen-et-al-2024>(4/4 | 104/153) ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion Models (Yuwen Chen et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--105153-urban-sound-propagation-a-benchmark-for-1-step-generative-modeling-of-complex-physical-systems-martin-spitznagel-et-al-2024>(1/2 | 105/153) Urban Sound Propagation: a Benchmark for 1-Step Generative Modeling of Complex Physical Systems (Martin Spitznagel et al., 2024)</a></li><li><a href=#22--106153-speech-driven-personalized-gesture-synthetics-harnessing-automatic-fuzzy-feature-inference-fan-zhang-et-al-2024>(2/2 | 106/153) Speech-driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference (Fan Zhang et al., 2024)</a></li></ul></li><li><a href=#cscr-4>cs.CR (4)</a><ul><li><a href=#14--107153-adversarial-knapsack-and-secondary-effects-of-common-information-for-cyber-operations-jon-goohs-et-al-2024>(1/4 | 107/153) Adversarial Knapsack and Secondary Effects of Common Information for Cyber Operations (Jon Goohs et al., 2024)</a></li><li><a href=#24--108153-enhancing-iot-security-against-ddos-attacks-through-federated-learning-ghazaleh-shirvani-et-al-2024>(2/4 | 108/153) Enhancing IoT Security Against DDoS Attacks through Federated Learning (Ghazaleh Shirvani et al., 2024)</a></li><li><a href=#34--109153-batch-oriented-element-wise-approximate-activation-for-privacy-preserving-neural-networks-peng-zhang-et-al-2024>(3/4 | 109/153) Batch-oriented Element-wise Approximate Activation for Privacy-Preserving Neural Networks (Peng Zhang et al., 2024)</a></li><li><a href=#44--110153-a-watermark-conditioned-diffusion-model-for-ip-protection-rui-min-et-al-2024>(4/4 | 110/153) A Watermark-Conditioned Diffusion Model for IP Protection (Rui Min et al., 2024)</a></li></ul></li><li><a href=#csmm-2>cs.MM (2)</a><ul><li><a href=#12--111153-mintrec20-a-large-scale-benchmark-dataset-for-multimodal-intent-recognition-and-out-of-scope-detection-in-conversations-hanlei-zhang-et-al-2024>(1/2 | 111/153) MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations (Hanlei Zhang et al., 2024)</a></li><li><a href=#22--112153-quality-aware-dynamic-resolution-adaptation-framework-for-adaptive-video-streaming-amritha-premkumar-et-al-2024>(2/2 | 112/153) Quality-Aware Dynamic Resolution Adaptation Framework for Adaptive Video Streaming (Amritha Premkumar et al., 2024)</a></li></ul></li><li><a href=#csit-4>cs.IT (4)</a><ul><li><a href=#14--113153-csi-transfer-from-sub-6g-to-mmwave-reduced-overhead-multi-user-hybrid-beamforming-weicao-deng-et-al-2024>(1/4 | 113/153) CSI Transfer From Sub-6G to mmWave: Reduced-Overhead Multi-User Hybrid Beamforming (Weicao Deng et al., 2024)</a></li><li><a href=#24--114153-distributed-multi-objective-dynamic-offloading-scheduling-for-air-ground-cooperative-mec-yang-huang-et-al-2024>(2/4 | 114/153) Distributed Multi-Objective Dynamic Offloading Scheduling for Air-Ground Cooperative MEC (Yang Huang et al., 2024)</a></li><li><a href=#34--115153-simultaneously-transmitting-and-reflecting-reconfigurable-intelligent-surfaces-empowered-cooperative-rate-splitting-with-user-relaying-kangchun-zhao-et-al-2024>(3/4 | 115/153) Simultaneously Transmitting and Reflecting Reconfigurable Intelligent Surfaces Empowered Cooperative Rate Splitting with User Relaying (Kangchun Zhao et al., 2024)</a></li><li><a href=#44--116153-bounding-the-graph-capacity-with-quantum-mechanics-and-finite-automata-alexander-meiburg-2024>(4/4 | 116/153) Bounding the Graph Capacity with Quantum Mechanics and Finite Automata (Alexander Meiburg, 2024)</a></li></ul></li><li><a href=#q-biogn-1>q-bio.GN (1)</a><ul><li><a href=#11--117153-stmcdi-masked-conditional-diffusion-model-with-graph-neural-network-for-spatial-transcriptomics-data-imputation-xiaoyu-li-et-al-2024>(1/1 | 117/153) stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for Spatial Transcriptomics Data Imputation (Xiaoyu Li et al., 2024)</a></li></ul></li><li><a href=#cshc-2>cs.HC (2)</a><ul><li><a href=#12--118153-human-centered-ai-for-indian-legal-text-analytics-sudipto-ghosh-et-al-2024>(1/2 | 118/153) Human Centered AI for Indian Legal Text Analytics (Sudipto Ghosh et al., 2024)</a></li><li><a href=#22--119153-from-melting-pots-to-misrepresentations-exploring-harms-in-generative-ai-sanjana-gautam-et-al-2024>(2/2 | 119/153) From Melting Pots to Misrepresentations: Exploring Harms in Generative AI (Sanjana Gautam et al., 2024)</a></li></ul></li><li><a href=#csir-1>cs.IR (1)</a><ul><li><a href=#11--120153-improving-the-robustness-of-dense-retrievers-against-typos-via-multi-positive-contrastive-learning-georgios-sidiropoulos-et-al-2024>(1/1 | 120/153) Improving the Robustness of Dense Retrievers Against Typos via Multi-Positive Contrastive Learning (Georgios Sidiropoulos et al., 2024)</a></li></ul></li><li><a href=#eessas-3>eess.AS (3)</a><ul><li><a href=#13--121153-initial-decoding-with-minimally-augmented-language-model-for-improved-lattice-rescoring-in-low-resource-asr-savitha-murthy-et-al-2024>(1/3 | 121/153) Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR (Savitha Murthy et al., 2024)</a></li><li><a href=#23--122153-fine-grained-engine-fault-sound-event-detection-using-multimodal-signals-dennis-fedorishin-et-al-2024>(2/3 | 122/153) Fine-Grained Engine Fault Sound Event Detection Using Multimodal Signals (Dennis Fedorishin et al., 2024)</a></li><li><a href=#33--123153-refining-knowledge-transfer-on-audio-image-temporal-agreement-for-audio-text-cross-retrieval-shunsuke-tsubaki-et-al-2024>(3/3 | 123/153) Refining Knowledge Transfer on Audio-Image Temporal Agreement for Audio-Text Cross Retrieval (Shunsuke Tsubaki et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--124153-inverse-learning-of-black-box-aggregator-for-robust-nash-equilibrium-guanpu-chen-et-al-2024>(1/1 | 124/153) Inverse learning of black-box aggregator for robust Nash equilibrium (Guanpu Chen et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--125153-computational-seismic-fracture-synthesis-of-tidal-barrage-using-enhanced-isotropic-plasticity-damage-mechanics-and-coupled-lagrangian-eulerian-multiphase-interaction-sayan-chowdhury-et-al-2024>(1/1 | 125/153) Computational Seismic Fracture Synthesis of Tidal Barrage using Enhanced Isotropic Plasticity Damage Mechanics and Coupled Lagrangian-Eulerian Multiphase Interaction (Sayan Chowdhury et al., 2024)</a></li></ul></li><li><a href=#eesssy-3>eess.SY (3)</a><ul><li><a href=#13--126153-stochastic-lp-string-stability-analysis-in-predecessor-following-platoons-under-packet-losses-alejandro-i-maass-et-al-2024>(1/3 | 126/153) Stochastic Lp string stability analysis in predecessor-following platoons under packet losses (Alejandro I. Maass et al., 2024)</a></li><li><a href=#23--127153-mpc-for-tracking-applied-to-rendezvous-with-non-cooperative-tumbling-targets-ensuring-stability-and-feasibility-jose-antonio-rebollo-et-al-2024>(2/3 | 127/153) MPC for Tracking applied to rendezvous with non-cooperative tumbling targets ensuring stability and feasibility (Jose Antonio Rebollo et al., 2024)</a></li><li><a href=#33--128153-extended-kalman-filtering-for-recursive-online-discrete-time-inverse-optimal-control-tian-zhao-et-al-2024>(3/3 | 128/153) Extended Kalman Filtering for Recursive Online Discrete-Time Inverse Optimal Control (Tian Zhao et al., 2024)</a></li></ul></li><li><a href=#physicsoptics-1>physics.optics (1)</a><ul><li><a href=#11--129153-multiplane-quantitative-phase-imaging-using-a-wavelength-multiplexed-diffractive-optical-processor-che-yung-shen-et-al-2024>(1/1 | 129/153) Multiplane Quantitative Phase Imaging Using a Wavelength-Multiplexed Diffractive Optical Processor (Che-Yung Shen et al., 2024)</a></li></ul></li><li><a href=#physicsplasm-ph-1>physics.plasm-ph (1)</a><ul><li><a href=#11--130153-stellarator-optimization-with-constraints-rory-conlin-et-al-2024>(1/1 | 130/153) Stellarator Optimization with Constraints (Rory Conlin et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--131153-circle-packing-problem-using-nature-inspired-optimization-techniques-pulkit-mundra-et-al-2024>(1/1 | 131/153) Circle Packing Problem Using Nature-Inspired Optimization Techniques (Pulkit Mundra et al., 2024)</a></li></ul></li><li><a href=#statml-4>stat.ML (4)</a><ul><li><a href=#14--132153-the-fallacy-of-minimizing-local-regret-in-the-sequential-task-setting-ziping-xu-et-al-2024>(1/4 | 132/153) The Fallacy of Minimizing Local Regret in the Sequential Task Setting (Ziping Xu et al., 2024)</a></li><li><a href=#24--133153-function-space-parameterization-of-neural-networks-for-sequential-learning-aidan-scannell-et-al-2024>(2/4 | 133/153) Function-space Parameterization of Neural Networks for Sequential Learning (Aidan Scannell et al., 2024)</a></li><li><a href=#34--134153-neural-kernel-conditional-mean-embeddings-eiki-shimizu-et-al-2024>(3/4 | 134/153) Neural-Kernel Conditional Mean Embeddings (Eiki Shimizu et al., 2024)</a></li><li><a href=#44--135153-a-primal-dual-algorithm-for-faster-distributionally-robust-optimization-ronak-mehta-et-al-2024>(4/4 | 135/153) A Primal-Dual Algorithm for Faster Distributionally Robust Optimization (Ronak Mehta et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--136153-modelling-co-evolution-of-resource-feedback-and-social-network-dynamics-in-human-environmental-systems-meghdad-saeedian-et-al-2024>(1/1 | 136/153) Modelling co-evolution of resource feedback and social network dynamics in human-environmental systems (Meghdad Saeedian et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--137153-a-new-coupled-electro-thermo-fluid-radiofrequency-model-of-cardiac-tissue-mathematical-modeling-analysis-and-numerical-simulation-mostafa-bendahmane-et-al-2024>(1/2 | 137/153) A new coupled electro-thermo-fluid radiofrequency model of cardiac tissue: Mathematical modeling, analysis and numerical simulation (Mostafa Bendahmane et al., 2024)</a></li><li><a href=#22--138153-high-order-well-balanced-arbitrary-lagrangian-eulerian-ader-discontinuous-galerkin-schemes-on-general-polygonal-moving-meshes-elena-gaburro-et-al-2024>(2/2 | 138/153) High order Well-Balanced Arbitrary-Lagrangian-Eulerian ADER discontinuous Galerkin schemes on general polygonal moving meshes (Elena Gaburro et al., 2024)</a></li></ul></li><li><a href=#csai-3>cs.AI (3)</a><ul><li><a href=#13--139153-game-and-reference-policy-combination-synthesis-for-epidemic-prevention-and-control-zhiyi-tan-et-al-2024>(1/3 | 139/153) Game and Reference: Policy Combination Synthesis for Epidemic Prevention and Control (Zhiyi Tan et al., 2024)</a></li><li><a href=#23--140153-scheduling-drone-and-mobile-charger-via-hybrid-action-deep-reinforcement-learning-jizhe-dou-et-al-2024>(2/3 | 140/153) Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning (Jizhe Dou et al., 2024)</a></li><li><a href=#33--141153-inducing-individual-students-learning-strategies-through-homomorphic-pomdps-huifan-gao-et-al-2024>(3/3 | 141/153) Inducing Individual Students&rsquo; Learning Strategies through Homomorphic POMDPs (Huifan Gao et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--142153-defa-efficient-deformable-attention-acceleration-via-pruning-assisted-grid-sampling-and-multi-scale-parallel-processing-yansong-xu-et-al-2024>(1/1 | 142/153) DEFA: Efficient Deformable Attention Acceleration via Pruning-Assisted Grid-Sampling and Multi-Scale Parallel Processing (Yansong Xu et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--143153-an-open-source-experimentation-framework-for-the-edge-cloud-continuum-georgios-koukis-et-al-2024>(1/1 | 143/153) An Open-Source Experimentation Framework for the Edge Cloud Continuum (Georgios Koukis et al., 2024)</a></li></ul></li><li><a href=#csse-2>cs.SE (2)</a><ul><li><a href=#12--144153-a-hypergraph-based-formalization-of-hierarchical-reactive-modules-and-a-compositional-verification-method-daisuke-ishii-2024>(1/2 | 144/153) A Hypergraph-based Formalization of Hierarchical Reactive Modules and a Compositional Verification Method (Daisuke Ishii, 2024)</a></li><li><a href=#22--145153-ipsynth-interprocedural-program-synthesis-for-software-security-implementation-ali-shokri-et-al-2024>(2/2 | 145/153) IPSynth: Interprocedural Program Synthesis for Software Security Implementation (Ali Shokri et al., 2024)</a></li></ul></li><li><a href=#quant-ph-3>quant-ph (3)</a><ul><li><a href=#13--146153-fedqnn-federated-learning-using-quantum-neural-networks-nouhaila-innan-et-al-2024>(1/3 | 146/153) FedQNN: Federated Learning using Quantum Neural Networks (Nouhaila Innan et al., 2024)</a></li><li><a href=#23--147153-quantumleak-stealing-quantum-neural-networks-from-cloud-based-nisq-machines-zhenxiao-fu-et-al-2024>(2/3 | 147/153) QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines (Zhenxiao Fu et al., 2024)</a></li><li><a href=#33--148153-multi-controlled-phase-gate-synthesis-with-zx-calculus-applied-to-neutral-atom-hardware-korbinian-staudacher-et-al-2024>(3/3 | 148/153) Multi-controlled Phase Gate Synthesis with ZX-calculus applied to Neutral Atom Hardware (Korbinian Staudacher et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--149153-a-comprehensive-review-of-latent-space-dynamics-identification-algorithms-for-intrusive-and-non-intrusive-reduced-order-modeling-christophe-bonneville-et-al-2024>(1/1 | 149/153) A Comprehensive Review of Latent Space Dynamics Identification Algorithms for Intrusive and Non-Intrusive Reduced-Order-Modeling (Christophe Bonneville et al., 2024)</a></li></ul></li><li><a href=#q-biomn-1>q-bio.MN (1)</a><ul><li><a href=#11--150153-identifying-the-attractors-of-gene-regulatory-networks-from-expression-data-under-uncertainty-an-interpretable-approach-alireza-rowhanimanesh-2024>(1/1 | 150/153) Identifying the Attractors of Gene Regulatory Networks from Expression Data under Uncertainty: An Interpretable Approach (Alireza Rowhanimanesh, 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--151153-on-extended-perfect-codes-konstantin-vorobev-2024>(1/1 | 151/153) On extended perfect codes (Konstantin Vorob&rsquo;ev, 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--152153-solving-the-multiobjective-quasi-clique-problem-daniela-scherer-dos-santos-et-al-2024>(1/1 | 152/153) Solving the Multiobjective Quasi-Clique Problem (Daniela Scherer dos Santos et al., 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#11--153153-approximation-ratio-of-the-min-degree-greedy-algorithm-for-maximum-independent-set-on-interval-and-chordal-graphs-steven-chaplick-et-al-2024>(1/1 | 153/153) Approximation Ratio of the Min-Degree Greedy Algorithm for Maximum Independent Set on Interval and Chordal Graphs (Steven Chaplick et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>