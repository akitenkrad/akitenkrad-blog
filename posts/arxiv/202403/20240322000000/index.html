<!doctype html><html><head><title>arXiv @ 2024.03.22</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.22"><meta property="og:description" content="Primary Categories cond-mat.mtrl-sci (1) cs.AI (5) cs.AR (1) cs.CE (3) cs.CG (1) cs.CL (33) cs.CR (9) cs.CV (89) cs.CY (4) cs.DB (2) cs.DC (5) cs.DL (1) cs.HC (5) cs.IR (6) cs.IT (2) cs.LG (32) cs.LO (3) cs.MA (3) cs.NI (2) cs.RO (23) cs.SC (1) cs.SD (5) cs.SE (9) cs.SI (5) eess.AS (2) eess.IV (3) eess.SY (14) math.NA (2) math.OC (4) physics.bio-ph (1) physics.chem-ph (1) physics.flu-dyn (1) q-bio.NC (1) q-bio.QM (1) q-fin."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240322000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-22T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-22T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.22"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240322000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Friday, Mar 22, 2024</p></div><div class=title><h1>arXiv @ 2024.03.22</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#csai-5>cs.AI (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#csce-3>cs.CE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#cscl-33>cs.CL (33)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#cscr-9>cs.CR (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#cscv-89>cs.CV (89)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#cscy-4>cs.CY (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#csdb-2>cs.DB (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#csdc-5>cs.DC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#csdl-1>cs.DL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#cshc-5>cs.HC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#csir-6>cs.IR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#cslg-32>cs.LG (32)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#cslo-3>cs.LO (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#csma-3>cs.MA (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#csro-23>cs.RO (23)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#cssc-1>cs.SC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#cssd-5>cs.SD (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#csse-9>cs.SE (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#cssi-5>cs.SI (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#eessiv-3>eess.IV (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#eesssy-14>eess.SY (14)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#mathoc-4>math.OC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#physicsbio-ph-1>physics.bio-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#physicschem-ph-1>physics.chem-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#q-fintr-1>q-fin.TR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/#statml-4>stat.ML (4)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th><th>eess.SY</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td>3</td><td>2</td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td>1</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Benchmarking</td><td>4</td><td>23</td><td>7</td><td>4</td><td>1</td></tr><tr><td>Black Box</td><td>1</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>ChatGPT</td><td>3</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Chatbot</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>2</td><td></td><td></td><td>1</td></tr><tr><td>Code Generation</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>10</td><td>1</td><td>1</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>12</td><td>2</td><td></td><td></td></tr><tr><td>Counter-factual</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Dialogue System</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Diffusion Model</td><td></td><td>20</td><td>1</td><td>1</td><td></td></tr><tr><td>Direct Preference Optimization</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Disambiguation</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Distribution Shift</td><td></td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td>1</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fact Verification</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fake News Detection</td><td>2</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Few-shot</td><td>2</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>12</td><td>10</td><td>4</td><td></td><td></td></tr><tr><td>Foundation Model</td><td>1</td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>GLUE</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT</td><td>2</td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>GPT-2</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-3</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Gemini</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>4</td><td>3</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td>5</td><td></td><td></td><td></td></tr><tr><td>Graph</td><td>1</td><td>3</td><td>5</td><td>3</td><td>2</td></tr><tr><td>Graph Attention Networks</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>5</td><td></td><td></td></tr><tr><td>Grounding</td><td>1</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>In-context Learning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>8</td><td>6</td><td>2</td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td>1</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Language Generation</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>30</td><td>12</td><td>5</td><td>2</td><td></td></tr><tr><td>Low-Resource</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Masked Language Model</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td></td><td>26</td><td>2</td><td>4</td><td>1</td></tr><tr><td>Multiple Instance Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Mutual Information</td><td>1</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>8</td><td></td><td></td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Open Information Extraction</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Open-Domain Dialogue</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Parameter Sharing</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Perplexity</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>6</td><td></td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Prompt</td><td>7</td><td>19</td><td>2</td><td>2</td><td></td></tr><tr><td>Prompt Learning</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Question Answering</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>6</td><td>3</td><td>1</td><td>1</td><td></td></tr><tr><td>Recommendation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td></td><td>2</td><td>3</td><td>2</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>RoBERTa</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Self-Distillation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>5</td><td>1</td><td></td><td></td></tr><tr><td>Sentence Embedding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td>1</td><td>11</td><td>11</td></tr><tr><td>Simulator</td><td></td><td></td><td>1</td><td>11</td><td>11</td></tr><tr><td>Sora</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Square Loss</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Style Transfer</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td>4</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>1</td><td>5</td><td>3</td><td>1</td><td></td></tr><tr><td>Text Generation</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>6</td><td></td><td></td><td></td></tr><tr><td>Tokenization</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td>1</td><td>1</td><td></td><td>1</td></tr><tr><td>Transformer</td><td>1</td><td>12</td><td>3</td><td>2</td><td></td></tr><tr><td>Unsupervised Learning</td><td>2</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>8</td><td></td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>9</td><td>1</td><td>1</td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Word Embedding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>3</td><td>3</td><td>1</td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=csir-6>cs.IR (6)</h2><h3 id=16--1286-harnessing-large-language-models-for-text-rich-sequential-recommendation-zhi-zheng-et-al-2024>(1/6 | 1/286) Harnessing Large Language Models for Text-Rich Sequential Recommendation (Zhi Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhi Zheng, Wenshuo Chao, Zhaopeng Qiu, Hengshu Zhu, Hui Xiong. (2024)<br><strong>Harnessing Large Language Models for Text-Rich Sequential Recommendation</strong><br><button class=copy-to-clipboard title="Harnessing Large Language Models for Text-Rich Sequential Recommendation" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 140<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Fine-tuning, Fine-tuning, Recommendation, Recommender System, Supervised Learning, Recurrent Neural Network, Recurrent Neural Network, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13325v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13325v1.pdf filename=2403.13325v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have been changing the paradigm of <b>Recommender</b> <b>Systems</b> (RS). However, when items in the <b>recommendation</b> scenarios contain rich textual information, such as product descriptions in online shopping or news headlines on social media, <b>LLMs</b> require longer texts to comprehensively depict the historical user behavior sequence. This poses significant challenges to <b>LLM-based</b> <b>recommenders,</b> <b>such</b> as over-length limitations, extensive time and space overheads, and suboptimal model performance. To this end, in this paper, we design a novel framework for harnessing <b>Large</b> <b>Language</b> <b>Models</b> for Text-Rich Sequential <b>Recommendation</b> <b>(LLM-TRSR).</b> Specifically, we first propose to segment the user historical behaviors and subsequently employ an <b>LLM-based</b> summarizer for summarizing these user behavior blocks. Particularly, drawing inspiration from the successful application of <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNN)</b> and <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNN)</b> models in user modeling, we introduce two unique <b>summarization</b> techniques in this paper, respectively hierarchical <b>summarization</b> and <b>recurrent</b> <b>summarization.</b> <b>Then,</b> we construct a <b>prompt</b> text encompassing the user preference summary, recent user interactions, and candidate item information into an <b>LLM-based</b> <b>recommender,</b> <b>which</b> is subsequently <b>fine-tuned</b> using <b>Supervised</b> <b>Fine-Tuning</b> (SFT) techniques to yield our final <b>recommendation</b> model. We also use Low-Rank Adaptation (LoRA) for Parameter-Efficient <b>Fine-Tuning</b> (PEFT). We conduct experiments on two public datasets, and the results clearly demonstrate the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=26--2286-a-large-language-model-enhanced-sequential-recommender-for-joint-video-and-comment-recommendation-bowen-zheng-et-al-2024>(2/6 | 2/286) A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation (Bowen Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Zheng, Zihan Lin, Enze Liu, Chen Yang, Enyang Bai, Cheng Ling, Wayne Xin Zhao, Ji-Rong Wen. (2024)<br><strong>A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation</strong><br><button class=copy-to-clipboard title="A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Recommendation, Recommender System, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13574v1.pdf filename=2403.13574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In online video platforms, reading or writing comments on interesting videos has become an essential part of the video watching experience. However, existing video <b>recommender</b> <b>systems</b> mainly model users&rsquo; interaction behaviors with videos, lacking consideration of comments in user behavior modeling. In this paper, we propose a novel <b>recommendation</b> approach called LSVCR by leveraging user interaction histories with both videos and comments, so as to jointly conduct personalized video and comment <b>recommendation.</b> Specifically, our approach consists of two key components, namely sequential <b>recommendation</b> (SR) model and supplemental <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> <b>recommender.</b> <b>The</b> SR model serves as the primary <b>recommendation</b> backbone (retained in deployment) of our approach, allowing for efficient user preference modeling. Meanwhile, we leverage the <b>LLM</b> <b>recommender</b> <b>as</b> a supplemental component (discarded in deployment) to better capture underlying user preferences from heterogeneous interaction behaviors. In order to integrate the merits of the SR model and the supplemental <b>LLM</b> <b>recommender,</b> <b>we</b> design a twostage training paradigm. The first stage is personalized preference alignment, which aims to align the preference representations from both components, thereby enhancing the semantics of the SR model. The second stage is <b>recommendation-oriented</b> <b>fine-tuning,</b> in which the alignment-enhanced SR model is <b>fine-tuned</b> according to specific objectives. Extensive experiments in both video and comment <b>recommendation</b> tasks demonstrate the effectiveness of LSVCR. Additionally, online A/B testing on the KuaiShou platform verifies the actual benefits brought by our approach. In particular, we achieve a significant overall gain of 4.13% in comment watch time.</p></p class="citation"></blockquote><h3 id=36--3286-desire-me-domain-enhanced-supervised-information-retrieval-using-mixture-of-experts-pranav-kasela-et-al-2024>(3/6 | 3/286) DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using Mixture-of-Experts (Pranav Kasela et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pranav Kasela, Gabriella Pasi, Raffaele Perego, Nicola Tonellotto. (2024)<br><strong>DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using Mixture-of-Experts</strong><br><button class=copy-to-clipboard title="DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using Mixture-of-Experts" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 60<br>Keywords: Graph Attention Networks, Dense Retrieval, Supervised Learning, Information Retrieval, Open-Domain Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13468v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13468v1.pdf filename=2403.13468v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Open-domain</b> <b>question</b> <b>answering</b> requires retrieval systems able to cope with the diverse and varied nature of <b>questions,</b> <b>providing</b> accurate answers across a broad spectrum of query types and topics. To deal with such topic heterogeneity through a unique model, we propose DESIRE-ME, a neural <b>information</b> <b>retrieval</b> model that leverages the Mixture-of-Experts framework to combine multiple specialized neural models. We rely on Wikipedia data to train an effective neural <b>gating</b> mechanism that classifies the incoming query and that weighs the predictions of the different domain-specific experts correspondingly. This allows DESIRE-ME to specialize adaptively in multiple domains. Through extensive experiments on publicly available datasets, we show that our proposal can effectively generalize domain-enhanced neural models. DESIRE-ME excels in handling <b>open-domain</b> <b>questions</b> <b>adaptively,</b> boosting by up to 12% in NDCG@10 and 22% in P@1, the underlying state-of-the-art <b>dense</b> <b>retrieval</b> model.</p></p class="citation"></blockquote><h3 id=46--4286-flickr30k-cfq-a-compact-and-fragmented-query-dataset-for-text-image-retrieval-haoyu-liu-et-al-2024>(4/6 | 4/286) Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image Retrieval (Haoyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Liu, Yaoxian Song, Xuwu Wang, Zhu Xiangru, Zhixu Li, Wei Song, Tiefeng Li. (2024)<br><strong>Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image Retrieval</strong><br><button class=copy-to-clipboard title="Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image Retrieval" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 43<br>Keywords: Multi-modal, Text2image, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13317v1.pdf filename=2403.13317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the explosive growth of <b>multi-modal</b> information on the Internet, unimodal search cannot satisfy the requirement of Internet applications. <b>Text-image</b> retrieval research is needed to realize high-quality and efficient retrieval between different modalities. Existing <b>text-image</b> retrieval research is mostly based on general <b>vision-language</b> datasets (e.g. MS-COCO, Flickr30K), in which the query utterance is rigid and unnatural (i.e. verbosity and formality). To overcome the shortcoming, we construct a new Compact and Fragmented Query challenge dataset (named Flickr30K-CFQ) to model <b>text-image</b> retrieval task considering multiple query content and style, including compact and fine-grained entity-relation corpus. We propose a novel query-enhanced <b>text-image</b> retrieval method using <b>prompt</b> engineering based on <b>LLM.</b> Experiments show that our proposed Flickr30-CFQ reveals the insufficiency of existing <b>vision-language</b> datasets in realistic <b>text-image</b> tasks. Our <b>LLM-based</b> Query-enhanced method applied on different existing <b>text-image</b> retrieval models improves query understanding performance both on public dataset and our challenge set Flickr30-CFQ with over 0.9% and 2.4% respectively. Our project can be available anonymously in <a href=https://sites.google.com/view/Flickr30K-cfq>https://sites.google.com/view/Flickr30K-cfq</a>.</p></p class="citation"></blockquote><h3 id=56--5286-an-analysis-on-matching-mechanisms-and-token-pruning-for-late-interaction-models-qi-liu-et-al-2024>(5/6 | 5/286) An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models (Qi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Liu, Gang Guo, Jiaxin Mao, Zhicheng Dou, Ji-Rong Wen, Hao Jiang, Xinyu Zhang, Zhao Cao. (2024)<br><strong>An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models</strong><br><button class=copy-to-clipboard title="An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Dense Retrieval, Pruning, Bag-of-Words, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13291v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13291v1.pdf filename=2403.13291v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of <b>pre-trained</b> <b>language</b> <b>models,</b> the <b>dense</b> <b>retrieval</b> models have become promising alternatives to the traditional retrieval models that rely on exact match and sparse <b>bag-of-words</b> representations. Different from most <b>dense</b> <b>retrieval</b> models using a bi-encoder to encode each query or document into a <b>dense</b> <b>vector,</b> the recently proposed late-interaction multi-vector models (i.e., ColBERT and COIL) achieve state-of-the-art retrieval effectiveness by using all token embeddings to represent documents and queries and modeling their relevance with a sum-of-max operation. However, these fine-grained representations may cause unacceptable storage overhead for practical search systems. In this study, we systematically analyze the matching mechanism of these late-interaction models and show that the sum-of-max operation heavily relies on the co-occurrence signals and some important words in the document. Based on these findings, we then propose several simple document <b>pruning</b> methods to reduce the storage overhead and compare the effectiveness of different <b>pruning</b> methods on different late-interaction models. We also leverage query <b>pruning</b> methods to further reduce the retrieval latency. We conduct extensive experiments on both in-domain and out-domain datasets and show that some of the used <b>pruning</b> methods can significantly improve the efficiency of these late-interaction models without substantially hurting their retrieval effectiveness.</p></p class="citation"></blockquote><h3 id=66--6286-a-semantic-search-engine-for-mathlib4-guoxiong-gao-et-al-2024>(6/6 | 6/286) A Semantic Search Engine for Mathlib4 (Guoxiong Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoxiong Gao, Haocheng Ju, Jiedong Jiang, Zihan Qin, Bin Dong. (2024)<br><strong>A Semantic Search Engine for Mathlib4</strong><br><button class=copy-to-clipboard title="A Semantic Search Engine for Mathlib4" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs-LO, cs.IR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13310v1.pdf filename=2403.13310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The interactive theorem prover, Lean, enables the verification of formal mathematical proofs and is backed by an expanding community. Central to this ecosystem is its mathematical library, mathlib4, which lays the groundwork for the formalization of an expanding range of mathematical theories. However, searching for theorems in mathlib4 can be challenging. To successfully search in mathlib4, users often need to be familiar with its naming conventions or documentation strings. Therefore, creating a semantic search engine that can be used easily by individuals with varying familiarity with mathlib4 is very important. In this paper, we present a semantic search engine for mathlib4 that accepts informal queries and finds the relevant theorems. We also establish a <b>benchmark</b> for assessing the performance of various search engines for mathlib4.</p></p class="citation"></blockquote><h2 id=cscv-89>cs.CV (89)</h2><h3 id=189--7286-mtp-advancing-remote-sensing-foundation-model-via-multi-task-pretraining-di-wang-et-al-2024>(1/89 | 7/286) MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining (Di Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Di Wang, Jing Zhang, Minqiang Xu, Lin Liu, Dongsheng Wang, Erzhong Gao, Chengxi Han, Haonan Guo, Bo Du, Dacheng Tao, Liangpei Zhang. (2024)<br><strong>MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining</strong><br><button class=copy-to-clipboard title="MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 110<br>Keywords: Vision Transformer, Object Detection, Convolution, Convolutional Neural Network, Fine-tuning, Foundation Model, Self-supervised Learning, Self-supervised Learning, Supervised Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13430v1.pdf filename=2403.13430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> have reshaped the landscape of Remote Sensing (RS) by enhancing various image interpretation tasks. Pretraining is an active research topic, encompassing <b>supervised</b> and <b>self-supervised</b> <b>learning</b> methods to initialize model weights effectively. However, transferring the pretrained models to downstream tasks may encounter task discrepancy due to their formulation of pretraining as image classification or <b>object</b> <b>discrimination</b> tasks. In this study, we explore the Multi-Task Pretraining (MTP) paradigm for RS <b>foundation</b> <b>models</b> to address this issue. Using a shared encoder and task-specific decoder architecture, we conduct multi-task <b>supervised</b> pretraining on the SAMRS dataset, encompassing semantic segmentation, instance segmentation, and rotated <b>object</b> <b>detection.</b> MTP supports both <b>convolutional</b> <b>neural</b> <b>networks</b> and <b>vision</b> <b>transformer</b> <b>foundation</b> <b>models</b> with over 300 million parameters. The pretrained models are <b>finetuned</b> on various RS downstream tasks, such as scene classification, horizontal and rotated <b>object</b> <b>detection,</b> semantic segmentation, and change detection. Extensive experiments across 14 datasets demonstrate the superiority of our models over existing ones of similar size and their competitive performance compared to larger state-of-the-art models, thus validating the effectiveness of MTP.</p></p class="citation"></blockquote><h3 id=289--8286-rar-retrieving-and-ranking-augmented-mllms-for-visual-recognition-ziyu-liu-et-al-2024>(2/89 | 8/286) RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition (Ziyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang. (2024)<br><strong>RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition</strong><br><button class=copy-to-clipboard title="RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 79<br>Keywords: Object Detection, Benchmarking, Contrastive Learning, Few-shot, Multi-modal, Multi-modal, Zero-shot, Image2text, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13805v1.pdf filename=2403.13805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>CLIP <b>(Contrastive</b> <b>Language-Image</b> Pre-training) uses <b>contrastive</b> <b>learning</b> from noise <b>image-text</b> pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items. Conversely, <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) excel at classifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora. However, the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size. To synergize the strengths of both approaches and enhance the few-shot/zero-shot recognition abilities for datasets characterized by extensive and fine-grained vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented method for MLLMs. We initially establish a <b>multi-modal</b> retriever based on CLIP to create and store explicit memory for different categories beyond the immediate context window. During inference, RAR retrieves the top-k similar results from the memory and uses MLLMs to rank and make the final predictions. Our proposed approach not only addresses the inherent limitations in fine-grained recognition but also preserves the model&rsquo;s comprehensive knowledge base, significantly boosting accuracy across a range of <b>vision-language</b> recognition tasks. Notably, our approach demonstrates a significant improvement in performance on 5 fine-grained visual recognition <b>benchmarks,</b> 11 <b>few-shot</b> image recognition datasets, and the 2 <b>object</b> <b>detection</b> datasets under the <b>zero-shot</b> recognition setting.</p></p class="citation"></blockquote><h3 id=389--9286-learning-from-models-and-data-for-visual-grounding-ruozhen-he-et-al-2024>(3/89 | 9/286) Learning from Models and Data for Visual Grounding (Ruozhen He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruozhen He, Paola Cascante-Bonilla, Ziyan Yang, Alexander C. Berg, Vicente Ordonez. (2024)<br><strong>Learning from Models and Data for Visual Grounding</strong><br><button class=copy-to-clipboard title="Learning from Models and Data for Visual Grounding" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 70<br>Keywords: Fine-tuning, Knowledge Transfer, Grounding, Text2image, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13804v1.pdf filename=2403.13804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce SynGround, a novel framework that combines data-driven learning and <b>knowledge</b> <b>transfer</b> from various <b>large-scale</b> <b>pretrained</b> <b>models</b> to enhance the visual <b>grounding</b> capabilities of a pretrained <b>vision-and-language</b> model. The <b>knowledge</b> <b>transfer</b> from the models initiates the generation of image descriptions through an image description generator. These descriptions serve dual purposes: they act as <b>prompts</b> for synthesizing images through a <b>text-to-image</b> generator, and as queries for synthesizing text, from which phrases are extracted using a <b>large</b> <b>language</b> <b>model.</b> Finally, we leverage an open-vocabulary object detector to generate synthetic bounding boxes for the synthetic images and texts. We <b>finetune</b> a pretrained <b>vision-and-language</b> model on this dataset by optimizing a mask-attention consistency objective that aligns region annotations with gradient-based model explanations. The resulting model improves the <b>grounding</b> capabilities of an off-the-shelf <b>vision-and-language</b> model. Particularly, SynGround improves the pointing game accuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on RefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to 63.67%.</p></p class="citation"></blockquote><h3 id=489--10286-multi-modal-hallucination-control-by-visual-information-grounding-alessandro-favero-et-al-2024>(4/89 | 10/286) Multi-Modal Hallucination Control by Visual Information Grounding (Alessandro Favero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto. (2024)<br><strong>Multi-Modal Hallucination Control by Visual Information Grounding</strong><br><button class=copy-to-clipboard title="Multi-Modal Hallucination Control by Visual Information Grounding" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 66<br>Keywords: Benchmarking, Direct Preference Optimization, Multi-modal, Mutual Information, Grounding, Visual Question Answering, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14003v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14003v1.pdf filename=2403.14003v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative <b>Vision-Language</b> Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image. We investigate this phenomenon, usually referred to as &ldquo;hallucination&rdquo; and show that it stems from an excessive reliance on the language prior. In particular, we show that as more tokens are generated, the reliance on the visual <b>prompt</b> decreases, and this behavior strongly correlates with the emergence of hallucinations. To reduce hallucinations, we introduce <b>Multi-Modal</b> <b>Mutual-Information</b> <b>Decoding</b> (M3ID), a new sampling method for <b>prompt</b> amplification. M3ID amplifies the influence of the reference image over the language prior, hence favoring the generation of tokens with higher <b>mutual</b> <b>information</b> with the visual <b>prompt.</b> M3ID can be applied to any pre-trained autoregressive VLM at inference time without necessitating further training and with minimal computational overhead. If training is an option, we show that M3ID can be paired with <b>Direct</b> <b>Preference</b> <b>Optimization</b> (DPO) to improve the model&rsquo;s reliance on the <b>prompt</b> image without requiring any labels. Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers. Specifically, for the LLaVA 13B model, M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on <b>VQA</b> <b>benchmarks</b> such as POPE by 21% and 24%.</p></p class="citation"></blockquote><h3 id=589--11286-agfsync-leveraging-ai-generated-feedback-for-preference-optimization-in-text-to-image-generation-jingkun-an-et-al-2024>(5/89 | 11/286) AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in Text-to-Image Generation (Jingkun An et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingkun An, Yinghao Zhu, Zongjian Li, Haoran Feng, Bohua Chen, Yemin Shi, Chengwei Pan. (2024)<br><strong>AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in Text-to-Image Generation" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Diffusion Model, Benchmarking, Direct Preference Optimization, Text2image, Visual Question Answering, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13352v1.pdf filename=2403.13352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-Image</b> (T2I) <b>diffusion</b> <b>models</b> have achieved remarkable success in image generation. Despite their progress, challenges remain in both <b>prompt-following</b> ability, image quality and lack of high-quality datasets, which are essential for refining these models. As acquiring labeled data is costly, we introduce AGFSync, a framework that enhances T2I <b>diffusion</b> <b>models</b> through <b>Direct</b> <b>Preference</b> <b>Optimization</b> (DPO) in a fully AI-driven approach. AGFSync utilizes <b>Vision-Language</b> Models (VLM) to assess image quality across style, coherence, and aesthetics, generating feedback data within an AI-driven loop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and SDXL, our extensive experiments on the TIFA dataset demonstrate notable improvements in <b>VQA</b> scores, aesthetic evaluations, and performance on the HPSv2 <b>benchmark,</b> consistently outperforming the base models. AGFSync&rsquo;s method of refining T2I <b>diffusion</b> <b>models</b> paves the way for scalable alignment techniques.</p></p class="citation"></blockquote><h3 id=689--12286-enhancing-fingerprint-image-synthesis-with-gans-diffusion-models-and-style-transfer-techniques-w-tang-et-al-2024>(6/89 | 12/286) Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and Style Transfer Techniques (W. Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>W. Tang, D. Figueroa, D. Liu, K. Johnsson, A. Sopasakis. (2024)<br><strong>Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and Style Transfer Techniques</strong><br><button class=copy-to-clipboard title="Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and Style Transfer Techniques" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 60<br>Keywords: Diffusion Model, Autoencoder, Generative Adversarial Network, Generative Adversarial Network, Image2text, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13916v1.pdf filename=2403.13916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present novel approaches involving <b>generative</b> <b>adversarial</b> <b>networks</b> and <b>diffusion</b> <b>models</b> in order to synthesize high quality, live and spoof fingerprint <b>images</b> <b>while</b> preserving features such as uniqueness and diversity. We generate live fingerprints from noise with a variety of methods, and we use <b>image</b> <b>translation</b> techniques to translate live fingerprint <b>images</b> <b>to</b> spoof. To generate different types of spoof <b>images</b> <b>based</b> on limited training data we incorporate <b>style</b> <b>transfer</b> techniques through a cycle <b>autoencoder</b> equipped with a Wasserstein metric along with Gradient Penalty (CycleWGAN-GP) in order to avoid mode collapse and instability. We find that when the spoof training data includes distinct spoof characteristics, it leads to improved live-to-spoof translation. We assess the diversity and realism of the generated live fingerprint <b>images</b> <b>mainly</b> through the Fr'echet Inception Distance (FID) and the False Acceptance Rate (FAR). Our best <b>diffusion</b> <b>model</b> achieved a FID of 15.78. The comparable WGAN-GP model achieved slightly higher FID while performing better in the uniqueness assessment due to a slightly lower FAR when matched against the training data, indicating better creativity. Moreover, we give example <b>images</b> <b>showing</b> that a DDPM model clearly can generate realistic fingerprint images.</p></p class="citation"></blockquote><h3 id=789--13286-reground-improving-textual-and-spatial-grounding-at-no-cost-yuseung-lee-et-al-2024>(7/89 | 13/286) ReGround: Improving Textual and Spatial Grounding at No Cost (Yuseung Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuseung Lee, Minhyuk Sung. (2024)<br><strong>ReGround: Improving Textual and Spatial Grounding at No Cost</strong><br><button class=copy-to-clipboard title="ReGround: Improving Textual and Spatial Grounding at No Cost" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Diffusion Model, Graph Attention Networks, Fine-tuning, Grounding, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13589v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13589v1.pdf filename=2403.13589v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When an image generation process is guided by both a text <b>prompt</b> and spatial cues, such as a set of bounding boxes, do these elements work in harmony, or does one dominate the other? Our analysis of a pretrained image <b>diffusion</b> <b>model</b> that integrates <b>gated</b> <b>self-attention</b> into the U-Net reveals that spatial <b>grounding</b> often outweighs textual <b>grounding</b> due to the sequential flow from <b>gated</b> <b>self-attention</b> to cross-attention. We demonstrate that such bias can be significantly mitigated without sacrificing accuracy in either <b>grounding</b> by simply rewiring the network architecture, changing from sequential to parallel for <b>gated</b> <b>self-attention</b> and cross-attention. This surprisingly simple yet effective solution does not require any <b>fine-tuning</b> of the network but significantly reduces the trade-off between the two <b>groundings.</b> Our experiments demonstrate significant improvements from the original GLIGEN to the rewired version in the trade-off between textual <b>grounding</b> and spatial <b>grounding.</b></p></p class="citation"></blockquote><h3 id=889--14286-t-pixel2mesh-combining-global-and-local-transformer-for-3d-mesh-generation-from-a-single-image-shijie-zhang-et-al-2024>(8/89 | 14/286) T-Pixel2Mesh: Combining Global and Local Transformer for 3D Mesh Generation from a Single Image (Shijie Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shijie Zhang, Boyan Jiang, Keke He, Junwei Zhu, Ying Tai, Chengjie Wang, Yinda Zhang, Yanwei Fu. (2024)<br><strong>T-Pixel2Mesh: Combining Global and Local Transformer for 3D Mesh Generation from a Single Image</strong><br><button class=copy-to-clipboard title="T-Pixel2Mesh: Combining Global and Local Transformer for 3D Mesh Generation from a Single Image" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 58<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Geometry, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13663v1.pdf filename=2403.13663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pixel2Mesh (P2M) is a classical approach for reconstructing 3D shapes from a single color image through coarse-to-fine mesh deformation. Although P2M is capable of generating plausible global shapes, its <b>Graph</b> <b>Convolution</b> <b>Network</b> <b>(GCN)</b> often produces overly smooth results, causing the loss of fine-grained <b>geometry</b> details. Moreover, P2M generates non-credible features for occluded regions and struggles with the domain gap from synthetic data to real-world images, which is a common challenge for single-view 3D reconstruction methods. To address these challenges, we propose a novel <b>Transformer-boosted</b> architecture, named T-Pixel2Mesh, inspired by the coarse-to-fine approach of P2M. Specifically, we use a global <b>Transformer</b> to control the holistic shape and a local <b>Transformer</b> to progressively refine the local <b>geometry</b> details with <b>graph-based</b> point upsampling. To enhance real-world reconstruction, we present the simple yet effective Linear Scale Search (LSS), which serves as <b>prompt</b> tuning during the input preprocessing. Our experiments on ShapeNet demonstrate state-of-the-art performance, while results on real-world data show the generalization capability.</p></p class="citation"></blockquote><h3 id=989--15286-ground-a-score-scaling-up-the-score-distillation-for-multi-attribute-editing-hangeol-chang-et-al-2024>(9/89 | 15/286) Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute Editing (Hangeol Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hangeol Chang, Jinho Chang, Jong Chul Ye. (2024)<br><strong>Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute Editing</strong><br><button class=copy-to-clipboard title="Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute Editing" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Knowledge Distillation, Grounding, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13551v1.pdf filename=2403.13551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite recent advancements in <b>text-to-image</b> <b>diffusion</b> <b>models</b> facilitating various image editing techniques, complex text <b>prompts</b> often lead to an oversight of some requests due to a bottleneck in processing text information. To tackle this challenge, we present Ground-A-Score, a simple yet powerful model-agnostic image editing method by incorporating <b>grounding</b> during score <b>distillation.</b> This approach ensures a precise reflection of intricate <b>prompt</b> requirements in the editing outcomes, taking into account the prior knowledge of the object locations within the image. Moreover, the selective application with a new penalty coefficient and contrastive loss helps to precisely target editing areas while preserving the integrity of the objects in the source image. Both qualitative assessments and quantitative analyses confirm that Ground-A-Score successfully adheres to the intricate details of extended and multifaceted <b>prompts,</b> ensuring high-quality outcomes that respect the original image attributes.</p></p class="citation"></blockquote><h3 id=1089--16286-improved-baselines-for-data-efficient-perceptual-augmentation-of-llms-théophane-vallaeys-et-al-2024>(10/89 | 16/286) Improved Baselines for Data-efficient Perceptual Augmentation of LLMs (Théophane Vallaeys et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Théophane Vallaeys, Mustafa Shukor, Matthieu Cord, Jakob Verbeek. (2024)<br><strong>Improved Baselines for Data-efficient Perceptual Augmentation of LLMs</strong><br><button class=copy-to-clipboard title="Improved Baselines for Data-efficient Perceptual Augmentation of LLMs" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13499v1.pdf filename=2403.13499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The abilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently progressed to unprecedented levels, paving the way to novel applications in a wide variety of areas. In computer vision, <b>LLMs</b> can be used to prime <b>vision-language</b> tasks such image captioning and <b>visual</b> <b>question</b> <b>answering</b> when coupled with pre-trained vision backbones. While different approaches have been explored to interface <b>LLMs</b> with ``perceptual backbones&rsquo;&rsquo; that process, e.g., <b>visual</b> <b>or</b> <b>audio</b> data, they are often explored for different tasks, different datasets, and using different perceptual backbones and language models, hindering direct comparison of the interfacing mechanisms. To remedy this lack of comparability between methods, we present an extensive experimental evaluation of different interfacing mechanisms, across multiple tasks (including image, video, and audio captioning as well as <b>visual</b> <b>question</b> <b>answering),</b> datasets and backbones, paying special attention to low-data settings. We find improved performance using existing mechanisms over state-of-the-art results, and identify a new interfacing mechanism that yields (near) optimal results across different tasks, while obtaining a 4x reduction in training time.</p></p class="citation"></blockquote><h3 id=1189--17286-editing-massive-concepts-in-text-to-image-diffusion-models-tianwei-xiong-et-al-2024>(11/89 | 17/286) Editing Massive Concepts in Text-to-Image Diffusion Models (Tianwei Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianwei Xiong, Yue Wu, Enze Xie, Yue Wu, Zhenguo Li, Xihui Liu. (2024)<br><strong>Editing Massive Concepts in Text-to-Image Diffusion Models</strong><br><button class=copy-to-clipboard title="Editing Massive Concepts in Text-to-Image Diffusion Models" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 43<br>Keywords: Diffusion Model, Benchmarking, Self-Distillation, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13807v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13807v1.pdf filename=2403.13807v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> <b>diffusion</b> <b>models</b> suffer from the risk of generating outdated, copyrighted, incorrect, and biased content. While previous methods have mitigated the issues on a small scale, it is essential to handle them simultaneously in larger-scale real-world scenarios. We propose a two-stage method, Editing Massive Concepts In <b>Diffusion</b> <b>Models</b> (EMCID). The first stage performs memory optimization for each individual concept with dual <b>self-distillation</b> from text alignment loss and <b>diffusion</b> <b>noise</b> prediction loss. The second stage conducts massive concept editing with multi-layer, closed form model editing. We further propose a comprehensive <b>benchmark,</b> named ImageNet Concept Editing <b>Benchmark</b> (ICEB), for evaluating massive concept editing for T2I models with two subtasks, free-form <b>prompts,</b> massive concept categories, and extensive evaluation metrics. Extensive experiments conducted on our proposed <b>benchmark</b> and previous <b>benchmarks</b> demonstrate the superior scalability of EMCID for editing up to 1,000 concepts, providing a practical approach for fast adjustment and re-deployment of T2I <b>diffusion</b> <b>models</b> in real-world applications.</p></p class="citation"></blockquote><h3 id=1289--18286-fmm-attack-a-flow-based-multi-modal-adversarial-attack-on-video-based-llms-jinmin-li-et-al-2024>(12/89 | 18/286) FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs (Jinmin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinmin Li, Kuofeng Gao, Yang Bai, Jingyun Zhang, Shu-tao Xia, Yisen Wang. (2024)<br><strong>FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs</strong><br><button class=copy-to-clipboard title="FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Multi-modal, Large Language Model, Large Language Model, Prompt, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13507v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13507v2.pdf filename=2403.13507v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the remarkable performance of video-based <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> their <b>adversarial</b> <b>threat</b> remains unexplored. To fill this gap, we propose the first <b>adversarial</b> <b>attack</b> tailored for video-based <b>LLMs</b> by crafting flow-based <b>multi-modal</b> <b>adversarial</b> <b>perturbations</b> on a small fraction of frames within a video, dubbed FMM-Attack. Extensive experiments show that our attack can effectively induce video-based <b>LLMs</b> to generate incorrect answers when videos are added with imperceptible <b>adversarial</b> <b>perturbations.</b> Intriguingly, our FMM-Attack can also induce garbling in the model output, <b>prompting</b> video-based <b>LLMs</b> to hallucinate. Overall, our observations inspire a further understanding of <b>multi-modal</b> robustness and safety-related feature alignment across different modalities, which is of great importance for various <b>large</b> <b>multi-modal</b> <b>models.</b> Our code is available at <a href=https://github.com/THU-Kingmin/FMM-Attack>https://github.com/THU-Kingmin/FMM-Attack</a>.</p></p class="citation"></blockquote><h3 id=1389--19286-ecosense-energy-efficient-intelligent-sensing-for-in-shore-ship-detection-through-edge-cloud-collaboration-wenjun-huang-et-al-2024>(13/89 | 19/286) EcoSense: Energy-Efficient Intelligent Sensing for In-Shore Ship Detection through Edge-Cloud Collaboration (Wenjun Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenjun Huang, Hanning Chen, Yang Ni, Arghavan Rezvani, Sanggeon Yun, Sungheon Jeon, Eric Pedley, Mohsen Imani. (2024)<br><strong>EcoSense: Energy-Efficient Intelligent Sensing for In-Shore Ship Detection through Edge-Cloud Collaboration</strong><br><button class=copy-to-clipboard title="EcoSense: Energy-Efficient Intelligent Sensing for In-Shore Ship Detection through Edge-Cloud Collaboration" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14027v1.pdf filename=2403.14027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting marine <b>objects</b> <b>inshore</b> presents challenges owing to algorithmic intricacies and complexities in system deployment. We propose a difficulty-aware edge-cloud collaborative sensing system that splits the task into <b>object</b> <b>localization</b> and fine-grained classification. <b>Objects</b> <b>are</b> classified either at the edge or within the cloud, based on their estimated difficulty. The framework comprises a low-power device-tailored front-end model for <b>object</b> <b>localization,</b> classification, and difficulty estimation, along with a <b>transformer-graph</b> <b>convolutional</b> <b>network-based</b> back-end model for fine-grained classification. Our system demonstrates superior performance (<a href=mailto:mAP@0.5>mAP@0.5</a> +4.3%}) on widely used marine <b>object</b> <b>detection</b> datasets, significantly reducing both data transmission volume (by 95.43%) and energy consumption (by 72.7%}) at the system level. We validate the proposed system across various embedded system platforms and in real-world scenarios involving drone deployment.</p></p class="citation"></blockquote><h3 id=1489--20286-when-cars-meet-drones-hyperbolic-federated-learning-for-source-free-domain-adaptation-in-adverse-weather-giulia-rizzoli-et-al-2024>(14/89 | 20/286) When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather (Giulia Rizzoli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giulia Rizzoli, Matteo Caligiuri, Donald Shenaj, Francesco Barbato, Pietro Zanuttigh. (2024)<br><strong>When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather</strong><br><button class=copy-to-clipboard title="When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Federated Learning, Supervised Learning, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13762v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13762v1.pdf filename=2403.13762v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>Federated</b> <b>Learning</b> (FL), multiple clients collaboratively train a global model without sharing private data. In semantic segmentation, the <b>Federated</b> <b>source</b> Free <b>Domain</b> <b>Adaptation</b> (FFreeDA) setting is of particular interest, where clients undergo <b>unsupervised</b> training after <b>supervised</b> pretraining at the server side. While few recent works address FL for autonomous vehicles, intrinsic real-world challenges such as the presence of adverse weather conditions and the existence of different autonomous agents are still unexplored. To bridge this gap, we address both problems and introduce a new <b>federated</b> <b>semantic</b> segmentation setting where both car and drone clients co-exist and collaborate. Specifically, we propose a novel approach for this setting which exploits a batch-norm weather-aware strategy to dynamically adapt the model to the different weather conditions, while hyperbolic space prototypes are used to align the heterogeneous client representations. Finally, we introduce FLYAWARE, the first semantic segmentation dataset with adverse weather data for aerial vehicles.</p></p class="citation"></blockquote><h3 id=1589--21286-h-vmunet-high-order-vision-mamba-unet-for-medical-image-segmentation-renkai-wu-et-al-2024>(15/89 | 21/286) H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation (Renkai Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renkai Wu, Yinghao Liu, Pengchen Liang, Qing Chang. (2024)<br><strong>H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13642v1.pdf filename=2403.13642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of medical image segmentation, variant models based on <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and Visual <b>Transformers</b> (ViTs) as the base modules have been very widely developed and applied. However, <b>CNNs</b> are often limited in their ability to deal with long sequences of information, while the low sensitivity of ViTs to local feature information and the problem of secondary computational complexity limit their development. Recently, the emergence of state-space models (SSMs), especially 2D-selective-scan (SS2D), has had an impact on the longtime dominance of traditional <b>CNNs</b> and ViTs as the foundational modules of visual neural networks. In this paper, we extend the adaptability of SS2D by proposing a High-order Vision Mamba UNet (H-vmunet) for medical image segmentation. Among them, the proposed High-order 2D-selective-scan (H-SS2D) progressively reduces the introduction of redundant information during SS2D operations through higher-order interactions. In addition, the proposed Local-SS2D module improves the learning ability of local features of SS2D at each order of interaction. We conducted comparison and ablation experiments on three publicly available medical image datasets (ISIC2017, Spleen, and CVC-ClinicDB), and the results all demonstrate the strong competitiveness of H-vmunet in medical image segmentation tasks. The code is available from <a href=https://github.com/wurenkai/H-vmunet>https://github.com/wurenkai/H-vmunet</a> .</p></p class="citation"></blockquote><h3 id=1689--22286-an-ai-assisted-skincare-routine-recommendation-system-in-xr-gowravi-malalur-rajegowda-et-al-2024>(16/89 | 22/286) An AI-Assisted Skincare Routine Recommendation System in XR (Gowravi Malalur Rajegowda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gowravi Malalur Rajegowda, Yannis Spyridis, Barbara Villarini, Vasileios Argyriou. (2024)<br><strong>An AI-Assisted Skincare Routine Recommendation System in XR</strong><br><button class=copy-to-clipboard title="An AI-Assisted Skincare Routine Recommendation System in XR" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13466v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13466v1.pdf filename=2403.13466v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there has been an increasing interest in the use of artificial intelligence (AI) and extended reality (XR) in the beauty industry. In this paper, we present an AI-assisted skin care <b>recommendation</b> system integrated into an XR platform. The system uses a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> to analyse an individual&rsquo;s skin type and recommend personalised skin care products in an immersive and interactive manner. Our methodology involves collecting data from individuals through a questionnaire and conducting skin analysis using a provided facial image in an immersive environment. This data is then used to train the <b>CNN</b> model, which recognises the skin type and existing issues and allows the <b>recommendation</b> engine to suggest personalised skin care products. We evaluate our system in terms of the accuracy of the <b>CNN</b> model, which achieves an average score of 93% in correctly classifying existing skin issues. Being integrated into an XR system, this approach has the potential to significantly enhance the beauty industry by providing immersive and engaging experiences to users, leading to more efficient and consistent skincare routines.</p></p class="citation"></blockquote><h3 id=1789--23286-few-shot-oriented-object-detection-with-memorable-contrastive-learning-in-remote-sensing-images-jiawei-zhou-et-al-2024>(17/89 | 23/286) Few-shot Oriented Object Detection with Memorable Contrastive Learning in Remote Sensing Images (Jiawei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Zhou, Wuzhou Li, Yi Cao, Hongtao Cai, Xiang Li. (2024)<br><strong>Few-shot Oriented Object Detection with Memorable Contrastive Learning in Remote Sensing Images</strong><br><button class=copy-to-clipboard title="Few-shot Oriented Object Detection with Memorable Contrastive Learning in Remote Sensing Images" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Contrastive Learning, Few-shot, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13375v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13375v1.pdf filename=2403.13375v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>object</b> <b>detection</b> (FSOD) has garnered significant research attention in the field of remote sensing due to its ability to reduce the dependency on large amounts of annotated data. However, two challenges persist in this area: (1) axis-aligned proposals, which can result in misalignment for arbitrarily oriented <b>objects,</b> <b>and</b> (2) the scarcity of annotated data still limits the performance for unseen <b>object</b> <b>categories.</b> To address these issues, we propose a novel FSOD method for remote sensing images called <b>Few-shot</b> Oriented <b>object</b> <b>detection</b> with Memorable <b>Contrastive</b> <b>learning</b> (FOMC). Specifically, we employ oriented bounding boxes instead of traditional horizontal bounding boxes to learn a better feature representation for arbitrary-oriented aerial <b>objects,</b> <b>leading</b> to enhanced detection performance. To the best of our knowledge, we are the first to address oriented <b>object</b> <b>detection</b> in the <b>few-shot</b> setting for remote sensing images. To address the challenging issue of <b>object</b> <b>misclassification,</b> we introduce a <b>supervised</b> <b>contrastive</b> <b>learning</b> module with a dynamically updated memory bank. This module enables the use of large batches of negative samples and enhances the model&rsquo;s capability to learn discriminative features for unseen classes. We conduct comprehensive experiments on the DOTA and HRSC2016 datasets, and our model achieves state-of-the-art performance on the <b>few-shot</b> oriented <b>object</b> <b>detection</b> task. Code and pretrained models will be released.</p></p class="citation"></blockquote><h3 id=1889--24286-detdiffusion-synergizing-generative-and-perceptive-models-for-enhanced-data-generation-and-perception-yibo-wang-et-al-2024>(18/89 | 24/286) DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception (Yibo Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yibo Wang, Ruiyuan Gao, Kai Chen, Kaiqiang Zhou, Yingjie Cai, Lanqing Hong, Zhenguo Li, Lihui Jiang, Dit-Yan Yeung, Qiang Xu, Kai Zhang. (2024)<br><strong>DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception</strong><br><button class=copy-to-clipboard title="DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Object Detection, Data Augmentation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13304v1.pdf filename=2403.13304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current perceptive models heavily depend on resource-intensive datasets, <b>prompting</b> the need for innovative solutions. Leveraging recent advances in <b>diffusion</b> <b>models,</b> synthetic <b>data,</b> <b>by</b> constructing image inputs from various annotations, proves beneficial for downstream tasks. While prior methods have separately addressed generative and perceptive models, DetDiffusion, for the first time, harmonizes both, tackling the challenges in generating effective <b>data</b> <b>for</b> perceptive models. To enhance image generation with perceptive models, we introduce perception-aware loss (P.A. loss) through segmentation, improving both quality and controllability. To boost the performance of specific perceptive models, our method customizes <b>data</b> <b>augmentation</b> by extracting and utilizing perception-aware attribute (P.A. Attr) during generation. Experimental results from the <b>object</b> <b>detection</b> task highlight DetDiffusion&rsquo;s superior performance, establishing a new state-of-the-art in layout-guided generation. Furthermore, image syntheses from DetDiffusion can effectively augment training <b>data,</b> <b>significantly</b> enhancing downstream detection performance.</p></p class="citation"></blockquote><h3 id=1989--25286-vl-mamba-exploring-state-space-models-for-multimodal-learning-yanyuan-qiao-et-al-2024>(19/89 | 25/286) VL-Mamba: Exploring State Space Models for Multimodal Learning (Yanyuan Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia Zhao, Mingzhen Sun, Qi Wu, Jing Liu. (2024)<br><strong>VL-Mamba: Exploring State Space Models for Multimodal Learning</strong><br><button class=copy-to-clipboard title="VL-Mamba: Exploring State Space Models for Multimodal Learning" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, LLaMA, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13600v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13600v1.pdf filename=2403.13600v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) have attracted widespread interest and have rich applications. However, the inherent attention mechanism in its <b>Transformer</b> structure requires quadratic complexity and results in expensive computational overhead. Therefore, in this work, we propose VL-Mamba, a <b>multimodal</b> <b>large</b> <b>language</b> <b>model</b> based on state space models, which have been shown to have great potential for long-sequence modeling with fast inference and linear scaling in sequence length. Specifically, we first replace the <b>transformer-based</b> backbone language model such as <b>LLama</b> or Vicuna with the pre-trained Mamba language model. Then, we empirically explore how to effectively apply the 2D vision selective scan mechanism for <b>multimodal</b> learning and the combinations of different vision encoders and variants of pretrained Mamba language models. The extensive experiments on diverse <b>multimodal</b> <b>benchmarks</b> with competitive performance show the effectiveness of our proposed VL-Mamba and demonstrate the great potential of applying state space models for <b>multimodal</b> learning tasks.</p></p class="citation"></blockquote><h3 id=2089--26286-aud-tgn-advancing-action-unit-detection-with-temporal-convolution-and-gpt-2-in-wild-audiovisual-contexts-jun-yu-et-al-2024>(20/89 | 26/286) AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and GPT-2 in Wild Audiovisual Contexts (Jun Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Yu, Zerui Zhang, Zhihong Wei, Gongpeng Zhao, Zhongpeng Cai, Yongqi Wang, Guochen Xie, Jichao Zhu, Wangyuan Zhu. (2024)<br><strong>AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and GPT-2 in Wild Audiovisual Contexts</strong><br><button class=copy-to-clipboard title="AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and GPT-2 in Wild Audiovisual Contexts" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Convolution, Multi-modal, Multi-modal, GPT, GPT-2<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13678v1.pdf filename=2403.13678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Leveraging the synergy of both audio data and visual data is essential for understanding human emotions and behaviors, especially in in-the-wild setting. Traditional methods for integrating such <b>multimodal</b> information often stumble, leading to less-than-ideal outcomes in the task of facial action unit detection. To overcome these shortcomings, we propose a novel approach utilizing audio-visual <b>multimodal</b> data. This method enhances audio feature extraction by leveraging Mel Frequency Cepstral Coefficients (MFCC) and Log-Mel spectrogram features alongside a pre-trained VGGish network. Moreover, this paper adaptively captures fusion features across modalities by modeling the temporal relationships, and ultilizes a pre-trained <b>GPT-2</b> model for sophisticated context-aware fusion of <b>multimodal</b> information. Our method notably improves the accuracy of AU detection by understanding the temporal and contextual nuances of the data, showcasing significant advancements in the comprehension of intricate scenarios. These findings underscore the potential of integrating temporal dynamics and contextual interpretation, paving the way for future research endeavors.</p></p class="citation"></blockquote><h3 id=2189--27286-recursive-cross-modal-attention-for-multimodal-fusion-in-dimensional-emotion-recognition-r-gnana-praveen-et-al-2024>(21/89 | 27/286) Recursive Cross-Modal Attention for Multimodal Fusion in Dimensional Emotion Recognition (R. Gnana Praveen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>R. Gnana Praveen, Jahangir Alam. (2024)<br><strong>Recursive Cross-Modal Attention for Multimodal Fusion in Dimensional Emotion Recognition</strong><br><button class=copy-to-clipboard title="Recursive Cross-Modal Attention for Multimodal Fusion in Dimensional Emotion Recognition" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-SD, cs.CV, eess-AS<br>Keyword Score: 36<br>Keywords: Convolution, Convolutional Neural Network, Multi-modal, Multi-modal, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13659v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13659v1.pdf filename=2403.13659v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> <b>emotion</b> <b>recognition</b> has recently gained a lot of attention since it can leverage diverse and complementary relationships over multiple modalities, such as audio, visual, and text. Most state-of-the-art methods for <b>multimodal</b> fusion rely on recurrent networks or conventional attention mechanisms that do not effectively leverage the complementary nature of the modalities. In this paper, we focus on dimensional <b>emotion</b> <b>recognition</b> based on the fusion of facial, vocal, and text modalities extracted from videos. Specifically, we propose a recursive cross-modal attention (RCMA) to effectively capture the complementary relationships across the modalities in a recursive fashion. The proposed model is able to effectively capture the inter-modal relationships by computing the cross-attention weights across the individual modalities and the joint representation of the other two modalities. To further improve the inter-modal relationships, the obtained attended features of the individual modalities are again fed as input to the cross-modal attention to refine the feature representations of the individual modalities. In addition to that, we have used Temporal <b>convolution</b> <b>networks</b> (TCNs) to capture the temporal modeling (intra-modal relationships) of the individual modalities. By deploying the TCNs as well cross-modal attention in a recursive fashion, we are able to effectively capture both intra- and inter-modal relationships across the audio, visual, and text modalities. Experimental results on validation-set videos from the AffWild2 dataset indicate that our proposed fusion model is able to achieve significant improvement over the baseline for the sixth challenge of Affective Behavior Analysis in-the-Wild 2024 (ABAW6) competition.</p></p class="citation"></blockquote><h3 id=2289--28286-what-if-counterfactual-inception-to-mitigate-hallucination-effects-in-large-multimodal-models-junho-kim-et-al-2024>(22/89 | 28/286) What if&mldr;?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models (Junho Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junho Kim, Yeon Ju Kim, Yong Man Ro. (2024)<br><strong>What if&mldr;?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models</strong><br><button class=copy-to-clipboard title="What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Counter-factual, Multi-modal, Multi-modal, Reasoning, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13513v1.pdf filename=2403.13513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a way of enhancing the reliability of Large <b>Multimodal</b> Models (LMMs) in addressing hallucination effects, where models generate incorrect or unrelated responses. Without additional <b>instruction</b> <b>tuning</b> paradigm, we introduce <b>Counterfactual</b> Inception, a novel method that implants <b>counterfactual</b> thoughts into LMMs using carefully chosen, misaligned <b>counterfactual</b> keywords. This method is grounded in the concept of <b>counterfactual</b> thinking, a cognitive process where humans consider alternative realities and outcomes. By applying this human-like <b>reasoning</b> mechanism to LMMs, we aim to reduce hallucination effects and improve the models&rsquo; trustworthiness. We also propose Dual-modality Verification Process (DVP), a rigorous framework for selecting optimal <b>counterfactual</b> keywords to trigger <b>counterfactual</b> thinking into LMMs, concurrently considering visual and linguistic context. Our extensive experiments across various LMMs, including both open-source and proprietary models, corroborate that our method significantly mitigates hallucination phenomena across different datasets.</p></p class="citation"></blockquote><h3 id=2389--29286-puzzlevqa-diagnosing-multimodal-reasoning-challenges-of-language-models-with-abstract-visual-patterns-yew-ken-chia-et-al-2024>(23/89 | 29/286) PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns (Yew Ken Chia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, Soujanya Poria. (2024)<br><strong>PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns</strong><br><button class=copy-to-clipboard title="PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, GPT, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13315v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13315v1.pdf filename=2403.13315v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>multimodal</b> <b>models</b> extend the impressive capabilities of <b>large</b> <b>language</b> <b>models</b> by integrating <b>multimodal</b> understanding abilities. However, it is not clear how they can emulate the general intelligence and <b>reasoning</b> ability of humans. As recognizing patterns and abstracting concepts are key to general intelligence, we introduce PuzzleVQA, a collection of puzzles based on abstract patterns. With this dataset, we evaluate <b>large</b> <b>multimodal</b> <b>models</b> with abstract patterns based on fundamental concepts, including colors, numbers, sizes, and shapes. Through our experiments on state-of-the-art <b>large</b> <b>multimodal</b> <b>models,</b> we find that they are not able to generalize well to simple abstract patterns. Notably, even <b>GPT-4V</b> cannot solve more than half of the puzzles. To diagnose the <b>reasoning</b> challenges in <b>large</b> <b>multimodal</b> <b>models,</b> we progressively guide the models with our ground truth <b>reasoning</b> explanations for visual perception, inductive <b>reasoning,</b> and deductive <b>reasoning.</b> Our systematic analysis finds that the main bottlenecks of <b>GPT-4V</b> are weaker visual perception and inductive <b>reasoning</b> abilities. Through this work, we hope to shed light on the limitations of <b>large</b> <b>multimodal</b> <b>models</b> and how they can better emulate human cognitive processes in the future (Our data and code will be released publicly at <a href=https://github.com/declare-lab/LLM-PuzzleTest)>https://github.com/declare-lab/LLM-PuzzleTest)</a>.</p></p class="citation"></blockquote><h3 id=2489--30286-portrait4d-v2-pseudo-multi-view-data-creates-better-4d-head-synthesizer-yu-deng-et-al-2024>(24/89 | 30/286) Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer (Yu Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Deng, Duomin Wang, Baoyuan Wang. (2024)<br><strong>Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer</strong><br><button class=copy-to-clipboard title="Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Vision Transformer, Geometry, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13570v1.pdf filename=2403.13570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a novel learning approach for feed-forward one-shot 4D head avatar synthesis. Different from existing methods that often learn from reconstructing monocular videos guided by 3DMM, we employ pseudo multi-view videos to learn a 4D head synthesizer in a data-driven manner, avoiding reliance on inaccurate 3DMM reconstruction that could be detrimental to the synthesis performance. The key idea is to first learn a 3D head synthesizer using synthetic multi-view images to convert monocular real videos into multi-view ones, and then utilize the pseudo multi-view videos to learn a 4D head synthesizer via cross-view self-reenactment. By leveraging a simple <b>vision</b> <b>transformer</b> backbone with motion-aware cross-attentions, our method exhibits superior performance compared to previous methods in terms of reconstruction fidelity, <b>geometry</b> consistency, and motion control accuracy. We hope our method offers novel insights into integrating 3D priors with 2D supervisions for improved 4D head avatar creation.</p></p class="citation"></blockquote><h3 id=2589--31286-leveraging-high-resolution-features-for-improved-deep-hashing-based-image-retrieval-aymene-berriche-et-al-2024>(25/89 | 31/286) Leveraging High-Resolution Features for Improved Deep Hashing-based Image Retrieval (Aymene Berriche et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aymene Berriche, Mehdi Adjal Zakaria, Riyadh Baghdadi. (2024)<br><strong>Leveraging High-Resolution Features for Improved Deep Hashing-based Image Retrieval</strong><br><button class=copy-to-clipboard title="Leveraging High-Resolution Features for Improved Deep Hashing-based Image Retrieval" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-IR, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13747v1.pdf filename=2403.13747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep hashing techniques have emerged as the predominant approach for efficient image retrieval. Traditionally, these methods utilize pre-trained <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> such as AlexNet and VGG-16 as feature extractors. However, the increasing complexity of datasets poses challenges for these backbone architectures in capturing meaningful features essential for effective image retrieval. In this study, we explore the efficacy of employing high-resolution features learned through state-of-the-art techniques for image retrieval tasks. Specifically, we propose a novel methodology that utilizes High-Resolution Networks (HRNets) as the backbone for the deep hashing task, termed High-Resolution Hashing Network (HHNet). Our approach demonstrates superior performance compared to existing methods across all tested <b>benchmark</b> datasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance improvement is more pronounced for complex datasets, which highlights the need to learn high-resolution features for intricate image retrieval tasks. Furthermore, we conduct a comprehensive analysis of different HRNet configurations and provide insights into the optimal architecture for the deep hashing task</p></p class="citation"></blockquote><h3 id=2689--32286-progressive-trajectory-matching-for-medical-dataset-distillation-zhen-yu-et-al-2024>(26/89 | 32/286) Progressive trajectory matching for medical dataset distillation (Zhen Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Yu, Yang Liu, Qingchao Chen. (2024)<br><strong>Progressive trajectory matching for medical dataset distillation</strong><br><button class=copy-to-clipboard title="Progressive trajectory matching for medical dataset distillation" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Foundation Model, Knowledge Distillation, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13469v1.pdf filename=2403.13469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is essential but challenging to share medical image datasets due to privacy issues, which prohibit building <b>foundation</b> <b>models</b> and <b>knowledge</b> <b>transfer.</b> In this paper, we propose a novel dataset <b>distillation</b> method to condense the original medical image datasets into a synthetic one that preserves useful information for building an analysis model without accessing the original datasets. Existing methods tackle only natural images by randomly matching parts of the training trajectories of the model parameters trained by the whole real datasets. However, through extensive experiments on medical image datasets, the training process is extremely unstable and achieves inferior <b>distillation</b> results. To solve these barriers, we propose to design a novel progressive trajectory matching strategy to improve the training stability for medical image dataset <b>distillation.</b> Additionally, it is observed that improved stability prevents the synthetic dataset diversity and final performance improvements. Therefore, we propose a dynamic overlap mitigation module that improves the synthetic dataset diversity by dynamically eliminating the overlap across different images and retraining parts of the synthetic images for better convergence. Finally, we propose a new medical image dataset <b>distillation</b> <b>benchmark</b> of various modalities and configurations to promote fair evaluations. It is validated that our proposed method achieves 8.33% improvement over previous state-of-the-art methods on average, and 11.7% improvement when ipc=2 (i.e., image per class is 2). Codes and <b>benchmarks</b> will be released.</p></p class="citation"></blockquote><h3 id=2789--33286-out-of-distribution-detection-using-peer-class-generated-by-large-language-model-k-huang-et-al-2024>(27/89 | 33/286) Out-of-Distribution Detection Using Peer-Class Generated by Large Language Model (K Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>K Huang, G Song, Hanwen Su, Jiyan Wang. (2024)<br><strong>Out-of-Distribution Detection Using Peer-Class Generated by Large Language Model</strong><br><button class=copy-to-clipboard title="Out-of-Distribution Detection Using Peer-Class Generated by Large Language Model" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Out-of-distribution, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13324v1.pdf filename=2403.13324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-distribution</b> (OOD) detection is a critical task to ensure the reliability and security of machine learning models deployed in real-world applications. Conventional methods for OOD detection that rely on single-modal information, often struggle to capture the rich variety of OOD instances. The primary difficulty in OOD detection arises when an input image has numerous similarities to a particular class in the in-distribution (ID) dataset, e.g., wolf to dog, causing the model to misclassify it. Nevertheless, it may be easy to distinguish these classes in the semantic domain. To this end, in this paper, a novel method called ODPC is proposed, in which specific <b>prompts</b> to generate OOD peer classes of ID semantics are designed by a <b>large</b> <b>language</b> <b>model</b> as an auxiliary modality to facilitate detection. Moreover, a contrastive loss based on OOD peer classes is devised to learn compact representations of ID classes and improve the clarity of boundaries between different classes. The extensive experiments on five <b>benchmark</b> datasets show that the method we propose can yield state-of-the-art results.</p></p class="citation"></blockquote><h3 id=2889--34286-dd-robustbench-an-adversarial-robustness-benchmark-for-dataset-distillation-yifan-wu-et-al-2024>(28/89 | 34/286) DD-RobustBench: An Adversarial Robustness Benchmark for Dataset Distillation (Yifan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Wu, Jiawei Du, Ping Liu, Yuewei Lin, Wenqing Cheng, Wei Xu. (2024)<br><strong>DD-RobustBench: An Adversarial Robustness Benchmark for Dataset Distillation</strong><br><button class=copy-to-clipboard title="DD-RobustBench: An Adversarial Robustness Benchmark for Dataset Distillation" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Knowledge Distillation, Knowledge Distillation, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13322v1.pdf filename=2403.13322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dataset <b>distillation</b> is an advanced technique aimed at compressing datasets into significantly smaller counterparts, while preserving formidable training performance. Significant efforts have been devoted to promote evaluation accuracy under limited compression ratio while overlooked the robustness of <b>distilled</b> dataset. In this work, we introduce a comprehensive <b>benchmark</b> that, to the best of our knowledge, is the most extensive to date for evaluating the <b>adversarial</b> <b>robustness</b> of <b>distilled</b> datasets in a unified way. Our <b>benchmark</b> significantly expands upon prior efforts by incorporating a wider range of dataset <b>distillation</b> methods, including the latest advancements such as TESLA and SRe2L, a diverse array of <b>adversarial</b> <b>attack</b> methods, and evaluations across a broader and more extensive collection of datasets such as ImageNet-1K. Moreover, we assessed the robustness of these <b>distilled</b> datasets against representative <b>adversarial</b> <b>attack</b> algorithms like PGD and AutoAttack, while exploring their resilience from a frequency perspective. We also discovered that incorporating <b>distilled</b> data into the training batches of the original dataset can yield to improvement of robustness.</p></p class="citation"></blockquote><h3 id=2989--35286-on-pretraining-data-diversity-for-self-supervised-learning-hasan-abed-al-kader-hammoud-et-al-2024>(29/89 | 35/286) On Pretraining Data Diversity for Self-Supervised Learning (Hasan Abed Al Kader Hammoud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hasan Abed Al Kader Hammoud, Tuhin Das, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem. (2024)<br><strong>On Pretraining Data Diversity for Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="On Pretraining Data Diversity for Self-Supervised Learning" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Distribution Shift, Distribution Shift, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13808v1.pdf filename=2403.13808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore the impact of training with more diverse datasets, characterized by the number of unique samples, on the performance of <b>self-supervised</b> <b>learning</b> (SSL) under a fixed computational budget. Our findings consistently demonstrate that increasing pretraining data diversity enhances SSL performance, albeit only when the <b>distribution</b> <b>distance</b> to the downstream data is minimal. Notably, even with an exceptionally large pretraining data diversity achieved through methods like web crawling or diffusion-generated data, among other ways, the <b>distribution</b> <b>shift</b> remains a challenge. Our experiments are comprehensive with seven SSL methods using large-scale datasets such as ImageNet and YFCC100M amounting to over 200 GPU days. Code and trained models will be available at <a href=https://github.com/hammoudhasan/DiversitySSL>https://github.com/hammoudhasan/DiversitySSL</a> .</p></p class="citation"></blockquote><h3 id=3089--36286-enhancing-gait-video-analysis-in-neurodegenerative-diseases-by-knowledge-augmentation-in-vision-language-model-diwei-wang-et-al-2024>(30/89 | 36/286) Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge Augmentation in Vision Language Model (Diwei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Diwei Wang, Kun Yuan, Candice Muller, Frédéric Blanc, Nicolas Padoy, Hyewon Seo. (2024)<br><strong>Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge Augmentation in Vision Language Model</strong><br><button class=copy-to-clipboard title="Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge Augmentation in Vision Language Model" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Prompt, Prompt Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13756v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13756v1.pdf filename=2403.13756v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a knowledge augmentation strategy for assessing the diagnostic groups and gait impairment from monocular gait videos. Based on a large-scale pre-trained Vision Language Model (VLM), our model learns and improves visual, textual, and numerical representations of patient gait videos, through a collective learning across three distinct modalities: gait videos, class-specific descriptions, and numerical gait parameters. Our specific contributions are two-fold: First, we adopt a knowledge-aware <b>prompt</b> <b>tuning</b> strategy to utilize the class-specific medical description in guiding the text <b>prompt</b> <b>learning.</b> Second, we integrate the paired gait parameters in the form of numerical texts to enhance the numeracy of the textual representation. Results demonstrate that our model not only significantly outperforms state-of-the-art (SOTA) in video-based classification tasks but also adeptly decodes the learned class-specific text features into natural language descriptions using the vocabulary of quantitative gait parameters. The code and the model will be made available at our project page.</p></p class="citation"></blockquote><h3 id=3189--37286-fostc3neta-lightweight-yolov5-based-on-the-network-structure-optimization-danqing-ma-et-al-2024>(31/89 | 37/286) Fostc3net:A Lightweight YOLOv5 Based On the Network Structure Optimization (Danqing Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danqing Ma, Shaojie Li, Bo Dang, Hengyi Zang, Xinqi Dong. (2024)<br><strong>Fostc3net:A Lightweight YOLOv5 Based On the Network Structure Optimization</strong><br><button class=copy-to-clipboard title="Fostc3net:A Lightweight YOLOv5 Based On the Network Structure Optimization" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13703v1.pdf filename=2403.13703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Transmission line detection technology is crucial for automatic monitoring and ensuring the safety of electrical facilities. The YOLOv5 series is currently one of the most advanced and widely used methods for <b>object</b> <b>detection.</b> However, it faces inherent challenges, such as high computational load on devices and insufficient detection accuracy. To address these concerns, this paper presents an enhanced lightweight YOLOv5 technique customized for mobile devices, specifically intended for identifying <b>objects</b> <b>associated</b> with transmission lines. The C3Ghost module is integrated into the <b>convolutional</b> <b>network</b> of YOLOv5 to reduce floating point operations per second (FLOPs) in the feature channel fusion process and improve feature expression performance. In addition, a FasterNet module is introduced to replace the c3 module in the YOLOv5 Backbone. The FasterNet module uses Partial <b>Convolutions</b> to process only a portion of the input channels, improving feature extraction efficiency and reducing computational overhead. To address the imbalance between simple and challenging samples in the dataset and the diversity of aspect ratios of bounding boxes, the wIoU v3 LOSS is adopted as the loss function. To validate the performance of the proposed approach, Experiments are conducted on a custom dataset of transmission line poles. The results show that the proposed model achieves a 1% increase in detection accuracy, a 13% reduction in FLOPs, and a 26% decrease in model parameters compared to the existing YOLOv5.In the ablation experiment, it was also discovered that while the Fastnet module and the CSghost module improved the precision of the original YOLOv5 baseline model, they caused a decrease in the mAP@.5-.95 metric. However, the improvement of the wIoUv3 loss function significantly mitigated the decline of the mAP@.5-.95 metric.</p></p class="citation"></blockquote><h3 id=3289--38286-retina-vision-transformer-retinavit-introducing-scaled-patches-into-vision-transformers-yuyang-shu-et-al-2024>(32/89 | 38/286) Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into Vision Transformers (Yuyang Shu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuyang Shu, Michael E. Bain. (2024)<br><strong>Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into Vision Transformers</strong><br><button class=copy-to-clipboard title="Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into Vision Transformers" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13677v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13677v1.pdf filename=2403.13677v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans see low and high spatial frequency components at the same time, and combine the information from both to form a visual scene. Drawing on this neuroscientific inspiration, we propose an altered <b>Vision</b> <b>Transformer</b> architecture where patches from scaled down versions of the input image are added to the input of the first <b>Transformer</b> Encoder layer. We name this model Retina <b>Vision</b> <b>Transformer</b> (RetinaViT) due to its inspiration from the human visual system. Our experiments show that when trained on the ImageNet-1K dataset with a moderate configuration, RetinaViT achieves a 3.3% performance improvement over the original ViT. We hypothesize that this improvement can be attributed to the inclusion of low spatial frequency components in the input, which improves the ability to capture structural features, and to select and forward important features to deeper layers. RetinaViT thereby opens doors to further investigations into vertical pathways and attention patterns.</p></p class="citation"></blockquote><h3 id=3389--39286-zodi-zero-shot-domain-adaptation-with-diffusion-based-image-transfer-hiroki-azuma-et-al-2024>(33/89 | 39/286) ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer (Hiroki Azuma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hiroki Azuma, Yusuke Matsui, Atsuto Maki. (2024)<br><strong>ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer</strong><br><button class=copy-to-clipboard title="ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Zero-shot, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13652v1.pdf filename=2403.13652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning models achieve high accuracy in segmentation tasks among others, yet <b>domain</b> <b>shift</b> often degrades the models&rsquo; performance, which can be critical in real-world scenarios where no target images are available. This paper proposes a <b>zero-shot</b> <b>domain</b> <b>adaptation</b> method based on <b>diffusion</b> <b>models,</b> called ZoDi, which is two-fold by the design: <b>zero-shot</b> image transfer and model adaptation. First, we utilize an off-the-shelf <b>diffusion</b> <b>model</b> to synthesize target-like images by transferring the <b>domain</b> <b>of</b> source images to the target <b>domain.</b> <b>In</b> this we specifically try to maintain the layout and content by utilising layout-to-image <b>diffusion</b> <b>models</b> with stochastic inversion. Secondly, we train the model using both source images and synthesized images with the original segmentation maps while maximizing the feature similarity of images from the two <b>domains</b> <b>to</b> learn <b>domain-robust</b> <b>representations.</b> Through experiments we show benefits of ZoDi in the task of image segmentation over state-of-the-art methods. It is also more applicable than existing CLIP-based methods because it assumes no specific backbone or models, and it enables to estimate the model&rsquo;s performance without target images by inspecting generated images. Our implementation will be publicly available.</p></p class="citation"></blockquote><h3 id=3489--40286-idadapter-learning-mixed-features-for-tuning-free-personalization-of-text-to-image-models-siying-cui-et-al-2024>(34/89 | 40/286) IDAdapter: Learning Mixed Features for Tuning-Free Personalization of Text-to-Image Models (Siying Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siying Cui, Jia Guo, Xiang An, Jiankang Deng, Yongle Zhao, Xinyu Wei, Ziyong Feng. (2024)<br><strong>IDAdapter: Learning Mixed Features for Tuning-Free Personalization of Text-to-Image Models</strong><br><button class=copy-to-clipboard title="IDAdapter: Learning Mixed Features for Tuning-Free Personalization of Text-to-Image Models" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13535v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13535v2.pdf filename=2403.13535v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Leveraging Stable Diffusion for the generation of personalized portraits has emerged as a powerful and noteworthy tool, enabling users to create high-fidelity, custom character avatars based on their specific <b>prompts.</b> However, existing personalization methods face challenges, including test-time <b>fine-tuning,</b> the requirement of multiple input images, low preservation of identity, and limited diversity in generated outcomes. To overcome these challenges, we introduce IDAdapter, a tuning-free approach that enhances the diversity and identity preservation in personalized image generation from a single face image. IDAdapter integrates a personalized concept into the generation process through a combination of textual and visual injections and a face identity loss. During the training phase, we incorporate mixed features from multiple reference images of a specific identity to enrich identity-related content details, guiding the model to generate images with more diverse styles, expressions, and angles compared to previous works. Extensive evaluations demonstrate the effectiveness of our method, achieving both diversity and identity fidelity in generated images.</p></p class="citation"></blockquote><h3 id=3589--41286-vstar-generative-temporal-nursing-for-longer-dynamic-video-synthesis-yumeng-li-et-al-2024>(35/89 | 41/286) VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis (Yumeng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yumeng Li, William Beluch, Margret Keuper, Dan Zhang, Anna Khoreva. (2024)<br><strong>VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis</strong><br><button class=copy-to-clipboard title="VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs-MM, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13501v1.pdf filename=2403.13501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite tremendous progress in the field of text-to-video (T2V) synthesis, open-sourced T2V <b>diffusion</b> <b>models</b> struggle to generate longer videos with dynamically varying and evolving content. They tend to synthesize quasi-static videos, ignoring the necessary visual change-over-time implied in the text <b>prompt.</b> At the same time, scaling these models to enable longer, more dynamic video synthesis often remains computationally intractable. To address this challenge, we introduce the concept of Generative Temporal Nursing (GTN), where we aim to alter the generative process on the fly during inference to improve control over the temporal dynamics and enable generation of longer videos. We propose a method for GTN, dubbed VSTAR, which consists of two key ingredients: 1) Video Synopsis <b>Prompting</b> (VSP) - automatic generation of a video synopsis based on the original single <b>prompt</b> leveraging <b>LLMs,</b> which gives accurate textual guidance to different visual states of longer videos, and 2) Temporal Attention Regularization (TAR) - a regularization technique to refine the temporal attention units of the pre-trained T2V <b>diffusion</b> <b>models,</b> which enables control over the video dynamics. We experimentally showcase the superiority of the proposed approach in generating longer, visually appealing videos over existing open-sourced T2V models. We additionally analyze the temporal attention maps realized with and without VSTAR, demonstrating the importance of applying our method to mitigate neglect of the desired visual change over time.</p></p class="citation"></blockquote><h3 id=3689--42286-iidm-image-to-image-diffusion-model-for-semantic-image-synthesis-feng-liu-et-al-2024>(36/89 | 42/286) IIDM: Image-to-Image Diffusion Model for Semantic Image Synthesis (Feng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Liu, Xiaobin-Chang. (2024)<br><strong>IIDM: Image-to-Image Diffusion Model for Semantic Image Synthesis</strong><br><button class=copy-to-clipboard title="IIDM: Image-to-Image Diffusion Model for Semantic Image Synthesis" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13378v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13378v1.pdf filename=2403.13378v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic image synthesis aims to generate high-quality images given semantic conditions, i.e. segmentation masks and style reference images. Existing methods widely adopt <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs).</b> <b>GANs</b> take all conditional inputs and directly synthesize images in a single forward step. In this paper, semantic image synthesis is treated as an image denoising task and is handled with a novel image-to-image <b>diffusion</b> <b>model</b> (IIDM). Specifically, the style reference is first contaminated with random noise and then progressively denoised by IIDM, guided by segmentation masks. Moreover, three techniques, refinement, color-transfer and model ensembles, are proposed to further boost the generation quality. They are plug-in inference modules and do not require additional training. Extensive experiments show that our IIDM outperforms existing state-of-the-art methods by clear margins. Further analysis is provided via detailed demonstrations. We have implemented IIDM based on the Jittor framework; code is available at <a href=https://github.com/ader47/jittor-jieke-semantic_images_synthesis>https://github.com/ader47/jittor-jieke-semantic_images_synthesis</a>.</p></p class="citation"></blockquote><h3 id=3789--43286-fissionfusion-fast-geometric-generation-and-hierarchical-souping-for-medical-image-analysis-santosh-sanjeev-et-al-2024>(37/89 | 43/286) FissionFusion: Fast Geometric Generation and Hierarchical Souping for Medical Image Analysis (Santosh Sanjeev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Santosh Sanjeev, Nuren Zhaksylyk, Ibrahim Almakky, Anees Ur Rehman Hashmi, Mohammad Areeb Qazi, Mohammad Yaqub. (2024)<br><strong>FissionFusion: Fast Geometric Generation and Hierarchical Souping for Medical Image Analysis</strong><br><button class=copy-to-clipboard title="FissionFusion: Fast Geometric Generation and Hierarchical Souping for Medical Image Analysis" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Out-of-distribution, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13341v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13341v1.pdf filename=2403.13341v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The scarcity of well-annotated medical datasets requires leveraging <b>transfer</b> <b>learning</b> from broader datasets like ImageNet or pre-trained models like CLIP. Model soups averages multiple <b>fine-tuned</b> models aiming to improve performance on In-Domain (ID) tasks and enhance robustness against <b>Out-of-Distribution</b> (OOD) datasets. However, applying these methods to the medical imaging domain faces challenges and results in suboptimal performance. This is primarily due to differences in error surface characteristics that stem from data complexities such as heterogeneity, domain shift, class imbalance, and distributional shifts between training and testing phases. To address this issue, we propose a hierarchical merging approach that involves local and global aggregation of models at various levels based on models&rsquo; hyperparameter configurations. Furthermore, to alleviate the need for training a large number of models in the hyperparameter search, we introduce a computationally efficient method using a cyclical learning rate scheduler to produce multiple models for aggregation in the weight space. Our method demonstrates significant improvements over the model souping approach across multiple datasets (around 6% gain in HAM10000 and CheXpert datasets) while maintaining low computational costs for model generation and selection. Moreover, we achieve better results on OOD datasets than model soups. The code is available at <a href=https://github.com/BioMedIA-MBZUAI/FissionFusion>https://github.com/BioMedIA-MBZUAI/FissionFusion</a>.</p></p class="citation"></blockquote><h3 id=3889--44286-rotary-position-embedding-for-vision-transformer-byeongho-heo-et-al-2024>(38/89 | 44/286) Rotary Position Embedding for Vision Transformer (Byeongho Heo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Byeongho Heo, Song Park, Dongyoon Han, Sangdoo Yun. (2024)<br><strong>Rotary Position Embedding for Vision Transformer</strong><br><button class=copy-to-clipboard title="Rotary Position Embedding for Vision Transformer" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13298v1.pdf filename=2403.13298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rotary Position Embedding (RoPE) performs remarkably on language models, especially for length extrapolation of <b>Transformers.</b> However, the impacts of RoPE on computer <b>vision</b> <b>domains</b> have been underexplored, even though RoPE appears capable of enhancing <b>Vision</b> <b>Transformer</b> (ViT) performance in a way similar to the language domain. This study provides a comprehensive analysis of RoPE when applied to ViTs, utilizing practical implementations of RoPE for 2D <b>vision</b> <b>data.</b> The analysis reveals that RoPE demonstrates impressive extrapolation performance, i.e., maintaining precision while increasing image resolution at inference. It eventually leads to performance improvement for ImageNet-1k, COCO detection, and ADE-20k segmentation. We believe this study provides thorough guidelines to apply RoPE into ViT, promising improved backbone performance with minimal extra computational overhead. Our code and pre-trained models are available at <a href=https://github.com/naver-ai/rope-vit>https://github.com/naver-ai/rope-vit</a></p></p class="citation"></blockquote><h3 id=3989--45286-adavipro-region-based-adaptive-visual-prompt-for-large-scale-models-adapting-mengyu-yang-et-al-2024>(39/89 | 45/286) AdaViPro: Region-based Adaptive Visual Prompt for Large-Scale Models Adapting (Mengyu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengyu Yang, Ye Tian, Lanshan Zhang, Xiao Liang, Xuming Ran, Wendong Wang. (2024)<br><strong>AdaViPro: Region-based Adaptive Visual Prompt for Large-Scale Models Adapting</strong><br><button class=copy-to-clipboard title="AdaViPro: Region-based Adaptive Visual Prompt for Large-Scale Models Adapting" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13282v1.pdf filename=2403.13282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>prompt-based</b> methods have emerged as a new alternative <code>parameter-efficient &lt;b>fine-tuning'&lt;/b> paradigm, which only &lt;b>fine-tunes&lt;/b> a small number of additional parameters while keeping the original model frozen. However, despite achieving notable results, existing &lt;b>prompt&lt;/b> methods mainly focus on </code>what to add&rsquo;, while overlooking the equally important aspect of <code>where to add', typically relying on the manually crafted placement. To this end, we propose a region-based Adaptive Visual &lt;b>Prompt,&lt;/b> named AdaViPro, which integrates the </code>where to add&rsquo; optimization of the <b>prompt</b> into the learning process. Specifically, we reconceptualize the `where to add&rsquo; optimization as a problem of regional decision-making. During inference, AdaViPro generates a regionalized mask map for the whole image, which is composed of 0 and 1, to designate whether to apply or discard the <b>prompt</b> in each specific area. Therefore, we employ Gumbel-Softmax sampling to enable AdaViPro&rsquo;s end-to-end learning through standard back-propagation. Extensive experiments demonstrate that our AdaViPro yields new efficiency and accuracy trade-offs for adapting pre-trained models.</p></p class="citation"></blockquote><h3 id=4089--46286-samct-segment-any-ct-allowing-labor-free-task-indicator-prompts-xian-lin-et-al-2024>(40/89 | 46/286) SAMCT: Segment Any CT Allowing Labor-Free Task-Indicator Prompts (Xian Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xian Lin, Yangyang Xiang, Zhehao Wang, Kwang-Ting Cheng, Zengqiang Yan, Li Yu. (2024)<br><strong>SAMCT: Segment Any CT Allowing Labor-Free Task-Indicator Prompts</strong><br><button class=copy-to-clipboard title="SAMCT: Segment Any CT Allowing Labor-Free Task-Indicator Prompts" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Foundation Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13258v1.pdf filename=2403.13258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Segment anything model (SAM), a <b>foundation</b> <b>model</b> with superior versatility and generalization across diverse segmentation tasks, has attracted widespread attention in medical imaging. However, it has been proved that SAM would encounter severe performance degradation due to the lack of medical knowledge in training and local feature encoding. Though several SAM-based models have been proposed for tuning SAM in medical imaging, they still suffer from insufficient feature extraction and highly rely on high-quality <b>prompts.</b> In this paper, we construct a large CT dataset consisting of 1.1M CT images and 5M masks from public datasets and propose a powerful <b>foundation</b> <b>model</b> SAMCT allowing labor-free <b>prompts.</b> Specifically, based on SAM, SAMCT is further equipped with a U-shaped <b>CNN</b> image encoder, a cross-branch interaction module, and a task-indicator <b>prompt</b> encoder. The U-shaped <b>CNN</b> image encoder works in parallel with the ViT image encoder in SAM to supplement local features. Cross-branch interaction enhances the feature expression capability of the <b>CNN</b> image encoder and the ViT image encoder by exchanging global perception and local features from one to the other. The task-indicator <b>prompt</b> encoder is a plug-and-play component to effortlessly encode task-related indicators into <b>prompt</b> embeddings. In this way, SAMCT can work in an automatic manner in addition to the semi-automatic interactive strategy in SAM. Extensive experiments demonstrate the superiority of SAMCT against the state-of-the-art task-specific and SAM-based medical <b>foundation</b> <b>models</b> on various tasks. The code, data, and models are released at <a href=https://github.com/xianlin7/SAMCT>https://github.com/xianlin7/SAMCT</a>.</p></p class="citation"></blockquote><h3 id=4189--47286-zigma-zigzag-mamba-diffusion-model-vincent-tao-hu-et-al-2024>(41/89 | 47/286) ZigMa: Zigzag Mamba Diffusion Model (Vincent Tao Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, Bjorn Ommer. (2024)<br><strong>ZigMa: Zigzag Mamba Diffusion Model</strong><br><button class=copy-to-clipboard title="ZigMa: Zigzag Mamba Diffusion Model" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 26<br>Keywords: Diffusion Model, Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13802v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13802v1.pdf filename=2403.13802v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>diffusion</b> <b>model</b> has long been plagued by scalability and quadratic complexity issues, especially within <b>transformer-based</b> structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to <b>transformer-based</b> baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\times 1024$ and UCF101, <b>MultiModal-CelebA-HQ,</b> and MS COCO $256\times 256$. Code will be released at <a href=https://taohu.me/zigma/>https://taohu.me/zigma/</a></p></p class="citation"></blockquote><h3 id=4289--48286-compress3d-a-compressed-latent-space-for-3d-generation-from-a-single-image-bowen-zhang-et-al-2024>(42/89 | 48/286) Compress3D: a Compressed Latent Space for 3D Generation from a Single Image (Bowen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Zhang, Tianyu Yang, Yu Li, Lei Zhang, Xi Zhao. (2024)<br><strong>Compress3D: a Compressed Latent Space for 3D Generation from a Single Image</strong><br><button class=copy-to-clipboard title="Compress3D: a Compressed Latent Space for 3D Generation from a Single Image" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Diffusion Model, Autoencoder, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13524v1.pdf filename=2403.13524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D generation has witnessed significant advancements, yet efficiently producing high-quality 3D assets from a single image remains challenging. In this paper, we present a triplane <b>autoencoder,</b> which encodes 3D models into a compact triplane latent space to effectively compress both the 3D <b>geometry</b> and texture information. Within the <b>autoencoder</b> framework, we introduce a 3D-aware cross-attention mechanism, which utilizes low-resolution latent representations to query features from a high-resolution 3D feature volume, thereby enhancing the representation capacity of the latent space. Subsequently, we train a <b>diffusion</b> <b>model</b> on this refined latent space. In contrast to solely relying on image embedding for 3D generation, our proposed method advocates for the simultaneous utilization of both image embedding and shape embedding as conditions. Specifically, the shape embedding is estimated via a <b>diffusion</b> <b>prior</b> model conditioned on the image embedding. Through comprehensive experiments, we demonstrate that our method outperforms state-of-the-art algorithms, achieving superior performance while requiring less training data and time. Our approach enables the generation of high-quality 3D assets in merely 7 seconds on a single A100 GPU.</p></p class="citation"></blockquote><h3 id=4389--49286-text-to-3d-shape-generation-han-hung-lee-et-al-2024>(43/89 | 49/286) Text-to-3D Shape Generation (Han-Hung Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han-Hung Lee, Manolis Savva, Angel X. Chang. (2024)<br><strong>Text-to-3D Shape Generation</strong><br><button class=copy-to-clipboard title="Text-to-3D Shape Generation" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Generative AI, Representation Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13289v1.pdf filename=2403.13289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have seen an explosion of work and interest in text-to-3D shape generation. Much of the progress is driven by advances in 3D <b>representations,</b> <b>large-scale</b> pretraining and <b>representation</b> <b>learning</b> for text and image data enabling <b>generative</b> <b>AI</b> models, and differentiable rendering. Computational systems that can perform text-to-3D shape generation have captivated the popular imagination as they enable non-expert users to easily create 3D content directly from text. However, there are still many limitations and challenges remaining in this problem space. In this state-of-the-art report, we provide a survey of the underlying technology and methods enabling text-to-3D shape generation to <b>summarize</b> the background literature. We then derive a systematic categorization of recent work on text-to-3D shape generation based on the type of supervision data required. Finally, we discuss limitations of the existing categories of methods, and delineate promising directions for future work.</p></p class="citation"></blockquote><h3 id=4489--50286-beyond-skeletons-integrative-latent-mapping-for-coherent-4d-sequence-generation-qitong-yang-et-al-2024>(44/89 | 50/286) Beyond Skeletons: Integrative Latent Mapping for Coherent 4D Sequence Generation (Qitong Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qitong Yang, Mingtao Feng, Zijie Wu, Shijie Sun, Weisheng Dong, Yaonan Wang, Ajmal Mian. (2024)<br><strong>Beyond Skeletons: Integrative Latent Mapping for Coherent 4D Sequence Generation</strong><br><button class=copy-to-clipboard title="Beyond Skeletons: Integrative Latent Mapping for Coherent 4D Sequence Generation" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Diffusion Model, Geometry, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13238v1.pdf filename=2403.13238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Directly learning to model 4D content, including shape, color and motion, is challenging. Existing methods depend on skeleton-based motion control and offer limited continuity in detail. To address this, we propose a novel framework that generates coherent 4D sequences with animation of 3D shapes under given conditions with dynamic evolution of shape and color over time through integrative latent mapping. We first employ an integrative latent unified representation to encode shape and color information of each detailed 3D <b>geometry</b> frame. The proposed skeleton-free latent 4D sequence joint representation allows us to leverage <b>diffusion</b> <b>models</b> in a low-dimensional space to control the generation of 4D sequences. Finally, temporally coherent 4D sequences are generated conforming well to the input images and text <b>prompts.</b> Extensive experiments on the ShapeNet, 3DBiCar and DeformingThings4D datasets for several tasks demonstrate that our method effectively learns to generate quality 3D shapes with color and 4D mesh animations, improving over the current state-of-the-art. Source code will be released.</p></p class="citation"></blockquote><h3 id=4589--51286-sptnet-an-efficient-alternative-framework-for-generalized-category-discovery-with-spatial-prompt-tuning-hongjun-wang-et-al-2024>(45/89 | 51/286) SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning (Hongjun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongjun Wang, Sagar Vaze, Kai Han. (2024)<br><strong>SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning</strong><br><button class=copy-to-clipboard title="SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13684v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13684v1.pdf filename=2403.13684v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generalized Category Discovery (GCD) aims to classify unlabelled images from both <code>seen' and </code>unseen&rsquo; classes by transferring knowledge from a set of labelled `seen&rsquo; class images. A key theme in existing GCD approaches is adapting large-scale pre-trained models for the GCD task. An alternate perspective, however, is to adapt the data representation itself for better alignment with the pre-trained model. As such, in this paper, we introduce a two-stage adaptation approach termed SPTNet, which iteratively optimizes model parameters (i.e., model-finetuning) and data parameters (i.e., <b>prompt</b> <b>learning).</b> Furthermore, we propose a novel spatial <b>prompt</b> <b>tuning</b> method (SPT) which considers the spatial property of image data, enabling the method to better focus on object parts, which can transfer between seen and unseen classes. We thoroughly evaluate our SPTNet on standard <b>benchmarks</b> and demonstrate that our method outperforms existing GCD methods. Notably, we find our method achieves an average accuracy of 61.4% on the SSB, surpassing prior state-of-the-art methods by approximately 10%. The improvement is particularly remarkable as our method yields extra parameters amounting to only 0.117% of those in the backbone architecture. Project page: <a href=https://visual-ai.github.io/sptnet>https://visual-ai.github.io/sptnet</a>.</p></p class="citation"></blockquote><h3 id=4689--52286-dancecamera3d-3d-camera-movement-synthesis-with-music-and-dance-zixuan-wang-et-al-2024>(46/89 | 52/286) DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance (Zixuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zixuan Wang, Jia Jia, Shikun Sun, Haozhe Wu, Rong Han, Zhenyu Li, Di Tang, Jiaqing Zhou, Jiebo Luo. (2024)<br><strong>DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance</strong><br><button class=copy-to-clipboard title="DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13667v1.pdf filename=2403.13667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Choreographers determine what the dances look like, while cameramen determine the final presentation of dances. Recently, various methods and datasets have showcased the feasibility of dance synthesis. However, camera movement synthesis with music and dance remains an unsolved challenging problem due to the scarcity of paired data. Thus, we present DCM, a new <b>multi-modal</b> 3D dataset, which for the first time combines camera movement with dance motion and music audio. This dataset encompasses 108 dance sequences (3.2 hours) of paired dance-camera-music data from the anime community, covering 4 music genres. With this dataset, we uncover that dance camera movement is multifaceted and human-centric, and possesses multiple influencing factors, making dance camera synthesis a more challenging task compared to camera or dance synthesis alone. To overcome these difficulties, we propose DanceCamera3D, a <b>transformer-based</b> <b>diffusion</b> <b>model</b> that incorporates a novel body attention loss and a condition separation strategy. For evaluation, we devise new metrics measuring camera movement quality, diversity, and dancer fidelity. Utilizing these metrics, we conduct extensive experiments on our DCM dataset, providing both quantitative and qualitative evidence showcasing the effectiveness of our DanceCamera3D model. Code and video demos are available at <a href=https://github.com/Carmenw1203/DanceCamera3D-Official>https://github.com/Carmenw1203/DanceCamera3D-Official</a>.</p></p class="citation"></blockquote><h3 id=4789--53286-exmap-leveraging-explainability-heatmaps-for-unsupervised-group-robustness-to-spurious-correlations-rwiddhi-chakraborty-et-al-2024>(47/89 | 53/286) ExMap: Leveraging Explainability Heatmaps for Unsupervised Group Robustness to Spurious Correlations (Rwiddhi Chakraborty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rwiddhi Chakraborty, Adrian Sletten, Michael Kampffmeyer. (2024)<br><strong>ExMap: Leveraging Explainability Heatmaps for Unsupervised Group Robustness to Spurious Correlations</strong><br><button class=copy-to-clipboard title="ExMap: Leveraging Explainability Heatmaps for Unsupervised Group Robustness to Spurious Correlations" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Clustering, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13870v1.pdf filename=2403.13870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Group robustness strategies aim to mitigate learned biases in deep learning models that arise from spurious correlations present in their training datasets. However, most existing methods rely on the access to the label distribution of the groups, which is time-consuming and expensive to obtain. As a result, <b>unsupervised</b> group robustness strategies are sought. Based on the insight that a trained model&rsquo;s classification strategies can be inferred accurately based on explainability heatmaps, we introduce ExMap, an <b>unsupervised</b> two stage mechanism designed to enhance group robustness in traditional classifiers. ExMap utilizes a <b>clustering</b> module to infer pseudo-labels based on a model&rsquo;s explainability heatmaps, which are then used during training in lieu of actual labels. Our empirical studies validate the efficacy of ExMap - We demonstrate that it bridges the performance gap with its <b>supervised</b> counterparts and outperforms existing partially <b>supervised</b> and <b>unsupervised</b> methods. Additionally, ExMap can be seamlessly integrated with existing group robustness learning strategies. Finally, we demonstrate its potential in tackling the emerging issue of multiple shortcut mitigation\footnote{Code available at \url{https://github.com/rwchakra/exmap}}.</p></p class="citation"></blockquote><h3 id=4889--54286-find-n-propagate-open-vocabulary-3d-object-detection-in-urban-environments-djamahl-etchegaray-et-al-2024>(48/89 | 54/286) Find n&rsquo; Propagate: Open-Vocabulary 3D Object Detection in Urban Environments (Djamahl Etchegaray et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Djamahl Etchegaray, Zi Huang, Tatsuya Harada, Yadan Luo. (2024)<br><strong>Find n&rsquo; Propagate: Open-Vocabulary 3D Object Detection in Urban Environments</strong><br><button class=copy-to-clipboard title="Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13556v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13556v1.pdf filename=2403.13556v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we tackle the limitations of current LiDAR-based 3D <b>object</b> <b>detection</b> systems, which are hindered by a restricted class vocabulary and the high costs associated with annotating new <b>object</b> <b>classes.</b> Our exploration of open-vocabulary (OV) learning in urban environments aims to capture novel instances using pre-trained <b>vision-language</b> models (VLMs) with multi-sensor data. We design and <b>benchmark</b> a set of four potential solutions as baselines, categorizing them into either top-down or bottom-up approaches based on their input data strategies. While effective, these methods exhibit certain limitations, such as missing novel <b>objects</b> <b>in</b> 3D box estimation or applying rigorous priors, leading to biases towards <b>objects</b> <b>near</b> the camera or of rectangular geometries. To overcome these limitations, we introduce a universal \textsc{Find n&rsquo; Propagate} approach for 3D OV tasks, aimed at maximizing the recall of novel <b>objects</b> <b>and</b> propagating this detection capability to more distant areas thereby progressively capturing more. In particular, we utilize a greedy box seeker to search against 3D novel boxes of varying orientations and depth in each generated frustum and ensure the reliability of newly identified boxes by cross alignment and density ranker. Additionally, the inherent bias towards camera-proximal <b>objects</b> <b>is</b> alleviated by the proposed remote simulator, which randomly diversifies pseudo-labeled novel instances in the self-training process, combined with the fusion of base samples in the memory bank. Extensive experiments demonstrate a 53% improvement in novel recall across diverse OV settings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold increase in Average Precision (AP) for novel <b>object</b> <b>classes.</b> The source code is made available in the supplementary material.</p></p class="citation"></blockquote><h3 id=4989--55286-scale-decoupled-distillation-shicai-wei-chunbo-luo-yang-luo-2024>(49/89 | 55/286) Scale Decoupled Distillation (Shicai Wei Chunbo Luo Yang Luo, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shicai Wei Chunbo Luo Yang Luo. (2024)<br><strong>Scale Decoupled Distillation</strong><br><button class=copy-to-clipboard title="Scale Decoupled Distillation" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13512v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13512v1.pdf filename=2403.13512v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Logit <b>knowledge</b> <b>distillation</b> attracts increasing attention due to its practicality in recent studies. However, it often suffers inferior performance compared to the feature <b>knowledge</b> <b>distillation.</b> In this paper, we argue that existing logit-based methods may be sub-optimal since they only leverage the global logit output that couples multiple semantic <b>knowledge.</b> <b>This</b> may transfer ambiguous <b>knowledge</b> <b>to</b> the student and mislead its learning. To this end, we propose a simple but effective method, i.e., Scale Decoupled <b>Distillation</b> (SDD), for logit <b>knowledge</b> <b>distillation.</b> SDD decouples the global logit output into multiple local logit outputs and establishes <b>distillation</b> pipelines for them. This helps the student to mine and inherit fine-grained and unambiguous logit <b>knowledge.</b> <b>Moreover,</b> the decoupled <b>knowledge</b> <b>can</b> be further divided into consistent and complementary logit <b>knowledge</b> <b>that</b> transfers the semantic information and sample ambiguity, respectively. By increasing the weight of complementary parts, SDD can guide the student to focus more on ambiguous samples, improving its discrimination ability. Extensive experiments on several <b>benchmark</b> datasets demonstrate the effectiveness of SDD for wide teacher-student pairs, especially in the fine-grained classification task. Code is available at: <a href=https://github.com/shicaiwei123/SDD-CVPR2024>https://github.com/shicaiwei123/SDD-CVPR2024</a></p></p class="citation"></blockquote><h3 id=5089--56286-sc-tune-unleashing-self-consistent-referential-comprehension-in-large-vision-language-models-tongtian-yue-et-al-2024>(50/89 | 56/286) SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models (Tongtian Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tongtian Yue, Jie Cheng, Longteng Guo, Xingyuan Dai, Zijia Zhao, Xingjian He, Gang Xiong, Yisheng Lv, Jing Liu. (2024)<br><strong>SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models</strong><br><button class=copy-to-clipboard title="SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13263v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13263v1.pdf filename=2403.13263v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent trends in Large Vision Language Models (LVLMs) research have been increasingly focusing on advancing beyond general image understanding towards more nuanced, object-level referential comprehension. In this paper, we present and delve into the self-consistency capability of LVLMs, a crucial aspect that reflects the models&rsquo; ability to both generate informative captions for specific objects and subsequently utilize these captions to accurately re-identify the objects in a closed-loop process. This capability significantly mirrors the precision and reliability of fine-grained visual-language understanding. Our findings reveal that the self-consistency level of existing LVLMs falls short of expectations, posing limitations on their practical applicability and potential. To address this gap, we introduce a novel <b>fine-tuning</b> paradigm named Self-Consistency Tuning (SC-Tune). It features the synergistic learning of a cyclic describer-locator system. This paradigm is not only data-efficient but also exhibits generalizability across multiple LVLMs. Through extensive experiments, we demonstrate that SC-Tune significantly elevates performance across a spectrum of object-level <b>vision-language</b> <b>benchmarks</b> and maintains competitive or improved performance on image-level <b>vision-language</b> <b>benchmarks.</b> Both our model and code will be publicly available at <a href=https://github.com/ivattyue/SC-Tune>https://github.com/ivattyue/SC-Tune</a>.</p></p class="citation"></blockquote><h3 id=5189--57286-nellie-automated-organelle-segmentation-tracking-and-hierarchical-feature-extraction-in-2d3d-live-cell-microscopy-austin-e-y-t-lefebvre-et-al-2024>(51/89 | 57/286) Nellie: Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy (Austin E. Y. T. Lefebvre et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Austin E. Y. T. Lefebvre, Gabriel Sturm, Ting-Yu Lin, Emily Stoops, Magdalena Preciado Lopez, Benjamin Kaufmann-Malaga, Kayley Hake. (2024)<br><strong>Nellie: Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy</strong><br><button class=copy-to-clipboard title="Nellie: Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, q-bio-QM<br>Keyword Score: 23<br>Keywords: Graph, Autoencoder, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13214v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13214v1.pdf filename=2403.13214v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The analysis of dynamic organelles remains a formidable challenge, though key to understanding biological processes. We introduce Nellie, an automated and unbiased pipeline for segmentation, tracking, and feature extraction of diverse intracellular structures. Nellie adapts to image metadata, eliminating user input. Nellie&rsquo;s preprocessing pipeline enhances structural contrast on multiple intracellular scales allowing for robust hierarchical segmentation of sub-organellar regions. Internal motion capture markers are generated and tracked via a radius-adaptive pattern matching scheme, and used as guides for sub-voxel flow interpolation. Nellie extracts a plethora of features at multiple hierarchical levels for deep and customizable analysis. Nellie features a Napari-based GUI that allows for code-free operation and visualization, while its modular open-source codebase invites customization by experienced users. We demonstrate Nellie&rsquo;s wide variety of use cases with two examples: unmixing multiple organelles from a single channel using feature-based classification and training an <b>unsupervised</b> <b>graph</b> <b>autoencoder</b> on mitochondrial multi-mesh <b>graphs</b> to quantify latent space embedding changes following ionomycin treatment.</p></p class="citation"></blockquote><h3 id=5289--58286-uncertainty-driven-active-learning-for-image-segmentation-in-underwater-inspection-luiza-ribeiro-marnet-et-al-2024>(52/89 | 58/286) Uncertainty Driven Active Learning for Image Segmentation in Underwater Inspection (Luiza Ribeiro Marnet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luiza Ribeiro Marnet, Yury Brodskiy, Stella Grasshof, Andrzej Wasowski. (2024)<br><strong>Uncertainty Driven Active Learning for Image Segmentation in Underwater Inspection</strong><br><button class=copy-to-clipboard title="Uncertainty Driven Active Learning for Image Segmentation in Underwater Inspection" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Active Learning, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14002v1.pdf filename=2403.14002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Active</b> <b>learning</b> aims to select the minimum amount of data to train a model that performs similarly to a model trained with the entire dataset. We study the potential of <b>active</b> <b>learning</b> for image segmentation in underwater infrastructure inspection tasks, where large amounts of data are typically collected. The pipeline inspection images are usually semantically repetitive but with great variations in quality. We use <b>mutual</b> <b>information</b> as the acquisition function, calculated using Monte Carlo dropout. To assess the effectiveness of the framework, DenseNet and HyperSeg are trained with the CamVid dataset using <b>active</b> <b>learning.</b> In addition, HyperSeg is trained with a pipeline inspection dataset of over 50,000 images. For the pipeline dataset, HyperSeg with <b>active</b> <b>learning</b> achieved 67.5% meanIoU using 12.5% of the data, and 61.4% with the same amount of randomly selected images. This shows that using <b>active</b> <b>learning</b> for segmentation models in underwater inspection tasks can lower the cost significantly.</p></p class="citation"></blockquote><h3 id=5389--59286-como-controllable-motion-generation-through-language-guided-pose-code-editing-yiming-huang-et-al-2024>(53/89 | 59/286) CoMo: Controllable Motion Generation through Language Guided Pose Code Editing (Yiming Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Huang, Weilin Wan, Yue Yang, Chris Callison-Burch, Mark Yatskar, Lingjie Liu. (2024)<br><strong>CoMo: Controllable Motion Generation through Language Guided Pose Code Editing</strong><br><button class=copy-to-clipboard title="CoMo: Controllable Motion Generation through Language Guided Pose Code Editing" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13900v1.pdf filename=2403.13900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-motion models excel at efficient human motion generation, but existing approaches lack fine-grained controllability over the generation process. Consequently, modifying subtle postures within a motion or inserting new actions at specific moments remains a challenge, limiting the applicability of these methods in diverse scenarios. In light of these challenges, we introduce CoMo, a Controllable Motion generation model, adept at accurately generating and editing motions by leveraging the knowledge priors of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Specifically, CoMo decomposes motions into discrete and semantically meaningful pose codes, with each code encapsulating the semantics of a body part, representing elementary information such as &ldquo;left knee slightly bent&rdquo;. Given textual inputs, CoMo autoregressively generates sequences of pose codes, which are then decoded into 3D motions. Leveraging pose codes as interpretable representations, an <b>LLM</b> can directly intervene in motion editing by adjusting the pose codes according to editing instructions. Experiments demonstrate that CoMo achieves competitive performance in motion generation compared to state-of-the-art models while, in human studies, CoMo substantially surpasses previous work in motion editing abilities.</p></p class="citation"></blockquote><h3 id=5489--60286-self-attention-based-semantic-decomposition-in-vector-symbolic-architectures-calvin-yeung-et-al-2024>(54/89 | 60/286) Self-Attention Based Semantic Decomposition in Vector Symbolic Architectures (Calvin Yeung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Calvin Yeung, Prathyush Poduval, Mohsen Imani. (2024)<br><strong>Self-Attention Based Semantic Decomposition in Vector Symbolic Architectures</strong><br><button class=copy-to-clipboard title="Self-Attention Based Semantic Decomposition in Vector Symbolic Architectures" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-SC, cs.CV<br>Keyword Score: 20<br>Keywords: Reasoning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13218v1.pdf filename=2403.13218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vector Symbolic Architectures (VSAs) have emerged as a novel framework for enabling interpretable machine learning algorithms equipped with the ability to reason and explain their decision processes. The basic idea is to represent discrete information through high dimensional random vectors. Complex data structures can be built up with operations over vectors such as the &ldquo;binding&rdquo; operation involving element-wise vector multiplication, which associates data together. The reverse task of decomposing the associated elements is a combinatorially hard task, with an exponentially large search space. The main algorithm for performing this search is the resonator network, inspired by Hopfield network-based memory search operations. In this work, we introduce a new variant of the resonator network, based on <b>self-attention</b> based update rules in the iterative search problem. This update rule, based on the Hopfield network with log-sum-exp energy function and norm-bounded states, is shown to substantially improve the performance and rate of convergence. As a result, our algorithm enables a larger capacity for associative memory, enabling applications in many tasks like perception based pattern recognition, scene decomposition, and object <b>reasoning.</b> We substantiate our algorithm with a thorough evaluation and comparisons to baselines.</p></p class="citation"></blockquote><h3 id=5589--61286-practical-end-to-end-optical-music-recognition-for-pianoform-music-jiří-mayer-et-al-2024>(55/89 | 61/286) Practical End-to-End Optical Music Recognition for Pianoform Music (Jiří Mayer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiří Mayer, Milan Straka, Jan Hajič jr., Pavel Pecina. (2024)<br><strong>Practical End-to-End Optical Music Recognition for Pianoform Music</strong><br><button class=copy-to-clipboard title="Practical End-to-End Optical Music Recognition for Pianoform Music" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-9; J-5, cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13763v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13763v1.pdf filename=2403.13763v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The majority of recent progress in Optical Music Recognition (OMR) has been achieved with Deep Learning methods, especially models following the end-to-end paradigm, reading input images and producing a linear sequence of tokens. Unfortunately, many music scores, especially piano music, cannot be easily converted to a linear sequence. This has led OMR researchers to use custom linearized encodings, instead of broadly accepted structured formats for music notation. Their diversity makes it difficult to compare the performance of OMR systems directly. To bring recent OMR model progress closer to useful results: (a) We define a sequential format called Linearized MusicXML, allowing to train an end-to-end model directly and maintaining close cohesion and compatibility with the industry-standard MusicXML format. (b) We create a dev and test set for <b>benchmarking</b> typeset OMR with MusicXML ground truth based on the OpenScore Lieder corpus. They contain 1,438 and 1,493 pianoform systems, each with an image from IMSLP. (c) We train and <b>fine-tune</b> an end-to-end model to serve as a baseline on the dataset and employ the TEDn metric to evaluate the model. We also test our model against the recently published synthetic pianoform dataset GrandStaff and surpass the state-of-the-art results.</p></p class="citation"></blockquote><h3 id=5689--62286-a-unified-optimal-transport-framework-for-cross-modal-retrieval-with-noisy-labels-haochen-han-et-al-2024>(56/89 | 62/286) A Unified Optimal Transport Framework for Cross-Modal Retrieval with Noisy Labels (Haochen Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haochen Han, Minnan Luo, Huan Liu, Fang Nan. (2024)<br><strong>A Unified Optimal Transport Framework for Cross-Modal Retrieval with Noisy Labels</strong><br><button class=copy-to-clipboard title="A Unified Optimal Transport Framework for Cross-Modal Retrieval with Noisy Labels" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-IR, cs-MM, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13480v1.pdf filename=2403.13480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-modal retrieval (CMR) aims to establish interaction between different modalities, among which <b>supervised</b> CMR is emerging due to its flexibility in learning semantic category discrimination. Despite the remarkable performance of previous <b>supervised</b> CMR methods, much of their success can be attributed to the well-annotated data. However, even for unimodal data, precise annotation is expensive and time-consuming, and it becomes more challenging with the <b>multimodal</b> scenario. In practice, massive <b>multimodal</b> data are collected from the Internet with coarse annotation, which inevitably introduces noisy labels. Training with such misleading labels would bring two key challenges &ndash; enforcing the <b>multimodal</b> samples to \emph{align incorrect semantics} and \emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To tackle these challenges, this work proposes UOT-RCL, a Unified framework based on Optimal Transport (OT) for Robust Cross-modal Retrieval. First, we propose a semantic alignment based on partial OT to progressively correct the noisy labels, where a novel cross-modal consistent cost function is designed to blend different modalities and provide precise transport cost. Second, to narrow the discrepancy in <b>multi-modal</b> data, an OT-based relation alignment is proposed to infer the semantic-level cross-modal matching. Both of these two components leverage the inherent correlation among <b>multi-modal</b> data to facilitate effective cost function. The experiments on three widely-used cross-modal retrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art approaches and significantly improves the robustness against noisy labels.</p></p class="citation"></blockquote><h3 id=5789--63286-depthfm-fast-monocular-depth-estimation-with-flow-matching-ming-gui-et-al-2024>(57/89 | 63/286) DepthFM: Fast Monocular Depth Estimation with Flow Matching (Ming Gui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Gui, Johannes S. Fischer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, Björn Ommer. (2024)<br><strong>DepthFM: Fast Monocular Depth Estimation with Flow Matching</strong><br><button class=copy-to-clipboard title="DepthFM: Fast Monocular Depth Estimation with Flow Matching" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13788v1.pdf filename=2403.13788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monocular depth estimation is crucial for numerous downstream vision tasks and applications. Current discriminative approaches to this problem are limited due to blurry artifacts, while state-of-the-art generative methods suffer from slow sampling due to their SDE nature. Rather than starting from noise, we seek a direct mapping from input image to depth map. We observe that this can be effectively framed using flow matching, since its straight trajectories through solution space offer efficiency and high quality. Our study demonstrates that a pre-trained image <b>diffusion</b> <b>model</b> can serve as an adequate prior for a flow matching depth model, allowing efficient training on only synthetic data to generalize to real images. We find that an auxiliary surface normals loss further improves the depth estimates. Due to the generative nature of our approach, our model reliably predicts the confidence of its depth estimates. On standard <b>benchmarks</b> of complex natural scenes, our lightweight approach exhibits state-of-the-art performance at favorable low computational cost despite only being trained on little synthetic data.</p></p class="citation"></blockquote><h3 id=5889--64286-hiercode-a-lightweight-hierarchical-codebook-for-zero-shot-chinese-text-recognition-yuyi-zhang-et-al-2024>(58/89 | 64/286) HierCode: A Lightweight Hierarchical Codebook for Zero-shot Chinese Text Recognition (Yuyi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuyi Zhang, Yuanzhi Zhu, Dezhi Peng, Peirong Zhang, Zhenhua Yang, Zhibo Yang, Cong Yao, Lianwen Jin. (2024)<br><strong>HierCode: A Lightweight Hierarchical Codebook for Zero-shot Chinese Text Recognition</strong><br><button class=copy-to-clipboard title="HierCode: A Lightweight Hierarchical Codebook for Zero-shot Chinese Text Recognition" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13761v1.pdf filename=2403.13761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text recognition, especially for complex scripts like Chinese, faces unique challenges due to its intricate character structures and vast vocabulary. Traditional one-hot encoding methods struggle with the representation of hierarchical radicals, recognition of Out-Of-Vocabulary (OOV) characters, and on-device deployment due to their computational intensity. To address these challenges, we propose HierCode, a novel and lightweight codebook that exploits the innate hierarchical nature of Chinese characters. HierCode employs a multi-hot encoding strategy, leveraging hierarchical binary tree encoding and prototype learning to create distinctive, informative representations for each character. This approach not only facilitates <b>zero-shot</b> recognition of OOV characters by utilizing shared radicals and structures but also excels in line-level recognition tasks by computing similarity with visual features, a notable advantage over existing methods. Extensive experiments across diverse <b>benchmarks,</b> including handwritten, scene, document, web, and ancient text, have showcased HierCode&rsquo;s superiority for both conventional and <b>zero-shot</b> Chinese character or text recognition, exhibiting state-of-the-art performance with significantly fewer parameters and fast inference speed.</p></p class="citation"></blockquote><h3 id=5989--65286-be-your-outpainter-mastering-video-outpainting-through-input-specific-adaptation-fu-yun-wang-et-al-2024>(59/89 | 65/286) Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation (Fu-Yun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, Hongsheng Li. (2024)<br><strong>Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation</strong><br><button class=copy-to-clipboard title="Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13745v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13745v1.pdf filename=2403.13745v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video outpainting is a challenging task, aiming at generating video content outside the viewport of the input video while maintaining inter-frame and intra-frame consistency. Existing methods fall short in either generation quality or flexibility. We introduce MOTIA Mastering Video Outpainting Through Input-Specific Adaptation, a <b>diffusion-based</b> <b>pipeline</b> that leverages both the intrinsic data-specific patterns of the source video and the image/video generative prior for effective outpainting. MOTIA comprises two main phases: input-specific adaptation and pattern-aware outpainting. The input-specific adaptation phase involves conducting efficient and effective pseudo outpainting learning on the single-shot source video. This process encourages the model to identify and learn patterns within the source video, as well as bridging the gap between standard generative processes and outpainting. The subsequent phase, pattern-aware outpainting, is dedicated to the generalization of these learned patterns to generate outpainting outcomes. Additional strategies including spatial-aware insertion and noise travel are proposed to better leverage the <b>diffusion</b> <b>model&rsquo;s</b> generative prior and the acquired video patterns from source videos. Extensive evaluations underscore MOTIA&rsquo;s superiority, outperforming existing state-of-the-art methods in widely recognized <b>benchmarks.</b> Notably, these advancements are achieved without necessitating extensive, task-specific tuning.</p></p class="citation"></blockquote><h3 id=6089--66286-orthcaps-an-orthogonal-capsnet-with-sparse-attention-routing-and-pruning-xinyu-geng-et-al-2024>(60/89 | 66/286) OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and Pruning (Xinyu Geng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Geng, Jiaming Wang, Jiawei Gong, Yuerong Xue, Jun Xu, Fanglin Chen, Xiaolin Huang. (2024)<br><strong>OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and Pruning</strong><br><button class=copy-to-clipboard title="OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and Pruning" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13351v1.pdf filename=2403.13351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Redundancy is a persistent challenge in Capsule Networks (CapsNet),leading to high computational costs and parameter counts. Although previous works have introduced <b>pruning</b> after the initial capsule layer, dynamic routing&rsquo;s fully connected nature and non-orthogonal weight matrices reintroduce redundancy in deeper layers. Besides, dynamic routing requires iterating to converge, further increasing computational demands. In this paper, we propose an Orthogonal Capsule Network (OrthCaps) to reduce redundancy, improve routing performance and decrease parameter counts. Firstly, an efficient pruned capsule layer is introduced to discard redundant capsules. Secondly, dynamic routing is replaced with orthogonal sparse attention routing, eliminating the need for iterations and fully connected structures. Lastly, weight matrices during routing are orthogonalized to sustain low capsule similarity, which is the first approach to introduce orthogonality into CapsNet as far as we know. Our experiments on baseline datasets affirm the efficiency and robustness of OrthCaps in classification tasks, in which ablation studies validate the criticality of each component. Remarkably, OrthCaps-Shallow outperforms other Capsule Network <b>benchmarks</b> on four datasets, utilizing only 110k parameters, which is a mere 1.25% of a standard Capsule Network&rsquo;s total. To the best of our knowledge, it achieves the smallest parameter count among existing Capsule Networks. Similarly, OrthCaps-Deep demonstrates competitive performance across four datasets, utilizing only 1.2% of the parameters required by its counterparts.</p></p class="citation"></blockquote><h3 id=6189--67286-tibix-leveraging-temporal-information-for-bidirectional-x-ray-and-report-generation-santosh-sanjeev-et-al-2024>(61/89 | 67/286) TiBiX: Leveraging Temporal Information for Bidirectional X-ray and Report Generation (Santosh Sanjeev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Santosh Sanjeev, Fadillah Adamsyah Maani, Arsen Abzhanov, Vijay Ram Papineni, Ibrahim Almakky, Bartłomiej W. Papież, Mohammad Yaqub. (2024)<br><strong>TiBiX: Leveraging Temporal Information for Bidirectional X-ray and Report Generation</strong><br><button class=copy-to-clipboard title="TiBiX: Leveraging Temporal Information for Bidirectional X-ray and Report Generation" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13343v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13343v1.pdf filename=2403.13343v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the emergence of vision language models in the medical imaging domain, numerous studies have focused on two dominant research activities: (1) report generation from Chest X-rays (CXR), and (2) synthetic scan generation from text or reports. Despite some research incorporating multi-view CXRs into the generative process, prior patient scans and reports have been generally disregarded. This can inadvertently lead to the leaving out of important medical information, thus affecting generation quality. To address this, we propose TiBiX: Leveraging Temporal information for Bidirectional X-ray and Report Generation. Considering previous scans, our approach facilitates bidirectional generation, primarily addressing two challenging problems: (1) generating the current image from the previous image and current report and (2) generating the current report based on both the previous and current images. Moreover, we extract and release a curated temporal <b>benchmark</b> dataset derived from the MIMIC-CXR dataset, which focuses on temporal data. Our comprehensive experiments and ablation studies explore the merits of incorporating prior CXRs and achieve state-of-the-art (SOTA) results on the report generation task. Furthermore, we attain on-par performance with SOTA image generation efforts, thus serving as a new baseline in longitudinal bidirectional CXR-to-report generation. The code is available at <a href=https://github.com/BioMedIA-MBZUAI/TiBiX>https://github.com/BioMedIA-MBZUAI/TiBiX</a>.</p></p class="citation"></blockquote><h3 id=6289--68286-efficient-scene-text-image-super-resolution-with-semantic-guidance-leowu-tomyenrique-et-al-2024>(62/89 | 68/286) Efficient scene text image super-resolution with semantic guidance (LeoWu TomyEnrique et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>LeoWu TomyEnrique, Xiangcheng Du, Kangliang Liu, Han Yuan, Zhao Zhou, Cheng Jin. (2024)<br><strong>Efficient scene text image super-resolution with semantic guidance</strong><br><button class=copy-to-clipboard title="Efficient scene text image super-resolution with semantic guidance" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13330v1.pdf filename=2403.13330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene text image super-resolution has significantly improved the accuracy of scene text recognition. However, many existing methods emphasize performance over efficiency and ignore the practical need for lightweight solutions in deployment scenarios. Faced with the issues, our work proposes an efficient framework called SGENet to facilitate deployment on resource-limited platforms. SGENet contains two branches: super-resolution branch and semantic guidance branch. We apply a lightweight pre-trained recognizer as a semantic extractor to enhance the understanding of text information. Meanwhile, we design the visual-semantic alignment module to achieve bidirectional alignment between image features and semantics, resulting in the generation of highquality prior guidance. We conduct extensive experiments on <b>benchmark</b> dataset, and the proposed SGENet achieves excellent performance with fewer computational costs. Code is available at <a href=https://github.com/SijieLiu518/SGENet>https://github.com/SijieLiu518/SGENet</a></p></p class="citation"></blockquote><h3 id=6389--69286-self-supervised-class-agnostic-motion-prediction-with-spatial-and-temporal-consistency-regularizations-kewei-wang-et-al-2024>(63/89 | 69/286) Self-Supervised Class-Agnostic Motion Prediction with Spatial and Temporal Consistency Regularizations (Kewei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kewei Wang, Yizheng Wu, Jun Cen, Zhiyu Pan, Xingyi Li, Zhe Wang, Zhiguo Cao, Guosheng Lin. (2024)<br><strong>Self-Supervised Class-Agnostic Motion Prediction with Spatial and Temporal Consistency Regularizations</strong><br><button class=copy-to-clipboard title="Self-Supervised Class-Agnostic Motion Prediction with Spatial and Temporal Consistency Regularizations" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Multi-modal, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13261v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13261v2.pdf filename=2403.13261v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The perception of motion behavior in a dynamic environment holds significant importance for autonomous driving systems, wherein class-agnostic motion prediction methods directly predict the motion of the entire point cloud. While most existing methods rely on fully-supervised learning, the manual labeling of point cloud data is laborious and time-consuming. Therefore, several annotation-efficient methods have been proposed to address this challenge. Although effective, these methods rely on weak annotations or additional <b>multi-modal</b> data like images, and the potential benefits inherent in the point cloud sequence are still underexplored. To this end, we explore the feasibility of <b>self-supervised</b> motion prediction with only unlabeled LiDAR point clouds. Initially, we employ an optimal transport solver to establish coarse correspondences between current and future point clouds as the coarse pseudo motion labels. Training models directly using such coarse labels leads to noticeable spatial and temporal prediction inconsistencies. To mitigate these issues, we introduce three simple spatial and temporal regularization losses, which facilitate the <b>self-supervised</b> training process effectively. Experimental results demonstrate the significant superiority of our approach over the state-of-the-art <b>self-supervised</b> methods.</p></p class="citation"></blockquote><h3 id=6489--70286-seffec-semantic-facial-feature-control-for-fine-grained-face-editing-florian-strohm-et-al-2024>(64/89 | 70/286) SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing (Florian Strohm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Strohm, Mihai Bâce, Markus Kaltenecker, Andreas Bulling. (2024)<br><strong>SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing</strong><br><button class=copy-to-clipboard title="SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13972v1.pdf filename=2403.13972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose Semantic Facial Feature Control (SeFFeC) - a novel method for fine-grained face shape editing. Our method enables the manipulation of human-understandable, semantic face features, such as nose length or mouth width, which are defined by different groups of facial landmarks. In contrast to existing methods, the use of facial landmarks enables precise measurement of the facial features, which then enables training SeFFeC without any manually annotated labels. SeFFeC consists of a <b>transformer-based</b> encoder network that takes a latent vector of a pre-trained generative model and a facial feature embedding as input, and learns to modify the latent vector to perform the desired face edit operation. To ensure that the desired feature measurement is changed towards the target value without altering uncorrelated features, we introduced a novel semantic face feature loss. Qualitative and quantitative results show that SeFFeC enables precise and fine-grained control of 23 facial features, some of which could not previously be controlled by other methods, without requiring manual annotations. Unlike existing methods, SeFFeC also provides deterministic control over the exact values of the facial features and more localised and disentangled face edits.</p></p class="citation"></blockquote><h3 id=6589--71286-radsplat-radiance-field-informed-gaussian-splatting-for-robust-real-time-rendering-with-900-fps-michael-niemeyer-et-al-2024>(65/89 | 71/286) RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS (Michael Niemeyer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico Tombari. (2024)<br><strong>RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS</strong><br><button class=copy-to-clipboard title="RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13806v1.pdf filename=2403.13806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in view synthesis and real-time rendering have achieved photorealistic quality at impressive rendering speeds. While Radiance Field-based methods achieve state-of-the-art quality in challenging scenarios such as in-the-wild captures and large-scale scenes, they often suffer from excessively high compute requirements linked to volumetric rendering. Gaussian Splatting-based methods, on the other hand, rely on rasterization and naturally achieve real-time rendering but suffer from brittle optimization heuristics that underperform on more challenging scenes. In this work, we present RadSplat, a lightweight method for robust real-time rendering of complex scenes. Our main contributions are threefold. First, we use radiance fields as a prior and supervision signal for optimizing point-based scene representations, leading to improved quality and more robust optimization. Next, we develop a novel <b>pruning</b> technique reducing the overall point count while maintaining high quality, leading to smaller and more compact scene representations with faster inference speeds. Finally, we propose a novel test-time filtering approach that further accelerates rendering and allows to scale to larger, house-sized scenes. We find that our method enables state-of-the-art synthesis of complex captures at 900+ FPS.</p></p class="citation"></blockquote><h3 id=6689--72286-bounding-box-stability-against-feature-dropout-reflects-detector-generalization-across-environments-yang-yang-et-al-2024>(66/89 | 72/286) Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments (Yang Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Yang, Wenhai Wang, Zhe Chen, Jifeng Dai, Liang Zheng. (2024)<br><strong>Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments</strong><br><button class=copy-to-clipboard title="Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13803v1.pdf filename=2403.13803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bounding boxes uniquely characterize <b>object</b> <b>detection,</b> where a good detector gives accurate bounding boxes of categories of interest. However, in the real-world where test ground truths are not provided, it is non-trivial to find out whether bounding boxes are accurate, thus preventing us from assessing the detector generalization ability. In this work, we find under feature map dropout, good detectors tend to output bounding boxes whose locations do not change much, while bounding boxes of poor detectors will undergo noticeable position changes. We compute the box stability score (BoS score) to reflect this stability. Specifically, given an image, we compute a normal set of bounding boxes and a second set after feature map dropout. To obtain BoS score, we use bipartite matching to find the corresponding boxes between the two sets and compute the average Intersection over Union (IoU) across the entire test set. We contribute to finding that BoS score has a strong, positive correlation with detection accuracy measured by mean average precision (mAP) under various test environments. This relationship allows us to predict the accuracy of detectors on various real-world test sets without accessing test ground truths, verified on canonical detection tasks such as vehicle detection and pedestrian detection. Code and data are available at <a href=https://github.com/YangYangGirl/BoS>https://github.com/YangYangGirl/BoS</a>.</p></p class="citation"></blockquote><h3 id=6789--73286-timerewind-rewinding-time-with-image-and-events-video-diffusion-jingxi-chen-et-al-2024>(67/89 | 73/286) TimeRewind: Rewinding Time with Image-and-Events Video Diffusion (Jingxi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingxi Chen, Brandon Y. Feng, Haoming Cai, Mingyang Xie, Christopher Metzler, Cornelia Fermuller, Yiannis Aloimonos. (2024)<br><strong>TimeRewind: Rewinding Time with Image-and-Events Video Diffusion</strong><br><button class=copy-to-clipboard title="TimeRewind: Rewinding Time with Image-and-Events Video Diffusion" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13800v1.pdf filename=2403.13800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the novel challenge of <code>rewinding'' time from a single captured image to recover the fleeting moments missed just before the shutter button is pressed. This problem poses a significant challenge in computer vision and computational photography, as it requires predicting plausible pre-capture motion from a single static frame, an inherently ill-posed task due to the high degree of freedom in potential pixel movements. We overcome this challenge by leveraging the emerging technology of neuromorphic event cameras, which capture motion information with high temporal resolution, and integrating this data with advanced image-to-video &lt;b>diffusion&lt;/b> &lt;b>models.&lt;/b> Our proposed framework introduces an event motion adaptor conditioned on event camera data, guiding the &lt;b>diffusion&lt;/b> &lt;b>model&lt;/b> to generate videos that are visually coherent and physically grounded in the captured events. Through extensive experimentation, we demonstrate the capability of our approach to synthesize high-quality videos that effectively </code>rewind&rsquo;&rsquo; time, showcasing the potential of combining event camera technology with generative models. Our work opens new avenues for research at the intersection of computer vision, computational photography, and generative modeling, offering a forward-thinking solution to capturing missed moments and enhancing future consumer cameras and smartphones. Please see the project page at <a href=https://timerewind.github.io/>https://timerewind.github.io/</a> for video results and code release.</p></p class="citation"></blockquote><h3 id=6889--74286-certified-human-trajectory-prediction-mohammadhossein-bahari-et-al-2024>(68/89 | 74/286) Certified Human Trajectory Prediction (Mohammadhossein Bahari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammadhossein Bahari, Saeed Saadatnejad, Amirhossein Asgari Farsangi, Seyed-Mohsen Moosavi-Dezfooli, Alexandre Alahi. (2024)<br><strong>Certified Human Trajectory Prediction</strong><br><button class=copy-to-clipboard title="Certified Human Trajectory Prediction" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13778v1.pdf filename=2403.13778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trajectory prediction plays an essential role in autonomous vehicles. While numerous strategies have been developed to enhance the robustness of trajectory prediction models, these methods are predominantly heuristic and do not offer guaranteed robustness against <b>adversarial</b> <b>attacks</b> and noisy observations. In this work, we propose a certification approach tailored for the task of trajectory prediction. To this end, we address the inherent challenges associated with trajectory prediction, including unbounded outputs, and mutli-modality, resulting in a model that provides guaranteed robustness. Furthermore, we integrate a denoiser into our method to further improve the performance. Through comprehensive evaluations, we demonstrate the effectiveness of the proposed technique across various baselines and using standard trajectory prediction datasets. The code will be made available online: <a href=https://s-attack.github.io/>https://s-attack.github.io/</a></p></p class="citation"></blockquote><h3 id=6989--75286-promamba-prompt-mamba-for-polyp-segmentation-jianhao-xie-et-al-2024>(69/89 | 75/286) ProMamba: Prompt-Mamba for polyp segmentation (Jianhao Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhao Xie, Ruofan Liao, Ziang Zhang, Sida Yi, Yuesheng Zhu, Guibo Luo. (2024)<br><strong>ProMamba: Prompt-Mamba for polyp segmentation</strong><br><button class=copy-to-clipboard title="ProMamba: Prompt-Mamba for polyp segmentation" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13660v1.pdf filename=2403.13660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting polyps through colonoscopy is an important task in medical image segmentation, which provides significant assistance and reference value for clinical surgery. However, accurate segmentation of polyps is a challenging task due to two main reasons. Firstly, polyps exhibit various shapes and colors. Secondly, the boundaries between polyps and their normal surroundings are often unclear. Additionally, significant differences between different datasets lead to limited generalization capabilities of existing methods. To address these issues, we propose a segmentation model based on <b>Prompt-Mamba,</b> which incorporates the latest Vision-Mamba and <b>prompt</b> technologies. Compared to previous models trained on the same dataset, our model not only maintains high segmentation accuracy on the validation part of the same dataset but also demonstrates superior accuracy on unseen datasets, exhibiting excellent generalization capabilities. Notably, we are the first to apply the Vision-Mamba architecture to polyp segmentation and the first to utilize <b>prompt</b> technology in a polyp segmentation model. Our model efficiently accomplishes segmentation tasks, surpassing previous state-of-the-art methods by an average of 5% across six datasets. Furthermore, we have developed multiple versions of our model with scaled parameter counts, achieving better performance than previous models even with fewer parameters. Our code and trained weights will be released soon.</p></p class="citation"></blockquote><h3 id=7089--76286-learning-user-embeddings-from-human-gaze-for-personalised-saliency-prediction-florian-strohm-et-al-2024>(70/89 | 76/286) Learning User Embeddings from Human Gaze for Personalised Saliency Prediction (Florian Strohm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Strohm, Mihai Bâce, Andreas Bulling. (2024)<br><strong>Learning User Embeddings from Human Gaze for Personalised Saliency Prediction</strong><br><button class=copy-to-clipboard title="Learning User Embeddings from Human Gaze for Personalised Saliency Prediction" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-HC, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13653v1.pdf filename=2403.13653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reusable embeddings of user behaviour have shown significant performance improvements for the personalised saliency prediction task. However, prior works require explicit user characteristics and preferences as input, which are often difficult to obtain. We present a novel method to extract user embeddings from pairs of natural images and corresponding saliency maps generated from a small amount of user-specific eye tracking data. At the core of our method is a Siamese <b>convolutional</b> neural encoder that learns the user embeddings by contrasting the image and personal saliency map pairs of different users. Evaluations on two public saliency datasets show that the generated embeddings have high discriminative power, are effective at refining universal saliency maps to the individual users, and generalise well across users and images. Finally, based on our model&rsquo;s ability to encode individual user characteristics, our work points towards other applications that can benefit from reusable embeddings of gaze behaviour.</p></p class="citation"></blockquote><h3 id=7189--77286-leveraging-feature-communication-in-federated-learning-for-remote-sensing-image-classification-anh-kiet-duong-et-al-2024>(71/89 | 77/286) Leveraging feature communication in federated learning for remote sensing image classification (Anh-Kiet Duong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anh-Kiet Duong, Hoàng-Ân Lê, Minh-Tan Pham. (2024)<br><strong>Leveraging feature communication in federated learning for remote sensing image classification</strong><br><button class=copy-to-clipboard title="Leveraging feature communication in federated learning for remote sensing image classification" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13575v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13575v1.pdf filename=2403.13575v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of <b>Federated</b> <b>Learning</b> (FL) applied to remote sensing image classification, this study introduces and assesses several innovative communication strategies. Our exploration includes feature-centric communication, pseudo-weight amalgamation, and a combined method utilizing both weights and features. Experiments conducted on two public scene classification datasets unveil the effectiveness of these strategies, showcasing accelerated convergence, heightened privacy, and reduced network information exchange. This research provides valuable insights into the implications of feature-centric communication in FL, offering potential applications tailored for remote sensing scenarios.</p></p class="citation"></blockquote><h3 id=7289--78286-diversity-aware-channel-pruning-for-stylegan-compression-jiwoo-chung-et-al-2024>(72/89 | 78/286) Diversity-aware Channel Pruning for StyleGAN Compression (Jiwoo Chung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiwoo Chung, Sangeek Hyun, Sang-Heon Shim, Jae-Pil Heo. (2024)<br><strong>Diversity-aware Channel Pruning for StyleGAN Compression</strong><br><button class=copy-to-clipboard title="Diversity-aware Channel Pruning for StyleGAN Compression" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13548v1.pdf filename=2403.13548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>StyleGAN has shown remarkable performance in unconditional image generation. However, its high computational cost poses a significant challenge for practical applications. Although recent efforts have been made to compress StyleGAN while preserving its performance, existing compressed models still lag behind the original model, particularly in terms of sample diversity. To overcome this, we propose a novel channel <b>pruning</b> method that leverages varying sensitivities of channels to latent vectors, which is a key factor in sample diversity. Specifically, by assessing channel importance based on their sensitivities to latent vector perturbations, our method enhances the diversity of samples in the compressed model. Since our method solely focuses on the channel <b>pruning</b> stage, it has complementary benefits with prior training schemes without additional training cost. Extensive experiments demonstrate that our method significantly enhances sample diversity across various datasets. Moreover, in terms of FID scores, our method not only surpasses state-of-the-art by a large margin but also achieves comparable scores with only half training iterations.</p></p class="citation"></blockquote><h3 id=7389--79286-scaling-diffusion-models-to-real-world-3d-lidar-scene-completion-lucas-nunes-et-al-2024>(73/89 | 79/286) Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion (Lucas Nunes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas Nunes, Rodrigo Marcuzzi, Benedikt Mersch, Jens Behley, Cyrill Stachniss. (2024)<br><strong>Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion</strong><br><button class=copy-to-clipboard title="Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13470v1.pdf filename=2403.13470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computer vision techniques play a central role in the perception stack of autonomous vehicles. Such methods are employed to perceive the vehicle surroundings given sensor data. 3D LiDAR sensors are commonly used to collect sparse 3D point clouds from the scene. However, compared to human perception, such systems struggle to deduce the unseen parts of the scene given those sparse point clouds. In this matter, the scene completion task aims at predicting the gaps in the LiDAR measurements to achieve a more complete scene representation. Given the promising results of recent <b>diffusion</b> <b>models</b> as generative models for images, we propose extending them to achieve scene completion from a single 3D LiDAR scan. Previous works used <b>diffusion</b> <b>models</b> over range images extracted from LiDAR data, directly applying image-based <b>diffusion</b> <b>methods.</b> Distinctly, we propose to directly operate on the points, reformulating the noising and denoising <b>diffusion</b> <b>process</b> such that it can efficiently work at scene scale. Together with our approach, we propose a regularization loss to stabilize the noise predicted during the denoising process. Our experimental evaluation shows that our method can complete the scene given a single LiDAR scan as input, producing a scene with more details compared to state-of-the-art scene completion methods. We believe that our proposed <b>diffusion</b> <b>process</b> formulation can support further research in <b>diffusion</b> <b>models</b> applied to scene-scale point cloud data.</p></p class="citation"></blockquote><h3 id=7489--80286-diversified-and-personalized-multi-rater-medical-image-segmentation-yicheng-wu-et-al-2024>(74/89 | 80/286) Diversified and Personalized Multi-rater Medical Image Segmentation (Yicheng Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yicheng Wu, Xiangde Luo, Zhe Xu, Xiaoqing Guo, Lie Ju, Zongyuan Ge, Wenjun Liao, Jianfei Cai. (2024)<br><strong>Diversified and Personalized Multi-rater Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Diversified and Personalized Multi-rater Medical Image Segmentation" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13417v1.pdf filename=2403.13417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Annotation ambiguity due to inherent data uncertainties such as blurred boundaries in medical scans and different observer expertise and preferences has become a major obstacle for training deep-learning based medical image segmentation models. To address it, the common practice is to gather multiple annotations from different experts, leading to the setting of multi-rater medical image segmentation. Existing works aim to either merge different annotations into the &ldquo;groundtruth&rdquo; that is often unattainable in numerous medical contexts, or generate diverse results, or produce personalized results corresponding to individual expert raters. Here, we bring up a more ambitious goal for multi-rater medical image segmentation, i.e., obtaining both diversified and personalized results. Specifically, we propose a two-stage framework named D-Persona (first Diversification and then Personalization). In Stage I, we exploit multiple given annotations to train a Probabilistic U-Net model, with a bound-constrained loss to improve the prediction diversity. In this way, a common latent space is constructed in Stage I, where different latent codes denote diversified expert opinions. Then, in Stage II, we design multiple attention-based projection heads to adaptively query the corresponding expert <b>prompts</b> from the shared latent space, and then perform the personalized medical image segmentation. We evaluated the proposed model on our in-house Nasopharyngeal Carcinoma dataset and the public lung nodule dataset (i.e., LIDC-IDRI). Extensive experiments demonstrated our D-Persona can provide diversified and personalized results at the same time, achieving new SOTA performance for multi-rater medical image segmentation. Our code will be released at <a href=https://github.com/ycwu1997/D-Persona>https://github.com/ycwu1997/D-Persona</a>.</p></p class="citation"></blockquote><h3 id=7589--81286-cell-tracking-in-c-elegans-with-cell-position-heatmap-based-alignment-and-pairwise-detection-kaito-shiku-et-al-2024>(75/89 | 81/286) Cell Tracking in C. elegans with Cell Position Heatmap-Based Alignment and Pairwise Detection (Kaito Shiku et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaito Shiku, Hiromitsu Shirai, Takeshi Ishihara, Ryoma Bise. (2024)<br><strong>Cell Tracking in C. elegans with Cell Position Heatmap-Based Alignment and Pairwise Detection</strong><br><button class=copy-to-clipboard title="Cell Tracking in C. elegans with Cell Position Heatmap-Based Alignment and Pairwise Detection" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13412v1.pdf filename=2403.13412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D cell tracking in a living organism has a crucial role in live cell image analysis. Cell tracking in C. elegans has two difficulties. First, cell migration in a consecutive frame is large since they move their head during scanning. Second, cell detection is often inconsistent in consecutive frames due to touching cells and low-contrast images, and these inconsistent detections affect the tracking performance worse. In this paper, we propose a cell tracking method to address these issues, which has two main contributions. First, we introduce cell position heatmap-based non-rigid alignment with test-time <b>fine-tuning,</b> which can warp the detected points to near the positions at the next frame. Second, we propose a pairwise detection method, which uses the information of detection results at the previous frame for detecting cells at the current frame. The experimental results demonstrate the effectiveness of each module, and the proposed method achieved the best performance in comparison.</p></p class="citation"></blockquote><h3 id=7689--82286-s2dm-sector-shaped-diffusion-models-for-video-generation-haoran-lang-et-al-2024>(76/89 | 82/286) S2DM: Sector-Shaped Diffusion Models for Video Generation (Haoran Lang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoran Lang, Yuxuan Ge, Zheng Tian. (2024)<br><strong>S2DM: Sector-Shaped Diffusion Models for Video Generation</strong><br><button class=copy-to-clipboard title="S2DM: Sector-Shaped Diffusion Models for Video Generation" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13408v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13408v2.pdf filename=2403.13408v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have achieved great success in image generation. However, when leveraging this idea for video generation, we face significant challenges in maintaining the consistency and continuity across video frames. This is mainly caused by the lack of an effective framework to align frames of videos with desired temporal features while preserving consistent semantic and stochastic features. In this work, we propose a novel Sector-Shaped <b>Diffusion</b> <b>Model</b> (S2DM) whose sector-shaped <b>diffusion</b> <b>region</b> is formed by a set of ray-shaped reverse <b>diffusion</b> <b>processes</b> starting at the same noise point. S2DM can generate a group of intrinsically related data sharing the same semantic and stochastic features while varying on temporal features with appropriate guided conditions. We apply S2DM to video generation tasks, and explore the use of optical flow as temporal conditions. Our experimental results show that S2DM outperforms many existing methods in the task of video generation without any temporal-feature modelling modules. For text-to-video generation tasks where temporal conditions are not explicitly given, we propose a two-stage generation strategy which can decouple the generation of temporal features from semantic-content features. We show that, without additional training, our model integrated with another temporal conditions generative model can still achieve comparable performance with existing works. Our results can be viewd at <a href=https://s2dm.github.io/S2DM/>https://s2dm.github.io/S2DM/</a>.</p></p class="citation"></blockquote><h3 id=7789--83286-counting-network-for-learning-from-majority-label-kaito-shiku-et-al-2024>(77/89 | 83/286) Counting Network for Learning from Majority Label (Kaito Shiku et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaito Shiku, Shinnosuke Matsuo, Daiki Suehiro, Ryoma Bise. (2024)<br><strong>Counting Network for Learning from Majority Label</strong><br><button class=copy-to-clipboard title="Counting Network for Learning from Majority Label" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Multiple Instance Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13370v1.pdf filename=2403.13370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper proposes a novel problem in multi-class <b>Multiple-Instance</b> <b>Learning</b> <b>(MIL)</b> called Learning from the Majority Label (LML). In LML, the majority class of instances in a bag is assigned as the bag&rsquo;s label. LML aims to classify instances using bag-level majority classes. This problem is valuable in various applications. Existing MIL methods are unsuitable for LML due to aggregating confidences, which may lead to inconsistency between the bag-level label and the label obtained by counting the number of instances for each class. This may lead to incorrect instance-level classification. We propose a counting network trained to produce the bag-level majority labels estimated by counting the number of instances for each class. This led to the consistency of the majority class between the network outputs and one obtained by counting the number of instances. Experimental results show that our counting network outperforms conventional MIL methods on four datasets The code is publicly available at <a href=https://github.com/Shiku-Kaito/Counting-Network-for-Learning-from-Majority-Label>https://github.com/Shiku-Kaito/Counting-Network-for-Learning-from-Majority-Label</a>.</p></p class="citation"></blockquote><h3 id=7889--84286-vid-tldr-training-free-token-merging-for-light-weight-video-transformer-joonmyung-choi-et-al-2024>(78/89 | 84/286) vid-TLDR: Training Free Token merging for Light-weight Video Transformer (Joonmyung Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joonmyung Choi, Sanghyeok Lee, Jaewon Chu, Minhyuk Choi, Hyunwoo J. Kim. (2024)<br><strong>vid-TLDR: Training Free Token merging for Light-weight Video Transformer</strong><br><button class=copy-to-clipboard title="vid-TLDR: Training Free Token merging for Light-weight Video Transformer" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13347v1.pdf filename=2403.13347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video <b>Transformers</b> have become the prevalent solution for various video downstream tasks with superior expressive power and flexibility. However, these video <b>transformers</b> suffer from heavy computational costs induced by the massive number of tokens across the entire video frames, which has been the major barrier to training the model. Further, the patches irrelevant to the main contents, e.g., backgrounds, degrade the generalization performance of models. To tackle these issues, we propose training free token merging for lightweight video <b>Transformer</b> (vid-TLDR) that aims to enhance the efficiency of video <b>Transformers</b> by merging the background tokens without additional training. For vid-TLDR, we introduce a novel approach to capture the salient regions in videos only with the attention map. Further, we introduce the saliency-aware token merging strategy by dropping the background tokens and sharpening the object scores. Our experiments show that vid-TLDR significantly mitigates the computational complexity of video <b>Transformers</b> while achieving competitive performance compared to the base model without vid-TLDR. Code is available at <a href=https://github.com/mlvlab/vid-TLDR>https://github.com/mlvlab/vid-TLDR</a>.</p></p class="citation"></blockquote><h3 id=7989--85286-amp-autoregressive-motion-prediction-revisited-with-next-token-prediction-for-autonomous-driving-xiaosong-jia-et-al-2024>(79/89 | 85/286) AMP: Autoregressive Motion Prediction Revisited with Next Token Prediction for Autonomous Driving (Xiaosong Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaosong Jia, Shaoshuai Shi, Zijun Chen, Li Jiang, Wenlong Liao, Tao He, Junchi Yan. (2024)<br><strong>AMP: Autoregressive Motion Prediction Revisited with Next Token Prediction for Autonomous Driving</strong><br><button class=copy-to-clipboard title="AMP: Autoregressive Motion Prediction Revisited with Next Token Prediction for Autonomous Driving" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13331v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13331v2.pdf filename=2403.13331v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As an essential task in autonomous driving (AD), motion prediction aims to predict the future states of surround objects for navigation. One natural solution is to estimate the position of other agents in a step-by-step manner where each predicted time-step is conditioned on both observed time-steps and previously predicted time-steps, i.e., autoregressive prediction. Pioneering works like SocialLSTM and MFP design their decoders based on this intuition. However, almost all state-of-the-art works assume that all predicted time-steps are independent conditioned on observed time-steps, where they use a single linear layer to generate positions of all time-steps simultaneously. They dominate most motion prediction leaderboards due to the simplicity of training MLPs compared to autoregressive networks. In this paper, we introduce the <b>GPT</b> style next token prediction into motion forecasting. In this way, the input and output could be represented in a unified space and thus the autoregressive prediction becomes more feasible. However, different from language data which is composed of homogeneous units -words, the elements in the driving scene could have complex spatial-temporal and semantic relations. To this end, we propose to adopt three factorized attention modules with different neighbors for information aggregation and different position encoding styles to capture their relations, e.g., encoding the transformation between coordinate systems for spatial relativity while adopting RoPE for temporal relativity. Empirically, by equipping with the aforementioned tailored designs, the proposed method achieves state-of-the-art performance in the Waymo Open Motion and Waymo Interaction datasets. Notably, AMP outperforms other recent autoregressive motion prediction methods: MotionLM and StateTransformer, which demonstrates the effectiveness of the proposed designs.</p></p class="citation"></blockquote><h3 id=8089--86286-laserhuman-language-guided-scene-aware-human-motion-generation-in-free-environment-peishan-cong-et-al-2024>(80/89 | 86/286) LaserHuman: Language-guided Scene-aware Human Motion Generation in Free Environment (Peishan Cong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peishan Cong, Ziyi Wang, Zhiyang Dou, Yiming Ren, Wei Yin, Kai Cheng, Yujing Sun, Xiaoxiao Long, Xinge Zhu, Yuexin Ma. (2024)<br><strong>LaserHuman: Language-guided Scene-aware Human Motion Generation in Free Environment</strong><br><button class=copy-to-clipboard title="LaserHuman: Language-guided Scene-aware Human Motion Generation in Free Environment" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13307v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13307v2.pdf filename=2403.13307v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language-guided scene-aware human motion generation has great significance for entertainment and robotics. In response to the limitations of existing datasets, we introduce LaserHuman, a pioneering dataset engineered to revolutionize Scene-Text-to-Motion research. LaserHuman stands out with its inclusion of genuine human motions within 3D environments, unbounded free-form natural language descriptions, a blend of indoor and outdoor scenarios, and dynamic, ever-changing scenes. Diverse modalities of capture data and rich annotations present great opportunities for the research of conditional motion generation, and can also facilitate the development of real-life applications. Moreover, to generate semantically consistent and physically plausible human motions, we propose a multi-conditional <b>diffusion</b> <b>model,</b> which is simple but effective, achieving state-of-the-art performance on existing datasets.</p></p class="citation"></blockquote><h3 id=8189--87286-building-optimal-neural-architectures-using-interpretable-knowledge-keith-g-mills-et-al-2024>(81/89 | 87/286) Building Optimal Neural Architectures using Interpretable Knowledge (Keith G. Mills et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keith G. Mills, Fred X. Han, Mohammad Salameh, Shengyao Lu, Chunhua Zhou, Jiao He, Fengyu Sun, Di Niu. (2024)<br><strong>Building Optimal Neural Architectures using Interpretable Knowledge</strong><br><button class=copy-to-clipboard title="Building Optimal Neural Architectures using Interpretable Knowledge" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13293v1.pdf filename=2403.13293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Architecture Search is a costly practice. The fact that a search space can span a vast number of design choices with each architecture evaluation taking nontrivial overhead makes it hard for an algorithm to sufficiently explore candidate networks. In this paper, we propose AutoBuild, a scheme which learns to align the latent embeddings of operations and architecture modules with the ground-truth performance of the architectures they appear in. By doing so, AutoBuild is capable of assigning interpretable importance scores to architecture modules, such as individual operation features and larger macro operation sequences such that high-performance neural networks can be constructed without any need for search. Through experiments performed on state-of-the-art image classification, segmentation, and Stable <b>Diffusion</b> <b>models,</b> we show that by mining a relatively small set of evaluated architectures, AutoBuild can learn to build high-quality architectures directly or help to reduce search space to focus on relevant areas, finding better architectures that outperform both the original labeled ones and ones found by search baselines. Code available at <a href=https://github.com/Ascend-Research/AutoBuild>https://github.com/Ascend-Research/AutoBuild</a></p></p class="citation"></blockquote><h3 id=8289--88286-mora-enabling-generalist-video-generation-via-a-multi-agent-framework-zhengqing-yuan-et-al-2024>(82/89 | 88/286) Mora: Enabling Generalist Video Generation via A Multi-Agent Framework (Zhengqing Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengqing Yuan, Ruoxi Chen, Zhaoxu Li, Haolong Jia, Lifang He, Chi Wang, Lichao Sun. (2024)<br><strong>Mora: Enabling Generalist Video Generation via A Multi-Agent Framework</strong><br><button class=copy-to-clipboard title="Mora: Enabling Generalist Video Generation via A Multi-Agent Framework" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Sora<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13248v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13248v2.pdf filename=2403.13248v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Sora</b> is the first large-scale generalist video generation model that garnered significant attention across society. Since its launch by OpenAI in February 2024, no other video generation models have paralleled {Sora}&rsquo;s performance or its capacity to support a broad spectrum of video generation tasks. Additionally, there are only a few fully published video generation models, with the majority being closed-source. To address this gap, this paper proposes a new multi-agent framework Mora, which incorporates several advanced visual AI agents to replicate generalist video generation demonstrated by <b>Sora.</b> In particular, Mora can utilize multiple visual agents and successfully mimic <b>Sora&rsquo;s</b> video generation capabilities in various tasks, such as (1) text-to-video generation, (2) text-conditional image-to-video generation, (3) extend generated videos, (4) video-to-video editing, (5) connect videos and (6) simulate digital worlds. Our extensive experimental results show that Mora achieves performance that is proximate to that of <b>Sora</b> in various tasks. However, there exists an obvious performance gap between our work and <b>Sora</b> when assessed holistically. In summary, we hope this project can guide the future trajectory of video generation through collaborative AI agents.</p></p class="citation"></blockquote><h3 id=8389--89286-correlation-clustering-of-organoid-images-jannik-presberger-et-al-2024>(83/89 | 89/286) Correlation Clustering of Organoid Images (Jannik Presberger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jannik Presberger, Rashmiparvathi Keshara, David Stein, Yung Hae Kim, Anne Grapin-Botton, Bjoern Andres. (2024)<br><strong>Correlation Clustering of Organoid Images</strong><br><button class=copy-to-clipboard title="Correlation Clustering of Organoid Images" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 8<br>Keywords: Clustering, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13376v1.pdf filename=2403.13376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In biological and medical research, scientists now routinely acquire microscopy images of hundreds of morphologically heterogeneous organoids and are then faced with the task of finding patterns in the image collection, i.e., subsets of organoids that appear similar and potentially represent the same morphological class. We adopt models and algorithms for correlating organoid images, i.e., for quantifying the similarity in appearance and <b>geometry</b> of the organoids they depict, and for <b>clustering</b> organoid images by consolidating conflicting correlations. For correlating organoid images, we adopt and compare two alternatives, a partial quadratic assignment problem and a twin network. For <b>clustering</b> organoid images, we employ the correlation <b>clustering</b> problem. Empirically, we learn the parameters of these models, infer a <b>clustering</b> of organoid images, and quantify the accuracy of the inferred clusters, with respect to a training set and a test set we contribute of state-of-the-art light microscopy images of organoids clustered manually by biologists.</p></p class="citation"></blockquote><h3 id=8489--90286-describe-and-dissect-interpreting-neurons-in-vision-networks-with-language-models-nicholas-bai-et-al-2024>(84/89 | 90/286) Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models (Nicholas Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicholas Bai, Rahul A. Iyer, Tuomas Oikarinen, Tsui-Wei Weng. (2024)<br><strong>Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models</strong><br><button class=copy-to-clipboard title="Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13771v1.pdf filename=2403.13771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose Describe-and-Dissect (DnD), a novel method to describe the roles of hidden neurons in vision networks. DnD utilizes recent advancements in <b>multimodal</b> deep learning to produce complex natural language descriptions, without the need for labeled training data or a predefined set of concepts to choose from. Additionally, DnD is training-free, meaning we don&rsquo;t train any new models and can easily leverage more capable general purpose models in the future. We have conducted extensive qualitative and quantitative analysis to show that DnD outperforms prior work by providing higher quality neuron descriptions. Specifically, our method on average provides the highest quality labels and is more than 2 times as likely to be selected as the best explanation for a neuron than the best baseline.</p></p class="citation"></blockquote><h3 id=8589--91286-unifying-local-and-global-multimodal-features-for-place-recognition-in-aliased-and-low-texture-environments-alberto-garcía-hernández-et-al-2024>(85/89 | 91/286) Unifying Local and Global Multimodal Features for Place Recognition in Aliased and Low-Texture Environments (Alberto García-Hernández et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alberto García-Hernández, Riccardo Giubilato, Klaus H. Strobl, Javier Civera, Rudolph Triebel. (2024)<br><strong>Unifying Local and Global Multimodal Features for Place Recognition in Aliased and Low-Texture Environments</strong><br><button class=copy-to-clipboard title="Unifying Local and Global Multimodal Features for Place Recognition in Aliased and Low-Texture Environments" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13395v1.pdf filename=2403.13395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Perceptual aliasing and weak textures pose significant challenges to the task of place recognition, hindering the performance of Simultaneous Localization and Mapping (SLAM) systems. This paper presents a novel model, called UMF (standing for Unifying Local and Global <b>Multimodal</b> Features) that 1) leverages multi-modality by cross-attention blocks between vision and LiDAR features, and 2) includes a re-ranking stage that re-orders based on local feature matching the top-k candidates retrieved using a global representation. Our experiments, particularly on sequences captured on a planetary-analogous environment, show that UMF outperforms significantly previous baselines in those challenging aliased environments. Since our work aims to enhance the reliability of SLAM in all situations, we also explore its performance on the widely used RobotCar dataset, for broader applicability. Code and models are available at <a href=https://github.com/DLR-RM/UMF>https://github.com/DLR-RM/UMF</a></p></p class="citation"></blockquote><h3 id=8689--92286-hyperfusion-a-hypernetwork-approach-to-multimodal-integration-of-tabular-and-medical-imaging-data-for-predictive-modeling-daniel-duenias-et-al-2024>(86/89 | 92/286) HyperFusion: A Hypernetwork Approach to Multimodal Integration of Tabular and Medical Imaging Data for Predictive Modeling (Daniel Duenias et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Duenias, Brennan Nichyporuk, Tal Arbel, Tammy Riklin Raviv. (2024)<br><strong>HyperFusion: A Hypernetwork Approach to Multimodal Integration of Tabular and Medical Imaging Data for Predictive Modeling</strong><br><button class=copy-to-clipboard title="HyperFusion: A Hypernetwork Approach to Multimodal Integration of Tabular and Medical Imaging Data for Predictive Modeling" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13319v1.pdf filename=2403.13319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of diverse clinical modalities such as medical imaging and the tabular data obtained by the patients&rsquo; Electronic Health Records (EHRs) is a crucial aspect of modern healthcare. The integrative analysis of multiple sources can provide a comprehensive understanding of a patient&rsquo;s condition and can enhance diagnoses and treatment decisions. Deep Neural Networks (DNNs) consistently showcase outstanding performance in a wide range of <b>multimodal</b> tasks in the medical domain. However, the complex endeavor of effectively merging medical imaging with clinical, demographic and genetic information represented as numerical tabular data remains a highly active and ongoing research pursuit. We present a novel framework based on hypernetworks to fuse clinical imaging and tabular data by conditioning the image processing on the EHR&rsquo;s values and measurements. This approach aims to leverage the complementary information present in these modalities to enhance the accuracy of various medical applications. We demonstrate the strength and the generality of our method on two different brain Magnetic Resonance Imaging (MRI) analysis tasks, namely, brain age prediction conditioned by subject&rsquo;s sex, and multiclass Alzheimer&rsquo;s Disease (AD) classification conditioned by tabular data. We show that our framework outperforms both single-modality models and state-of-the-art MRI-tabular data fusion methods. The code, enclosed to this manuscript will be made publicly available.</p></p class="citation"></blockquote><h3 id=8789--93286-congeo-robust-cross-view-geo-localization-across-ground-view-variations-li-mi-et-al-2024>(87/89 | 93/286) ConGeo: Robust Cross-view Geo-localization across Ground View Variations (Li Mi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Mi, Chang Xu, Javiera Castillo-Navarro, Syrielle Montariol, Wen Yang, Antoine Bosselut, Devis Tuia. (2024)<br><strong>ConGeo: Robust Cross-view Geo-localization across Ground View Variations</strong><br><button class=copy-to-clipboard title="ConGeo: Robust Cross-view Geo-localization across Ground View Variations" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13965v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13965v1.pdf filename=2403.13965v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-view geo-localization aims at localizing a ground-level query image by matching it to its corresponding geo-referenced aerial view. In real-world scenarios, the task requires accommodating diverse ground images captured by users with varying orientations and reduced field of views (FoVs). However, existing learning pipelines are orientation-specific or FoV-specific, demanding separate model training for different ground view variations. Such models heavily depend on the North-aligned spatial correspondence and predefined FoVs in the training data, compromising their robustness across different settings. To tackle this challenge, we propose ConGeo, a single- and cross-modal Contrastive method for Geo-localization: it enhances robustness and consistency in feature representations to improve a model&rsquo;s invariance to orientation and its resilience to FoV variations, by enforcing proximity between ground view variations of the same location. As a generic learning objective for cross-view geo-localization, when integrated into state-of-the-art pipelines, ConGeo significantly boosts the performance of three base models on four geo-localization <b>benchmarks</b> for diverse ground view variations and outperforms competing methods that train separate models for each ground view variation.</p></p class="citation"></blockquote><h3 id=8889--94286-fast-poly-a-fast-polyhedral-framework-for-3d-multi-object-tracking-xiaoyu-li-et-al-2024>(88/89 | 94/286) Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking (Xiaoyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyu Li, Dedong Liu, Lijun Zhao, Yitao Wu, Xian Wu, Jinghan Gao. (2024)<br><strong>Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking</strong><br><button class=copy-to-clipboard title="Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13443v1.pdf filename=2403.13443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D Multi-Object Tracking (MOT) captures stable and comprehensive motion states of surrounding obstacles, essential for robotic perception. However, current 3D trackers face issues with accuracy and latency consistency. In this paper, we propose Fast-Poly, a fast and effective filter-based method for 3D MOT. Building upon our previous work Poly-MOT, Fast-Poly addresses object rotational anisotropy in 3D space, enhances local computation densification, and leverages parallelization technique, improving inference speed and precision. Fast-Poly is extensively tested on two large-scale tracking <b>benchmarks</b> with Python implementation. On the nuScenes dataset, Fast-Poly achieves new state-of-the-art performance with 75.8% AMOTA among all methods and can run at 34.2 FPS on a personal CPU. On the Waymo dataset, Fast-Poly exhibits competitive accuracy with 63.6% MOTA and impressive inference speed (35.5 FPS). The source code is publicly available at <a href=https://github.com/lixiaoyu2000/FastPoly>https://github.com/lixiaoyu2000/FastPoly</a>.</p></p class="citation"></blockquote><h3 id=8989--95286-adaptive-critical-subgraph-mining-for-cognitive-impairment-conversion-prediction-with-t1-mri-based-brain-network-yilin-leng-et-al-2024>(89/89 | 95/286) Adaptive Critical Subgraph Mining for Cognitive Impairment Conversion Prediction with T1-MRI-based Brain Network (Yilin Leng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yilin Leng, Wenju Cui, Bai Chen, Xi Jiang, Shuangqing Chen, Jian Zheng. (2024)<br><strong>Adaptive Critical Subgraph Mining for Cognitive Impairment Conversion Prediction with T1-MRI-based Brain Network</strong><br><button class=copy-to-clipboard title="Adaptive Critical Subgraph Mining for Cognitive Impairment Conversion Prediction with T1-MRI-based Brain Network" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13338v1.pdf filename=2403.13338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prediction the conversion to early-stage dementia is critical for mitigating its progression but remains challenging due to subtle cognitive impairments and structural brain changes. Traditional T1-weighted magnetic resonance imaging (T1-MRI) research focus on identifying brain atrophy regions but often fails to address the intricate connectivity between them. This limitation underscores the necessity of focuing on inter-regional connectivity for a comprehensive understand of the brain&rsquo;s complex network. Moreover, there is a pressing demand for methods that adaptively preserve and extract critical information, particularly specialized subgraph mining techniques for brain networks. These are essential for developing high-quality feature representations that reveal critical spatial impacts of structural brain changes and its topology. In this paper, we propose Brain-SubGNN, a novel <b>graph</b> representation network to mine and enhance critical subgraphs based on T1-MRI. This network provides a subgraph-level interpretation, enhancing interpretability and insights for <b>graph</b> analysis. The process begins by extracting node features and a correlation matrix between nodes to construct a task-oriented brain network. Brain-SubGNN then adaptively identifies and enhances critical subgraphs, capturing both loop and neighbor subgraphs. This method reflects the loop topology and local changes, indicative of long-range connections, and maintains local and global brain attributes. Extensive experiments validate the effectiveness and advantages of Brain-SubGNN, demonstrating its potential as a powerful tool for understanding and diagnosing early-stage dementia. Source code is available at <a href=https://github.com/Leng-10/Brain-SubGNN>https://github.com/Leng-10/Brain-SubGNN</a>.</p></p class="citation"></blockquote><h2 id=cscl-33>cs.CL (33)</h2><h3 id=133--96286-paramanu-ayn-an-efficient-novel-generative-and-instruction-tuned-language-model-for-indian-legal-case-documents-mitodru-niyogi-et-al-2024>(1/33 | 96/286) PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents (Mitodru Niyogi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mitodru Niyogi, Arnab Bhattacharya. (2024)<br><strong>PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents</strong><br><button class=copy-to-clipboard title="PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 90<br>Keywords: GPT, GPT-3, GPT-3.5, Question Answering, Reasoning, Instruction Tuning, Perplexity, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13681v1.pdf filename=2403.13681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present PARAMANU-AYN, a language model based exclusively on case documents of the Supreme Court of India, the Constitution of India, and the Indian Penal Code. The novel Auto Regressive (AR) decoder based model is pretrained from scratch at a context size of 8192. We evaluated our pretrained legal model on <b>perplexity</b> metrics. We also <b>instruction-tuned</b> <b>our</b> pretrained model on a set of 10,763 <b>instructions</b> <b>covering</b> various legal tasks such as legal <b>reasoning,</b> judgement explanation, legal clause generation, legal drafting, legal contract drafting, case <b>summarization,</b> constitutional <b>question-answering,</b> <b>etc.</b> We also evaluated the responses of <b>prompts</b> for <b>instruction-tuned</b> <b>models</b> by <b>GPT-3.5-Turbo</b> on clarity, relevance, completeness, and legal <b>reasoning</b> metrics in a scale of 10. Our model can be run on CPU and achieved 42.46 tokens/sec CPU inference speed. We found that our models, despite not being pretrained on legal books, various legal contracts, and legal documents, were able to learn the domain knowledge required for drafting various legal contracts and legal clauses, and generalize to draft legal contracts and legal clauses with limited <b>instruction</b> <b>tuning.</b> Hence, we conclude that for a strong domain-specialized generative language model (such as legal), very large amounts of data are not required to develop models from scratch. We believe that this work is the first attempt to make a dedicated generative legal language model from scratch for Indian Supreme Court jurisdiction or in legal NLP overall. We plan to release our Paramanu-Ayn model at <a href=https://www.bharatgpts.com>https://www.bharatgpts.com</a>.</p></p class="citation"></blockquote><h3 id=233--97286-facilitating-pornographic-text-detection-for-open-domain-dialogue-systems-via-knowledge-distillation-of-large-language-models-huachuan-qiu-et-al-2024>(2/33 | 97/286) Facilitating Pornographic Text Detection for Open-Domain Dialogue Systems via Knowledge Distillation of Large Language Models (Huachuan Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huachuan Qiu, Shuai Zhang, Hongliang He, Anqi Li, Zhenzhong Lan. (2024)<br><strong>Facilitating Pornographic Text Detection for Open-Domain Dialogue Systems via Knowledge Distillation of Large Language Models</strong><br><button class=copy-to-clipboard title="Facilitating Pornographic Text Detection for Open-Domain Dialogue Systems via Knowledge Distillation of Large Language Models" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Knowledge Distillation, Knowledge Distillation, ChatGPT, GPT, GPT-4, Chatbot, Dialogue System, Open-Domain Dialogue, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13250v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13250v1.pdf filename=2403.13250v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pornographic content occurring in human-machine interaction <b>dialogues</b> <b>can</b> cause severe side effects for users in <b>open-domain</b> <b>dialogue</b> <b>systems.</b> However, research on detecting pornographic language within human-machine interaction <b>dialogues</b> <b>is</b> an important subject that is rarely studied. To advance in this direction, we introduce CensorChat, a <b>dialogue</b> <b>monitoring</b> dataset aimed at detecting whether the <b>dialogue</b> <b>session</b> contains pornographic content. To this end, we collect real-life human-machine interaction <b>dialogues</b> <b>in</b> the wild and break them down into single utterances and single-turn <b>dialogues,</b> <b>with</b> the last utterance spoken by the <b>chatbot.</b> We propose utilizing <b>knowledge</b> <b>distillation</b> of <b>large</b> <b>language</b> <b>models</b> to annotate the dataset. Specifically, first, the raw dataset is annotated by four open-source <b>large</b> <b>language</b> <b>models,</b> with the majority vote determining the label. Second, we use <b>ChatGPT</b> to update the empty label from the first step. Third, to ensure the quality of the validation and test sets, we utilize <b>GPT-4</b> for label calibration. If the current label does not match the one generated by <b>GPT-4,</b> we employ a self-criticism strategy to verify its correctness. Finally, to facilitate the detection of pornographic text, we develop a series of text classifiers using a pseudo-labeled dataset. Detailed data analysis demonstrates that leveraging <b>knowledge</b> <b>distillation</b> techniques with <b>large</b> <b>language</b> <b>models</b> provides a practical and cost-efficient method for developing pornographic text detectors.</p></p class="citation"></blockquote><h3 id=333--98286-information-theoretic-distillation-for-reference-less-summarization-jaehun-jung-et-al-2024>(3/33 | 98/286) Information-Theoretic Distillation for Reference-less Summarization (Jaehun Jung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaehun Jung, Ximing Lu, Liwei Jiang, Faeze Brahman, Peter West, Pang Wei Koh, Yejin Choi. (2024)<br><strong>Information-Theoretic Distillation for Reference-less Summarization</strong><br><button class=copy-to-clipboard title="Information-Theoretic Distillation for Reference-less Summarization" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Knowledge Distillation, Knowledge Distillation, Mutual Information, Supervised Learning, Unsupervised Learning, ChatGPT, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13780v1.pdf filename=2403.13780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The current winning recipe for automatic <b>summarization</b> is using proprietary large-scale language models <b>(LLMs)</b> such as <b>ChatGPT</b> as is, or imitation learning from them as teacher models. While increasingly ubiquitous dependence on such large-scale language models is convenient, there remains an important question of whether small-scale models could have achieved competitive results, if we were to seek an alternative learning method &ndash; that allows for a more cost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a novel framework to <b>distill</b> a powerful summarizer based on the information-theoretic objective for <b>summarization,</b> without relying on either the <b>LLM&rsquo;s</b> capability or human-written references. To achieve this, we first propose a novel formulation of the desiderata of <b>summarization</b> (saliency, faithfulness and brevity) through the lens of <b>mutual</b> <b>information</b> between the original document and the summary. Based on this formulation, we start off from Pythia-2.8B as the teacher model, which is not yet capable of <b>summarization,</b> then self-train the model to optimize for the information-centric measures of ideal summaries. <b>Distilling</b> from the improved teacher, we arrive at a compact but powerful summarizer with only 568M parameters that performs competitively against <b>ChatGPT,</b> without ever relying on <b>ChatGPT&rsquo;s</b> capabilities. Extensive analysis demonstrates that our approach outperforms in-domain <b>supervised</b> models in human evaluation, let alone state-of-the-art <b>unsupervised</b> methods, and wins over <b>ChatGPT</b> in controllable <b>summarization.</b></p></p class="citation"></blockquote><h3 id=433--99286-clinical-information-extraction-for-low-resource-languages-with-few-shot-learning-using-pre-trained-language-models-and-prompting-phillip-richter-pechanski-et-al-2024>(4/33 | 99/286) Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting (Phillip Richter-Pechanski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Phillip Richter-Pechanski, Philipp Wiesenbach, Dominic M. Schwab, Christina Kiriakou, Nicolas Geis, Christoph Dieterich, Anette Frank. (2024)<br><strong>Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting</strong><br><button class=copy-to-clipboard title="Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Few-shot Learning, Low-Resource, Information Retrieval, Domain Adaptation, Masked Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13369v1.pdf filename=2403.13369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic extraction of medical <b>information</b> <b>from</b> clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent advances in <b>domain-adaptation</b> <b>and</b> <b>prompting</b> methods showed promising results with minimal training data using lightweight <b>masked</b> <b>language</b> <b>models,</b> which are suited for well-established interpretability methods. We are first to present a systematic evaluation of these methods in a <b>low-resource</b> setting, by performing multi-class section classification on German doctor&rsquo;s letters. We conduct extensive class-wise evaluations supported by Shapley values, to validate the quality of our small training data set and to ensure the interpretability of model predictions. We demonstrate that a lightweight, <b>domain-adapted</b> <b>pretrained</b> <b>model,</b> <b>prompted</b> with just 20 shots, outperforms a traditional classification model by 30.5% accuracy. Our results serve as a process-oriented guideline for clinical <b>information</b> <b>extraction</b> projects working with <b>low-resource.</b></p></p class="citation"></blockquote><h3 id=533--100286-llama-meets-eu-investigating-the-european-political-spectrum-through-the-lens-of-llms-ilias-chalkidis-et-al-2024>(5/33 | 100/286) Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs (Ilias Chalkidis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ilias Chalkidis, Stephanie Brandl. (2024)<br><strong>Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs</strong><br><button class=copy-to-clipboard title="Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, LLaMA, Reasoning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13592v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13592v2.pdf filename=2403.13592v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Instruction-finetuned <b>Large</b> <b>Language</b> <b>Models</b> inherit clear political leanings that have been shown to influence downstream task performance. We expand this line of research beyond the two-party system in the US and audit <b>Llama</b> Chat in the context of EU politics in various settings to analyze the model&rsquo;s political knowledge and its ability to reason in context. We adapt, i.e., further <b>fine-tune,</b> <b>Llama</b> Chat on speeches of individual euro-parties from debates in the European Parliament to reevaluate its political leaning based on the EUandI questionnaire. <b>Llama</b> Chat shows considerable knowledge of national parties&rsquo; positions and is capable of <b>reasoning</b> in context. The adapted, party-specific, models are substantially re-aligned towards respective positions which we see as a starting point for using chat-based <b>LLMs</b> as data-driven conversational engines to assist research in political science.</p></p class="citation"></blockquote><h3 id=633--101286-sumtra-a-differentiable-pipeline-for-few-shot-cross-lingual-summarization-jacob-parnell-et-al-2024>(6/33 | 101/286) SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization (Jacob Parnell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacob Parnell, Inigo Jauregi Unanue, Massimo Piccardi. (2024)<br><strong>SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization</strong><br><button class=copy-to-clipboard title="SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, Fine-tuning, Fine-tuning, Zero-shot, Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13240v1.pdf filename=2403.13240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-lingual <b>summarization</b> (XLS) generates summaries in a language different from that of the input documents (e.g., English to Spanish), allowing speakers of the target language to gain a concise view of their content. In the present day, the predominant approach to this task is to take a performing, pretrained multilingual language model (LM) and <b>fine-tune</b> it for XLS on the language pairs of interest. However, the scarcity of <b>fine-tuning</b> samples makes this approach challenging in some cases. For this reason, in this paper we propose revisiting the <b>summarize-and-translate</b> pipeline, where the <b>summarization</b> and translation tasks are performed in a sequence. This approach allows reusing the many, publicly-available resources for monolingual <b>summarization</b> and translation, obtaining a very competitive <b>zero-shot</b> performance. In addition, the proposed pipeline is completely differentiable end-to-end, allowing it to take advantage of <b>few-shot</b> <b>fine-tuning,</b> where available. Experiments over two contemporary and widely adopted XLS datasets (CrossSum and WikiLingua) have shown the remarkable <b>zero-shot</b> performance of the proposed approach, and also its strong <b>few-shot</b> performance compared to an equivalent multilingual LM baseline, that the proposed approach has been able to outperform in many languages with only 10% of the <b>fine-tuning</b> samples.</p></p class="citation"></blockquote><h3 id=733--102286-teacher-student-training-for-debiasing-general-permutation-debiasing-for-large-language-models-adian-liusie-et-al-2024>(7/33 | 102/286) Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models (Adian Liusie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adian Liusie, Yassir Fathullah, Mark J. F. Gales. (2024)<br><strong>Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models</strong><br><button class=copy-to-clipboard title="Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 55<br>Keywords: Black Box, Knowledge Distillation, Knowledge Distillation, Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13590v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13590v1.pdf filename=2403.13590v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated impressive <b>zero-shot</b> capabilities and versatility in NLP tasks, however they sometimes fail to maintain crucial invariances for specific tasks. One example is permutation sensitivity, where <b>LLMs&rsquo;</b> outputs may significantly vary depending on the order of the input options. While debiasing techniques can mitigate these issues, and yield better performance and reliability, they often come with a high computational cost at inference. This paper addresses this inefficiency at inference time. The aim is to <b>distill</b> the capabilities of a computationally intensive, debiased, teacher model into a more compact student model. We explore two variants of student models: one based on pure <b>distillation,</b> and the other on an error-correction approach for more complex tasks, where the student corrects a single biased decision from the teacher to achieve a debiased output. Our approach is general and can be applied to both <b>black-box</b> <b>and</b> white-box <b>LLMs.</b> Furthermore, we demonstrate that our compact, encoder-only student models can outperform their larger, biased teacher counterparts, achieving better results with significantly fewer parameters.</p></p class="citation"></blockquote><h3 id=833--103286-on-prompt-sensitivity-of-chatgpt-in-affective-computing-mostafa-m-amin-et-al-2024>(8/33 | 103/286) On Prompt Sensitivity of ChatGPT in Affective Computing (Mostafa M. Amin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mostafa M. Amin, Björn W. Schuller. (2024)<br><strong>On Prompt Sensitivity of ChatGPT in Affective Computing</strong><br><button class=copy-to-clipboard title="On Prompt Sensitivity of ChatGPT in Affective Computing" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Foundation Model, ChatGPT, Sentiment Analysis, Text Generation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14006v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14006v1.pdf filename=2403.14006v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have demonstrated the emerging capabilities of <b>foundation</b> <b>models</b> like <b>ChatGPT</b> in several fields, including affective computing. However, accessing these emerging capabilities is facilitated through <b>prompt</b> engineering. Despite the existence of some <b>prompting</b> techniques, the field is still rapidly evolving and many <b>prompting</b> ideas still require investigation. In this work, we introduce a method to evaluate and investigate the sensitivity of the performance of <b>foundation</b> <b>models</b> based on different <b>prompts</b> or generation parameters. We perform our evaluation on <b>ChatGPT</b> within the scope of affective computing on three major problems, namely <b>sentiment</b> <b>analysis,</b> toxicity detection, and sarcasm detection. First, we carry out a sensitivity analysis on pivotal parameters in auto-regressive <b>text</b> <b>generation,</b> specifically the temperature parameter $T$ and the top-$p$ parameter in Nucleus sampling, dictating how conservative or creative the model should be during generation. Furthermore, we explore the efficacy of several <b>prompting</b> ideas, where we explore how giving different incentives or structures affect the performance. Our evaluation takes into consideration performance measures on the affective computing tasks, and the effectiveness of the model to follow the stated instructions, hence generating easy-to-parse responses to be smoothly used in downstream applications.</p></p class="citation"></blockquote><h3 id=933--104286-leveraging-linguistically-enhanced-embeddings-for-open-information-extraction-fauzan-farooqui-et-al-2024>(9/33 | 104/286) Leveraging Linguistically Enhanced Embeddings for Open Information Extraction (Fauzan Farooqui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fauzan Farooqui, Thanmay Jayakumar, Pulkit Mathur, Mansi Radke. (2024)<br><strong>Leveraging Linguistically Enhanced Embeddings for Open Information Extraction</strong><br><button class=copy-to-clipboard title="Leveraging Linguistically Enhanced Embeddings for Open Information Extraction" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Information Retrieval, Open Information Extraction, Pre-trained Language Model, Pre-trained Language Model, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13903v1.pdf filename=2403.13903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Open</b> <b>Information</b> <b>Extraction</b> (OIE) is a structured prediction (SP) task in Natural Language Processing (NLP) that aims to extract structured $n$-ary tuples - usually subject-relation-object triples - from free text. The <b>word</b> <b>embeddings</b> in the input text can be enhanced with linguistic features, usually Part-of-Speech (PoS) and Syntactic Dependency Parse (SynDP) labels. However, past enhancement techniques cannot leverage the power of <b>pretrained</b> <b>language</b> <b>models</b> <b>(PLMs),</b> which themselves have been hardly used for OIE. To bridge this gap, we are the first to leverage linguistic features with a Seq2Seq <b>PLM</b> for OIE. We do so by introducing two methods - Weighted Addition and Linearized Concatenation. Our work can give any neural OIE architecture the key performance boost from both <b>PLMs</b> and linguistic features in one go. In our settings, this shows wide improvements of up to 24.9%, 27.3% and 14.9% on Precision, Recall and F1 scores respectively over the baseline. Beyond this, we address other important challenges in the field: to reduce compute overheads with the features, we are the first ones to exploit Semantic Dependency Parse (SemDP) tags; to address flaws in current datasets, we create a clean synthetic dataset; finally, we contribute the first known study of OIE behaviour in SP models.</p></p class="citation"></blockquote><h3 id=1033--105286-train--constrain-phonologically-informed-tongue-twister-generation-from-topics-and-paraphrases-tyler-loakman-et-al-2024>(10/33 | 105/286) Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases (Tyler Loakman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tyler Loakman, Chen Tang, Chenghua Lin. (2024)<br><strong>Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases</strong><br><button class=copy-to-clipboard title="Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Language Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13901v1.pdf filename=2403.13901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous work in phonologically and phonetically grounded <b>language</b> <b>generation</b> has mainly focused on domains such as puns and poetry. In this article, we present new work on the generation of tongue-twisters - a form of <b>language</b> <b>that</b> is required to be conditioned on a phoneme level to maximize sound overlap, whilst maintaining semantic consistency with an input topic and still being grammatically correct. We present TwisterLister, a pipeline for generating phonologically informed tongue-twisters from <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> that we use to generate TwistList 2.0, the largest annotated dataset of tongue-twisters to date, consisting of 17K+ examples from a combination of human and <b>LLM</b> authors. Our generation pipeline involves the use of a phonologically constrained vocabulary alongside <b>LLM</b> <b>prompting</b> to generate novel, non-derivative tongue-twister examples. We additionally present the results of automatic and human evaluation of smaller models trained on our generated dataset to demonstrate the extent to which phonologically motivated <b>language</b> <b>types</b> can be generated without explicit injection of phonological knowledge. Additionally, we introduce a Phoneme-Aware Constrained Decoding module (PACD) that can be integrated into any causal <b>language</b> <b>model</b> and demonstrate that this method generates good quality tongue-twisters both with and without <b>fine-tuning</b> the underlying <b>language</b> <b>model.</b> We also design and implement a range of automatic metrics for the task of tongue-twister generation that is phonologically motivated and captures the unique essence of tongue-twisters based on Phonemic Edit Distance (PED).</p></p class="citation"></blockquote><h3 id=1133--106286-do-not-worry-if-you-do-not-have-data-building-pretrained-language-models-using-translationese-meet-doshi-et-al-2024>(11/33 | 106/286) Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese (Meet Doshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meet Doshi, Raj Dabre, Pushpak Bhattacharyya. (2024)<br><strong>Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese</strong><br><button class=copy-to-clipboard title="Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Natural Language Generation, Natural Language Understanding, Neural Machine Translation, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13638v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13638v2.pdf filename=2403.13638v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the utility of Translationese as synthetic data created using <b>machine</b> <b>translation</b> for pre-training language models (LMs). Pre-training requires vast amounts of monolingual data, which is mostly unavailable for languages other than English. Recently, there has been a growing interest in using synthetic data to address this data scarcity. We take the case of English and Indic languages and translate web-crawled monolingual documents (clean) into the target language. Then, we train language models containing 28M and 85M parameters on this translationese data (synthetic). We show that their performance on downstream <b>natural</b> <b>language</b> <b>understanding</b> and generative tasks is only 3.56% poorer on NLU tasks and 1.51% on <b>NLG</b> tasks than LMs <b>pre-trained</b> <b>on</b> <b>clean</b> data. Further, we propose the use of lightweight TinyLMs <b>pre-trained</b> <b>on</b> <b>clean</b> data to filter synthetic data efficiently which significantly improves the performance of our models. We also find that LMs trained on synthetic data strongly benefit from extended pretraining on a tiny fraction (10%) of clean data. We release the data we collected and created as a part of this work, IndicMonoDoc, the largest collection of monolingual document-level corpora, which we hope will help bridge the gap between English and non-English performance for <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=1233--107286-instruction-multi-constraint-molecular-generation-using-a-teacher-student-large-language-model-peng-zhou-et-al-2024>(12/33 | 107/286) Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model (Peng Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Zhou, Jianmin Wang, Chunyan Li, Zixu Wang, Yiping Liu, Siqi Sun, Jianxin Lin, Longyue Wang, Xiangxiang Zeng. (2024)<br><strong>Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model</strong><br><button class=copy-to-clipboard title="Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Knowledge Distillation, Zero-shot, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13244v1.pdf filename=2403.13244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While various models and computational tools have been proposed for structure and property analysis of molecules, generating molecules that conform to all desired structures and properties remains a challenge. Here, we introduce a multi-constraint molecular generation <b>large</b> <b>language</b> <b>model,</b> TSMMG, which, akin to a student, incorporates <b>knowledge</b> <b>from</b> various small models and tools, namely, the &rsquo;teachers&rsquo;. To train TSMMG, we construct a <b>large</b> <b>set</b> <b>of</b> text-molecule pairs by extracting molecular <b>knowledge</b> <b>from</b> these &rsquo;teachers&rsquo;, enabling it to generate novel molecules that conform to the descriptions through various text <b>prompts.</b> We experimentally show that TSMMG remarkably performs in generating molecules meeting complex, natural language-described property requirements across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% and success ratio of 88.08%, 65.27%, and 61.44%, respectively. The model also exhibits adaptability through <b>zero-shot</b> testing, creating molecules that satisfy combinations of properties that have not been encountered. It can comprehend text inputs with various language styles, extending beyond the confines of outlined <b>prompts,</b> as confirmed through empirical validation. Additionally, the <b>knowledge</b> <b>distillation</b> feature of TSMMG contributes to the continuous enhancement of small models, while the innovative approach to dataset construction effectively addresses the issues of data scarcity and quality, which positions TSMMG as a promising tool in the domains of drug discovery and materials science. Code is available at <a href=https://github.com/HHW-zhou/TSMMG>https://github.com/HHW-zhou/TSMMG</a>.</p></p class="citation"></blockquote><h3 id=1333--108286-ax-to-grind-urdu-benchmark-dataset-for-urdu-fake-news-detection-sheetal-harris-et-al-2024>(13/33 | 108/286) Ax-to-Grind Urdu: Benchmark Dataset for Urdu Fake News Detection (Sheetal Harris et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sheetal Harris, Jinshuo Liu, Hassan Jalil Hadi, Yue Cao. (2024)<br><strong>Ax-to-Grind Urdu: Benchmark Dataset for Urdu Fake News Detection</strong><br><button class=copy-to-clipboard title="Ax-to-Grind Urdu: Benchmark Dataset for Urdu Fake News Detection" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, RoBERTa, Fact Verification, Fake News Detection, Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14037v1.pdf filename=2403.14037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Misinformation can seriously impact society, affecting anything from public opinion to institutional confidence and the political horizon of a state. <b>Fake</b> <b>News</b> <b>(FN)</b> proliferation on online websites and Online Social Networks (OSNs) has increased profusely. Various <b>fact-checking</b> <b>websites</b> include news in English and barely provide information about FN in regional languages. Thus the Urdu FN purveyors cannot be discerned using factchecking portals. SOTA approaches for <b>Fake</b> <b>News</b> <b>Detection</b> (FND) count upon appropriately labelled and large datasets. FND in regional and resource-constrained languages lags due to the lack of limited-sized datasets and legitimate lexical resources. The previous datasets for Urdu FND are limited-sized, domain-restricted, publicly unavailable and not manually verified where the news is translated from English into Urdu. In this paper, we curate and contribute the first largest publicly available dataset for Urdu FND, Ax-to-Grind Urdu, to bridge the identified gaps and limitations of existing Urdu datasets in the literature. It constitutes 10,083 <b>fake</b> <b>and</b> <b>real</b> news on fifteen domains collected from leading and authentic Urdu newspapers and news channel websites in Pakistan and India. FN for the Ax-to-Grind dataset is collected from websites and crowdsourcing. The dataset contains news items in Urdu from the year 2017 to the year 2023. Expert journalists annotated the dataset. We <b>benchmark</b> the dataset with an ensemble model of mBERT,XLNet, and XLM <b>RoBERTa.</b> The selected models are originally trained on multilingual large corpora. The results of the proposed model are based on performance metrics, F1-score, accuracy, precision, recall and MCC value.</p></p class="citation"></blockquote><h3 id=1433--109286-ethiollm-multilingual-large-language-models-for-ethiopian-languages-with-task-evaluation-atnafu-lambebo-tonja-et-al-2024>(14/33 | 109/286) EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation (Atnafu Lambebo Tonja et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atnafu Lambebo Tonja, Israel Abebe Azime, Tadesse Destaw Belay, Mesay Gemeda Yigezu, Moges Ahmed Mehamed, Abinew Ali Ayele, Ebrahim Chekol Jibril, Michael Melese Woldeyohannis, Olga Kolesnikova, Philipp Slusallek, Dietrich Klakow, Shengwu Xiong, Seid Muhie Yimam. (2024)<br><strong>EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation</strong><br><button class=copy-to-clipboard title="EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Low-Resource, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13737v1.pdf filename=2403.13737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have gained popularity recently due to their outstanding performance in various downstream Natural Language Processing (NLP) tasks. However, <b>low-resource</b> languages are still lagging behind current state-of-the-art (SOTA) developments in the field of NLP due to insufficient resources to train <b>LLMs.</b> Ethiopian languages exhibit remarkable linguistic diversity, encompassing a wide array of scripts, and are imbued with profound religious and cultural significance. This paper introduces EthioLLM &ndash; multilingual <b>large</b> <b>language</b> <b>models</b> for five Ethiopian languages (Amharic, Ge&rsquo;ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark &ndash; a new <b>benchmark</b> dataset for various downstream NLP tasks. We evaluate the performance of these models across five downstream NLP tasks. We open-source our multilingual language models, new <b>benchmark</b> datasets for various downstream tasks, and task-specific <b>fine-tuned</b> language models and discuss the performance of the models. Our dataset and models are available at the <a href=https://huggingface.co/EthioNLP>https://huggingface.co/EthioNLP</a> repository.</p></p class="citation"></blockquote><h3 id=1533--110286-evaluating-unsupervised-dimensionality-reduction-methods-for-pretrained-sentence-embeddings-gaifan-zhang-et-al-2024>(15/33 | 110/286) Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained Sentence Embeddings (Gaifan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaifan Zhang, Yi Zhou, Danushka Bollegala. (2024)<br><strong>Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained Sentence Embeddings</strong><br><button class=copy-to-clipboard title="Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained Sentence Embeddings" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Unsupervised Learning, Sentence Embedding, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14001v1.pdf filename=2403.14001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Sentence</b> <b>embeddings</b> produced by <b>Pretrained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> have received wide attention from the NLP community due to their superior performance when representing texts in numerous downstream applications. However, the high dimensionality of the <b>sentence</b> <b>embeddings</b> produced by <b>PLMs</b> is problematic when representing large numbers of <b>sentences</b> <b>in</b> memory- or compute-constrained devices. As a solution, we evaluate <b>unsupervised</b> dimensionality reduction methods to reduce the dimensionality of <b>sentence</b> <b>embeddings</b> produced by <b>PLMs.</b> Our experimental results show that simple methods such as Principal Component Analysis (PCA) can reduce the dimensionality of <b>sentence</b> <b>embeddings</b> by almost $50%$, without incurring a significant loss in performance in multiple downstream tasks. Surprisingly, reducing the dimensionality further improves performance over the original high-dimensional versions for the <b>sentence</b> <b>embeddings</b> produced by some <b>PLMs</b> in some tasks.</p></p class="citation"></blockquote><h3 id=1633--111286-chain-of-interaction-enhancing-large-language-models-for-psychiatric-behavior-understanding-by-dyadic-contexts-guangzeng-han-et-al-2024>(16/33 | 111/286) Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts (Guangzeng Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangzeng Han, Weisi Liu, Xiaolei Huang, Brian Borsari. (2024)<br><strong>Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts</strong><br><button class=copy-to-clipboard title="Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13786v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13786v1.pdf filename=2403.13786v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic coding patient behaviors is essential to support decision making for psychotherapists during the motivational interviewing (MI), a collaborative communication intervention approach to address psychiatric issues, such as alcohol and drug addiction. While the behavior coding task has rapidly adapted machine learning to predict patient states during the MI sessions, lacking of domain-specific knowledge and overlooking patient-therapist interactions are major challenges in developing and deploying those models in real practice. To encounter those challenges, we introduce the Chain-of-Interaction (CoI) <b>prompting</b> method aiming to contextualize <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for psychiatric decision support by the dyadic interactions. The CoI <b>prompting</b> approach systematically breaks down the coding task into three key <b>reasoning</b> steps, extract patient engagement, learn therapist question strategies, and integrates dyadic interactions between patients and therapists. This approach enables <b>large</b> <b>language</b> <b>models</b> to leverage the coding scheme, patient state, and domain knowledge for patient behavioral coding. Experiments on real-world datasets can prove the effectiveness and flexibility of our <b>prompting</b> method with multiple state-of-the-art <b>LLMs</b> over existing <b>prompting</b> baselines. We have conducted extensive ablation analysis and demonstrate the critical role of dyadic interactions in applying <b>LLMs</b> for psychotherapy behavior understanding.</p></p class="citation"></blockquote><h3 id=1733--112286-dynamic-reward-adjustment-in-multi-reward-reinforcement-learning-for-counselor-reflection-generation-do-june-min-et-al-2024>(17/33 | 112/286) Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for Counselor Reflection Generation (Do June Min et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Do June Min, Veronica Perez-Rosas, Kenneth Resnicow, Rada Mihalcea. (2024)<br><strong>Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for Counselor Reflection Generation</strong><br><button class=copy-to-clipboard title="Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for Counselor Reflection Generation" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Bandit Algorithm, Reinforcement Learning, Language Generation, Natural Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13578v1.pdf filename=2403.13578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study the problem of multi-reward <b>reinforcement</b> <b>learning</b> to jointly optimize for multiple text qualities for <b>natural</b> <b>language</b> <b>generation.</b> We focus on the task of counselor reflection generation, where we optimize the generators to simultaneously improve the fluency, coherence, and reflection quality of generated counselor responses. We introduce two novel <b>bandit</b> methods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining rewards into a single value and optimizing them simultaneously. Specifically, we employ non-contextual and contextual multi-arm <b>bandits</b> to dynamically adjust multiple reward weights during training. Through automatic and manual evaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt, outperform existing naive and <b>bandit</b> baselines, showcasing their potential for enhancing <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=1833--113286-llamafactory-unified-efficient-fine-tuning-of-100-language-models-yaowei-zheng-et-al-2024>(18/33 | 113/286) LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models (Yaowei Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Yongqiang Ma. (2024)<br><strong>LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</strong><br><button class=copy-to-clipboard title="LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13372v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13372v2.pdf filename=2403.13372v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient <b>fine-tuning</b> is vital for adapting <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the <b>fine-tuning</b> of 100+ <b>LLMs</b> without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and <b>text</b> <b>generation</b> tasks. It has been released at <a href=https://github.com/hiyouga/LLaMA-Factory>https://github.com/hiyouga/LLaMA-Factory</a> and already received over 13,000 stars and 1,600 forks.</p></p class="citation"></blockquote><h3 id=1933--114286-leanreasoner-boosting-complex-logical-reasoning-with-lean-dongwei-jiang-et-al-2024>(19/33 | 114/286) LeanReasoner: Boosting Complex Logical Reasoning with Lean (Dongwei Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongwei Jiang, Marcio Fonseca, Shay B. Cohen. (2024)<br><strong>LeanReasoner: Boosting Complex Logical Reasoning with Lean</strong><br><button class=copy-to-clipboard title="LeanReasoner: Boosting Complex Logical Reasoning with Lean" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13312v1.pdf filename=2403.13312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> often struggle with complex logical <b>reasoning</b> due to logical inconsistencies and the inherent difficulty of such <b>reasoning.</b> We use Lean, a theorem proving framework, to address these challenges. By formalizing logical <b>reasoning</b> problems into theorems within Lean, we can solve them by proving or disproving the corresponding theorems. This method reduces the risk of logical inconsistencies with the help of Lean&rsquo;s symbolic solver. It also enhances our ability to treat complex <b>reasoning</b> tasks by using Lean&rsquo;s extensive library of theorem proofs. Our method achieves state-of-the-art performance on the FOLIO dataset and achieves performance near this level on ProofWriter. Notably, these results were accomplished by <b>fine-tuning</b> on fewer than 100 in-domain samples for each dataset.</p></p class="citation"></blockquote><h3 id=2033--115286-arcees-mergekit-a-toolkit-for-merging-large-language-models-charles-goddard-et-al-2024>(20/33 | 115/286) Arcee&rsquo;s MergeKit: A Toolkit for Merging Large Language Models (Charles Goddard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, Jacob Solawetz. (2024)<br><strong>Arcee&rsquo;s MergeKit: A Toolkit for Merging Large Language Models</strong><br><button class=copy-to-clipboard title="Arcee's MergeKit: A Toolkit for Merging Large Language Models" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Transfer Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13257v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13257v2.pdf filename=2403.13257v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid expansion of the open-source language model landscape presents an opportunity to merge the competencies of these model checkpoints by combining their parameters. Advances in <b>transfer</b> <b>learning,</b> the process of <b>fine-tuning</b> pretrained models for specific tasks, has resulted in the development of vast amounts of task-specific models, typically specialized in individual tasks and unable to utilize each other&rsquo;s strengths. Model merging facilitates the creation of multitask models without the need for additional training, offering a promising avenue for enhancing model performance and versatility. By preserving the intrinsic capabilities of the original models, model merging addresses complex challenges in AI - including the difficulties of catastrophic forgetting and multitask learning. To support this expanding area of research, we introduce MergeKit, a comprehensive, open-source library designed to facilitate the application of model merging strategies. MergeKit offers an extensible framework to efficiently merge models on any hardware, providing utility to researchers and practitioners. To date, thousands of models have been merged by the open-source community, leading to the creation of some of the worlds most powerful open-source model checkpoints, as assessed by the Open <b>LLM</b> Leaderboard. The library is accessible at <a href=https://github.com/arcee-ai/MergeKit>https://github.com/arcee-ai/MergeKit</a>.</p></p class="citation"></blockquote><h3 id=2133--116286-roleinteract-evaluating-the-social-interaction-of-role-playing-agents-hongzhan-chen-et-al-2024>(21/33 | 116/286) RoleInteract: Evaluating the Social Interaction of Role-Playing Agents (Hongzhan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, Fei Huang, Jingren Zhou. (2024)<br><strong>RoleInteract: Evaluating the Social Interaction of Role-Playing Agents</strong><br><button class=copy-to-clipboard title="RoleInteract: Evaluating the Social Interaction of Role-Playing Agents" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13679v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13679v3.pdf filename=2403.13679v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce RoleInteract, the first <b>benchmark</b> designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. The <b>benchmark</b> is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question <b>prompts</b> and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this <b>benchmark</b> using mainstream open-source and closed-source <b>LLMs.</b> We find that agents excelling in individual level does not imply their proficiency in group level. Moreover, the behavior of individuals may drift as a result of the influence exerted by other agents within the group. Experimental results on RoleInteract confirm its significance as a testbed for assessing the social interaction of role-playing conversational agents. The <b>benchmark</b> is publicly accessible at <a href=https://github.com/X-PLUG/RoleInteract>https://github.com/X-PLUG/RoleInteract</a>.</p></p class="citation"></blockquote><h3 id=2233--117286-grounding-spatial-relations-in-text-only-language-models-gorka-azkune-et-al-2024>(22/33 | 117/286) Grounding Spatial Relations in Text-Only Language Models (Gorka Azkune et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gorka Azkune, Ander Salaberria, Eneko Agirre. (2024)<br><strong>Grounding Spatial Relations in Text-Only Language Models</strong><br><button class=copy-to-clipboard title="Grounding Spatial Relations in Text-Only Language Models" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Grounding, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13666v1.pdf filename=2403.13666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper shows that text-only Language Models (LM) can learn to ground spatial relations like &ldquo;left of&rdquo; or &ldquo;below&rdquo; if they are provided with explicit location information of objects and they are properly trained to leverage those locations. We perform experiments on a verbalized version of the Visual Spatial <b>Reasoning</b> (VSR) dataset, where images are coupled with textual statements which contain real or fake spatial relations between two objects of the image. We verbalize the images using an off-the-shelf object detector, adding location tokens to every object label to represent their bounding boxes in textual form. Given the small size of VSR, we do not observe any improvement when using locations, but pretraining the LM over a synthetic dataset automatically derived by us improves results significantly when using location tokens. We thus show that locations allow LMs to ground spatial relations, with our text-only LMs outperforming <b>Vision-and-Language</b> Models and setting the new state-of-the-art for the VSR dataset. Our analysis show that our text-only LMs can generalize beyond the relations seen in the synthetic dataset to some extent, learning also more useful information than that encoded in the spatial rules we used to create the synthetic dataset itself.</p></p class="citation"></blockquote><h3 id=2333--118286-hyacinth6b-a-large-language-model-for-traditional-chinese-chih-wei-song-et-al-2024>(23/33 | 118/286) Hyacinth6B: A large language model for Traditional Chinese (Chih-Wei Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chih-Wei Song, Yin-Te Tsai. (2024)<br><strong>Hyacinth6B: A large language model for Traditional Chinese</strong><br><button class=copy-to-clipboard title="Hyacinth6B: A large language model for Traditional Chinese" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13334v1.pdf filename=2403.13334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research&rsquo;s primary motivation of this study is to address the high hardware and computational demands typically associated with <b>LLMs.Therefore,our</b> goal is to find a balance between model lightness and performance,striving to maximize performance while using a comparatively lightweight model. Hyacinth6B was developed with this objective in mind,aiming to fully leverage the core capabilities of <b>LLMs</b> without incurring substantial resource costs, effectively pushing the boundaries of smaller model&rsquo;s performance. The training approach involves parameter efficient <b>finetuning</b> using the LoRA method.</p></p class="citation"></blockquote><h3 id=2433--119286-aflora-adaptive-freezing-of-low-rank-adaptation-in-parameter-efficient-fine-tuning-of-large-models-zeyu-liu-et-al-2024>(24/33 | 119/286) AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models (Zeyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Liu, Souvik Kundu, Anni Li, Junrui Wan, Lianghao Jiang, Peter Anthony Beerel. (2024)<br><strong>AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models</strong><br><button class=copy-to-clipboard title="AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, GLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13269v1.pdf filename=2403.13269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel Parameter-Efficient <b>Fine-Tuning</b> (PEFT) method, dubbed as Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel freezing score, we the incrementally freeze these projection matrices during <b>fine-tuning</b> to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85%$ as evaluated on <b>GLUE</b> <b>benchmark</b> while yeilding up to $9.5\times$ fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the trainability requirements of LoRA paths at different modules and the freezing schedule for the different projection matrices. Code will be released.</p></p class="citation"></blockquote><h3 id=2533--120286-reverse-training-to-nurse-the-reversal-curse-olga-golovneva-et-al-2024>(25/33 | 120/286) Reverse Training to Nurse the Reversal Curse (Olga Golovneva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, Sainbayar Sukhbaatar. (2024)<br><strong>Reverse Training to Nurse the Reversal Curse</strong><br><button class=copy-to-clipboard title="Reverse Training to Nurse the Reversal Curse" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13799v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13799v1.pdf filename=2403.13799v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have a surprising failure: when trained on &ldquo;A has a feature B&rdquo;, they do not generalize to &ldquo;B is a feature of A&rdquo;, which is termed the Reversal Curse. Even when training with trillions of tokens this issue still appears due to Zipf&rsquo;s law - hence even if we train on the entire internet. This work proposes an alternative training scheme, called reverse training, whereby all words are used twice, doubling the amount of available tokens. The <b>LLM</b> is trained in both forward and reverse directions by reversing the training strings while preserving (i.e., not reversing) chosen substrings, such as entities. We show that data-matched reverse-trained models provide superior performance to standard models on standard tasks, and compute-matched reverse-trained models provide far superior performance on reversal tasks, helping resolve the reversal curse issue.</p></p class="citation"></blockquote><h3 id=2633--121286-what-explains-the-success-of-cross-modal-fine-tuning-with-orca-paloma-garcía-de-herreros-et-al-2024>(26/33 | 121/286) What explains the success of cross-modal fine-tuning with ORCA? (Paloma García-de-Herreros et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paloma García-de-Herreros, Vagrant Gautam, Philipp Slusallek, Dietrich Klakow, Marius Mosbach. (2024)<br><strong>What explains the success of cross-modal fine-tuning with ORCA?</strong><br><button class=copy-to-clipboard title="What explains the success of cross-modal fine-tuning with ORCA?" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13537v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13537v1.pdf filename=2403.13537v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>ORCA (Shen et al., 2023) is a recent technique for cross-modal <b>fine-tuning,</b> i.e., applying pre-trained <b>transformer</b> models to modalities beyond their training data. The technique consists primarily of training an embedder and <b>fine-tuning</b> the embedder and model. Despite its high performance on a variety of downstream tasks, we do not understand precisely how each of these components contribute to ORCA&rsquo;s success. Therefore, we run a series of ablations and find that embedder training does not help 2D tasks at all, contrary to what the original paper posits. In 1D tasks, some amount of embedder training is necessary but more is not better. In 4 out of 6 datasets we experiment with, it is model <b>fine-tuning</b> that makes the biggest difference. Through our ablations and baselines, we contribute a better understanding of the individual components of ORCA.</p></p class="citation"></blockquote><h3 id=2733--122286-how-gender-interacts-with-political-values-a-case-study-on-czech-bert-models-adnan-al-ali-et-al-2024>(27/33 | 122/286) How Gender Interacts with Political Values: A Case Study on Czech BERT Models (Adnan Al Ali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adnan Al Ali, Jindřich Libovický. (2024)<br><strong>How Gender Interacts with Political Values: A Case Study on Czech BERT Models</strong><br><button class=copy-to-clipboard title="How Gender Interacts with Political Values: A Case Study on Czech BERT Models" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 20<br>Keywords: BERT, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13514v1.pdf filename=2403.13514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural language models, which reach state-of-the-art results on most natural language processing tasks, are trained on large text corpora that inevitably contain value-burdened content and often capture undesirable biases, which the models reflect. This case study focuses on the political biases of pre-trained encoders in Czech and compares them with a representative value survey. Because Czech is a gendered language, we also measure how the grammatical gender coincides with responses to men and women in the survey. We introduce a novel method for measuring the model&rsquo;s perceived political values. We find that the models do not assign statement probability following value-driven <b>reasoning,</b> and there is no systematic difference between feminine and masculine sentences. We conclude that <b>BERT-sized</b> models do not manifest systematic alignment with political values and that the biases observed in the models are rather due to superficial imitation of training data patterns than systematic value beliefs encoded in the models.</p></p class="citation"></blockquote><h3 id=2833--123286-an-entropy-based-text-watermarking-detection-method-yijian-lu-et-al-2024>(28/33 | 123/286) An Entropy-based Text Watermarking Detection Method (Yijian Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijian Lu, Aiwei Liu, Dianzhi Yu, Jingjing Li, Irwin King. (2024)<br><strong>An Entropy-based Text Watermarking Detection Method</strong><br><button class=copy-to-clipboard title="An Entropy-based Text Watermarking Detection Method" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13485v1.pdf filename=2403.13485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Currently, text watermarking algorithms for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can embed hidden features to texts generated by <b>LLMs</b> to facilitate subsequent detection, thus alleviating the problem of misuse of <b>LLMs.</b> Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we proposed that the influence of token entropy should be fully considered in the watermark detection process, that is, the weight of each token should be adjusted according to its entropy during watermark detection, rather than setting the weight of all tokens to the same value as in previous methods. Specifically, we proposed an Entropy-based Watermark Detection (EWD) that gives higher-entropy tokens higher weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free and fully automated. %In actual detection, we use a proxy-LLM to calculate the entropy of each token, without the need to use the original <b>LLM.</b> In the experiment, we found that our method can achieve better detection performance in low-entropy scenarios, and our method is also general and can be applied to texts with different entropy distributions. Our code and data will be available online.</p></p class="citation"></blockquote><h3 id=2933--124286-technical-report-competition-solution-for-bettermixture-shuaijiang-zhao-et-al-2024>(29/33 | 124/286) Technical Report: Competition Solution For BetterMixture (Shuaijiang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuaijiang Zhao, Xiaoquan Fang. (2024)<br><strong>Technical Report: Competition Solution For BetterMixture</strong><br><button class=copy-to-clipboard title="Technical Report: Competition Solution For BetterMixture" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13233v1.pdf filename=2403.13233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of flourishing <b>large-scale</b> <b>models,</b> <b>the</b> challenge of selecting and optimizing datasets from the vast and complex sea of data, to enhance the performance of <b>large</b> <b>language</b> <b>models</b> within the constraints of limited computational resources, has become paramount. This paper details our solution for the BetterMixture challenge, which focuses on the <b>fine-tuning</b> data mixing for <b>large</b> <b>language</b> <b>models.</b> Our approach, which secured third place, incorporates data deduplication, low-level and high-level quality filtering, and diversity selection. The foundation of our solution is Ke-Data-Juicer, an extension of Data-Juicer, demonstrating its robust capabilities in handling and optimizing data for <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=3033--125286-a-new-massive-multilingual-dataset-for-high-performance-language-technologies-ona-de-gibert-et-al-2024>(30/33 | 125/286) A New Massive Multilingual Dataset for High-Performance Language Technologies (Ona de Gibert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ona de Gibert, Graeme Nail, Nikolay Arefyev, Marta Bañón, Jelmer van der Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, Mikko Aulamo, Gema Ramírez-Sánchez, Andrey Kutuzov, Sampo Pyysalo, Stephan Oepen, Jörg Tiedemann. (2024)<br><strong>A New Massive Multilingual Dataset for High-Performance Language Technologies</strong><br><button class=copy-to-clipboard title="A New Massive Multilingual Dataset for High-Performance Language Technologies" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14009v1.pdf filename=2403.14009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the HPLT (High Performance Language Technologies) language resources, a new massive multilingual dataset including both monolingual and bilingual corpora extracted from CommonCrawl and previously unused web crawls from the Internet Archive. We describe our methods for data acquisition, management and processing of large corpora, which rely on open-source software tools and high-performance computing. Our monolingual collection focuses on low- to medium-resourced languages and covers 75 languages and a total of ~5.6 trillion word tokens de-duplicated on the document level. Our English-centric parallel corpus is derived from its monolingual counterpart and covers 18 language pairs and more than 96 million aligned sentence pairs with roughly 1.4 billion English tokens. The HPLT language resources are one of the largest open text corpora ever released, providing a great resource for language modeling and <b>machine</b> <b>translation</b> training. We publicly release the corpora, the software, and the tools used in this work.</p></p class="citation"></blockquote><h3 id=3133--126286-reducing-large-language-model-bias-with-emphasis-on-restricted-industries-automated-dataset-augmentation-and-prejudice-quantification-devam-mondal-et-al-2024>(31/33 | 126/286) Reducing Large Language Model Bias with Emphasis on &lsquo;Restricted Industries&rsquo;: Automated Dataset Augmentation and Prejudice Quantification (Devam Mondal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Devam Mondal, Carlo Lipizzi. (2024)<br><strong>Reducing Large Language Model Bias with Emphasis on &lsquo;Restricted Industries&rsquo;: Automated Dataset Augmentation and Prejudice Quantification</strong><br><button class=copy-to-clipboard title="Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13925v1.pdf filename=2403.13925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the growing capabilities of <b>large</b> <b>language</b> <b>models,</b> there exists concerns about the biases they develop. In this paper, we propose a novel, automated mechanism for debiasing through specified dataset augmentation in the lens of bias producers and in the context of &lsquo;restricted industries&rsquo; with limited data. We additionally create two new additional metrics, the mb-index and db-index, to quantify bias, considering the idea that bias occurs due to both intrinsic model architecture and dataset.</p></p class="citation"></blockquote><h3 id=3233--127286-different-tokenization-schemes-lead-to-comparable-performance-in-spanish-number-agreement-catherine-arnett-et-al-2024>(32/33 | 127/286) Different Tokenization Schemes Lead to Comparable Performance in Spanish Number Agreement (Catherine Arnett et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Catherine Arnett, Pamela D. Rivière, Tyler A. Chang, Sean Trott. (2024)<br><strong>Different Tokenization Schemes Lead to Comparable Performance in Spanish Number Agreement</strong><br><button class=copy-to-clipboard title="Different Tokenization Schemes Lead to Comparable Performance in Spanish Number Agreement" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13754v1.pdf filename=2403.13754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The relationship between language model <b>tokenization</b> and performance is an open area of research. Here, we investigate how different <b>tokenization</b> schemes impact number agreement in Spanish plurals. We find that morphologically-aligned <b>tokenization</b> performs similarly to other <b>tokenization</b> schemes, even when induced artificially for words that would not be tokenized that way during training. We then present exploratory analyses demonstrating that language model embeddings for different plural <b>tokenizations</b> have similar distributions along the embedding space axis that maximally distinguishes singular and plural nouns. Our results suggest that morphologically-aligned <b>tokenization</b> is a viable <b>tokenization</b> approach, and existing models already generalize some morphological patterns to new items. However, our results indicate that morphological <b>tokenization</b> is not strictly required for performance.</p></p class="citation"></blockquote><h3 id=3333--128286-erst-a-signaled-graph-theory-of-discourse-relations-and-organization-amir-zeldes-et-al-2024>(33/33 | 128/286) eRST: A Signaled Graph Theory of Discourse Relations and Organization (Amir Zeldes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Zeldes, Tatsuya Aoyama, Yang Janet Liu, Siyao Peng, Debopam Das, Luke Gessler. (2024)<br><strong>eRST: A Signaled Graph Theory of Discourse Relations and Organization</strong><br><button class=copy-to-clipboard title="eRST: A Signaled Graph Theory of Discourse Relations and Organization" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13560v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13560v1.pdf filename=2403.13560v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this article we present Enhanced Rhetorical Structure Theory (eRST), a new theoretical framework for computational discourse analysis, based on an expansion of Rhetorical Structure Theory (RST). The framework encompasses discourse relation <b>graphs</b> with tree-breaking, nonprojective and concurrent relations, as well as implicit and explicit signals which give explainable rationales to our analyses. We survey shortcomings of RST and other existing frameworks, such as Segmented Discourse Representation Theory (SDRT), the Penn Discourse Treebank (PDTB) and Discourse Dependencies, and address these using constructs in the proposed theory. We provide annotation, search and visualization tools for data, and present and evaluate a freely available corpus of English annotated according to our framework, encompassing 12 spoken and written genres with over 200K tokens. Finally, we discuss automatic parsing, evaluation metrics and applications for data in our framework.</p></p class="citation"></blockquote><h2 id=csse-9>cs.SE (9)</h2><h3 id=19--129286-enhancing-code-generation-performance-of-smaller-models-by-distilling-the-reasoning-ability-of-llms-zhihong-sun-et-al-2024>(1/9 | 129/286) Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning Ability of LLMs (Zhihong Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihong Sun, Chen Lyu, Bolun Li, Yao Wan, Hongyu Zhang, Ge Li, Zhi Jin. (2024)<br><strong>Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning Ability of LLMs</strong><br><button class=copy-to-clipboard title="Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning Ability of LLMs" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: D-2-3, cs-SE, cs.SE<br>Keyword Score: 83<br>Keywords: Benchmarking, Fine-tuning, Knowledge Distillation, Knowledge Distillation, Code Generation, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13271v1.pdf filename=2403.13271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have recently made significant advances in <b>code</b> <b>generation</b> through the &lsquo;Chain-of-Thought&rsquo; <b>prompting</b> technique. This technique empowers the model to autonomously devise &ldquo;solution plans&rdquo; to tackle intricate programming challenges, thereby improving its performance in <b>code</b> <b>generation.</b> Nevertheless, smaller models have been struggling to keep up with <b>LLMs</b> in deducing these plans, adversely affecting their <b>code</b> <b>generation</b> capabilities. Given the considerable size and associated deployment costs, along with concerns about data security, many teams opt for deploying smaller models for <b>code</b> <b>generation.</b> Consequently, there arises a compelling need for transferring <b>LLMs&rsquo;</b> <b>code</b> <b>generation</b> <b>reasoning</b> abilities to the smaller models. In this paper, we propose the CodePLAN framework, which aims to transfer <b>LLMs&rsquo;</b> <b>reasoning</b> capabilities to smaller models through <b>distillation.</b> We adopt a multi-task learning approach, jointly undertaking <b>code</b> <b>generation</b> and solution plan generation tasks, to enhance the <b>code</b> <b>generation</b> capabilities of the smaller model. To ensure the superior quality of the solution plans, we advocate for the utilization of backward <b>reasoning</b> and plan sampling strategies. Our experiments show that in comparison to the conventional <b>fine-tuning</b> approach, our approach improves the smaller model&rsquo;s <b>code</b> <b>generation</b> performance (measured in pass@1 metric) by over 130% on the challenging APPS <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=29--130286-genetic-auto-prompt-learning-for-pre-trained-code-intelligence-language-models-chengzhe-feng-et-al-2024>(2/9 | 130/286) Genetic Auto-prompt Learning for Pre-trained Code Intelligence Language Models (Chengzhe Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengzhe Feng, Yanan Sun, Ke Li, Pan Zhou, Jiancheng Lv, Aojun Lu. (2024)<br><strong>Genetic Auto-prompt Learning for Pre-trained Code Intelligence Language Models</strong><br><button class=copy-to-clipboard title="Genetic Auto-prompt Learning for Pre-trained Code Intelligence Language Models" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: Pre-trained Language Model, Pre-trained Language Model, Prompt, Prompt Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13588v1.pdf filename=2403.13588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs),</b> a popular approach for code intelligence, continue to grow in size, the computational cost of their usage has become prohibitively expensive. <b>Prompt</b> <b>learning,</b> a recent development in the field of natural language processing, emerges as a potential solution to address this challenge. In this paper, we investigate the effectiveness of <b>prompt</b> <b>learning</b> in code intelligence tasks. We unveil its reliance on manually designed <b>prompts,</b> <b>which</b> often require significant human effort and expertise. Moreover, we discover existing automatic <b>prompt</b> <b>design</b> methods are very limited to code intelligence tasks due to factors including gradient dependence, high computational demands, and limited applicability. To effectively address both issues, we propose Genetic Auto <b>Prompt</b> <b>(GenAP),</b> which utilizes an elaborate genetic algorithm to automatically design <b>prompts.</b> <b>With</b> GenAP, non-experts can effortlessly generate superior <b>prompts</b> <b>compared</b> to meticulously manual-designed ones. GenAP operates without the need for gradients or additional computational costs, rendering it gradient-free and cost-effective. Moreover, GenAP supports both understanding and generation types of code intelligence tasks, exhibiting great applicability. We conduct GenAP on three popular code intelligence <b>PLMs</b> with three canonical code intelligence tasks including defect prediction, code <b>summarization,</b> and code translation. The results suggest that GenAP can effectively automate the process of designing <b>prompts.</b> <b>Specifically,</b> GenAP outperforms all other methods across all three tasks (e.g., improving accuracy by an average of 2.13% for defect prediction). To the best of our knowledge, GenAP is the first work to automatically design <b>prompts</b> <b>for</b> code intelligence <b>PLMs.</b></p></p class="citation"></blockquote><h3 id=39--131286-conline-complex-code-generation-and-refinement-with-online-searching-and-correctness-testing-xinyi-he-et-al-2024>(3/9 | 131/286) CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing (Xinyi He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyi He, Jiaru Zou, Yun Lin, Mengyu Zhou, Shi Han, Zejian Yuan, Dongmei Zhang. (2024)<br><strong>CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing</strong><br><button class=copy-to-clipboard title="CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-LG, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Code Generation, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13583v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13583v1.pdf filename=2403.13583v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have revolutionized <b>code</b> <b>generation</b> ability by converting natural language descriptions into executable <b>code.</b> <b>However,</b> generating complex <b>code</b> <b>within</b> real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CONLINE framework, which enhances <b>code</b> <b>generation</b> by incorporating planned online searches for <b>information</b> <b>retrieval</b> and automated correctness testing for iterative refinement. CONLINE also serializes the complex inputs and outputs to improve comprehension and generate test case to ensure the framework&rsquo;s adaptability for real-world applications. CONLINE is validated through rigorous experiments on the DS-1000 and ClassEval datasets. It shows that CONLINE substantially improves the quality of complex <b>code</b> <b>generation,</b> highlighting its potential to enhance the practicality and reliability of <b>LLMs</b> in generating intricate code.</p></p class="citation"></blockquote><h3 id=49--132286-creative-and-correct-requesting-diverse-code-solutions-from-ai-foundation-models-scott-blyth-et-al-2024>(4/9 | 132/286) Creative and Correct: Requesting Diverse Code Solutions from AI Foundation Models (Scott Blyth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Scott Blyth, Markus Wagner, Christoph Treude. (2024)<br><strong>Creative and Correct: Requesting Diverse Code Solutions from AI Foundation Models</strong><br><button class=copy-to-clipboard title="Creative and Correct: Requesting Diverse Code Solutions from AI Foundation Models" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: D-2-3, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Foundation Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13259v1.pdf filename=2403.13259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI <b>foundation</b> <b>models</b> have the capability to produce a wide array of responses to a single <b>prompt,</b> a feature that is highly beneficial in software engineering to generate diverse code solutions. However, this advantage introduces a significant trade-off between diversity and correctness. In software engineering tasks, diversity is key to exploring design spaces and fostering creativity, but the practical value of these solutions is heavily dependent on their correctness. Our study systematically investigates this trade-off using experiments with HumanEval tasks, exploring various parameter settings and <b>prompting</b> strategies. We assess the diversity of code solutions using similarity metrics from the code clone community. The study identifies combinations of parameters and strategies that strike an optimal balance between diversity and correctness, situated on the Pareto front of this trade-off space. These findings offer valuable insights for software engineers on how to effectively use AI <b>foundation</b> <b>models</b> to generate code solutions that are diverse and accurate.</p></p class="citation"></blockquote><h3 id=59--133286-reinforcement-learning-for-online-testing-of-autonomous-driving-systems-a-replication-and-extension-study-luca-giamattei-et-al-2024>(5/9 | 133/286) Reinforcement Learning for Online Testing of Autonomous Driving Systems: a Replication and Extension Study (Luca Giamattei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Giamattei, Matteo Biagiola, Roberto Pietrantuono, Stefano Russo, Paolo Tonella. (2024)<br><strong>Reinforcement Learning for Online Testing of Autonomous Driving Systems: a Replication and Extension Study</strong><br><button class=copy-to-clipboard title="Reinforcement Learning for Online Testing of Autonomous Driving Systems: a Replication and Extension Study" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-LG, cs-RO, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13729v1.pdf filename=2403.13729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a recent study, <b>Reinforcement</b> <b>Learning</b> (RL) used in combination with many-objective search, has been shown to outperform alternative techniques (random search and many-objective search) for online testing of Deep Neural Network-enabled systems. The empirical evaluation of these techniques was conducted on a state-of-the-art Autonomous Driving System (ADS). This work is a replication and extension of that empirical study. Our replication shows that RL does not outperform pure random test generation in a comparison conducted under the same settings of the original study, but with no confounding factor coming from the way collisions are measured. Our extension aims at eliminating some of the possible reasons for the poor performance of RL observed in our replication: (1) the presence of reward components providing contrasting or useless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning) which requires discretization of an intrinsically continuous state space. Results show that our new RL agent is able to converge to an effective policy that outperforms random testing. Results also highlight other possible improvements, which open to further investigations on how to best leverage RL for online ADS testing.</p></p class="citation"></blockquote><h3 id=69--134286-hylimo-a-hybrid-live-synchronized-modular-diagramming-editor-as-ide-extension-for-technical-and-scientific-publications-niklas-krieger-et-al-2024>(6/9 | 134/286) HyLiMo: A Hybrid Live-Synchronized Modular Diagramming Editor as IDE Extension for Technical and Scientific Publications (Niklas Krieger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niklas Krieger, Sandro Speth, Steffen Becker. (2024)<br><strong>HyLiMo: A Hybrid Live-Synchronized Modular Diagramming Editor as IDE Extension for Technical and Scientific Publications</strong><br><button class=copy-to-clipboard title="HyLiMo: A Hybrid Live-Synchronized Modular Diagramming Editor as IDE Extension for Technical and Scientific Publications" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13711v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13711v1.pdf filename=2403.13711v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Creating suitable diagrams for technical and scientific publications is challenging and time-consuming, as manual control over the layout is required to communicate information effectively. Existing diagramming tools usually allow modeling the diagrams via a textual domain-specific language (DSL) that can be rendered and auto-layouted or via a graphical editor. While auto-layout is fast, the results are often not satisfying for most publications. However, graphical editors are time-consuming to create large diagrams. The blended or hybrid modeling concept enables creating diagrams efficiently using a DSL and editing the rendered diagram via the graphical editor for <b>fine-tuning.</b> However, hybrid modeling editors are limited to individual diagram types and do not save the layout and style information in the textual description. Therefore, we propose HyLiMo, a hybrid live-synchronized modular diagramming editor. In HyLiMo, diagrams are created using an internal DSL and live synchronized with an interactive graphical editor for the rendered diagram, allowing a straightforward layout and style change, which is stored in the DSL code. HyLiMo is independent of specific diagram types, but we offer specific functionality for UML class diagrams. Using the language server protocol, we implement it as a web app and IDE extension. The results of our user study indicate that such an approach enables fast and precise diagramming.</p></p class="citation"></blockquote><h3 id=79--135286-fastflip-compositional-error-injection-analysis-keyur-joshi-et-al-2024>(7/9 | 135/286) FastFlip: Compositional Error Injection Analysis (Keyur Joshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keyur Joshi, Rahul Singh, Tommaso Bassetto, Sarita Adve, Darko Marinov, Sasa Misailovic. (2024)<br><strong>FastFlip: Compositional Error Injection Analysis</strong><br><button class=copy-to-clipboard title="FastFlip: Compositional Error Injection Analysis" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13989v1.pdf filename=2403.13989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Instruction-level error injection analyses aim to find instructions where errors often lead to unacceptable outcomes like Silent Data Corruptions (SDCs). These analyses require significant time, which is especially problematic if developers wish to regularly analyze software that evolves over time. We present FastFlip, a combination of empirical error injection and symbolic SDC propagation analyses that enables fast, compositional error injection analysis of evolving programs. FastFlip calculates how SDCs propagate across program sections and correctly accounts for unexpected side effects that can occur due to errors. Using FastFlip, we analyze five <b>benchmarks,</b> plus two modified versions of each <b>benchmark.</b> FastFlip speeds up the analysis of incrementally modified programs by $3.2\times$ (geomean). FastFlip selects a set of instructions to protect against SDCs that minimizes the runtime cost of protection while protecting against a developer-specified target fraction of all SDC-causing errors.</p></p class="citation"></blockquote><h3 id=89--136286-motorease-automated-detection-of-motor-impairment-accessibility-issues-in-mobile-app-uis-arun-krishnavajjala-et-al-2024>(8/9 | 136/286) MotorEase: Automated Detection of Motor Impairment Accessibility Issues in Mobile App UIs (Arun Krishnavajjala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arun Krishnavajjala, SM Hasan Mansur, Justin Jose, Kevin Moran. (2024)<br><strong>MotorEase: Automated Detection of Motor Impairment Accessibility Issues in Mobile App UIs</strong><br><button class=copy-to-clipboard title="MotorEase: Automated Detection of Motor Impairment Accessibility Issues in Mobile App UIs" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CV, cs-HC, cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13690v1.pdf filename=2403.13690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research has begun to examine the potential of automatically finding and fixing accessibility issues that manifest in software. However, while recent work makes important progress, it has generally been skewed toward identifying issues that affect users with certain disabilities, such as those with visual or hearing impairments. However, there are other groups of users with different types of disabilities that also need software tooling support to improve their experience. As such, this paper aims to automatically identify accessibility issues that affect users with motor-impairments. To move toward this goal, this paper introduces a novel approach, called MotorEase, capable of identifying accessibility issues in mobile app UIs that impact motor-impaired users. Motor-impaired users often have limited ability to interact with touch-based devices, and instead may make use of a switch or other assistive mechanism &ndash; hence UIs must be designed to support both limited touch gestures and the use of assistive devices. MotorEase adapts computer vision and text processing techniques to enable a semantic understanding of app UI screens, enabling the detection of violations related to four popular, previously unexplored UI design guidelines that support motor-impaired users, including: (i) visual touch target size, (ii) expanding sections, (iii) persisting elements, and (iv) adjacent icon visual distance. We evaluate MotorEase on a newly derived <b>benchmark,</b> called MotorCheck, that contains 555 manually annotated examples of violations to the above accessibility guidelines, across 1599 screens collected from 70 applications via a mobile app testing tool. Our experiments illustrate that MotorEase is able to identify violations with an average accuracy of ~90%, and a false positive rate of less than 9%, outperforming baseline techniques.</p></p class="citation"></blockquote><h3 id=99--137286-specification-mining-for-smart-contracts-with-trace-slicing-and-predicate-abstraction-ye-liu-et-al-2024>(9/9 | 137/286) Specification Mining for Smart Contracts with Trace Slicing and Predicate Abstraction (Ye Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Liu, Yi Li, Cyrille Artho, Yixuan Liu. (2024)<br><strong>Specification Mining for Smart Contracts with Trace Slicing and Predicate Abstraction</strong><br><button class=copy-to-clipboard title="Specification Mining for Smart Contracts with Trace Slicing and Predicate Abstraction" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13279v1.pdf filename=2403.13279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Smart contracts are computer programs running on blockchains to implement Decentralized Applications.The absence of contract specifications hinders routine tasks, such as contract understanding and testing. Inthis work, we propose a specification mining approach to infer contract specifications from past transactionhistories. Our approach derives high-level behavioral automata of function invocations, accompanied byprogram invariants statistically inferred from the transaction histories. We implemented our approach as toolSmConand evaluated it on eleven well-studied Azure <b>benchmark</b> smart contracts and six popular real-worldDApp smart contracts. The experiments show thatSmConmines reasonably accurate specifications that canbe used to facilitate DApp understanding and development in terms of document maintenance and test suite improvement.</p></p class="citation"></blockquote><h2 id=cslg-32>cs.LG (32)</h2><h3 id=132--138286-from-representational-harms-to-quality-of-service-harms-a-case-study-on-llama-2-safety-safeguards-khaoula-chehbouni-et-al-2024>(1/32 | 138/286) From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards (Khaoula Chehbouni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khaoula Chehbouni, Megha Roshan, Emmanuel Ma, Futian Andrew Wei, Afaf Taik, Jackie CK Cheung, Golnoosh Farnadi. (2024)<br><strong>From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards</strong><br><button class=copy-to-clipboard title="From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CY, cs-LG, cs.LG<br>Keyword Score: 80<br>Keywords: Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Supervised Learning, LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13213v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13213v2.pdf filename=2403.13213v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent progress in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as <b>supervised</b> safety-oriented <b>fine-tuning</b> and leveraging safe <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback,</b> multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluating models on already mitigated biases. Using the case of <b>Llama</b> 2 as an example, we illustrate how <b>LLMs&rsquo;</b> safety responses can still encode harmful assumptions. To do so, we create a set of non-toxic <b>prompts,</b> which we then use to evaluate <b>Llama</b> models. Through our new taxonomy of <b>LLMs</b> responses to users, we observe that the safety/helpfulness trade-offs are more pronounced for certain demographic groups which can lead to quality-of-service harms for marginalized populations.</p></p class="citation"></blockquote><h3 id=232--139286-rewardbench-evaluating-reward-models-for-language-modeling-nathan-lambert-et-al-2024>(2/32 | 139/286) RewardBench: Evaluating Reward Models for Language Modeling (Nathan Lambert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi. (2024)<br><strong>RewardBench: Evaluating Reward Models for Language Modeling</strong><br><button class=copy-to-clipboard title="RewardBench: Evaluating Reward Models for Language Modeling" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Benchmarking, Direct Preference Optimization, Out-of-distribution, Reinforcement Learning from Human Feedback, Instruction Following, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13787v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13787v1.pdf filename=2403.13787v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reward models (RMs) are at the crux of successful <b>RLHF</b> to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. To date, very few descriptors of capabilities, training methods, or open-source reward models exist. In this paper, we present RewardBench, a <b>benchmark</b> dataset and code-base for evaluation, to enhance scientific understanding of reward models. The RewardBench dataset is a collection of <b>prompt-win-lose</b> trios spanning chat, <b>reasoning,</b> and safety, to <b>benchmark</b> how reward models perform on challenging, structured and <b>out-of-distribution</b> queries. We created specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another. On the RewardBench leaderboard, we evaluate reward models trained with a variety of methods, such as the <b>direct</b> <b>MLE</b> <b>training</b> of classifiers and the implicit reward modeling of <b>Direct</b> <b>Preference</b> <b>Optimization</b> (DPO), and on a spectrum of datasets. We present many findings on propensity for refusals, <b>reasoning</b> limitations, and <b>instruction</b> <b>following</b> shortcomings of various reward models towards a better understanding of the <b>RLHF</b> process.</p></p class="citation"></blockquote><h3 id=332--140286-real-representation-enhanced-analytic-learning-for-exemplar-free-class-incremental-learning-run-he-et-al-2024>(3/32 | 140/286) REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning (Run He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Run He, Huiping Zhuang, Di Fang, Yizhu Chen, Kai Tong, Cen Chen. (2024)<br><strong>REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning</strong><br><button class=copy-to-clipboard title="REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Contrastive Learning, Knowledge Distillation, Knowledge Distillation, Self-supervised Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13522v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13522v1.pdf filename=2403.13522v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exemplar-free class-incremental learning (EFCIL) aims to mitigate catastrophic forgetting in class-incremental learning without available historical data. Compared with its counterpart (replay-based CIL) that stores historical samples, the EFCIL suffers more from forgetting issues under the exemplar-free constraint. In this paper, inspired by the recently developed analytic learning (AL) based CIL, we propose a representation enhanced analytic learning (REAL) for EFCIL. The REAL constructs a dual-stream base pretraining (DS-BPT) and a representation enhancing <b>distillation</b> (RED) process to enhance the representation of the extractor. The DS-BPT pretrains model in streams of both <b>supervised</b> <b>learning</b> and <b>self-supervised</b> <b>contrastive</b> <b>learning</b> (SSCL) for base knowledge extraction. The RED process <b>distills</b> the <b>supervised</b> <b>knowledge</b> to the SSCL pretrained backbone and facilitates a subsequent AL-basd CIL that converts the CIL to a recursive least-square problem. Our method addresses the issue of insufficient discriminability in representations of unseen data caused by a frozen backbone in the existing AL-based CIL. Empirical results on various datasets including CIFAR-100, ImageNet-100 and ImageNet-1k, demonstrate that our REAL outperforms the state-of-the-arts in EFCIL, and achieves comparable or even more superior performance compared with the replay-based methods.</p></p class="citation"></blockquote><h3 id=432--141286-adaptive-ensembles-of-fine-tuned-transformers-for-llm-generated-text-detection-zhixin-lai-et-al-2024>(4/32 | 141/286) Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection (Zhixin Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhixin Lai, Xuesheng Zhang, Suiyao Chen. (2024)<br><strong>Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection</strong><br><button class=copy-to-clipboard title="Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Fine-tuning, Out-of-distribution, Transformer, Fake News Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13335v1.pdf filename=2403.13335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have reached human-like proficiency in generating diverse textual content, underscoring the necessity for effective <b>fake</b> <b>text</b> detection to avoid potential risks such as <b>fake</b> <b>news</b> in social media. Previous research has mostly tested single models on in-distribution datasets, limiting our understanding of how these models perform on different types of data for <b>LLM-generated</b> text detection task. We researched this by testing five specialized <b>transformer-based</b> models on both in-distribution and <b>out-of-distribution</b> datasets to better assess their performance and generalizability. Our results revealed that single <b>transformer-based</b> classifiers achieved decent performance on in-distribution dataset but limited generalization ability on <b>out-of-distribution</b> dataset. To improve it, we combined the individual classifiers models using adaptive ensemble algorithms, which improved the average accuracy significantly from 91.8% to 99.2% on an in-distribution test set and from 62.9% to 72.5% on an <b>out-of-distribution</b> test set. The results indicate the effectiveness, good generalization ability, and great potential of adaptive ensemble algorithms in <b>LLM-generated</b> text detection.</p></p class="citation"></blockquote><h3 id=532--142286-multimodal-variational-autoencoder-for-low-cost-cardiac-hemodynamics-instability-detection-mohammod-n-i-suvon-et-al-2024>(5/32 | 142/286) Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics Instability Detection (Mohammod N. I. Suvon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammod N. I. Suvon, Prasun C. Tripathi, Wenrui Fan, Shuo Zhou, Xianyuan Liu, Samer Alabed, Venet Osmani, Andrew J. Swift, Chen Chen, Haiping Lu. (2024)<br><strong>Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics Instability Detection</strong><br><button class=copy-to-clipboard title="Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics Instability Detection" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 46<br>Keywords: Autoencoder, Fine-tuning, Fine-tuning, Multi-modal, Multi-modal, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13658v1.pdf filename=2403.13658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in non-invasive detection of cardiac hemodynamic instability (CHDI) primarily focus on applying machine learning techniques to a single data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite their potential, these approaches often fall short especially when the size of labeled patient data is limited, a common challenge in the medical domain. Furthermore, only a few studies have explored <b>multimodal</b> methods to study CHDI, which mostly rely on costly modalities such as cardiac MRI and echocardiogram. In response to these limitations, we propose a novel <b>multimodal</b> <b>variational</b> <b>autoencoder</b> ($\text{CardioVAE}<em>\text{X,G}$) to integrate low-cost chest X-ray (CXR) and electrocardiogram (ECG) modalities with pre-training on a large unlabeled dataset. Specifically, $\text{CardioVAE}</em>\text{X,G}$ introduces a novel tri-stream pre-training strategy to learn both shared and modality-specific features, thus enabling <b>fine-tuning</b> with both unimodal and <b>multimodal</b> datasets. We pre-train $\text{CardioVAE}<em>\text{X,G}$ on a large, unlabeled dataset of $50,982$ subjects from a subset of MIMIC database and then <b>fine-tune</b> the pre-trained model on a labeled dataset of $795$ subjects from the ASPIRE registry. Comprehensive evaluations against existing methods show that $\text{CardioVAE}</em>\text{X,G}$ offers promising performance (AUROC $=0.79$ and Accuracy $=0.77$), representing a significant step forward in non-invasive prediction of CHDI. Our model also excels in producing fine interpretations of predictions directly associated with clinical features, thereby supporting clinical decision-making.</p></p class="citation"></blockquote><h3 id=632--143286-capsule-neural-networks-as-noise-stabilizer-for-time-series-data-soyeon-kim-et-al-2024>(6/32 | 143/286) Capsule Neural Networks as Noise Stabilizer for Time Series Data (Soyeon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soyeon Kim, Jihyeon Seong, Hyunkyung Han, Jaesik Choi. (2024)<br><strong>Capsule Neural Networks as Noise Stabilizer for Time Series Data</strong><br><button class=copy-to-clipboard title="Capsule Neural Networks as Noise Stabilizer for Time Series Data" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13867v1.pdf filename=2403.13867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Capsule Neural Networks utilize capsules, which bind neurons into a single vector and learn position equivariant features, which makes them more robust than original <b>Convolutional</b> <b>Neural</b> <b>Networks.</b> CapsNets employ an affine transformation matrix and dynamic routing with coupling coefficients to learn robustly. In this paper, we investigate the effectiveness of CapsNets in analyzing highly sensitive and noisy time series sensor data. To demonstrate CapsNets robustness, we compare their performance with original <b>CNNs</b> on electrocardiogram data, a medical time series sensor data with complex patterns and noise. Our study provides empirical evidence that CapsNets function as noise stabilizers, as investigated by manual and <b>adversarial</b> <b>attack</b> experiments using the fast gradient sign method and three manual attacks, including offset shifting, gradual drift, and temporal lagging. In summary, CapsNets outperform <b>CNNs</b> in both manual and <b>adversarial</b> <b>attacked</b> data. Our findings suggest that CapsNets can be effectively applied to various sensor systems to improve their resilience to noise attacks. These results have significant implications for designing and implementing robust machine learning models in real world applications. Additionally, this study contributes to the effectiveness of CapsNet models in handling noisy data and highlights their potential for addressing the challenges of noise data in time series analysis.</p></p class="citation"></blockquote><h3 id=732--144286-diffusion-model-for-data-driven-black-box-optimization-zihao-li-et-al-2024>(7/32 | 144/286) Diffusion Model for Data-Driven Black-Box Optimization (Zihao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Li, Hui Yuan, Kaixuan Huang, Chengzhuo Ni, Yinyu Ye, Minshuo Chen, Mengdi Wang. (2024)<br><strong>Diffusion Model for Data-Driven Black-Box Optimization</strong><br><button class=copy-to-clipboard title="Diffusion Model for Data-Driven Black-Box Optimization" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 35<br>Keywords: Diffusion Model, Bandit Algorithm, Black Box, Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13219v1.pdf filename=2403.13219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> has redefined artificial intelligence, enabling the creation of innovative content and customized solutions that drive business practices into a new era of efficiency and creativity. In this paper, we focus on <b>diffusion</b> <b>models,</b> a powerful <b>generative</b> <b>AI</b> technology, and investigate their potential for <b>black-box</b> <b>optimization</b> over complex structured variables. Consider the practical scenario where one wants to optimize some structured design in a high-dimensional space, based on massive unlabeled data (representing design variables) and a small labeled dataset. We study two practical types of labels: 1) noisy measurements of a real-valued reward function and 2) human preference based on pairwise comparisons. The goal is to generate new designs that are near-optimal and preserve the designed latent structures. Our proposed method reformulates the design optimization problem into a conditional sampling problem, which allows us to leverage the power of <b>diffusion</b> <b>models</b> for modeling complex distributions. In particular, we propose a reward-directed conditional <b>diffusion</b> <b>model,</b> to be trained on the mixed data, for sampling a near-optimal solution conditioned on high predicted rewards. Theoretically, we establish sub-optimality error bounds for the generated designs. The sub-optimality gap nearly matches the optimal guarantee in off-policy <b>bandits,</b> demonstrating the efficiency of reward-directed <b>diffusion</b> <b>models</b> for <b>black-box</b> <b>optimization.</b> Moreover, when the data admits a low-dimensional latent subspace structure, our model efficiently generates high-fidelity designs that closely respect the latent structure. We provide empirical experiments validating our model in decision-making and content-creation tasks.</p></p class="citation"></blockquote><h3 id=832--145286-machine-learning-optimized-approach-for-parameter-selection-in-meshfree-simulations-paulami-banerjee-et-al-2024>(8/32 | 145/286) Machine Learning Optimized Approach for Parameter Selection in MESHFREE Simulations (Paulami Banerjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paulami Banerjee, Mohan Padmanabha, Chaitanya Sanghavi, Isabel Michel, Simone Gramsch. (2024)<br><strong>Machine Learning Optimized Approach for Parameter Selection in MESHFREE Simulations</strong><br><button class=copy-to-clipboard title="Machine Learning Optimized Approach for Parameter Selection in MESHFREE Simulations" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-flu-dyn<br>Keyword Score: 30<br>Keywords: Active Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13672v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13672v1.pdf filename=2403.13672v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Meshfree <b>simulation</b> methods are emerging as compelling alternatives to conventional mesh-based approaches, particularly in the fields of Computational Fluid Dynamics (CFD) and continuum mechanics. In this publication, we provide a comprehensive overview of our research combining Machine Learning (ML) and Fraunhofer&rsquo;s MESHFREE software (<a href=https://www.meshfree.eu>www.meshfree.eu</a>), a powerful tool utilizing a numerical point cloud in a Generalized Finite Difference Method (GFDM). This tool enables the effective handling of complex flow domains, moving geometries, and free surfaces, while allowing users to finely tune local refinement and quality parameters for an optimal balance between computation time and results accuracy. However, manually determining the optimal parameter combination poses challenges, especially for less experienced users. We introduce a novel ML-optimized approach, using <b>active</b> <b>learning,</b> regression trees, and visualization on MESHFREE <b>simulation</b> data, demonstrating the impact of input combinations on results quality and computation time. This research contributes valuable insights into parameter optimization in meshfree <b>simulations,</b> enhancing accessibility and usability for a broader user base in scientific and engineering applications.</p></p class="citation"></blockquote><h3 id=932--146286-towards-principled-representation-learning-from-videos-for-reinforcement-learning-dipendra-misra-et-al-2024>(9/32 | 146/286) Towards Principled Representation Learning from Videos for Reinforcement Learning (Dipendra Misra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dipendra Misra, Akanksha Saran, Tengyang Xie, Alex Lamb, John Langford. (2024)<br><strong>Towards Principled Representation Learning from Videos for Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Towards Principled Representation Learning from Videos for Reinforcement Learning" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Contrastive Learning, Reinforcement Learning, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13765v1.pdf filename=2403.13765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study pre-training <b>representations</b> <b>for</b> decision-making using video data, which is abundantly available for tasks such as game agents and software testing. Even though significant empirical advances have been made on this problem, a theoretical understanding remains absent. We initiate the theoretical investigation into principled approaches for <b>representation</b> <b>learning</b> and focus on learning the latent state <b>representations</b> <b>of</b> the underlying MDP using video data. We study two types of settings: one where there is iid noise in the observation, and a more challenging setting where there is also the presence of exogenous noise, which is non-iid noise that is temporally correlated, such as the motion of people or cars in the background. We study three commonly used approaches: autoencoding, temporal <b>contrastive</b> <b>learning,</b> and forward modeling. We prove upper bounds for temporal <b>contrastive</b> <b>learning</b> and forward modeling in the presence of only iid noise. We show that these approaches can learn the latent state and use it to do efficient downstream RL with polynomial sample complexity. When exogenous noise is also present, we establish a lower bound result showing that the sample complexity of learning from video data can be exponentially worse than learning from action-labeled trajectory data. This partially explains why <b>reinforcement</b> <b>learning</b> with video pre-training is hard. We evaluate these <b>representational</b> <b>learning</b> methods in two visual domains, yielding results that are consistent with our theoretical findings.</p></p class="citation"></blockquote><h3 id=1032--147286-sparse-implementation-of-versatile-graph-informed-layers-francesco-della-santa-2024>(10/32 | 147/286) Sparse Implementation of Versatile Graph-Informed Layers (Francesco Della Santa, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Della Santa. (2024)<br><strong>Sparse Implementation of Versatile Graph-Informed Layers</strong><br><button class=copy-to-clipboard title="Sparse Implementation of Versatile Graph-Informed Layers" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T07, 03D32, cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13781v1.pdf filename=2403.13781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have emerged as effective tools for learning tasks on <b>graph-structured</b> <b>data.</b> <b>Recently,</b> <b>Graph-Informed</b> <b>(GI)</b> <b>layers</b> were introduced to address regression tasks on <b>graph</b> <b>nodes,</b> <b>extending</b> their applicability beyond classic <b>GNNs.</b> However, existing implementations of GI layers lack efficiency due to dense memory allocation. This paper presents a sparse implementation of GI layers, leveraging the sparsity of adjacency matrices to reduce memory usage significantly. Additionally, a versatile general form of GI layers is introduced, enabling their application to subsets of <b>graph</b> <b>nodes.</b> <b>The</b> proposed sparse implementation improves the concrete computational efficiency and scalability of the GI layers, permitting to build deeper <b>Graph-Informed</b> <b>Neural</b> <b>Networks</b> (GINNs) and facilitating their scalability to larger <b>graphs.</b></p></p class="citation"></blockquote><h3 id=1132--148286-hierarchical-gaussian-mixture-normalizing-flow-modeling-for-unified-anomaly-detection-xincheng-yao-et-al-2024>(11/32 | 148/286) Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection (Xincheng Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xincheng Yao, Ruoqi Li, Zefeng Qian, Lu Wang, Chongyang Zhang. (2024)<br><strong>Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection</strong><br><button class=copy-to-clipboard title="Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Anomaly Detection, Benchmarking, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13349v1.pdf filename=2403.13349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unified <b>anomaly</b> <b>detection</b> (AD) is one of the most challenges for <b>anomaly</b> <b>detection,</b> where one unified model is trained with normal samples from multiple classes with the objective to detect anomalies in these classes. For such a challenging task, popular normalizing flow (NF) based AD methods may fall into a &ldquo;homogeneous mapping&rdquo; issue,where the NF-based AD models are biased to generate similar latent representations for both normal and abnormal features, and thereby lead to a high missing rate of anomalies. In this paper, we propose a novel Hierarchical Gaussian mixture normalizing flow modeling method for accomplishing unified <b>Anomaly</b> <b>Detection,</b> which we call HGAD. Our HGAD consists of two key components: inter-class Gaussian mixture modeling and intra-class mixed class centers learning. Compared to the previous NF-based AD methods, the hierarchical Gaussian mixture modeling approach can bring stronger representation capability to the latent space of normalizing flows, so that even complex multi-class distribution can be well represented and learned in the latent space. In this way, we can avoid mapping different class distributions into the same single Gaussian prior, thus effectively avoiding or mitigating the &ldquo;homogeneous mapping&rdquo; issue. We further indicate that the more distinguishable different class centers, the more conducive to avoiding the bias issue. Thus, we further propose a <b>mutual</b> <b>information</b> maximization loss for better structuring the latent feature space. We evaluate our method on four real-world AD <b>benchmarks,</b> where we can significantly improve the previous NF-based AD methods and also outperform the SOTA unified AD methods.</p></p class="citation"></blockquote><h3 id=1232--149286-unifews-unified-entry-wise-sparsification-for-efficient-graph-neural-network-ningyi-liao-et-al-2024>(12/32 | 149/286) Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural Network (Ningyi Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ningyi Liao, Zihao Yu, Siqiang Luo. (2024)<br><strong>Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural Network</strong><br><button class=copy-to-clipboard title="Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural Network" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DB, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13268v1.pdf filename=2403.13268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have shown promising performance in various <b>graph</b> <b>learning</b> <b>tasks,</b> but at the cost of resource-intensive computations. The primary overhead of <b>GNN</b> update stems from <b>graph</b> <b>propagation</b> <b>and</b> weight transformation, both involving operations on <b>graph-scale</b> <b>matrices.</b> <b>Previous</b> studies attempt to reduce the computational budget by leveraging <b>graph-level</b> <b>or</b> <b>network-level</b> sparsification techniques, resulting in downsized <b>graph</b> <b>or</b> <b>weights.</b> In this work, we propose Unifews, which unifies the two operations in an entry-wise manner considering individual matrix elements, and conducts joint edge-weight sparsification to enhance learning efficiency. The entry-wise design of Unifews enables adaptive compression across <b>GNN</b> layers with progressively increased sparsity, and is applicable to a variety of architectural designs with on-the-fly operation simplification. Theoretically, we establish a novel framework to characterize sparsified <b>GNN</b> learning in view of a <b>graph</b> <b>optimization</b> <b>process,</b> and prove that Unifews effectively approximates the learning objective with bounded error and reduced computational load. We conduct extensive experiments to evaluate the performance of our method in diverse settings. Unifews is advantageous in jointly removing more than 90% of edges and weight entries with comparable or better accuracy than baseline models. The sparsification offers remarkable efficiency improvements including 10-20x matrix operation reduction and up to 100x acceleration in <b>graph</b> <b>propagation</b> <b>time</b> for the largest <b>graph</b> <b>at</b> <b>the</b> billion-edge scale.</p></p class="citation"></blockquote><h3 id=1332--150286-decentralized-federated-learning-model-update-tracking-under-imperfect-information-sharing-vishnu-pandi-chellapandi-et-al-2024>(13/32 | 150/286) Decentralized Federated Learning: Model Update Tracking Under Imperfect Information Sharing (Vishnu Pandi Chellapandi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vishnu Pandi Chellapandi, Antesh Upadhyay, Abolfazl Hashemi, Stanislaw H. Żak. (2024)<br><strong>Decentralized Federated Learning: Model Update Tracking Under Imperfect Information Sharing</strong><br><button class=copy-to-clipboard title="Decentralized Federated Learning: Model Update Tracking Under Imperfect Information Sharing" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Federated Learning, Parameter Sharing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13247v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13247v1.pdf filename=2403.13247v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A novel Decentralized Noisy Model Update Tracking <b>Federated</b> <b>Learning</b> algorithm (FedNMUT) is proposed, which is tailored to function efficiently in the presence of noisy communication channels that reflect imperfect information exchange. This algorithm uses gradient tracking to minimize the impact of data heterogeneity while minimizing communication overhead. The proposed algorithm incorporates noise into its <b>parameters</b> <b>to</b> mimic the conditions of noisy communication channels, thereby enabling consensus among clients through a communication <b>graph</b> topology in such challenging environments. FedNMUT prioritizes <b>parameter</b> <b>sharing</b> and noise incorporation to increase the resilience of decentralized learning systems against noisy communications. Through theoretical and empirical validation, it is demonstrated that the performance of FedNMUT is superior compared to the existing state-of-the-art methods and conventional <b>parameter-mixing</b> <b>approaches</b> in dealing with imperfect information sharing. This proves the capability of the proposed algorithm to counteract the negative effects of communication noise in a decentralized learning framework.</p></p class="citation"></blockquote><h3 id=1432--151286-bridge-the-modality-and-capacity-gaps-in-vision-language-model-selection-chao-yi-et-al-2024>(14/32 | 151/286) Bridge the Modality and Capacity Gaps in Vision-Language Model Selection (Chao Yi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Yi, De-Chuan Zhan, Han-Jia Ye. (2024)<br><strong>Bridge the Modality and Capacity Gaps in Vision-Language Model Selection</strong><br><button class=copy-to-clipboard title="Bridge the Modality and Capacity Gaps in Vision-Language Model Selection" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13797v1.pdf filename=2403.13797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vision Language Models (VLMs) excel in <b>zero-shot</b> image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. Thus, a promising <b>zero-shot</b> image classification strategy is selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset&rsquo;s images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the &ldquo;Modality Gap&rdquo; &ndash; the disparity in VLM&rsquo;s embeddings across two different modalities, making text a less reliable substitute for images; and the &ldquo;Capability Gap&rdquo; &ndash; the discrepancy between the VLM&rsquo;s overall ranking and its ranking for target dataset, hindering direct prediction of a model&rsquo;s dataset-specific performance from its general performance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the negative impact of these two gaps. SWAB first adopts optimal transport to capture the relevance between open-source datasets and target dataset with a transportation matrix. It then uses this matrix to transfer useful statistics of VLMs from open-source datasets to the target dataset for bridging those two gaps and enhancing the VLM&rsquo;s capacity estimation for VLM selection. Experiments across various VLMs and image classification datasets validate SWAB&rsquo;s effectiveness.</p></p class="citation"></blockquote><h3 id=1532--152286-does-differentially-private-synthetic-data-lead-to-synthetic-discoveries-ileana-montoya-perez-et-al-2024>(15/32 | 152/286) Does Differentially Private Synthetic Data Lead to Synthetic Discoveries? (Ileana Montoya Perez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ileana Montoya Perez, Parisa Movahedi, Valtteri Nieminen, Antti Airola, Tapio Pahikkala. (2024)<br><strong>Does Differentially Private Synthetic Data Lead to Synthetic Discoveries?</strong><br><button class=copy-to-clipboard title="Does Differentially Private Synthetic Data Lead to Synthetic Discoveries?" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13612v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13612v1.pdf filename=2403.13612v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: Synthetic data has been proposed as a solution for sharing anonymized versions of sensitive biomedical datasets. Ideally, synthetic data should preserve the structure and statistical properties of the original data, while protecting the privacy of the individual subjects. <b>Differential</b> <b>privacy</b> (DP) is currently considered the gold standard approach for balancing this trade-off. Objectives: The aim of this study is to evaluate the Mann-Whitney U test on DP-synthetic biomedical data in terms of Type I and Type II errors, in order to establish whether statistical hypothesis testing performed on privacy preserving synthetic data is likely to lead to loss of test&rsquo;s validity or decreased power. Methods: We evaluate the Mann-Whitney U test on DP-synthetic data generated from real-world data, including a prostate cancer dataset (n=500) and a cardiovascular dataset (n=70 000), as well as on data drawn from two Gaussian distributions. Five different DP-synthetic data generation methods are evaluated, including two basic DP histogram release methods and MWEM, Private-PGM, and DP <b>GAN</b> algorithms. Conclusion: Most of the tested DP-synthetic data generation methods showed inflated Type I error, especially at privacy budget levels of $\epsilon\leq 1$. This result calls for caution when releasing and analyzing DP-synthetic data: low p-values may be obtained in statistical tests simply as a byproduct of the noise added to protect privacy. A DP smoothed histogram-based synthetic data generation method was shown to produce valid Type I error for all privacy levels tested but required a large original dataset size and a modest privacy budget ($\epsilon\geq 5$) in order to have reasonable Type II error levels.</p></p class="citation"></blockquote><h3 id=1632--153286-the-bid-picture-auction-inspired-multi-player-generative-adversarial-networks-training-joo-yong-shim-et-al-2024>(16/32 | 153/286) The Bid Picture: Auction-Inspired Multi-player Generative Adversarial Networks Training (Joo Yong Shim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joo Yong Shim, Jean Seong Bjorn Choe, Jong-Kook Kim. (2024)<br><strong>The Bid Picture: Auction-Inspired Multi-player Generative Adversarial Networks Training</strong><br><button class=copy-to-clipboard title="The Bid Picture: Auction-Inspired Multi-player Generative Adversarial Networks Training" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13866v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13866v1.pdf filename=2403.13866v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article proposes auction-inspired multi-player <b>generative</b> <b>adversarial</b> <b>networks</b> training, which mitigates the mode collapse problem of <b>GANs.</b> Mode collapse occurs when an over-fitted generator generates a limited range of samples, often concentrating on a small subset of the data distribution. Despite the restricted diversity of generated samples, the discriminator can still be deceived into distinguishing these samples as real samples from the actual distribution. In the absence of external standards, a model cannot recognize its failure during the training phase. We extend the two-player game of <b>generative</b> <b>adversarial</b> <b>networks</b> to the multi-player game. During the training, the values of each model are determined by the bids submitted by other players in an auction-like process.</p></p class="citation"></blockquote><h3 id=1732--154286-diffimpute-tabular-data-imputation-with-denoising-diffusion-probabilistic-model-yizhu-wen-et-al-2024>(17/32 | 154/286) DiffImpute: Tabular Data Imputation With Denoising Diffusion Probabilistic Model (Yizhu Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yizhu Wen, Kai Yi, Jing Ke, Yiqing Shen. (2024)<br><strong>DiffImpute: Tabular Data Imputation With Denoising Diffusion Probabilistic Model</strong><br><button class=copy-to-clipboard title="DiffImpute: Tabular Data Imputation With Denoising Diffusion Probabilistic Model" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DB, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Probabilistic Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13863v1.pdf filename=2403.13863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tabular data plays a crucial role in various domains but often suffers from missing values, thereby curtailing its potential utility. Traditional imputation techniques frequently yield suboptimal results and impose substantial computational burdens, leading to inaccuracies in subsequent modeling tasks. To address these challenges, we propose DiffImpute, a novel Denoising Diffusion <b>Probabilistic</b> <b>Model</b> (DDPM). Specifically, DiffImpute is trained on complete tabular datasets, ensuring that it can produce credible imputations for missing entries without undermining the authenticity of the existing data. Innovatively, it can be applied to various settings of Missing Completely At Random (MCAR) and Missing At Random (MAR). To effectively handle the tabular features in DDPM, we tailor four tabular denoising networks, spanning MLP, ResNet, <b>Transformer,</b> and U-Net. We also propose Harmonization to enhance coherence between observed and imputed data by infusing the data back and denoising them multiple times during the sampling stage. To enable efficient inference while maintaining imputation performance, we propose a refined non-Markovian sampling process that works along with Harmonization. Empirical evaluations on seven diverse datasets underscore the prowess of DiffImpute. Specifically, when paired with the <b>Transformer</b> as the denoising network, it consistently outperforms its competitors, boasting an average ranking of 1.7 and the most minimal standard deviation. In contrast, the next best method lags with a ranking of 2.8 and a standard deviation of 0.9. The code is available at <a href=https://github.com/Dendiiiii/DiffImpute>https://github.com/Dendiiiii/DiffImpute</a>.</p></p class="citation"></blockquote><h3 id=1832--155286-divide-conquer-transformer-learning-for-predicting-electric-vehicle-charging-events-using-smart-meter-data-fucai-ke-et-al-2024>(18/32 | 155/286) Divide-Conquer Transformer Learning for Predicting Electric Vehicle Charging Events Using Smart Meter Data (Fucai Ke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fucai Ke, Hao Wang. (2024)<br><strong>Divide-Conquer Transformer Learning for Predicting Electric Vehicle Charging Events Using Smart Meter Data</strong><br><button class=copy-to-clipboard title="Divide-Conquer Transformer Learning for Predicting Electric Vehicle Charging Events Using Smart Meter Data" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13246v1.pdf filename=2403.13246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting electric vehicle (EV) charging events is crucial for load scheduling and energy management, promoting seamless transportation electrification and decarbonization. While prior studies have focused on EV charging demand prediction, primarily for public charging stations using historical charging data, home charging prediction is equally essential. However, existing prediction methods may not be suitable due to the unavailability of or limited access to home charging data. To address this research gap, inspired by the concept of non-intrusive load monitoring (NILM), we develop a home charging prediction method using historical smart meter data. Different from NILM detecting EV charging that has already occurred, our method provides predictive information of future EV charging occurrences, thus enhancing its utility for charging management. Specifically, our method, leverages a <b>self-attention</b> mechanism-based <b>transformer</b> model, employing a ``divide-conquer&rsquo;&rsquo; strategy, to process historical meter data to effectively and learn EV charging representation for charging occurrence prediction. Our method enables prediction at one-minute interval hour-ahead. Experimental results demonstrate the effectiveness of our method, achieving consistently high accuracy of over 96.81% across different prediction time spans. Notably, our method achieves high prediction performance solely using smart meter data, making it a practical and suitable solution for grid operators.</p></p class="citation"></blockquote><h3 id=1932--156286-spatial-temporal-graph-representation-learning-for-tactical-networks-future-state-prediction-liu-junhua-et-al-2024>(19/32 | 156/286) Spatial-Temporal Graph Representation Learning for Tactical Networks Future State Prediction (Liu Junhua et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liu Junhua, Albrethsen Justin, Goh Lincoln, Yau David, Lim Kwan Hui. (2024)<br><strong>Spatial-Temporal Graph Representation Learning for Tactical Networks Future State Prediction</strong><br><button class=copy-to-clipboard title="Spatial-Temporal Graph Representation Learning for Tactical Networks Future State Prediction" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 18<br>Keywords: Graph, Representation Learning, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13872v1.pdf filename=2403.13872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Resource allocation in tactical ad-hoc networks presents unique challenges due to their dynamic and multi-hop nature. Accurate prediction of future network connectivity is essential for effective resource allocation in such environments. In this paper, we introduce the Spatial-Temporal <b>Graph</b> Encoder-Decoder (STGED) framework for Tactical Communication Networks that leverages both spatial and temporal features of network states to learn latent tactical behaviors effectively. STGED hierarchically utilizes <b>graph-based</b> attention mechanism to spatially encode a series of communication network states, leverages a <b>recurrent</b> <b>neural</b> <b>network</b> to temporally encode the evolution of states, and a fully-connected feed-forward network to decode the connectivity in the future state. Through extensive experiments, we demonstrate that STGED consistently outperforms baseline models by large margins across different time-steps input, achieving an accuracy of up to 99.2% for the future state prediction task of tactical communication networks.</p></p class="citation"></blockquote><h3 id=2032--157286-weisfeiler-and-leman-go-loopy-a-new-hierarchy-for-graph-representational-learning-raffaele-paolino-et-al-2024>(20/32 | 157/286) Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning (Raffaele Paolino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raffaele Paolino, Sohir Maskey, Pascal Welke, Gitta Kutyniok. (2024)<br><strong>Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning</strong><br><button class=copy-to-clipboard title="Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13749v1.pdf filename=2403.13749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce $r$-loopy Weisfeiler-Leman ($r$-$\ell{}$WL), a novel hierarchy of <b>graph</b> isomorphism tests and a corresponding <b>GNN</b> framework, $r$-$\ell{}$MPNN, that can count cycles up to length $r + 2$. Most notably, we show that $r$-$\ell{}$WL can count homomorphisms of cactus <b>graphs.</b> This strictly extends classical 1-WL, which can only count homomorphisms of trees and, in fact, is incomparable to $k$-WL for any fixed $k$. We empirically validate the expressive and counting power of the proposed $r$-$\ell{}$MPNN on several synthetic datasets and present state-of-the-art predictive performance on various real-world datasets. The code is available at <a href=https://github.com/RPaolino/loopy>https://github.com/RPaolino/loopy</a></p></p class="citation"></blockquote><h3 id=2132--158286-adversarial-attacks-and-defenses-in-automated-control-systems-a-comprehensive-benchmark-vitaliy-pozdnyakov-et-al-2024>(21/32 | 158/286) Adversarial Attacks and Defenses in Automated Control Systems: A Comprehensive Benchmark (Vitaliy Pozdnyakov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vitaliy Pozdnyakov, Aleksandr Kovalenko, Ilya Makarov, Mikhail Drobyshevskiy, Kirill Lukyanov. (2024)<br><strong>Adversarial Attacks and Defenses in Automated Control Systems: A Comprehensive Benchmark</strong><br><button class=copy-to-clipboard title="Adversarial Attacks and Defenses in Automated Control Systems: A Comprehensive Benchmark" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6; I-2-1, cs-CR, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 13<br>Keywords: Benchmarking, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13502v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13502v2.pdf filename=2403.13502v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrating machine learning into Automated Control Systems (ACS) enhances decision-making in industrial process management. One of the limitations to the widespread adoption of these technologies in industry is the vulnerability of neural networks to <b>adversarial</b> <b>attacks.</b> This study explores the threats in deploying deep learning models for fault diagnosis in ACS using the Tennessee Eastman Process dataset. By evaluating three neural networks with different architectures, we subject them to six types of <b>adversarial</b> <b>attacks</b> and explore five different defense methods. Our results highlight the strong vulnerability of models to <b>adversarial</b> <b>samples</b> and the varying effectiveness of defense strategies. We also propose a novel protection approach by combining multiple defense methods and demonstrate it&rsquo;s efficacy. This research contributes several insights into securing machine learning within ACS, ensuring robust fault diagnosis in industrial processes.</p></p class="citation"></blockquote><h3 id=2232--159286-optimal-transport-for-fairness-archival-data-repair-using-small-research-data-sets-abigail-langbridge-et-al-2024>(22/32 | 159/286) Optimal Transport for Fairness: Archival Data Repair using Small Research Data Sets (Abigail Langbridge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abigail Langbridge, Anthony Quinn, Robert Shorten. (2024)<br><strong>Optimal Transport for Fairness: Archival Data Repair using Small Research Data Sets</strong><br><button class=copy-to-clipboard title="Optimal Transport for Fairness: Archival Data Repair using Small Research Data Sets" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG, math-ST, stat-TH<br>Keyword Score: 13<br>Keywords: Benchmarking, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13864v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13864v1.pdf filename=2403.13864v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advent of the AI Act and other regulations, there is now an urgent need for algorithms that repair unfairness in training data. In this paper, we define <b>fairness</b> in terms of conditional independence between protected attributes ($S$) and features ($X$), given unprotected attributes ($U$). We address the important setting in which torrents of archival data need to be repaired, using only a small proportion of these data, which are $S|U$-labelled (the research data). We use the latter to design optimal transport (OT)-based repair plans on interpolated supports. This allows {\em off-sample}, labelled, archival data to be repaired, subject to stationarity assumptions. It also significantly reduces the size of the supports of the OT plans, with correspondingly large savings in the cost of their design and of their {\em sequential/} application to the off-sample data. We provide detailed experimental results with simulated and <b>benchmark</b> real data (the Adult data set). Our performance figures demonstrate effective repair &ndash; in the sense of quenching conditional dependence &ndash; of large quantities of off-sample, labelled (archival) data.</p></p class="citation"></blockquote><h3 id=2332--160286-a-unified-and-general-framework-for-continual-learning-zhenyi-wang-et-al-2024>(23/32 | 160/286) A Unified and General Framework for Continual Learning (Zhenyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyi Wang, Yan Li, Li Shen, Heng Huang. (2024)<br><strong>A Unified and General Framework for Continual Learning</strong><br><button class=copy-to-clipboard title="A Unified and General Framework for Continual Learning" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13249v1.pdf filename=2403.13249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>Learning</b> (CL) focuses on learning from dynamic and changing data distributions while retaining previously acquired knowledge. Various methods have been developed to address the challenge of catastrophic forgetting, including regularization-based, Bayesian-based, and memory-replay-based techniques. However, these methods lack a unified framework and common terminology for describing their approaches. This research aims to bridge this gap by introducing a comprehensive and overarching framework that encompasses and reconciles these existing methodologies. Notably, this new framework is capable of encompassing established CL approaches as special instances within a unified and general optimization objective. An intriguing finding is that despite their diverse origins, these methods share common mathematical structures. This observation highlights the compatibility of these seemingly distinct techniques, revealing their interconnectedness through a shared underlying optimization objective. Moreover, the proposed general framework introduces an innovative concept called refresh learning, specifically designed to enhance the CL performance. This novel approach draws inspiration from neuroscience, where the human brain often sheds outdated information to improve the retention of crucial knowledge and facilitate the acquisition of new information. In essence, refresh learning operates by initially unlearning current data and subsequently relearning it. It serves as a versatile plug-in that seamlessly integrates with existing CL methods, offering an adaptable and effective enhancement to the learning process. Extensive experiments on CL <b>benchmarks</b> and theoretical analysis demonstrate the effectiveness of the proposed refresh learning. Code is available at \url{https://github.com/joey-wang123/CL-refresh-learning}.</p></p class="citation"></blockquote><h3 id=2432--161286-machine-learning-based-layer-wise-detection-of-overheating-anomaly-in-lpbf-using-photodiode-data-nazmul-hasan-et-al-2024>(24/32 | 161/286) Machine Learning-based Layer-wise Detection of Overheating Anomaly in LPBF using Photodiode Data (Nazmul Hasan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nazmul Hasan, Apurba Kumar Saha, Andrew Wessman, Mohammed Shafae. (2024)<br><strong>Machine Learning-based Layer-wise Detection of Overheating Anomaly in LPBF using Photodiode Data</strong><br><button class=copy-to-clipboard title="Machine Learning-based Layer-wise Detection of Overheating Anomaly in LPBF using Photodiode Data" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-AP<br>Keyword Score: 13<br>Keywords: Anomaly Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13861v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13861v1.pdf filename=2403.13861v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Overheating <b>anomaly</b> <b>detection</b> is essential for the quality and reliability of parts produced by laser powder bed fusion (LPBF) additive manufacturing (AM). In this research, we focus on the detection of overheating anomalies using photodiode sensor data. Photodiode sensors can collect high-frequency data from the melt pool, reflecting the process dynamics and thermal history. Hence, the proposed method offers a machine learning (ML) framework to utilize photodiode sensor data for layer-wise detection of overheating anomalies. In doing so, three sets of features are extracted from the raw photodiode data: MSMM (mean, standard deviation, median, maximum), MSQ (mean, standard deviation, quartiles), and MSD (mean, standard deviation, deciles). These three datasets are used to train several ML classifiers. Cost-sensitive learning is used to handle the class imbalance between the &ldquo;anomalous&rdquo; layers (affected by overheating) and &ldquo;nominal&rdquo; layers in the <b>benchmark</b> dataset. To boost detection accuracy, our proposed ML framework involves utilizing the majority voting ensemble (MVE) approach. The proposed method is demonstrated using a case study including an open <b>benchmark</b> dataset of photodiode measurements from an LPBF specimen with deliberate overheating anomalies at some layers. The results from the case study demonstrate that the MSD features yield the best performance for all classifiers, and the MVE classifier (with a mean F1-score of 0.8654) surpasses the individual ML classifiers. Moreover, our machine learning methodology achieves superior results (9.66% improvement in mean F1-score) in detecting layer-wise overheating anomalies, surpassing the existing methods in the literature that use the same <b>benchmark</b> dataset.</p></p class="citation"></blockquote><h3 id=2532--162286-multi-criteria-approach-for-selecting-an-explanation-from-the-set-of-counterfactuals-produced-by-an-ensemble-of-explainers-ignacy-stępka-et-al-2024>(25/32 | 162/286) Multi-criteria approach for selecting an explanation from the set of counterfactuals produced by an ensemble of explainers (Ignacy Stępka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ignacy Stępka, Mateusz Lango, Jerzy Stefanowski. (2024)<br><strong>Multi-criteria approach for selecting an explanation from the set of counterfactuals produced by an ensemble of explainers</strong><br><button class=copy-to-clipboard title="Multi-criteria approach for selecting an explanation from the set of counterfactuals produced by an ensemble of explainers" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13940v1.pdf filename=2403.13940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Counterfactuals</b> are widely used to explain ML model predictions by providing alternative scenarios for obtaining the more desired predictions. They can be generated by a variety of methods that optimize different, sometimes conflicting, quality measures and produce quite different solutions. However, choosing the most appropriate explanation method and one of the generated <b>counterfactuals</b> is not an easy task. Instead of forcing the user to test many different explanation methods and analysing conflicting solutions, in this paper, we propose to use a multi-stage ensemble approach that will select single <b>counterfactual</b> based on the multiple-criteria analysis. It offers a compromise solution that scores well on several popular quality measures. This approach exploits the dominance relation and the ideal point decision aid method, which selects one <b>counterfactual</b> from the Pareto front. The conducted experiments demonstrated that the proposed approach generates fully actionable <b>counterfactuals</b> with attractive compromise values of the considered quality measures.</p></p class="citation"></blockquote><h3 id=2632--163286-evaluating-frontier-models-for-dangerous-capabilities-mary-phuong-et-al-2024>(26/32 | 163/286) Evaluating Frontier Models for Dangerous Capabilities (Mary Phuong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael, Sarah Hodkinson, Heidi Howard, Tom Lieberum, Ramana Kumar, Maria Abi Raad, Albert Webson, Lewis Ho, Sharon Lin, Sebastian Farquhar, Marcus Hutter, Gregoire Deletang, Anian Ruoss, Seliem El-Sayed, Sasha Brown, Anca Dragan, Rohin Shah, Allan Dafoe, Toby Shevlane. (2024)<br><strong>Evaluating Frontier Models for Dangerous Capabilities</strong><br><button class=copy-to-clipboard title="Evaluating Frontier Models for Dangerous Capabilities" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Gemini<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13793v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13793v1.pdf filename=2403.13793v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To understand the risks posed by a new AI system, we must understand what it can and cannot do. Building on prior work, we introduce a programme of new &ldquo;dangerous capability&rdquo; evaluations and pilot them on <b>Gemini</b> 1.0 models. Our evaluations cover four areas: (1) persuasion and deception; (2) cyber-security; (3) self-proliferation; and (4) self-reasoning. We do not find evidence of strong dangerous capabilities in the models we evaluated, but we flag early warning signs. Our goal is to help advance a rigorous science of dangerous capability evaluation, in preparation for future models.</p></p class="citation"></blockquote><h3 id=2732--164286-the-model-openness-framework-promoting-completeness-and-openness-for-reproducibility-transparency-and-usability-in-ai-matt-white-et-al-2024>(27/32 | 164/286) The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI (Matt White et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matt White, Ibrahim Haddad, Cailean Osborne, Xiao-Yang, Liu, Ahmed Abdelmonsef, Sachin Varghese. (2024)<br><strong>The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI</strong><br><button class=copy-to-clipboard title="The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs-SE, cs.LG<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13784v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13784v2.pdf filename=2403.13784v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many &ldquo;open-source&rdquo; GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as &ldquo;openwashing.&rdquo; We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adopted without restrictions. Wide adoption of the MOF will foster a more open AI ecosystem, accelerating research, innovation, and adoption.</p></p class="citation"></blockquote><h3 id=2832--165286-integrating-large-language-models-for-severity-classification-in-traffic-incident-management-a-machine-learning-approach-artur-grigorev-et-al-2024>(28/32 | 165/286) Integrating Large Language Models for Severity Classification in Traffic Incident Management: A Machine Learning Approach (Artur Grigorev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Artur Grigorev, Khaled Saleh, Yuming Ou, Adriana-Simona Mihaita. (2024)<br><strong>Integrating Large Language Models for Severity Classification in Traffic Incident Management: A Machine Learning Approach</strong><br><button class=copy-to-clipboard title="Integrating Large Language Models for Severity Classification in Traffic Incident Management: A Machine Learning Approach" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13547v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13547v1.pdf filename=2403.13547v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study evaluates the impact of <b>large</b> <b>language</b> <b>models</b> on enhancing machine learning processes for managing traffic incidents. It examines the extent to which features generated by modern language models improve or match the accuracy of predictions when classifying the severity of incidents using accident reports. Multiple comparisons performed between combinations of language models and machine learning algorithms, including Gradient Boosted Decision Trees, Random Forests, and Extreme Gradient Boosting. Our research uses both conventional and language model-derived features from texts and incident reports, and their combinations to perform severity classification. Incorporating features from language models with those directly obtained from incident reports has shown to improve, or at least match, the performance of machine learning techniques in assigning severity levels to incidents, particularly when employing Random Forests and Extreme Gradient Boosting methods. This comparison was quantified using the F1-score over uniformly sampled data sets to obtain balanced severity classes. The primary contribution of this research is in the demonstration of how <b>Large</b> <b>Language</b> <b>Models</b> can be integrated into machine learning workflows for incident management, thereby simplifying feature extraction from unstructured text and enhancing or matching the precision of severity predictions using conventional machine learning pipeline. The engineering application of this research is illustrated through the effective use of these language processing models to refine the modelling process for incident severity classification. This work provides significant insights into the application of language processing capabilities in combination with traditional data for improving machine learning pipelines in the context of classifying incident severity.</p></p class="citation"></blockquote><h3 id=2932--166286-have-you-poisoned-my-data-defending-neural-networks-against-data-poisoning-fabio-de-gaspari-et-al-2024>(29/32 | 166/286) Have You Poisoned My Data? Defending Neural Networks against Data Poisoning (Fabio De Gaspari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabio De Gaspari, Dorjan Hitaj, Luigi V. Mancini. (2024)<br><strong>Have You Poisoned My Data? Defending Neural Networks against Data Poisoning</strong><br><button class=copy-to-clipboard title="Have You Poisoned My Data? Defending Neural Networks against Data Poisoning" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13523v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13523v1.pdf filename=2403.13523v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The unprecedented availability of training data fueled the rapid development of powerful neural networks in recent years. However, the need for such large amounts of data leads to potential threats such as poisoning attacks: adversarial manipulations of the training data aimed at compromising the learned model to achieve a given adversarial goal. This paper investigates defenses against clean-label poisoning attacks and proposes a novel approach to detect and filter poisoned datapoints in the <b>transfer</b> <b>learning</b> setting. We define a new characteristic vector representation of datapoints and show that it effectively captures the intrinsic properties of the data distribution. Through experimental analysis, we demonstrate that effective poisons can be successfully differentiated from clean points in the characteristic vector space. We thoroughly evaluate our proposed approach and compare it to existing state-of-the-art defenses using multiple architectures, datasets, and poison budgets. Our evaluation shows that our proposal outperforms existing approaches in defense rate and final trained model performance across all experimental settings.</p></p class="citation"></blockquote><h3 id=3032--167286-byzantine-resilient-federated-learning-with-adaptivity-to-data-heterogeneity-shiyuan-zuo-et-al-2024>(30/32 | 167/286) Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity (Shiyuan Zuo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q. S. Quek. (2024)<br><strong>Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity</strong><br><button class=copy-to-clipboard title="Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13374v1.pdf filename=2403.13374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper deals with <b>federated</b> <b>learning</b> (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and $\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optimal solution is proved to obtainable as data heterogeneity vanishes. Experimental results corroborate the robustness of RAGA to Byzantine attacks and verifies the advantage of RAGA over baselines on convergence performance under various intensity of Byzantine attacks, for heterogeneous dataset.</p></p class="citation"></blockquote><h3 id=3132--168286-probabilistic-forecasting-with-stochastic-interpolants-and-föllmer-processes-yifan-chen-et-al-2024>(31/32 | 168/286) Probabilistic Forecasting with Stochastic Interpolants and Föllmer Processes (Yifan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Chen, Mark Goldstein, Mengjian Hua, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden. (2024)<br><strong>Probabilistic Forecasting with Stochastic Interpolants and Föllmer Processes</strong><br><button class=copy-to-clipboard title="Probabilistic Forecasting with Stochastic Interpolants and Föllmer Processes" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 5<br>Keywords: Square Loss<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13724v1.pdf filename=2403.13724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a framework for probabilistic forecasting of dynamical systems based on generative modeling. Given observations of the system state over time, we formulate the forecasting problem as sampling from the conditional distribution of the future system state given its current state. To this end, we leverage the framework of stochastic interpolants, which facilitates the construction of a generative model between an arbitrary base distribution and the target. We design a fictitious, non-physical stochastic dynamics that takes as initial condition the current system state and produces as output a sample from the target conditional distribution in finite time and without bias. This process therefore maps a point mass centered at the current state onto a probabilistic ensemble of forecasts. We prove that the drift coefficient entering the stochastic differential equation (SDE) achieving this task is non-singular, and that it can be learned efficiently by <b>square</b> <b>loss</b> regression over the time-series data. We show that the drift and the diffusion coefficients of this SDE can be adjusted after training, and that a specific choice that minimizes the impact of the estimation error gives a F"ollmer process. We highlight the utility of our approach on several complex, high-dimensional forecasting problems, including stochastically forced Navier-Stokes and video prediction on the KTH and CLEVRER datasets.</p></p class="citation"></blockquote><h3 id=3232--169286-tackling-noisy-labels-with-network-parameter-additive-decomposition-jingyi-wang-et-al-2024>(32/32 | 169/286) Tackling Noisy Labels with Network Parameter Additive Decomposition (Jingyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyi Wang, Xiaobo Xia, Long Lan, Xinghao Wu, Jun Yu, Wenjing Yang, Bo Han, Tongliang Liu. (2024)<br><strong>Tackling Noisy Labels with Network Parameter Additive Decomposition</strong><br><button class=copy-to-clipboard title="Tackling Noisy Labels with Network Parameter Additive Decomposition" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13241v1.pdf filename=2403.13241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given data with noisy labels, over-parameterized deep networks suffer overfitting mislabeled data, resulting in poor generalization. The memorization effect of deep networks shows that although the networks have the ability to memorize all noisy data, they would first memorize clean training data, and then gradually memorize mislabeled training data. A simple and effective method that exploits the memorization effect to combat noisy labels is early stopping. However, early stopping cannot distinguish the memorization of clean data and mislabeled data, resulting in the network still inevitably overfitting mislabeled data in the early training stage.In this paper, to decouple the memorization of clean data and mislabeled data, and further reduce the side effect of mislabeled data, we perform additive decomposition on network parameters. Namely, all parameters are additively decomposed into two groups, i.e., parameters $\mathbf{w}$ are decomposed as $\mathbf{w}=\bm{\sigma}+\bm{\gamma}$. Afterward, the parameters $\bm{\sigma}$ are considered to memorize clean data, while the parameters $\bm{\gamma}$ are considered to memorize mislabeled data. Benefiting from the memorization effect, the updates of the parameters $\bm{\sigma}$ are encouraged to fully memorize clean data in early training, and then discouraged with the increase of training epochs to reduce interference of mislabeled data. The updates of the parameters $\bm{\gamma}$ are the opposite. In testing, only the parameters $\bm{\sigma}$ are employed to enhance generalization. Extensive experiments on both simulated and real-world <b>benchmarks</b> confirm the superior performance of our method.</p></p class="citation"></blockquote><h2 id=csro-23>cs.RO (23)</h2><h3 id=123--170286-natural-language-as-polices-reasoning-for-coordinate-level-embodied-control-with-llms-yusuke-mikami-et-al-2024>(1/23 | 170/286) Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs (Yusuke Mikami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yusuke Mikami, Andrew Melnik, Jun Miura, Ville Hautamäki. (2024)<br><strong>Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs</strong><br><button class=copy-to-clipboard title="Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: I-2-9; I-2-7, cs-AI, cs-CL, cs-RO, cs.RO<br>Keyword Score: 66<br>Keywords: Benchmarking, Multi-modal, Simulation, Simulator, Code Generation, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13801v1.pdf filename=2403.13801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We demonstrate experimental results with <b>LLMs</b> that address robotics action planning problems. Recently, <b>LLMs</b> have been applied in robotics action planning, particularly using a <b>code</b> <b>generation</b> approach that converts complex high-level instructions into mid-level policy <b>codes.</b> <b>In</b> contrast, our approach acquires text descriptions of the task and scene objects, then formulates action planning through natural language <b>reasoning,</b> and outputs coordinate level control commands, thus reducing the necessity for intermediate representation <b>code</b> <b>as</b> policies. Our approach is evaluated on a <b>multi-modal</b> <b>prompt</b> <b>simulation</b> <b>benchmark,</b> demonstrating that our <b>prompt</b> engineering experiments with natural language <b>reasoning</b> significantly enhance success rates compared to its absence. Furthermore, our approach illustrates the potential for natural language descriptions to transfer robotics skills from known tasks to previously unseen tasks.</p></p class="citation"></blockquote><h3 id=223--171286-waypoint-based-reinforcement-learning-for-robot-manipulation-tasks-shaunak-a-mehta-et-al-2024>(2/23 | 171/286) Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks (Shaunak A. Mehta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaunak A. Mehta, Soheil Habibian, Dylan P. Losey. (2024)<br><strong>Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks</strong><br><button class=copy-to-clipboard title="Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 53<br>Keywords: Bandit Algorithm, Benchmarking, Markov Decision Process, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13281v1.pdf filename=2403.13281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robot arms should be able to learn new tasks. One framework here is <b>reinforcement</b> <b>learning,</b> where the robot is given a reward function that encodes the task, and the robot autonomously learns actions to maximize its reward. Existing approaches to <b>reinforcement</b> <b>learning</b> often frame this problem as a <b>Markov</b> <b>decision</b> <b>process,</b> and learn a policy (or a hierarchy of policies) to complete the task. These policies reason over hundreds of fine-grained actions that the robot arm needs to take: e.g., moving slightly to the right or rotating the end-effector a few degrees. But the manipulation tasks that we want robots to perform can often be broken down into a small number of high-level motions: e.g., reaching an object or turning a handle. In this paper we therefore propose a waypoint-based approach for model-free <b>reinforcement</b> <b>learning.</b> Instead of learning a low-level policy, the robot now learns a trajectory of waypoints, and then interpolates between those waypoints using existing controllers. Our key novelty is framing this waypoint-based setting as a sequence of multi-armed <b>bandits:</b> each <b>bandit</b> problem corresponds to one waypoint along the robot&rsquo;s motion. We theoretically show that an ideal solution to this reformulation has lower regret bounds than standard frameworks. We also introduce an approximate posterior sampling solution that builds the robot&rsquo;s motion one waypoint at a time. Results across <b>benchmark</b> <b>simulations</b> and two real-world experiments suggest that this proposed approach learns new tasks more quickly than state-of-the-art baselines. See videos here: <a href=https://youtu.be/MMEd-lYfq4Y>https://youtu.be/MMEd-lYfq4Y</a></p></p class="citation"></blockquote><h3 id=323--172286-manipose-a-comprehensive-benchmark-for-pose-aware-object-manipulation-in-robotics-qiaojun-yu-et-al-2024>(3/23 | 172/286) ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation in Robotics (Qiaojun Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiaojun Yu, Ce Hao, Junbo Wang, Wenhai Liu, Liu Liu, Yao Mu, Yang You, Hengxu Yan, Cewu Lu. (2024)<br><strong>ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation in Robotics</strong><br><button class=copy-to-clipboard title="ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation in Robotics" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 43<br>Keywords: Benchmarking, Simulation, Simulator, ChatGPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13365v1.pdf filename=2403.13365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic manipulation in everyday scenarios, especially in unstructured environments, requires skills in pose-aware object manipulation (POM), which adapts robots&rsquo; grasping and handling according to an object&rsquo;s 6D pose. Recognizing an object&rsquo;s position and orientation is crucial for effective manipulation. For example, if a mug is lying on its side, it&rsquo;s more effective to grasp it by the rim rather than the handle. Despite its importance, research in POM skills remains limited, because learning manipulation skills requires pose-varying <b>simulation</b> environments and datasets. This paper introduces ManiPose, a pioneering <b>benchmark</b> designed to advance the study of pose-varying manipulation tasks. ManiPose encompasses: 1) <b>Simulation</b> environments for POM feature tasks ranging from 6D pose-specific pick-and-place of single objects to cluttered scenes, further including interactions with articulated objects. 2) A comprehensive dataset featuring geometrically consistent and manipulation-oriented 6D pose labels for 2936 real-world scanned rigid objects and 100 articulated objects across 59 categories. 3) A baseline for POM, leveraging the inferencing abilities of <b>LLM</b> (e.g., <b>ChatGPT)</b> to analyze the relationship between 6D pose and task-specific requirements, offers enhanced pose-aware grasp prediction and motion planning capabilities. Our <b>benchmark</b> demonstrates notable advancements in pose estimation, pose-aware manipulation, and real-robot skill transfer, setting new standards for POM research. We will open-source the ManiPose <b>benchmark</b> with the final version paper, inviting the community to engage with our resources, available at our website:https://sites.google.com/view/manipose.</p></p class="citation"></blockquote><h3 id=423--173286-loss-regularizing-robotic-terrain-classification-shakti-deo-kumar-et-al-2024>(4/23 | 173/286) Loss Regularizing Robotic Terrain Classification (Shakti Deo Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shakti Deo Kumar, Sudhanshu Tripathi, Krishna Ujjwal, Sarvada Sakshi Jha, Suddhasil De. (2024)<br><strong>Loss Regularizing Robotic Terrain Classification</strong><br><button class=copy-to-clipboard title="Loss Regularizing Robotic Terrain Classification" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Convolution, Supervised Learning, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13695v1.pdf filename=2403.13695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Locomotion mechanics of legged robots are suitable when pacing through difficult terrains. Recognising terrains for such robots are important to fully yoke the versatility of their movements. Consequently, robotic terrain classification becomes significant to classify terrains in real time with high accuracy. The conventional classifiers suffer from overfitting problem, low accuracy problem, high variance problem, and not suitable for live dataset. On the other hand, classifying a growing dataset is difficult for <b>convolution</b> based terrain classification. <b>Supervised</b> recurrent models are also not practical for this classification. Further, the existing recurrent architectures are still evolving to improve accuracy of terrain classification based on live variable-length sensory data collected from legged robots. This paper proposes a new semi-supervised method for terrain classification of legged robots, avoiding preprocessing of <b>long</b> <b>variable-length</b> <b>dataset.</b> <b>The</b> proposed method has a stacked <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>architecture,</b> including a new loss regularization. The proposed method solves the existing problems and improves accuracy. Comparison with the existing architectures show the improvements.</p></p class="citation"></blockquote><h3 id=523--174286-clipswarm-generating-drone-shows-from-text-prompts-with-vision-language-models-pablo-pueyo-et-al-2024>(5/23 | 174/286) CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language Models (Pablo Pueyo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pablo Pueyo, Eduardo Montijano, Ana C. Murillo, Mac Schwager. (2024)<br><strong>CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language Models</strong><br><button class=copy-to-clipboard title="CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language Models" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13467v1.pdf filename=2403.13467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces CLIPSwarm, a new algorithm designed to automate the modeling of swarm drone formations based on natural language. The algorithm begins by enriching a provided word, to compose a text <b>prompt</b> that serves as input to an iterative approach to find the formation that best matches the provided word. The algorithm iteratively refines formations of robots to align with the textual description, employing different steps for &ldquo;exploration&rdquo; and &ldquo;exploitation&rdquo;. Our framework is currently evaluated on simple formation targets, limited to contour shapes. A formation is visually represented through alpha-shape contours and the most representative color is automatically found for the input word. To measure the similarity between the description and the visual representation of the formation, we use CLIP [1], encoding text and images into vectors and assessing their similarity. Subsequently, the algorithm rearranges the formation to visually represent the word more effectively, within the given constraints of available drones. Control actions are then assigned to the drones, ensuring robotic behavior and collision-free movement. Experimental results demonstrate the system&rsquo;s efficacy in accurately modeling robot formations from natural language descriptions. The algorithm&rsquo;s versatility is showcased through the execution of drone shows in photorealistic <b>simulation</b> with varying shapes. We refer the reader to the supplementary video for a visual reference of the results.</p></p class="citation"></blockquote><h3 id=623--175286-germ-a-generalist-robotic-model-with-mixture-of-experts-for-quadruped-robot-wenxuan-song-et-al-2024>(6/23 | 175/286) GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot (Wenxuan Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenxuan Song, Han Zhao, Pengxiang Ding, Can Cui, Shangke Lyu, Yaning Fan, Donglin Wang. (2024)<br><strong>GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot</strong><br><button class=copy-to-clipboard title="GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Multi-modal, Offline Reinforcement Learning, Reinforcement Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13358v1.pdf filename=2403.13358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-task robot learning holds significant importance in tackling diverse and complex scenarios. However, current approaches are hindered by performance issues and difficulties in collecting training datasets. In this paper, we propose GeRM (Generalist Robotic Model). We utilize <b>offline</b> <b>reinforcement</b> <b>learning</b> to optimize data utilization strategies to learn from both demonstrations and sub-optimal data, thus surpassing the limitations of human demonstrations. Thereafter, we employ a <b>transformer-based</b> VLA network to process <b>multi-modal</b> inputs and output actions. By introducing the Mixture-of-Experts structure, GeRM allows faster inference speed with higher whole model capacity, and thus resolves the issue of limited RL parameters, enhancing model performance in multi-task learning while controlling computational costs. Through a series of experiments, we demonstrate that GeRM outperforms other methods across all tasks, while also validating its efficiency in both training and inference processes. Additionally, we uncover its potential to acquire emergent skills. Additionally, we contribute the QUARD-Auto dataset, collected automatically to support our training approach and foster advancements in multi-task quadruped robot learning. This work presents a new paradigm for reducing the cost of collecting robot data and driving progress in the multi-task learning community.</p></p class="citation"></blockquote><h3 id=723--176286-visual-imitation-learning-of-task-oriented-object-grasping-and-rearrangement-yichen-cai-et-al-2024>(7/23 | 176/286) Visual Imitation Learning of Task-Oriented Object Grasping and Rearrangement (Yichen Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichen Cai, Jianfeng Gao, Christoph Pohl, Tamim Asfour. (2024)<br><strong>Visual Imitation Learning of Task-Oriented Object Grasping and Rearrangement</strong><br><button class=copy-to-clipboard title="Visual Imitation Learning of Task-Oriented Object Grasping and Rearrangement" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Few-shot, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14000v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14000v1.pdf filename=2403.14000v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Task-oriented object grasping and rearrangement are critical skills for robots to accomplish different real-world manipulation tasks. However, they remain challenging due to partial observations of the objects and shape variations in categorical objects. In this paper, we propose the Multi-feature Implicit Model (MIMO), a novel object representation that encodes multiple spatial features between a point and an object in an implicit neural field. Training such a model on multiple features ensures that it embeds the object shapes consistently in different aspects, thus improving its performance in object shape reconstruction from partial observation, shape similarity measure, and modeling spatial relations between objects. Based on MIMO, we propose a framework to learn task-oriented object grasping and rearrangement from single or multiple human demonstration videos. The evaluations in <b>simulation</b> show that our approach outperforms the state-of-the-art methods for multi- and single-view observations. Real-world experiments demonstrate the efficacy of our approach in one- and <b>few-shot</b> imitation learning of manipulation tasks.</p></p class="citation"></blockquote><h3 id=823--177286-a-contact-model-based-on-denoising-diffusion-to-learn-variable-impedance-control-for-contact-rich-manipulation-masashi-okada-et-al-2024>(8/23 | 177/286) A Contact Model based on Denoising Diffusion to Learn Variable Impedance Control for Contact-rich Manipulation (Masashi Okada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masashi Okada, Mayumi Komatsu, Tadahiro Taniguchi. (2024)<br><strong>A Contact Model based on Denoising Diffusion to Learn Variable Impedance Control for Contact-rich Manipulation</strong><br><button class=copy-to-clipboard title="A Contact Model based on Denoising Diffusion to Learn Variable Impedance Control for Contact-rich Manipulation" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Diffusion Model, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13221v1.pdf filename=2403.13221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, a novel approach is proposed for learning robot control in contact-rich tasks such as wiping, by developing <b>Diffusion</b> <b>Contact</b> Model (DCM). Previous methods of learning such tasks relied on impedance control with time-varying stiffness tuning by performing Bayesian optimization by trial-and-error with robots. The proposed approach aims to reduce the cost of robot operation by predicting the robot contact trajectories from the variable stiffness inputs and using neural models. However, contact dynamics are inherently highly nonlinear, and their <b>simulation</b> requires iterative computations such as convex optimization. Moreover, approximating such computations by using finite-layer neural models is difficult. To overcome these limitations, the proposed DCM used the denoising <b>diffusion</b> <b>models</b> that could simulate the complex dynamics via iterative computations of multi-step denoising, thus improving the prediction accuracy. Stiffness tuning experiments conducted in simulated and real environments showed that the DCM achieved comparable performance to a conventional robot-based optimization method while reducing the number of robot trials.</p></p class="citation"></blockquote><h3 id=923--178286-embedding-pose-graph-enabling-3d-foundation-model-capabilities-with-a-compact-representation-hugues-thomas-et-al-2024>(9/23 | 178/286) Embedding Pose Graph, Enabling 3D Foundation Model Capabilities with a Compact Representation (Hugues Thomas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hugues Thomas, Jian Zhang. (2024)<br><strong>Embedding Pose Graph, Enabling 3D Foundation Model Capabilities with a Compact Representation</strong><br><button class=copy-to-clipboard title="Embedding Pose Graph, Enabling 3D Foundation Model Capabilities with a Compact Representation" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Graph, Foundation Model, Disambiguation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13777v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13777v1.pdf filename=2403.13777v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the Embedding Pose <b>Graph</b> (EPG), an innovative method that combines the strengths of <b>foundation</b> <b>models</b> with a simple 3D representation suitable for robotics applications. Addressing the need for efficient spatial understanding in robotics, EPG provides a compact yet powerful approach by attaching <b>foundation</b> <b>model</b> features to the nodes of a pose <b>graph.</b> Unlike traditional methods that rely on bulky data formats like voxel grids or point clouds, EPG is lightweight and scalable. It facilitates a range of robotic tasks, including open-vocabulary querying, <b>disambiguation,</b> image-based querying, language-directed navigation, and re-localization in 3D environments. We showcase the effectiveness of EPG in handling these tasks, demonstrating its capacity to improve how robots interact with and navigate through complex spaces. Through both qualitative and quantitative assessments, we illustrate EPG&rsquo;s strong performance and its ability to outperform existing methods in re-localization. Our work introduces a crucial step forward in enabling robots to efficiently understand and operate within large-scale 3D spaces.</p></p class="citation"></blockquote><h3 id=1023--179286-open-access-nao-oan-a-ros2-based-software-framework-for-hri-applications-with-the-nao-robot-antonio-bono-et-al-2024>(10/23 | 179/286) Open Access NAO (OAN): a ROS2-based software framework for HRI applications with the NAO robot (Antonio Bono et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonio Bono, Kenji Brameld, Luigi D&rsquo;Alfonso, Giuseppe Fedele. (2024)<br><strong>Open Access NAO (OAN): a ROS2-based software framework for HRI applications with the NAO robot</strong><br><button class=copy-to-clipboard title="Open Access NAO (OAN): a ROS2-based software framework for HRI applications with the NAO robot" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: GPT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13960v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13960v1.pdf filename=2403.13960v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a new software framework for HRI experimentation with the sixth version of the common NAO robot produced by the United Robotics Group. Embracing the common demand of researchers for better performance and new features for NAO, the authors took advantage of the ability to run ROS2 onboard on the NAO to develop a framework independent of the APIs provided by the manufacturer. Such a system provides NAO with not only the basic skills of a humanoid robot such as walking and reproducing movements of interest but also features often used in HRI such as: speech recognition/synthesis, face and object detention, and the use of Generative Pre-trained <b>Transformer</b> <b>(GPT)</b> models for conversation. The developed code is therefore configured as a ready-to-use but also highly expandable and improvable tool thanks to the possibilities provided by the ROS community.</p></p class="citation"></blockquote><h3 id=1123--180286-a-convex-formulation-of-frictional-contact-for-the-material-point-method-and-rigid-bodies-zeshun-zong-et-al-2024>(11/23 | 180/286) A Convex Formulation of Frictional Contact for the Material Point Method and Rigid Bodies (Zeshun Zong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeshun Zong, Chenfanfu Jiang, Xuchen Han. (2024)<br><strong>A Convex Formulation of Frictional Contact for the Material Point Method and Rigid Bodies</strong><br><button class=copy-to-clipboard title="A Convex Formulation of Frictional Contact for the Material Point Method and Rigid Bodies" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13783v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13783v2.pdf filename=2403.13783v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a novel convex formulation that seamlessly integrates the Material Point Method (MPM) with articulated rigid body dynamics in frictional contact scenarios. We extend the linear corotational hyperelastic model into the realm of elastoplasticity and include an efficient return mapping algorithm. This approach is particularly effective for MPM <b>simulations</b> involving significant deformation and topology changes, while preserving the convexity of the optimization problem. Our method ensures global convergence, enabling the use of large <b>simulation</b> time steps without compromising robustness. We have validated our approach through rigorous testing and performance evaluations, highlighting its superior capabilities in managing complex <b>simulations</b> relevant to robotics. Compared to previous MPM based robotic simulators, our method significantly improves the stability of contact resolution &ndash; a critical factor in robot manipulation tasks. We make our method available in the open-source robotics toolkit, Drake.</p></p class="citation"></blockquote><h3 id=1223--181286-what-matters-for-active-texture-recognition-with-vision-based-tactile-sensors-alina-böhm-et-al-2024>(12/23 | 181/286) What Matters for Active Texture Recognition With Vision-Based Tactile Sensors (Alina Böhm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alina Böhm, Tim Schneider, Boris Belousov, Alap Kshirsagar, Lisa Lin, Katja Doerschner, Knut Drewing, Constantin A. Rothkopf, Jan Peters. (2024)<br><strong>What Matters for Active Texture Recognition With Vision-Based Tactile Sensors</strong><br><button class=copy-to-clipboard title="What Matters for Active Texture Recognition With Vision-Based Tactile Sensors" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Data Augmentation, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13701v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13701v1.pdf filename=2403.13701v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores active sensing strategies that employ vision-based tactile sensors for robotic perception and classification of fabric textures. We formalize the active sampling problem in the context of tactile fabric recognition and provide an implementation of information-theoretic exploration strategies based on minimizing predictive entropy and variance of <b>probabilistic</b> <b>models.</b> Through ablation studies and human experiments, we investigate which components are crucial for quick and reliable texture recognition. Along with the active sampling strategies, we evaluate neural network architectures, representations of uncertainty, influence of <b>data</b> <b>augmentation,</b> and dataset variability. By evaluating our method on a previously published Active Clothing Perception Dataset and on a real robotic system, we establish that the choice of the active exploration strategy has only a minor influence on the recognition accuracy, whereas <b>data</b> <b>augmentation</b> and dropout rate play a significantly larger role. In a comparison study, while humans achieve 66.9% recognition accuracy, our best approach reaches 90.0% in under 5 touches, highlighting that vision-based tactile sensors are highly effective for fabric texture recognition.</p></p class="citation"></blockquote><h3 id=1323--182286-reward-driven-automated-curriculum-learning-for-interaction-aware-self-driving-at-unsignalized-intersections-zengqi-peng-et-al-2024>(13/23 | 182/286) Reward-Driven Automated Curriculum Learning for Interaction-Aware Self-Driving at Unsignalized Intersections (Zengqi Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zengqi Peng, Xiao Zhou, Lei Zheng, Yubin Wang, Jun Ma. (2024)<br><strong>Reward-Driven Automated Curriculum Learning for Interaction-Aware Self-Driving at Unsignalized Intersections</strong><br><button class=copy-to-clipboard title="Reward-Driven Automated Curriculum Learning for Interaction-Aware Self-Driving at Unsignalized Intersections" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Curriculum Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13674v1.pdf filename=2403.13674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present a reward-driven automated <b>curriculum</b> <b>reinforcement</b> <b>learning</b> approach for interaction-aware self-driving at unsignalized intersections, taking into account the uncertainties associated with surrounding vehicles (SVs). These uncertainties encompass the uncertainty of SVs&rsquo; driving intention and also the quantity of SVs. To deal with this problem, the <b>curriculum</b> <b>set</b> is specifically designed to accommodate a progressively increasing number of SVs. By implementing an automated <b>curriculum</b> <b>selection</b> mechanism, the importance weights are rationally allocated across various curricula, thereby facilitating improved sample efficiency and training outcomes. Furthermore, the reward function is meticulously designed to guide the agent towards effective policy exploration. Thus the proposed framework could proactively address the above uncertainties at unsignalized intersections by employing the automated <b>curriculum</b> <b>learning</b> technique that progressively increases task difficulty, and this ensures safe self-driving through effective interaction with SVs. Comparative experiments are conducted in $Highway_Env$, and the results indicate that our approach achieves the highest task success rate, attains strong robustness to initialization parameters of the <b>curriculum</b> <b>selection</b> module, and exhibits superior adaptability to diverse situational configurations at unsignalized intersections. Furthermore, the effectiveness of the proposed method is validated using the high-fidelity CARLA simulator.</p></p class="citation"></blockquote><h3 id=1423--183286-centroidal-state-estimation-based-on-the-koopman-embedding-for-dynamic-legged-locomotion-shahram-khorshidi-et-al-2024>(14/23 | 183/286) Centroidal State Estimation based on the Koopman Embedding for Dynamic Legged Locomotion (Shahram Khorshidi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shahram Khorshidi, Murad Dawood, Maren Bennewitz. (2024)<br><strong>Centroidal State Estimation based on the Koopman Embedding for Dynamic Legged Locomotion</strong><br><button class=copy-to-clipboard title="Centroidal State Estimation based on the Koopman Embedding for Dynamic Legged Locomotion" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13366v1.pdf filename=2403.13366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a novel approach to centroidal state estimation, which plays a crucial role in predictive model-based control strategies for dynamic legged locomotion. Our approach uses the Koopman operator theory to transform the robot&rsquo;s complex nonlinear dynamics into a linear system, by employing dynamic mode decomposition and deep learning for model construction. We evaluate both models on their linearization accuracy and capability to capture both fast and slow dynamic system responses. We then select the most suitable model for estimation purposes, and integrate it within a moving horizon estimator. This estimator is formulated as a convex quadratic program, to facilitate robust, real-time centroidal state estimation. Through extensive <b>simulation</b> experiments on a quadruped robot executing various dynamic gaits, our data-driven framework outperforms conventional filtering techniques based on nonlinear dynamics. Our estimator addresses challenges posed by force/torque measurement noise in highly dynamic motions and accurately recovers the centroidal states, demonstrating the adaptability and effectiveness of the Koopman-based linear representation for complex locomotive behaviors. Importantly, our model based on dynamic mode decomposition, trained with two locomotion patterns (trot and jump), successfully estimates the centroidal states for a different motion (bound) without retraining.</p></p class="citation"></blockquote><h3 id=1523--184286-robotics-meets-fluid-dynamics-a-characterization-of-the-induced-airflow-around-a-quadrotor-leonard-bauersfeld-et-al-2024>(15/23 | 184/286) Robotics meets Fluid Dynamics: A Characterization of the Induced Airflow around a Quadrotor (Leonard Bauersfeld et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonard Bauersfeld, Koen Muller, Dominic Ziegler, Filippo Coletti, Davide Scaramuzza. (2024)<br><strong>Robotics meets Fluid Dynamics: A Characterization of the Induced Airflow around a Quadrotor</strong><br><button class=copy-to-clipboard title="Robotics meets Fluid Dynamics: A Characterization of the Induced Airflow around a Quadrotor" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13321v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13321v1.pdf filename=2403.13321v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread adoption of quadrotors for diverse applications, from agriculture to public safety, necessitates an understanding of the aerodynamic disturbances they create. This paper introduces a computationally lightweight model for estimating the time-averaged magnitude of the induced flow below quadrotors in hover. Unlike related approaches that rely on expensive computational fluid dynamics (CFD) <b>simulations</b> or time-consuming empirical measurements, our method leverages classical theory from turbulent flows. By analyzing over 9 hours of flight data from drones of varying sizes within a large motion capture system, we show that the combined flow from all propellers of the drone is well-approximated by a turbulent jet. Through the use of a novel normalization and scaling, we have developed and experimentally validated a unified model that describes the mean velocity field of the induced flow for different drone sizes. The model accurately describes the far-field airflow in a very large volume below the drone which is difficult to simulate in CFD. Our model, which requires only the drone&rsquo;s mass, propeller size, and drone size for calculations, offers a practical tool for dynamic planning in multi-agent scenarios, ensuring safer operations near humans and optimizing sensor placements.</p></p class="citation"></blockquote><h3 id=1623--185286-look-before-you-leap-socially-acceptable-high-speed-ground-robot-navigation-in-crowded-hallways-lakshay-sharma-et-al-2024>(16/23 | 185/286) Look Before You Leap: Socially Acceptable High-Speed Ground Robot Navigation in Crowded Hallways (Lakshay Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lakshay Sharma, Jonathan P. How. (2024)<br><strong>Look Before You Leap: Socially Acceptable High-Speed Ground Robot Navigation in Crowded Hallways</strong><br><button class=copy-to-clipboard title="Look Before You Leap: Socially Acceptable High-Speed Ground Robot Navigation in Crowded Hallways" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13284v1.pdf filename=2403.13284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To operate safely and efficiently, autonomous warehouse/delivery robots must be able to accomplish tasks while navigating in dynamic environments and handling the large uncertainties associated with the motions/behaviors of other robots and/or humans. A key scenario in such environments is the hallway problem, where robots must operate in the same narrow corridor as human traffic going in one or both directions. Traditionally, robot planners have tended to focus on socially acceptable behavior in the hallway scenario at the expense of performance. This paper proposes a planner that aims to address the consequent &ldquo;robot freezing problem&rdquo; in hallways by allowing for &ldquo;peek-and-pass&rdquo; maneuvers. We then go on to demonstrate in <b>simulation</b> how this planner improves robot time to goal without violating social norms. Finally, we show initial hardware demonstrations of this planner in the real world.</p></p class="citation"></blockquote><h3 id=1723--186286-a-rule-compliance-path-planner-for-lane-merge-scenarios-based-on-responsibility-sensitive-safety-pengfei-lin-et-al-2024>(17/23 | 186/286) A Rule-Compliance Path Planner for Lane-Merge Scenarios Based on Responsibility-Sensitive Safety (Pengfei Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengfei Lin, Ehsan Javanmardi, Yuze Jiang, Manabu Tsukada. (2024)<br><strong>A Rule-Compliance Path Planner for Lane-Merge Scenarios Based on Responsibility-Sensitive Safety</strong><br><button class=copy-to-clipboard title="A Rule-Compliance Path Planner for Lane-Merge Scenarios Based on Responsibility-Sensitive Safety" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13251v1.pdf filename=2403.13251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lane merging is one of the critical tasks for self-driving cars, and how to perform lane-merge maneuvers effectively and safely has become one of the important standards in measuring the capability of autonomous driving systems. However, due to the ambiguity in driving intentions and right-of-way issues, the lane merging process in autonomous driving remains deficient in terms of maintaining or ceding the right-of-way and attributing liability, which could result in protracted durations for merging and problems such as trajectory oscillation. Hence, we present a rule-compliance path planner (RCPP) for lane-merge scenarios, which initially employs the extended responsibility-sensitive safety (RSS) to elucidate the right-of-way, followed by the potential field-based sigmoid planner for path generation. In the <b>simulation,</b> we have validated the efficacy of the proposed algorithm. The algorithm demonstrated superior performance over previous approaches in aspects such as merging time (Saved 72.3%), path length (reduced 53.4%), and eliminating the trajectory oscillation.</p></p class="citation"></blockquote><h3 id=1823--187286-workload-estimation-for-unknown-tasks-a-survey-of-machine-learning-under-distribution-shift-josh-bhagat-smith-et-al-2024>(18/23 | 187/286) Workload Estimation for Unknown Tasks: A Survey of Machine Learning Under Distribution Shift (Josh Bhagat Smith et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Josh Bhagat Smith, Julie A. Adams. (2024)<br><strong>Workload Estimation for Unknown Tasks: A Survey of Machine Learning Under Distribution Shift</strong><br><button class=copy-to-clipboard title="Workload Estimation for Unknown Tasks: A Survey of Machine Learning Under Distribution Shift" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-HC, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13318v1.pdf filename=2403.13318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human-robot teams involve humans and robots collaborating to achieve tasks under various environmental conditions. Successful teaming will require robots to adapt autonomously to a human teammate&rsquo;s internal state. An important element of such adaptation is the ability to estimate the human teammates&rsquo; workload in unknown situations. Existing workload models use machine learning to model the relationships between physiological metrics and workload; however, these methods are susceptible to individual differences and are heavily influenced by other factors. These methods cannot generalize to unknown tasks, as they rely on standard machine learning approaches that assume data consists of independent and identically distributed (IID) samples. This assumption does not necessarily hold for estimating workload for new tasks. A survey of non-IID machine learning techniques is presented, where commonly used techniques are evaluated using three criteria: portability, model complexity, and adaptability. These criteria are used to argue which techniques are most applicable for estimating workload for unknown tasks in dynamic, real-time environments.</p></p class="citation"></blockquote><h3 id=1923--188286-amco-adaptive-multimodal-coupling-of-vision-and-proprioception-for-quadruped-robot-navigation-in-outdoor-environments-mohamed-elnoor-et-al-2024>(19/23 | 188/286) AMCO: Adaptive Multimodal Coupling of Vision and Proprioception for Quadruped Robot Navigation in Outdoor Environments (Mohamed Elnoor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Elnoor, Kasun Weerakoon, Adarsh Jagan Sathyamoorthy, Tianrui Guan, Vignesh Rajagopal, Dinesh Manocha. (2024)<br><strong>AMCO: Adaptive Multimodal Coupling of Vision and Proprioception for Quadruped Robot Navigation in Outdoor Environments</strong><br><button class=copy-to-clipboard title="AMCO: Adaptive Multimodal Coupling of Vision and Proprioception for Quadruped Robot Navigation in Outdoor Environments" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13235v1.pdf filename=2403.13235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present AMCO, a novel navigation method for quadruped robots that adaptively combines vision-based and proprioception-based perception capabilities. Our approach uses three cost maps: general knowledge map; traversability history map; and current proprioception map; which are derived from a robot&rsquo;s vision and proprioception data, and couples them to obtain a coupled traversability cost map for navigation. The general knowledge map encodes terrains semantically segmented from visual sensing, and represents a terrain&rsquo;s typically expected traversability. The traversability history map encodes the robot&rsquo;s recent proprioceptive measurements on a terrain and its semantic segmentation as a cost map. Further, the robot&rsquo;s present proprioceptive measurement is encoded as a cost map in the current proprioception map. As the general knowledge map and traversability history map rely on semantic segmentation, we evaluate the reliability of the visual sensory data by estimating the brightness and motion blur of input RGB images and accordingly combine the three cost maps to obtain the coupled traversability cost map used for navigation. Leveraging this adaptive coupling, the robot can depend on the most reliable input modality available. Finally, we present a novel planner that selects appropriate gaits and velocities for traversing challenging outdoor environments using the coupled traversability cost map. We demonstrate AMCO&rsquo;s navigation performance in different real-world outdoor environments and observe 10.8%-34.9% reduction w.r.t. two stability metrics, and up to 50% improvement in terms of success rate compared to current navigation methods.</p></p class="citation"></blockquote><h3 id=2023--189286-policed-rl-learning-closed-loop-robot-control-policies-with-provable-satisfaction-of-hard-constraints-jean-baptiste-bouvier-et-al-2024>(20/23 | 189/286) POLICEd RL: Learning Closed-Loop Robot Control Policies with Provable Satisfaction of Hard Constraints (Jean-Baptiste Bouvier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jean-Baptiste Bouvier, Kartik Nagpal, Negar Mehr. (2024)<br><strong>POLICEd RL: Learning Closed-Loop Robot Control Policies with Provable Satisfaction of Hard Constraints</strong><br><button class=copy-to-clipboard title="POLICEd RL: Learning Closed-Loop Robot Control Policies with Provable Satisfaction of Hard Constraints" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13297v1.pdf filename=2403.13297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we seek to learn a robot policy guaranteed to satisfy state constraints. To encourage constraint satisfaction, existing RL algorithms typically rely on Constrained Markov Decision Processes and discourage constraint violations through reward shaping. However, such soft constraints cannot offer verifiable safety guarantees. To address this gap, we propose POLICEd RL, a novel RL algorithm explicitly designed to enforce affine hard constraints in closed-loop with a <b>black-box</b> <b>environment.</b> Our key insight is to force the learned policy to be affine around the unsafe set and use this affine region as a repulsive buffer to prevent trajectories from violating the constraint. We prove that such policies exist and guarantee constraint satisfaction. Our proposed framework is applicable to both systems with continuous and discrete state and action spaces and is agnostic to the choice of the RL training algorithm. Our results demonstrate the capacity of POLICEd RL to enforce hard constraints in robotic tasks while significantly outperforming existing methods.</p></p class="citation"></blockquote><h3 id=2123--190286-dba-fusion-tightly-integrating-deep-dense-visual-bundle-adjustment-with-multiple-sensors-for-large-scale-localization-and-mapping-yuxuan-zhou-et-al-2024>(21/23 | 190/286) DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with Multiple Sensors for Large-Scale Localization and Mapping (Yuxuan Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Zhou, Xingxing Li, Shengyu Li, Xuanbin Wang, Shaoquan Feng, Yuxuan Tan. (2024)<br><strong>DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with Multiple Sensors for Large-Scale Localization and Mapping</strong><br><button class=copy-to-clipboard title="DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with Multiple Sensors for Large-Scale Localization and Mapping" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13714v1.pdf filename=2403.13714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual simultaneous localization and mapping (VSLAM) has broad applications, with state-of-the-art methods leveraging deep neural networks for better robustness and applicability. However, there is a lack of research in fusing these learning-based methods with multi-sensor information, which could be indispensable to push related applications to large-scale and complex scenarios. In this paper, we tightly integrate the trainable deep dense bundle adjustment (DBA) with multi-sensor information through a factor <b>graph.</b> In the framework, recurrent optical flow and DBA are performed among sequential images. The Hessian information derived from DBA is fed into a generic factor <b>graph</b> for multi-sensor fusion, which employs a sliding window and supports probabilistic marginalization. A pipeline for visual-inertial integration is firstly developed, which provides the minimum ability of metric-scale localization and mapping. Furthermore, other sensors (e.g., global navigation satellite system) are integrated for driftless and geo-referencing functionality. Extensive tests are conducted on both public datasets and self-collected datasets. The results validate the superior localization performance of our approach, which enables real-time dense mapping in large-scale environments. The code has been made open-source (<a href=https://github.com/GREAT-WHU/DBA-Fusion)>https://github.com/GREAT-WHU/DBA-Fusion)</a>.</p></p class="citation"></blockquote><h3 id=2223--191286-lace-lhmp-airflow-modelling-inspired-long-term-human-motion-prediction-by-enhancing-laminar-characteristics-in-human-flow-yufei-zhu-et-al-2024>(22/23 | 191/286) LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction By Enhancing Laminar Characteristics in Human Flow (Yufei Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufei Zhu, Han Fan, Andrey Rudenko, Martin Magnusson, Erik Schaffernicht, Achim J. Lilienthal. (2024)<br><strong>LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction By Enhancing Laminar Characteristics in Human Flow</strong><br><button class=copy-to-clipboard title="LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction By Enhancing Laminar Characteristics in Human Flow" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13640v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13640v1.pdf filename=2403.13640v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long-term human motion prediction (LHMP) is essential for safely operating autonomous robots and vehicles in populated environments. It is fundamental for various applications, including motion planning, tracking, human-robot interaction and safety monitoring. However, accurate prediction of human trajectories is challenging due to complex factors, including, for example, social norms and environmental conditions. The influence of such factors can be captured through Maps of Dynamics (MoDs), which encode spatial motion patterns learned from (possibly scattered and partial) past observations of motion in the environment and which can be used for data-efficient, interpretable motion prediction (MoD-LHMP). To address the limitations of prior work, especially regarding accuracy and sensitivity to anomalies in long-term prediction, we propose the Laminar Component Enhanced LHMP approach (LaCE-LHMP). Our approach is inspired by data-driven airflow modelling, which estimates laminar and turbulent flow components and uses predominantly the laminar components to make flow predictions. Based on the hypothesis that human trajectory patterns also manifest laminar flow (that represents predictable motion) and turbulent flow components (that reflect more unpredictable and arbitrary motion), LaCE-LHMP extracts the laminar patterns in human dynamics and uses them for human motion prediction. We demonstrate the superior prediction performance of LaCE-LHMP through <b>benchmark</b> comparisons with state-of-the-art LHMP methods, offering an unconventional perspective and a more intuitive understanding of human movement patterns.</p></p class="citation"></blockquote><h3 id=2323--192286-enhancing-security-in-multi-robot-systems-through-co-observation-planning-reachability-analysis-and-network-flow-ziqi-yang-et-al-2024>(23/23 | 192/286) Enhancing Security in Multi-Robot Systems through Co-Observation Planning, Reachability Analysis, and Network Flow (Ziqi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqi Yang, Roberto Tron. (2024)<br><strong>Enhancing Security in Multi-Robot Systems through Co-Observation Planning, Reachability Analysis, and Network Flow</strong><br><button class=copy-to-clipboard title="Enhancing Security in Multi-Robot Systems through Co-Observation Planning, Reachability Analysis, and Network Flow" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-MA, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13266v1.pdf filename=2403.13266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses security challenges in multi-robot systems (MRS) where adversaries may compromise robot control, risking unauthorized access to forbidden areas. We propose a novel multi-robot optimal planning algorithm that integrates mutual observations and introduces reachability constraints for enhanced security. This ensures that, even with adversarial movements, compromised robots cannot breach forbidden regions without missing scheduled co-observations. The reachability constraint uses ellipsoidal over-approximation for efficient intersection checking and gradient computation. To enhance system resilience and tackle feasibility challenges, we also introduce sub-teams. These cohesive units replace individual robot assignments along each route, enabling redundant robots to deviate for co-observations across different trajectories, securing multiple sub-teams without requiring modifications. We formulate the cross-trajectory co-observation plan by solving a network flow coverage problem on the checkpoint <b>graph</b> generated from the original unsecured MRS trajectories, providing the same security guarantees against plan-deviation attacks. We demonstrate the effectiveness and robustness of our proposed algorithm, which significantly strengthens the security of multi-robot systems in the face of adversarial threats.</p></p class="citation"></blockquote><h2 id=csai-5>cs.AI (5)</h2><h3 id=15--193286-polaris-a-safety-focused-llm-constellation-architecture-for-healthcare-subhabrata-mukherjee-et-al-2024>(1/5 | 193/286) Polaris: A Safety-focused LLM Constellation Architecture for Healthcare (Subhabrata Mukherjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subhabrata Mukherjee, Paul Gamble, Markel Sanz Ausin, Neel Kant, Kriti Aggarwal, Neha Manjunath, Debajyoti Datta, Zhengliang Liu, Jiayuan Ding, Sophia Busacca, Cezanne Bianco, Swapnil Sharma, Rae Lasko, Michelle Voisard, Sanchay Harneja, Darya Filippova, Gerry Meixiong, Kevin Cha, Amir Youssefi, Meyhaa Buvanesh, Howard Weingram, Sebastian Bierman-Lytle, Harpreet Singh Mangat, Kim Parikh, Saad Godil, Alex Miller. (2024)<br><strong>Polaris: A Safety-focused LLM Constellation Architecture for Healthcare</strong><br><button class=copy-to-clipboard title="Polaris: A Safety-focused LLM Constellation Architecture for Healthcare" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, LLaMA, Question Answering, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13313v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13313v1.pdf filename=2403.13313v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop Polaris, the first safety-focused <b>LLM</b> constellation for real-time patient-AI healthcare conversations. Unlike prior <b>LLM</b> works in healthcare focusing on tasks like <b>question</b> <b>answering,</b> our work specifically focuses on long multi-turn voice conversations. Our one-trillion parameter constellation system is composed of several multibillion parameter <b>LLMs</b> as co-operative agents: a stateful primary agent that focuses on driving an engaging conversation and several specialist support agents focused on healthcare tasks performed by nurses to increase safety and reduce hallucinations. We develop a sophisticated training protocol for iterative co-training of the agents that optimize for diverse objectives. We train our models on proprietary data, clinical care plans, healthcare regulatory documents, medical manuals, and other medical <b>reasoning</b> documents. We align our models to speak like medical professionals, using organic healthcare conversations and simulated ones between patient actors and experienced nurses. This allows our system to express unique capabilities such as rapport building, trust building, empathy and bedside manner. Finally, we present the first comprehensive clinician evaluation of an <b>LLM</b> system for healthcare. We recruited over 1100 U.S. licensed nurses and over 130 U.S. licensed physicians to perform end-to-end conversational evaluations of our system by posing as patients and rating the system on several measures. We demonstrate Polaris performs on par with human nurses on aggregate across dimensions such as medical safety, clinical readiness, conversational quality, and bedside manner. Additionally, we conduct a challenging task-based evaluation of the individual specialist support agents, where we demonstrate our <b>LLM</b> agents significantly outperform a much larger general-purpose <b>LLM</b> <b>(GPT-4)</b> as well as from its own medium-size class <b>(LLaMA-2</b> 70B).</p></p class="citation"></blockquote><h3 id=25--194286-hyperllava-dynamic-visual-and-language-expert-tuning-for-multimodal-large-language-models-wenqiao-zhang-et-al-2024>(2/5 | 194/286) HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models (Wenqiao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun Shu, Haoyuan Li, Lei Zhang, He Wanggui, Hao Zhou, Zheqi Lv, Hao Jiang, Juncheng Li, Siliang Tang, Yueting Zhuang. (2024)<br><strong>HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CV, cs.AI<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Instruction Tuning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13447v1.pdf filename=2403.13447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements indicate that scaling up <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) effectively enhances performance on downstream <b>multimodal</b> tasks. The prevailing MLLM paradigm, \emph{e.g.}, LLaVA, transforms visual features into text-like tokens using a \emph{static} <b>vision-language</b> mapper, thereby enabling \emph{static} <b>LLMs</b> to develop the capability to comprehend visual information through visual <b>instruction</b> <b>tuning.</b> Although promising, the \emph{static} tuning strategy~\footnote{The static tuning refers to the trained model with static parameters.} that shares the same parameters may constrain performance across different downstream <b>multimodal</b> tasks. In light of this, we introduce HyperLLaVA, which involves adaptive tuning of the projector and <b>LLM</b> parameters, in conjunction with a dynamic visual expert and language expert, respectively. These experts are derived from HyperNetworks, which generates adaptive parameter shifts through visual and language guidance, enabling dynamic projector and <b>LLM</b> modeling in two-stage training. Our experiments demonstrate that our solution significantly surpasses LLaVA on existing MLLM <b>benchmarks,</b> including MME, MMBench, SEED-Bench, and LLaVA-Bench. ~\footnote{Our project is available on the link <a href=https://github.com/DCDmllm/HyperLLaVA%7D>https://github.com/DCDmllm/HyperLLaVA}</a>.</p></p class="citation"></blockquote><h3 id=35--195286-motion-generation-from-fine-grained-textual-descriptions-kunhang-li-et-al-2024>(3/5 | 195/286) Motion Generation from Fine-grained Textual Descriptions (Kunhang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kunhang Li, Yansong Feng. (2024)<br><strong>Motion Generation from Fine-grained Textual Descriptions</strong><br><button class=copy-to-clipboard title="Motion Generation from Fine-grained Textual Descriptions" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CV, cs-RO, cs.AI<br>Keyword Score: 40<br>Keywords: GPT, GPT-3, GPT-3.5, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13518v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13518v1.pdf filename=2403.13518v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of text2motion is to generate motion sequences from given textual descriptions, where a model should explore the interactions between natural language instructions and human body movements. While most existing works are confined to coarse-grained motion descriptions (e.g., &ldquo;A man squats.&rdquo;), fine-grained ones specifying movements of relevant body parts are barely explored. Models trained with coarse texts may not be able to learn mappings from fine-grained motion-related words to motion primitives, resulting in the failure in generating motions from unseen descriptions. In this paper, we build a large-scale language-motion dataset with fine-grained textual descriptions, FineHumanML3D, by feeding <b>GPT-3.5-turbo</b> with delicate <b>prompts.</b> Accordingly, we design a new text2motion model, FineMotionDiffuse, which makes full use of fine-grained textual information. Our experiments show that FineMotionDiffuse trained on FineHumanML3D acquires good results in quantitative evaluation. We also find this model can better generate spatially/chronologically composite motions by learning the implicit mappings from simple descriptions to the corresponding basic motions.</p></p class="citation"></blockquote><h3 id=45--196286-agent-group-chat-an-interactive-group-chat-simulacra-for-better-eliciting-collective-emergent-behavior-zhouhong-gu-et-al-2024>(4/5 | 196/286) Agent Group Chat: An Interactive Group Chat Simulacra For Better Eliciting Collective Emergent Behavior (Zhouhong Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai, Hao Shen, Jiangjie Chen, Zheyu Ye, Yifei Dai, Yan Gao, Yao Hu, Hongwei Feng, Yanghua Xiao. (2024)<br><strong>Agent Group Chat: An Interactive Group Chat Simulacra For Better Eliciting Collective Emergent Behavior</strong><br><button class=copy-to-clipboard title="Agent Group Chat: An Interactive Group Chat Simulacra For Better Eliciting Collective Emergent Behavior" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CY, cs.AI<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, N-gram<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13433v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13433v1.pdf filename=2403.13433v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To investigate the role of language in human collective behaviors, we developed the Agent Group Chat <b>simulation</b> to simulate linguistic interactions among multi-agent in different settings. Agents are asked to free chat in this <b>simulation</b> for their own purposes based on their character setting, aiming to see agents exhibit emergent behaviours that are both unforeseen and significant. Four narrative scenarios, Inheritance Disputes, Law Court Debates, Philosophical Discourses, Movie Casting Contention, are integrated into Agent Group Chat to evaluate its support for diverse storylines. By configuring specific environmental settings within Agent Group Chat, we are able to assess whether agents exhibit behaviors that align with human expectations. We evaluate the disorder within the environment by computing the <b>n-gram</b> Shannon entropy of all the content speak by characters. Our findings reveal that under the premise of agents possessing substantial alignment with human expectations, facilitating more extensive information exchange within the <b>simulation</b> ensures greater orderliness amidst diversity, which leads to the emergence of more unexpected and meaningful emergent behaviors. The code is open source in <a href=https://github.com/MikeGu721/AgentGroup>https://github.com/MikeGu721/AgentGroup</a>, and online platform will be open soon.</p></p class="citation"></blockquote><h3 id=55--197286-multi-robot-connected-fermat-spiral-coverage-jingtao-tang-et-al-2024>(5/5 | 197/286) Multi-Robot Connected Fermat Spiral Coverage (Jingtao Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingtao Tang, Hang Ma. (2024)<br><strong>Multi-Robot Connected Fermat Spiral Coverage</strong><br><button class=copy-to-clipboard title="Multi-Robot Connected Fermat Spiral Coverage" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-MA, cs-RO, cs.AI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13311v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13311v1.pdf filename=2403.13311v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the Multi-Robot Connected Fermat Spiral (MCFS), a novel algorithmic framework for Multi-Robot Coverage Path Planning (MCPP) that adapts Connected Fermat Spiral (CFS) from the computer graphics community to multi-robot coordination for the first time. MCFS uniquely enables the orchestration of multiple robots to generate coverage paths that contour around arbitrarily shaped obstacles, a feature that is notably lacking in traditional methods. Our framework not only enhances area coverage and optimizes task performance, particularly in terms of makespan, for workspaces rich in irregular obstacles but also addresses the challenges of path continuity and curvature critical for non-holonomic robots by generating smooth paths without decomposing the workspace. MCFS solves MCPP by constructing a <b>graph</b> of isolines and transforming MCPP into a combinatorial optimization problem, aiming to minimize the makespan while covering all vertices. Our contributions include developing a unified CFS version for scalable and adaptable MCPP, extending it to MCPP with novel optimization techniques for cost reduction and path continuity and smoothness, and demonstrating through extensive experiments that MCFS outperforms existing MCPP methods in makespan, path curvature, coverage ratio, and overlapping ratio. Our research marks a significant step in MCPP, showcasing the fusion of computer graphics and automated planning principles to advance the capabilities of multi-robot systems in complex environments. Our code is available at <a href=https://github.com/reso1/MCFS>https://github.com/reso1/MCFS</a>.</p></p class="citation"></blockquote><h2 id=cshc-5>cs.HC (5)</h2><h3 id=15--198286-reading-users-minds-from-what-they-say-an-investigation-into-llm-based-empathic-mental-inference-qihao-zhu-et-al-2024>(1/5 | 198/286) Reading Users&rsquo; Minds from What They Say: An Investigation into LLM-based Empathic Mental Inference (Qihao Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qihao Zhu, Leah Chong, Maria Yang, Jianxi Luo. (2024)<br><strong>Reading Users&rsquo; Minds from What They Say: An Investigation into LLM-based Empathic Mental Inference</strong><br><button class=copy-to-clipboard title="Reading Users' Minds from What They Say: An Investigation into LLM-based Empathic Mental Inference" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CL, cs-HC, cs.HC<br>Keyword Score: 43<br>Keywords: Benchmarking, Zero-shot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13301v1.pdf filename=2403.13301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In human-centered design, developing a comprehensive and in-depth understanding of user experiences, i.e., empathic understanding, is paramount for designing products that truly meet human needs. Nevertheless, accurately comprehending the real underlying mental states of a <b>large</b> <b>human</b> <b>population</b> remains a significant challenge today. This difficulty mainly arises from the trade-off between depth and scale of user experience research: gaining in-depth insights from a small group of users does not easily scale to a larger population, and vice versa. This paper investigates the use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for performing mental inference tasks, specifically inferring users&rsquo; underlying goals and fundamental psychological needs (FPNs). Baseline and <b>benchmark</b> datasets were collected from human users and designers to develop an empathic accuracy metric for measuring the mental inference performance of <b>LLMs.</b> The empathic accuracy of inferring goals and FPNs of different <b>LLMs</b> with varied <b>zero-shot</b> <b>prompt</b> engineering techniques are experimented against that of human designers. Experimental results suggest that <b>LLMs</b> can infer and understand the underlying goals and FPNs of users with performance comparable to that of human designers, suggesting a promising avenue for enhancing the scalability of empathic design approaches through the integration of advanced artificial intelligence technologies. This work has the potential to significantly augment the toolkit available to designers during human-centered design, enabling the development of both <b>large-scale</b> <b>and</b> <b>in-depth</b> understanding of users&rsquo; experiences.</p></p class="citation"></blockquote><h3 id=25--199286-vcounselor-a-psychological-intervention-chat-agent-based-on-a-knowledge-enhanced-large-language-model-h-zhang-et-al-2024>(2/5 | 199/286) VCounselor: A Psychological Intervention Chat Agent Based on a Knowledge-Enhanced Large Language Model (H. Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>H. Zhang, Z. Qiao, H. Wang, B. Duan, J. Yin. (2024)<br><strong>VCounselor: A Psychological Intervention Chat Agent Based on a Knowledge-Enhanced Large Language Model</strong><br><button class=copy-to-clipboard title="VCounselor: A Psychological Intervention Chat Agent Based on a Knowledge-Enhanced Large Language Model" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: J-4, cs-AI, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Fine-tuning, ChatGPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13553v1.pdf filename=2403.13553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational artificial intelligence can already independently engage in brief conversations with clients with psychological problems and provide evidence-based psychological interventions. The main objective of this study is to improve the effectiveness and credibility of the <b>large</b> <b>language</b> <b>model</b> in psychological intervention by creating a specialized agent, the VCounselor, to address the limitations observed in popular <b>large</b> <b>language</b> <b>models</b> such as <b>ChatGPT</b> in domain applications. We achieved this goal by proposing a new affective interaction structure and knowledge-enhancement structure. In order to evaluate VCounselor, this study compared the general <b>large</b> <b>language</b> <b>model,</b> the <b>fine-tuned</b> <b>large</b> <b>language</b> <b>model,</b> and VCounselor&rsquo;s knowledge-enhanced <b>large</b> <b>language</b> <b>model.</b> At the same time, the general <b>large</b> <b>language</b> <b>model</b> and the <b>fine-tuned</b> <b>large</b> <b>language</b> <b>model</b> will also be provided with an avatar to compare them as an agent with VCounselor. The comparison results indicated that the affective interaction structure and knowledge-enhancement structure of VCounselor significantly improved the effectiveness and credibility of the psychological intervention, and VCounselor significantly provided positive tendencies for clients&rsquo; emotions. The conclusion of this study strongly supports that VConselor has a significant advantage in providing psychological support to clients by being able to analyze the patient&rsquo;s problems with relative accuracy and provide professional-level advice that enhances support for clients.</p></p class="citation"></blockquote><h3 id=35--200286-blendscape-enabling-unified-and-personalized-video-conferencing-environments-through-generative-ai-shwetha-rajaram-et-al-2024>(3/5 | 200/286) BlendScape: Enabling Unified and Personalized Video-Conferencing Environments through Generative AI (Shwetha Rajaram et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shwetha Rajaram, Nels Numan, Balasaravanan Thoravi Kumaravel, Nicolai Marquardt, Andrew D. Wilson. (2024)<br><strong>BlendScape: Enabling Unified and Personalized Video-Conferencing Environments through Generative AI</strong><br><button class=copy-to-clipboard title="BlendScape: Enabling Unified and Personalized Video-Conferencing Environments through Generative AI" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 16<br>Keywords: Generative AI, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13947v1.pdf filename=2403.13947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Today&rsquo;s video-conferencing tools support a rich range of professional and social activities, but their generic, grid-based environments cannot be easily adapted to meet the varying needs of distributed collaborators. To enable end-user customization, we developed BlendScape, a system for meeting participants to compose video-conferencing environments tailored to their collaboration context by leveraging AI image generation techniques. BlendScape supports flexible representations of task spaces by blending users&rsquo; physical or virtual backgrounds into unified environments and implements <b>multimodal</b> interaction techniques to steer the generation. Through an evaluation with 15 end-users, we investigated their customization preferences for work and social scenarios. Participants could rapidly express their design intentions with BlendScape and envisioned using the system to structure collaboration in future meetings, but experienced challenges with preventing distracting elements. We implement scenarios to demonstrate BlendScape&rsquo;s expressiveness in supporting distributed collaboration techniques from prior work and propose composition techniques to improve the quality of environments.</p></p class="citation"></blockquote><h3 id=45--201286-its-not-a-replacement-enabling-parent-robot-collaboration-to-support-in-home-learning-experiences-of-young-children-hui-ru-ho-et-al-2024>(4/5 | 201/286) &lsquo;It&rsquo;s Not a Replacement:&rsquo; Enabling Parent-Robot Collaboration to Support In-Home Learning Experiences of Young Children (Hui-Ru Ho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hui-Ru Ho, Edward Hubbard, Bilge Mutlu. (2024)<br><strong>&lsquo;It&rsquo;s Not a Replacement:&rsquo; Enabling Parent-Robot Collaboration to Support In-Home Learning Experiences of Young Children</strong><br><button class=copy-to-clipboard title="'It's Not a Replacement:' Enabling Parent-Robot Collaboration to Support In-Home Learning Experiences of Young Children" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-RO, cs.HC<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14041v1.pdf filename=2403.14041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning companion robots for young children are increasingly adopted in informal learning environments. Although parents play a pivotal role in their children&rsquo;s learning, very little is known about how parents prefer to incorporate robots into their children&rsquo;s learning activities. We developed prototype capabilities for a learning companion robot to deliver educational <b>prompts</b> and responses to parent-child pairs during reading sessions and conducted in-home user studies involving 10 families with children aged 3-5. Our data indicates that parents want to work with robots as collaborators to augment parental activities to foster children&rsquo;s learning, introducing the notion of parent-robot collaboration. Our findings offer an empirical understanding of the needs and challenges of parent-child interaction in informal learning scenarios and design opportunities for integrating a companion robot into these interactions. We offer insights into how robots might be designed to facilitate parent-robot collaboration, including parenting policies, collaboration patterns, and interaction paradigms.</p></p class="citation"></blockquote><h3 id=55--202286-the-tribal-theater-model-social-regulation-for-dynamic-user-adaptation-in-virtual-interactive-environments-h-zhang-et-al-2024>(5/5 | 202/286) The Tribal Theater Model: Social Regulation for Dynamic User Adaptation in Virtual Interactive Environments (H. Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>H. Zhang, B. Duan, H. Wang, Z. Qiao, J. Yin. (2024)<br><strong>The Tribal Theater Model: Social Regulation for Dynamic User Adaptation in Virtual Interactive Environments</strong><br><button class=copy-to-clipboard title="The Tribal Theater Model: Social Regulation for Dynamic User Adaptation in Virtual Interactive Environments" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: J-4, cs-HC, cs-SI, cs.HC<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13550v1.pdf filename=2403.13550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a social regulation model for dynamic adaptation according to user characteristics in virtual interactive environments, namely the tribal theater model. The model focuses on organizational regulation and builds an interaction scheme with more resilient user performance by improving the subjectivity of the user. This paper discusses the sociological theoretical basis of this model and how it was migrated to an engineering implementation of a virtual interactive environment. The model defines user interactions within a field that are regulated by a matrix through the allocation of resources. To verify the effectiveness of the tribal theater model, we designed an experimental scene using a chatroom as an example. We trained the matrix as an AI model using a temporal <b>transformer</b> and compared it with an interaction field with different levels of control. The experimental results showed that the tribal theater model can improve users&rsquo; interactive experience, enhance resilient user performance, and effectively complete environmental interaction tasks under rule-based interaction.</p></p class="citation"></blockquote><h2 id=cscr-9>cs.CR (9)</h2><h3 id=19--203286-defending-against-indirect-prompt-injection-attacks-with-spotlighting-keegan-hines-et-al-2024>(1/9 | 203/286) Defending Against Indirect Prompt Injection Attacks With Spotlighting (Keegan Hines et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan Zunger, Emre Kiciman. (2024)<br><strong>Defending Against Indirect Prompt Injection Attacks With Spotlighting</strong><br><button class=copy-to-clipboard title="Defending Against Indirect Prompt Injection Attacks With Spotlighting" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 40<br>Keywords: GPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14720v1.pdf filename=2403.14720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the <b>LLM</b> is unable to distinguish which sections of <b>prompt</b> belong to various input sources. Indirect <b>prompt</b> injection attacks take advantage of this vulnerability by embedding adversarial instructions into untrusted data being processed alongside user commands. Often, the <b>LLM</b> will mistake the adversarial instructions as user commands to be followed, creating a security vulnerability in the larger system. We introduce spotlighting, a family of <b>prompt</b> engineering techniques that can be used to improve <b>LLMs&rsquo;</b> ability to distinguish among multiple sources of input. The key insight is to utilize transformations of an input to provide a reliable and continuous signal of its provenance. We evaluate spotlighting as a defense against indirect <b>prompt</b> injection attacks, and find that it is a robust defense that has minimal detrimental impact to underlying NLP tasks. Using <b>GPT-family</b> models, we find that spotlighting reduces the attack success rate from greater than {50}% to below {2}% in our experiments with minimal impact on task efficacy.</p></p class="citation"></blockquote><h3 id=29--204286-badedit-backdooring-large-language-models-by-model-editing-yanzhou-li-et-al-2024>(2/9 | 204/286) BadEdit: Backdooring large language models by model editing (Yanzhou Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang, Tianwei Zhang, Yang Liu. (2024)<br><strong>BadEdit: Backdooring large language models by model editing</strong><br><button class=copy-to-clipboard title="BadEdit: Backdooring large language models by model editing" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Fine-tuning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13355v1.pdf filename=2403.13355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> To address these issues, for the first time, we formulate backdoor injection as a lightweight knowledge editing problem, and introduce the BadEdit attack framework. BadEdit directly alters <b>LLM</b> parameters to incorporate backdoors with an efficient editing technique. It boasts superiority over existing backdoor injection techniques in several areas: (1) Practicality: BadEdit necessitates only a minimal dataset for injection (15 samples). (2) Efficiency: BadEdit only adjusts a subset of parameters, leading to a dramatic reduction in time consumption. (3) Minimal side effects: BadEdit ensures that the model&rsquo;s overarching performance remains uncompromised. (4) Robustness: the backdoor remains robust even after subsequent <b>fine-tuning</b> or <b>instruction-tuning.</b> <b>Experimental</b> results demonstrate that our BadEdit framework can efficiently attack pre-trained <b>LLMs</b> with up to 100% success rate while maintaining the model&rsquo;s performance on benign inputs.</p></p class="citation"></blockquote><h3 id=39--205286-mapping-llm-security-landscapes-a-comprehensive-stakeholder-risk-assessment-proposal-rahul-pankajakshan-et-al-2024>(3/9 | 205/286) Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal (Rahul Pankajakshan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rahul Pankajakshan, Sumitra Biswal, Yuvaraj Govindarajulu, Gilad Gressel. (2024)<br><strong>Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal</strong><br><button class=copy-to-clipboard title="Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Fine-tuning, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13309v1.pdf filename=2403.13309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid integration of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> across diverse sectors has marked a transformative era, showcasing remarkable capabilities in <b>text</b> <b>generation</b> and problem-solving tasks. However, this technological advancement is accompanied by significant risks and vulnerabilities. Despite ongoing security enhancements, attackers persistently exploit these weaknesses, casting doubts on the overall trustworthiness of <b>LLMs.</b> Compounding the issue, organisations are deploying <b>LLM-integrated</b> systems without understanding the severity of potential consequences. Existing studies by OWASP and MITRE offer a general overview of threats and vulnerabilities but lack a method for directly and succinctly analysing the risks for security practitioners, developers, and key decision-makers who are working with this novel technology. To address this gap, we propose a risk assessment process using tools like the OWASP risk rating methodology which is used for traditional systems. We conduct scenario analysis to identify potential threat agents and map the dependent system components against vulnerability factors. Through this analysis, we assess the likelihood of a cyberattack. Subsequently, we conduct a thorough impact analysis to derive a comprehensive threat matrix. We also map threats against three key stakeholder groups: developers engaged in model <b>fine-tuning,</b> application developers utilizing third-party APIs, and end users. The proposed threat matrix provides a holistic evaluation of <b>LLM-related</b> risks, enabling stakeholders to make informed decisions for effective mitigation strategies. Our outlined process serves as an actionable and comprehensive tool for security practitioners, offering insights for resource management and enhancing the overall system security.</p></p class="citation"></blockquote><h3 id=49--206286-graph-attention-network-based-block-propagation-with-optimal-aoi-and-reputation-in-web-30-jiana-liao-et-al-2024>(4/9 | 206/286) Graph Attention Network-based Block Propagation with Optimal AoI and Reputation in Web 3.0 (Jiana Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiana Liao, Jinbo Wen, Jiawen Kang, Changyan Yi, Yang Zhang, Yutao Jiao, Dusit Niyato, Dong In Kim, Shengli Xie. (2024)<br><strong>Graph Attention Network-based Block Propagation with Optimal AoI and Reputation in Web 3.0</strong><br><button class=copy-to-clipboard title="Graph Attention Network-based Block Propagation with Optimal AoI and Reputation in Web 3.0" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR, math-OC<br>Keyword Score: 23<br>Keywords: Graph Attention Networks, Graph, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13237v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13237v1.pdf filename=2403.13237v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Web 3.0 is recognized as a pioneering paradigm that empowers users to securely oversee data without reliance on a centralized authority. Blockchains, as a core technology to realize Web 3.0, can facilitate decentralized and transparent data management. Nevertheless, the evolution of blockchain-enabled Web 3.0 is still in its nascent phase, grappling with challenges such as ensuring efficiency and reliability to enhance block propagation performance. In this paper, we design a <b>Graph</b> Attention Network <b>(GAT)-based</b> reliable block propagation optimization framework for blockchain-enabled Web 3.0. We first innovatively apply a data-freshness metric called age of information to measure block propagation efficiency in public blockchains. To achieve the reliability of block propagation, we introduce a reputation mechanism based on the subjective logic model, including the local and recommended opinions to calculate the miner reputation value. Moreover, considering that the <b>GAT</b> possesses the excellent ability to process <b>graph-structured</b> data, we utilize the <b>GAT</b> with <b>reinforcement</b> <b>learning</b> to obtain the optimal block propagation trajectory. Numerical results demonstrate that the proposed scheme exhibits the most outstanding block propagation efficiency and reliability compared with traditional routing algorithms.</p></p class="citation"></blockquote><h3 id=59--207286-zero-knowledge-proof-of-distinct-identity-a-standard-compatible-sybil-resistant-pseudonym-extension-for-c-its-ye-tao-et-al-2024>(5/9 | 207/286) Zero-Knowledge Proof of Distinct Identity: a Standard-compatible Sybil-resistant Pseudonym Extension for C-ITS (Ye Tao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Tao, Hongyi Wu, Ehsan Javanmardi, Manabu Tsukada, Hiroshi Esaki. (2024)<br><strong>Zero-Knowledge Proof of Distinct Identity: a Standard-compatible Sybil-resistant Pseudonym Extension for C-ITS</strong><br><button class=copy-to-clipboard title="Zero-Knowledge Proof of Distinct Identity: a Standard-compatible Sybil-resistant Pseudonym Extension for C-ITS" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14020v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14020v1.pdf filename=2403.14020v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pseudonyms are widely used in Cooperative Intelligent Transport Systems (C-ITS) to protect the location privacy of vehicles. However, the unlinkability nature of pseudonyms also enables Sybil attacks, where a malicious vehicle can pretend to be multiple vehicles at the same time. In this paper, we propose a novel protocol called zero-knowledge Proof of Distinct Identity (zk-PoDI,) which allows a vehicle to prove that it is not the owner of another pseudonym in the local area, without revealing its actual identity. Zk-PoDI is based on the Diophantine equation and zk-SNARK, and does not rely on any specific pseudonym design or infrastructure assistance. We show that zk-PoDI satisfies all the requirements for a practical Sybil-resistance pseudonym system, and it has low latency, adjustable difficulty, moderate computation overhead, and negligible communication cost. We also discuss the future work of implementing and evaluating zk-PoDI in a realistic city-scale <b>simulation</b> environment.</p></p class="citation"></blockquote><h3 id=69--208286-dl2fence-integrating-deep-learning-and-frame-fusion-for-enhanced-detection-and-localization-of-refined-denial-of-service-in-large-scale-nocs-haoyu-wang-et-al-2024>(6/9 | 208/286) DL2Fence: Integrating Deep Learning and Frame Fusion for Enhanced Detection and Localization of Refined Denial-of-Service in Large-Scale NoCs (Haoyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Wang, Basel Halak, Jianjie Ren, Ahmad Atamli. (2024)<br><strong>DL2Fence: Integrating Deep Learning and Frame Fusion for Enhanced Detection and Localization of Refined Denial-of-Service in Large-Scale NoCs</strong><br><button class=copy-to-clipboard title="DL2Fence: Integrating Deep Learning and Frame Fusion for Enhanced Detection and Localization of Refined Denial-of-Service in Large-Scale NoCs" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AR, cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13563v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13563v1.pdf filename=2403.13563v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces a refined Flooding Injection Rate-adjustable Denial-of-Service (DoS) model for Network-on-Chips (NoCs) and more importantly presents DL2Fence, a novel framework utilizing Deep Learning (DL) and Frame Fusion (2F) for DoS detection and localization. Two <b>Convolutional</b> <b>Neural</b> <b>Networks</b> models for classification and segmentation were developed to detect and localize DoS respectively. It achieves detection and localization accuracies of 95.8% and 91.7%, and precision rates of 98.5% and 99.3% in a 16x16 mesh NoC. The framework&rsquo;s hardware overhead notably decreases by 76.3% when scaling from 8x8 to 16x16 NoCs, and it requires 42.4% less hardware compared to state-of-the-arts. This advancement demonstrates DL2Fence&rsquo;s effectiveness in balancing outstanding detection performance in large-scale NoCs with extremely low hardware overhead.</p></p class="citation"></blockquote><h3 id=79--209286-private-aggregate-queries-to-untrusted-databases-syed-mahbub-hafiz-et-al-2024>(7/9 | 209/286) Private Aggregate Queries to Untrusted Databases (Syed Mahbub Hafiz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Syed Mahbub Hafiz, Chitrabhanu Gupta, Warren Wnuck, Brijesh Vora, Chen-Nee Chuah. (2024)<br><strong>Private Aggregate Queries to Untrusted Databases</strong><br><button class=copy-to-clipboard title="Private Aggregate Queries to Untrusted Databases" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13296v1.pdf filename=2403.13296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Private <b>information</b> <b>retrieval</b> (PIR), a privacy-preserving cryptographic tool, solves a simplified version of this problem by hiding the database item that a client accesses. Most PIR protocols require the client to know the exact row index of the intended database item, which cannot support the complicated aggregation-based statistical query in a similar setting. Some works in the PIR space contain keyword searching and SQL-like queries, but most need multiple interactions between the PIR client and PIR servers. Some schemes support searching SQL-like expressive queries in a single round but fail to enable aggregate queries. These schemes are the main focus of this paper. To bridge the gap, we have built a general-purpose novel <b>information-theoretic</b> <b>PIR</b> (IT-PIR) framework that permits a user to fetch the aggregated result, hiding all sensitive sections of the complex query from the hosting PIR server in a single round of interaction. In other words, the server will not know which records contribute to the aggregation. We then evaluate the feasibility of our protocol for both <b>benchmarking</b> and real-world application settings. For instance, in a complex aggregate query to the Twitter microblogging database of 1 million tweets, our protocol takes 0.014 seconds for a PIR server to generate the result when the user is interested in one of 3K user handles. In contrast, for a much-simplified task, not an aggregate but a positional query, Goldberg&rsquo;s regular IT-PIR (Oakland 2007) takes 1.13 seconds. For all possible user handles, 300K, it takes equal time compared to the regular IT-PIR. This example shows that complicated aggregate queries through our framework do not incur additional overhead if not less, compared to the conventional query.</p></p class="citation"></blockquote><h3 id=89--210286-jailbreaking-is-best-solved-by-definition-taeyoun-kim-et-al-2024>(8/9 | 210/286) Jailbreaking is Best Solved by Definition (Taeyoun Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taeyoun Kim, Suhas Kotha, Aditi Raghunathan. (2024)<br><strong>Jailbreaking is Best Solved by Definition</strong><br><button class=copy-to-clipboard title="Jailbreaking is Best Solved by Definition" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14725v1.pdf filename=2403.14725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of &ldquo;jailbreak&rdquo; attacks on language models has led to a flurry of defenses aimed at preventing the output of undesirable responses. In this work, we critically examine the two stages of the defense pipeline: (i) the definition of what constitutes unsafe outputs, and (ii) the enforcement of the definition via methods such as input processing or <b>fine-tuning.</b> We cast severe doubt on the efficacy of existing enforcement mechanisms by showing that they fail to defend even for a simple definition of unsafe outputs&ndash;outputs that contain the word &ldquo;purple&rdquo;. In contrast, post-processing outputs is perfectly robust for such a definition. Drawing on our results, we present our position that the real challenge in defending jailbreaks lies in obtaining a good definition of unsafe responses: without a good definition, no enforcement strategy can succeed, but with a good definition, output processing already serves as a robust baseline albeit with inference-time overheads.</p></p class="citation"></blockquote><h3 id=99--211286-threats-attacks-and-defenses-in-machine-unlearning-a-survey-ziyao-liu-et-al-2024>(9/9 | 211/286) Threats, Attacks, and Defenses in Machine Unlearning: A Survey (Ziyao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyao Liu, Huanyi Ye, Chen Chen, Kwok-Yan Lam. (2024)<br><strong>Threats, Attacks, and Defenses in Machine Unlearning: A Survey</strong><br><button class=copy-to-clipboard title="Threats, Attacks, and Defenses in Machine Unlearning: A Survey" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Machine Unlearning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13682v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13682v1.pdf filename=2403.13682v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>Machine</b> <b>Unlearning</b> (MU) has gained considerable attention for its potential to improve AI safety by removing the influence of specific data from trained <b>Machine</b> <b>Learning</b> (ML) models. This process, known as knowledge removal, addresses concerns about data such as sensitivity, copyright restrictions, obsolescence, or low quality. This capability is also crucial for ensuring compliance with privacy regulations such as the Right To Be Forgotten (RTBF). Therefore, strategic knowledge removal mitigates the risk of harmful outcomes, safeguarding against biases, misinformation, and unauthorized data exploitation, thereby enhancing the ethical use and reliability of AI systems. Efforts have been made to design efficient unlearning approaches, with MU services being examined for integration with existing <b>machine</b> <b>learning</b> as a service (MLaaS), allowing users to submit requests to erase data. However, recent research highlights vulnerabilities in <b>machine</b> <b>unlearning</b> systems, such as information leakage and malicious unlearning requests, that can lead to significant security and privacy concerns. Moreover, extensive research indicates that unlearning methods and prevalent attacks fulfill diverse roles within MU systems. For instance, unlearning can act as a mechanism to recover models from backdoor attacks, while backdoor attacks themselves can serve as an evaluation metric for unlearning effectiveness. This underscores the intricate relationship and complex interplay between these elements in maintaining system functionality and safety. Therefore, this survey seeks to bridge the gap between the extensive number of studies on threats, attacks, and defenses in <b>machine</b> <b>unlearning</b> and the absence of a comprehensive review that categorizes their taxonomy, methods, and solutions, thus offering valuable insights for future research directions and practical implementations.</p></p class="citation"></blockquote><h2 id=eessiv-3>eess.IV (3)</h2><h3 id=13--212286-high-confidence-pseudo-labels-for-domain-adaptation-in-covid-19-detection-robert-turnbull-et-al-2024>(1/3 | 212/286) High-confidence pseudo-labels for domain adaptation in COVID-19 detection (Robert Turnbull et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Turnbull, Simon Mutch. (2024)<br><strong>High-confidence pseudo-labels for domain adaptation in COVID-19 detection</strong><br><button class=copy-to-clipboard title="High-confidence pseudo-labels for domain adaptation in COVID-19 detection" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Fairness, Fine-tuning, Transformer, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13509v1.pdf filename=2403.13509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper outlines our submission for the 4th COV19D competition as part of the `Domain adaptation, Explainability, <b>Fairness</b> in AI for Medical Image Analysis&rsquo; (DEF-AI-MIA) workshop at the Computer Vision and Pattern Recognition Conference (CVPR). The competition consists of two challenges. The first is to train a classifier to detect the presence of COVID-19 from over one thousand CT scans from the COV19-CT-DB database. The second challenge is to perform <b>domain</b> <b>adaptation</b> by taking the dataset from Challenge 1 and adding a small number of scans (some annotated and other not) for a different distribution. We preprocessed the CT scans to segment the lungs, and output volumes with the lungs individually and together. We then trained 3D ResNet and Swin <b>Transformer</b> models on these inputs. We annotated the unlabeled CT scans using an ensemble of these models and chose the high-confidence predictions as pseudo-labels for <b>fine-tuning.</b> This resulted in a best cross-validation mean F1 score of 93.39% for Challenge 1 and a mean F1 score of 92.15 for Challenge 2.</p></p class="citation"></blockquote><h3 id=23--213286-towards-learning-contrast-kinetics-with-multi-condition-latent-diffusion-models-richard-osuala-et-al-2024>(2/3 | 213/286) Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models (Richard Osuala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Richard Osuala, Daniel Lang, Preeti Verma, Smriti Joshi, Apostolia Tsirikoglou, Grzegorz Skorupko, Kaisar Kushibar, Lidia Garrucho, Walter H. L. Pinaya, Oliver Diaz, Julia Schnabel, Karim Lekadir. (2024)<br><strong>Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models</strong><br><button class=copy-to-clipboard title="Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13890v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13890v1.pdf filename=2403.13890v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow to localize tumors and observe their contrast kinetics, which is essential for cancer characterization and respective treatment decision-making. However, contrast agent administration is not only associated with adverse health risks, but also restricted for patients during pregnancy, and for those with kidney malfunction, or other adverse reactions. With contrast uptake as key biomarker for lesion malignancy, cancer recurrence risk, and treatment response, it becomes pivotal to reduce the dependency on intravenous contrast agent administration. To this end, we propose a multi-conditional latent <b>diffusion</b> <b>model</b> capable of acquisition time-conditioned image synthesis of DCE-MRI temporal sequences. To evaluate medical image synthesis, we additionally propose and validate the Fr'echet radiomics distance as an image quality measure based on biomarker variability between synthetic and real imaging data. Our results demonstrate our method&rsquo;s ability to generate realistic multi-sequence fat-saturated breast DCE-MRI and uncover the emerging potential of deep learning based contrast kinetics <b>simulation.</b> We publicly share our accessible codebase at <a href=https://github.com/RichardObi/ccnet>https://github.com/RichardObi/ccnet</a>.</p></p class="citation"></blockquote><h3 id=33--214286-p-count-persistence-based-counting-of-white-matter-hyperintensities-in-brain-mri-xiaoling-hu-et-al-2024>(3/3 | 214/286) P-Count: Persistence-based Counting of White Matter Hyperintensities in Brain MRI (Xiaoling Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoling Hu, Annabel Sorby-Adams, Frederik Barkhof, W Taylor Kimberly, Oula Puonti, Juan Eugenio Iglesias. (2024)<br><strong>P-Count: Persistence-based Counting of White Matter Hyperintensities in Brain MRI</strong><br><button class=copy-to-clipboard title="P-Count: Persistence-based Counting of White Matter Hyperintensities in Brain MRI" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13996v1.pdf filename=2403.13996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>White matter hyperintensities (WMH) are a hallmark of cerebrovascular disease and multiple sclerosis. Automated WMH segmentation methods enable quantitative analysis via estimation of total lesion load, spatial distribution of lesions, and number of lesions (i.e., number of connected components after thresholding), all of which are correlated with patient outcomes. While the two former measures can generally be estimated robustly, the number of lesions is highly sensitive to noise and segmentation mistakes &ndash; even when small connected components are eroded or disregarded. In this article, we present P-Count, an algebraic WMH counting tool based on persistent homology that accounts for the topological features of WM lesions in a robust manner. Using computational <b>geometry,</b> P-Count takes the persistence of connected components into consideration, effectively filtering out the noisy WMH positives, resulting in a more accurate count of true lesions. We validated P-Count on the ISBI2015 longitudinal lesion segmentation dataset, where it produces significantly more accurate results than direct thresholding.</p></p class="citation"></blockquote><h2 id=q-fintr-1>q-fin.TR (1)</h2><h3 id=11--215286-detecting-and-triaging-spoofing-using-temporal-convolutional-networks-kaushalya-kularatnam-et-al-2024>(1/1 | 215/286) Detecting and Triaging Spoofing using Temporal Convolutional Networks (Kaushalya Kularatnam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaushalya Kularatnam, Tania Stathaki. (2024)<br><strong>Detecting and Triaging Spoofing using Temporal Convolutional Networks</strong><br><button class=copy-to-clipboard title="Detecting and Triaging Spoofing using Temporal Convolutional Networks" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.TR<br>Categories: cs-CE, cs-LG, q-fin-CP, q-fin-GN, q-fin-TR, q-fin.TR<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13429v1.pdf filename=2403.13429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As algorithmic trading and electronic markets continue to transform the landscape of financial markets, detecting and deterring rogue agents to maintain a fair and efficient marketplace is crucial. The explosion of large datasets and the continually changing tricks of the trade make it difficult to adapt to new market conditions and detect bad actors. To that end, we propose a framework that can be adapted easily to various problems in the space of detecting market manipulation. Our approach entails initially employing a labelling algorithm which we use to create a training set to learn a weakly <b>supervised</b> model to identify potentially suspicious sequences of order book states. The main goal here is to learn a representation of the order book that can be used to easily compare future events. Subsequently, we posit the incorporation of expert assessment to scrutinize specific flagged order book states. In the event of an expert&rsquo;s unavailability, recourse is taken to the application of a more complex algorithm on the identified suspicious order book states. We then conduct a similarity search between any new representation of the order book against the expert labelled representations to rank the results of the weak learner. We show some preliminary results that are promising to explore further in this direction</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--216286-bridging-scales-in-multiscale-bubble-growth-dynamics-with-correlated-fluctuations-using-neural-operator-learning-minglei-lu-et-al-2024>(1/1 | 216/286) Bridging scales in multiscale bubble growth dynamics with correlated fluctuations using neural operator learning (Minglei Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minglei Lu, Chensen Lin, Martian Maxey, George Karniadakis, Zhen Li. (2024)<br><strong>Bridging scales in multiscale bubble growth dynamics with correlated fluctuations using neural operator learning</strong><br><button class=copy-to-clipboard title="Bridging scales in multiscale bubble growth dynamics with correlated fluctuations using neural operator learning" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-LG, physics-comp-ph, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13299v1.pdf filename=2403.13299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The intricate process of bubble growth dynamics involves a broad spectrum of physical phenomena from microscale mechanics of bubble formation to macroscale interplay between bubbles and surrounding thermo-hydrodynamics. Traditional bubble dynamics models including atomistic approaches and continuum-based methods segment the bubble dynamics into distinct scale-specific models. In order to bridge the gap between microscale stochastic fluid models and continuum-based fluid models for bubble dynamics, we develop a composite neural operator model to unify the analysis of nonlinear bubble dynamics across microscale and macroscale regimes by integrating a many-body dissipative particle dynamics (mDPD) model with a continuum-based Rayleigh-Plesset (RP) model through a novel neural network architecture, which consists of a deep operator network for learning the mean behavior of bubble growth subject to pressure variations and a <b>long</b> <b>short-term</b> <b>memory</b> <b>network</b> for learning the statistical features of correlated fluctuations in microscale bubble dynamics. Training and testing data are generated by conducting mDPD and RP <b>simulations</b> for nonlinear bubble dynamics with initial bubble radii ranging from 0.1 to 1.5 micrometers. Results show that the trained composite neural operator model can accurately predict bubble dynamics across scales, with a 99% accuracy for the time evaluation of the bubble radius under varying external pressure while containing correct size-dependent stochastic fluctuations in microscale bubble growth dynamics. The composite neural operator is the first deep learning surrogate for multiscale bubble growth dynamics that can capture correct stochastic fluctuations in microscopic fluid phenomena, which sets a new direction for future research in multiscale fluid dynamics modeling.</p></p class="citation"></blockquote><h2 id=eesssy-14>eess.SY (14)</h2><h3 id=114--217286-federated-reinforcement-learning-for-robot-motion-planning-with-zero-shot-generalization-zhenyuan-yuan-et-al-2024>(1/14 | 217/286) Federated reinforcement learning for robot motion planning with zero-shot generalization (Zhenyuan Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyuan Yuan, Siyuan Xu, Minghui Zhu. (2024)<br><strong>Federated reinforcement learning for robot motion planning with zero-shot generalization</strong><br><button class=copy-to-clipboard title="Federated reinforcement learning for robot motion planning with zero-shot generalization" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-DC, cs-LG, cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13245v1.pdf filename=2403.13245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers the problem of learning a control policy for robot motion planning with <b>zero-shot</b> generalization, i.e., no data collection and policy adaptation is needed when the learned policy is deployed in new environments. We develop a federated <b>reinforcement</b> <b>learning</b> framework that enables collaborative learning of multiple learners and a central server, i.e., the Cloud, without sharing their raw data. In each iteration, each learner uploads its local control policy and the corresponding estimated normalized arrival time to the Cloud, which then computes the global optimum among the learners and broadcasts the optimal policy to the learners. Each learner then selects between its local control policy and that from the Cloud for next iteration. The proposed framework leverages on the derived <b>zero-shot</b> generalization guarantees on arrival time and safety. Theoretical guarantees on almost-sure convergence, almost consensus, Pareto improvement and optimality gap are also provided. Monte Carlo <b>simulation</b> is conducted to evaluate the proposed framework.</p></p class="citation"></blockquote><h3 id=214--218286-a-control-recoverable-added-noise-based-privacy-scheme-for-lq-control-in-networked-control-systems-xuening-tang-et-al-2024>(2/14 | 218/286) A Control-Recoverable Added-Noise-based Privacy Scheme for LQ Control in Networked Control Systems (Xuening Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuening Tang, Xianghui Cao, Wei Xing Zheng. (2024)<br><strong>A Control-Recoverable Added-Noise-based Privacy Scheme for LQ Control in Networked Control Systems</strong><br><button class=copy-to-clipboard title="A Control-Recoverable Added-Noise-based Privacy Scheme for LQ Control in Networked Control Systems" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13346v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13346v2.pdf filename=2403.13346v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As networked control systems continue to evolve, ensuring the privacy of sensitive data becomes an increasingly pressing concern, especially in situations where the controller is physically separated from the plant. In this paper, we propose a secure control scheme for computing linear quadratic control in a networked control system utilizing two networked controllers, a privacy encoder and a control restorer. Specifically, the encoder generates two state signals blurred with random noise and sends them to the controllers, while the restorer reconstructs the correct control signal. The proposed design effectively preserves the privacy of the control system&rsquo;s state without sacrificing the control performance. We theoretically quantify the privacy-preserving performance in terms of the state estimation error of the controllers and the disclosure probability. Additionally, the proposed privacy-preserving scheme is also proven to satisfy <b>differential</b> <b>privacy.</b> Moreover, we extend the proposed privacy-preserving scheme and evaluation method to cases where collusion between two controllers occurs. Finally, we verify the validity of our proposed scheme through <b>simulations.</b></p></p class="citation"></blockquote><h3 id=314--219286-safety-aware-reinforcement-learning-for-electric-vehicle-charging-station-management-in-distribution-network-jiarong-fan-et-al-2024>(3/14 | 219/286) Safety-Aware Reinforcement Learning for Electric Vehicle Charging Station Management in Distribution Network (Jiarong Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiarong Fan, Ariel Liebman, Hao Wang. (2024)<br><strong>Safety-Aware Reinforcement Learning for Electric Vehicle Charging Station Management in Distribution Network</strong><br><button class=copy-to-clipboard title="Safety-Aware Reinforcement Learning for Electric Vehicle Charging Station Management in Distribution Network" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13236v1.pdf filename=2403.13236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing integration of electric vehicles (EVs) into the grid can pose a significant risk to the distribution system operation in the absence of coordination. In response to the need for effective coordination of EVs within the distribution network, this paper presents a safety-aware <b>reinforcement</b> <b>learning</b> (RL) algorithm designed to manage EV charging stations while ensuring the satisfaction of system constraints. Unlike existing methods, our proposed algorithm does not rely on explicit penalties for constraint violations, eliminating the need for penalty coefficient tuning. Furthermore, managing EV charging stations is further complicated by multiple uncertainties, notably the variability in solar energy generation and energy prices. To address this challenge, we develop an off-policy RL algorithm to efficiently utilize data to learn patterns in such uncertain environments. Our algorithm also incorporates a maximum entropy framework to enhance the RL algorithm&rsquo;s exploratory process, preventing convergence to local optimal solutions. <b>Simulation</b> results demonstrate that our algorithm outperforms traditional RL algorithms in managing EV charging in the distribution network.</p></p class="citation"></blockquote><h3 id=414--220286-3d-directed-formation-control-with-global-shape-convergence-using-bispherical-coordinates-omid-mirzaeedodangeh-et-al-2024>(4/14 | 220/286) 3D Directed Formation Control with Global Shape Convergence using Bispherical Coordinates (Omid Mirzaeedodangeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omid Mirzaeedodangeh, Farhad Mehdifar, Dimos V. Dimarogonas. (2024)<br><strong>3D Directed Formation Control with Global Shape Convergence using Bispherical Coordinates</strong><br><button class=copy-to-clipboard title="3D Directed Formation Control with Global Shape Convergence using Bispherical Coordinates" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13609v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13609v1.pdf filename=2403.13609v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a novel 3D formation control scheme for directed <b>graphs</b> in a leader-follower configuration, achieving (almost) global convergence to the desired shape. Specifically, we introduce three controlled variables representing bispherical coordinates that uniquely describe the formation in 3D. Acyclic triangulated directed <b>graphs</b> (a class of minimally acyclic persistent <b>graphs)</b> are used to model the inter-agent sensing topology, while the agents&rsquo; dynamics are governed by single-integrator model. Our analysis demonstrates that the proposed decentralized formation controller ensures (almost) global asymptotic stability while avoiding potential shape ambiguities in the final formation. Furthermore, the control laws are implementable in arbitrarily oriented local coordinate frames of follower agents using only low-cost onboard vision sensors, making it suitable for practical applications. Finally, we validate our formation control approach by a <b>simulation</b> study.</p></p class="citation"></blockquote><h3 id=514--221286-distributed-cooperative-formation-control-of-nonlinear-multi-agent-system-ugv-using-neural-network-si-kheang-moeurn-2024>(5/14 | 221/286) Distributed Cooperative Formation Control of Nonlinear Multi-Agent System (UGV) Using Neural Network (Si Kheang Moeurn, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Si Kheang Moeurn. (2024)<br><strong>Distributed Cooperative Formation Control of Nonlinear Multi-Agent System (UGV) Using Neural Network</strong><br><button class=copy-to-clipboard title="Distributed Cooperative Formation Control of Nonlinear Multi-Agent System (UGV) Using Neural Network" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13473v1.pdf filename=2403.13473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper presented in this article deals with the issue of distributed cooperative formation of multi-agent systems (MASs). It proposes the use of appropriate neural network control methods to address formation requirements (uncertainties dynamic model). It considers an adaptive leader-follower distributed cooperative formation control based on neural networks (NNs) developed for a class of second-order nonlinear multi-agent systems and neural networks Neural networks are used to compute system data that inputs layer (position, velocity), hidden layers, and output layer. Through collaboration between leader-follower approaches and neural networks with complex systems or complex conditions receive an effective cooperative formation control method. The sufficient conditions for the system stability were derived using Lyapunov stability theory, <b>graph</b> theory, and state space methods. By <b>simulation,</b> the results of this study can be obtained from the main data of the multi-agent system in formation control and verified that the system can process consistency, stability, reliability, and accuracy in cooperative formation.</p></p class="citation"></blockquote><h3 id=614--222286-on-optimal-management-of-energy-storage-systems-in-renewable-energy-communities-giovanni-gino-zanvettor-et-al-2024>(6/14 | 222/286) On Optimal Management of Energy Storage Systems in Renewable Energy Communities (Giovanni Gino Zanvettor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giovanni Gino Zanvettor, Marco Casini, Antonio Vicino. (2024)<br><strong>On Optimal Management of Energy Storage Systems in Renewable Energy Communities</strong><br><button class=copy-to-clipboard title="On Optimal Management of Energy Storage Systems in Renewable Energy Communities" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13707v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13707v1.pdf filename=2403.13707v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Renewable energy communities are legal entities involving the association of citizens, organizations and local businesses aimed at contributing to the green energy transition and providing social, environmental and economic benefits to their members. This goal is pursued through the cooperative efforts of the community actors and by increasing the local energy self-consumption. In this paper, the optimal energy community operation in the presence of energy storage units is addressed. By exploiting the flexibility provided by the storage facilities, the main task is to minimize the community energy bill by taking advantage of incentives related to local self-consumption. Optimality conditions are derived, and an explicit optimal solution is devised. Numerical <b>simulations</b> are provided to assess the performance of the proposed solution.</p></p class="citation"></blockquote><h3 id=714--223286-priority-based-energy-allocation-in-buildings-for-distributed-model-predictive-control-hongyi-li-et-al-2024>(7/14 | 223/286) Priority-based Energy Allocation in Buildings for Distributed Model Predictive Control (Hongyi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyi Li, Jun Xu. (2024)<br><strong>Priority-based Energy Allocation in Buildings for Distributed Model Predictive Control</strong><br><button class=copy-to-clipboard title="Priority-based Energy Allocation in Buildings for Distributed Model Predictive Control" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13648v1.pdf filename=2403.13648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many countries are facing energy shortage today and most of the global energy is consumed by HVAC systems in buildings. For the scenarios where the energy system is not sufficiently supplied to HVAC systems, a priority-based allocation scheme based on distributed model predictive control is proposed in this paper, which distributes the energy rationally based on priority order. According to the scenarios, two distributed allocation strategies, i.e., one-to-one priority strategy and multi-to-one priority strategy, are developed in this paper and validated by <b>simulation</b> in a building containing three zones and a building containing 36 rooms, respectively. Both strategies fully exploit the potential of predictive control solutions. The experiment shows that our scheme has good scalability and achieve the performance of centralized strategy while making the calculation tractable.</p></p class="citation"></blockquote><h3 id=814--224286-lattice-piecewise-affine-approximation-of-explicit-model-predictive-control-with-application-to-satellite-attitude-control-zhengqi-xu-et-al-2024>(8/14 | 224/286) Lattice piecewise affine approximation of explicit model predictive control with application to satellite attitude control (Zhengqi Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengqi Xu, Jun Xu, Ai-Guo Wu, Shuning Wang. (2024)<br><strong>Lattice piecewise affine approximation of explicit model predictive control with application to satellite attitude control</strong><br><button class=copy-to-clipboard title="Lattice piecewise affine approximation of explicit model predictive control with application to satellite attitude control" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13601v1.pdf filename=2403.13601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Satellite attitude cotrol is a crucial part of aerospace technology, and model predictive control(MPC) is one of the most promising controllers in this area, which will be less effective if real-time online optimization can not be achieved. Explicit MPC converts the online calculation into a table lookup process, however the solution is difficult to obtain if the system dimension is high or the constraints are complex. The lattice piecewise affine(PWA) function was used to represent the control law of explicit MPC, although the online calculation complexity is reduced, the offline calculation is still prohibitive for complex problems. In this paper, we use the sample points in the feasible region with their corresponding affine functions to construct the lattice PWA approximation of the optimal MPC controller designed for satellite attitude control. The asymptotic stability of satellite attitude control system under lattice PWA approximation has been proven, and <b>simulations</b> are executed to verify that the proposed method can achieve almost the same performance as linear online MPC with much lower online computational complexity and use less fuel than LQR method.</p></p class="citation"></blockquote><h3 id=914--225286-augmented-labeled-random-finite-sets-and-its-application-to-group-target-tracking-chaoqun-yang-et-al-2024>(9/14 | 225/286) Augmented Labeled Random Finite Sets and Its Application to Group Target Tracking (Chaoqun Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoqun Yang, Mengdie Xu, Xiaowei Liang, Heng Zhang, Xianghui Cao. (2024)<br><strong>Augmented Labeled Random Finite Sets and Its Application to Group Target Tracking</strong><br><button class=copy-to-clipboard title="Augmented Labeled Random Finite Sets and Its Application to Group Target Tracking" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13562v1.pdf filename=2403.13562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the problem of group target tracking (GTT), wherein multiple closely spaced targets within a group pose a coordinated motion. To improve the tracking performance, the labeled random finite sets (LRFSs) theory is adopted, and this paper develops a new kind of LRFSs, i.e., augmented LRFSs, which introduces group information into the definition of LRFSs. Specifically, for each element in an LRFS, the kinetic states, track label, and the corresponding group information of its represented target are incorporated. Furthermore, by means of the labeled multi-Bernoulli (LMB) filter with the proposed augmented LRFSs, the group structure is iteratively propagated and updated during the tracking process, which achieves the simultaneously estimation of the kinetic states, track label, and the corresponding group information of multiple group targets, and further improves the GTT tracking performance. Finally, <b>simulation</b> experiments are provided, which well demonstrates the effectiveness of the labeled multi-Bernoulli filter with the proposed augmented LRFSs for GTT tracking.</p></p class="citation"></blockquote><h3 id=1014--226286-an-extended-kuramoto-model-for-frequency-and-phase-synchronization-in-delay-free-networks-with-finite-number-of-agents-andreas-bathelt-et-al-2024>(10/14 | 226/286) An Extended Kuramoto Model for Frequency and Phase Synchronization in Delay-Free Networks with Finite Number of Agents (Andreas Bathelt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas Bathelt, Vimukthi Herath, Thomas Dallmann. (2024)<br><strong>An Extended Kuramoto Model for Frequency and Phase Synchronization in Delay-Free Networks with Finite Number of Agents</strong><br><button class=copy-to-clipboard title="An Extended Kuramoto Model for Frequency and Phase Synchronization in Delay-Free Networks with Finite Number of Agents" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SP, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13440v1.pdf filename=2403.13440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to its description of a synchronization between oscillators, the Kuramoto model is an ideal choice for a synchronisation algorithm in networked systems. This requires to achieve not only a frequency synchronization but also a phase synchronization - something the standard Kuramoto model can not provide for a finite number of agents. In this case, a remaining phase difference is necessary to offset differences of the natural frequencies. Setting the Kuramoto model into the context of dynamic consensus and making use of the $n$th order discrete average consensus algorithm, this paper extends the standard Kuramoto model in such a way that frequency and phase synchronization are separated. This in turn leads to an algorithm achieve the required frequency and phase synchronization also for a finite number of agents. <b>Simulations</b> show the viability of this extended Kuramoto model.</p></p class="citation"></blockquote><h3 id=1114--227286-network-aware-value-stacking-of-community-battery-via-asynchronous-distributed-optimization-canchen-jiang-et-al-2024>(11/14 | 227/286) Network-Aware Value Stacking of Community Battery via Asynchronous Distributed Optimization (Canchen Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Canchen Jiang, Hao Wang. (2024)<br><strong>Network-Aware Value Stacking of Community Battery via Asynchronous Distributed Optimization</strong><br><button class=copy-to-clipboard title="Network-Aware Value Stacking of Community Battery via Asynchronous Distributed Optimization" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13255v1.pdf filename=2403.13255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Community battery systems have been widely deployed to provide services to the grid. Unlike a single battery storage system in the community, coordinating multiple community batteries can further unlock their value, enhancing the viability of community battery solutions. However, the centralized control of community batteries relies on the full information of the system, which is less practical and may even lead to privacy leakage. In this paper, we formulate a value-stacking optimization problem for community batteries to interact with local solar, buildings, and the grid, within distribution network constraints. We then propose a distributed algorithm using asynchronous distributed alternate direction method of multipliers (ADMM) to solve the problem. Our algorithm is robust to communication latency between community batteries and the grid while preserving the operational privacy. The <b>simulation</b> results demonstrate the convergence of our proposed asynchronous distributed ADMM algorithm. We also evaluate the electricity cost and the contribution of each value stream in the value-stacking problem for community batteries using real-world data.</p></p class="citation"></blockquote><h3 id=1214--228286-bayesian-physics-informed-neural-networks-for-system-identification-of-inverter-dominated-power-systems-simon-stock-et-al-2024>(12/14 | 228/286) Bayesian Physics-informed Neural Networks for System Identification of Inverter-dominated Power Systems (Simon Stock et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Stock, Davood Babazadeh, Christian Becker, Spyros Chatzivasileiadis. (2024)<br><strong>Bayesian Physics-informed Neural Networks for System Identification of Inverter-dominated Power Systems</strong><br><button class=copy-to-clipboard title="Bayesian Physics-informed Neural Networks for System Identification of Inverter-dominated Power Systems" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13602v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13602v1.pdf filename=2403.13602v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While the uncertainty in generation and demand increases, accurately estimating the dynamic characteristics of power systems becomes crucial for employing the appropriate control actions to maintain their stability. In our previous work, we have shown that Bayesian Physics-informed Neural Networks (BPINNs) outperform conventional system identification methods in identifying the power system dynamic behavior under measurement noise. This paper takes the next natural step and addresses the more significant challenge, exploring how BPINN perform in estimating power system dynamics under increasing uncertainty from many Inverter-based Resources (IBRs) connected to the grid. These introduce a different type of uncertainty, compared to noisy measurements. The BPINN combines the advantages of Physics-informed Neural Networks (PINNs), such as inverse problem applicability, with Bayesian approaches for uncertainty quantification. We explore the BPINN performance on a wide range of systems, starting from a single machine infinite bus (SMIB) system and 3-bus system to extract important insights, to the 14-bus CIGRE distribution grid, and the large IEEE 118-bus system. We also investigate approaches that can accelerate the BPINN training, such as pretraining and <b>transfer</b> <b>learning.</b> Throughout this paper, we show that in presence of uncertainty, the BPINN achieves orders of magnitude lower errors than the widely popular method for system identification SINDy and significantly lower errors than PINN, while <b>transfer</b> <b>learning</b> helps reduce training time by up to 80 %.</p></p class="citation"></blockquote><h3 id=1314--229286-clustering-heuristics-for-robust-energy-capacitated-vehicle-routing-problem-ecvrp-mark-pustilnik-et-al-2024>(13/14 | 229/286) Clustering Heuristics for Robust Energy Capacitated Vehicle Routing Problem (ECVRP) (Mark Pustilnik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mark Pustilnik, Francesco Borrelli. (2024)<br><strong>Clustering Heuristics for Robust Energy Capacitated Vehicle Routing Problem (ECVRP)</strong><br><button class=copy-to-clipboard title="Clustering Heuristics for Robust Energy Capacitated Vehicle Routing Problem (ECVRP)" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 6<br>Keywords: Benchmarking, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13906v1.pdf filename=2403.13906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper presents an approach to solving the Robust Energy Capacitated Vehicle Routing Problem (RECVRP), focusing on electric vehicles and their limited battery capacity. A finite number of customers, each with their own demand, have to be serviced by an electric vehicle fleet while ensuring that none of the vehicles run out of energy. The time and energy it takes to travel between any two points is modeled as a random variable with known distribution. We propose a Mixed Integer Program (MIP) for computing an exact solution and introduce <b>clustering</b> heuristics to enhance the solution speed. This enables efficient re-planning of routes in dynamic scenarios. The methodology transforms the RECVRP into smaller problems, yielding good quality solutions quickly compared to existing methods. We demonstrate the effectiveness of this approach using a well-known <b>benchmark</b> problem set as well as a set of randomly generated problems.</p></p class="citation"></blockquote><h3 id=1414--230286-macroscopic-pricing-schemes-for-the-utilization-of-pool-ride-hailing-vehicles-in-bus-lanes-lynn-fayed-et-al-2024>(14/14 | 230/286) Macroscopic pricing schemes for the utilization of pool ride-hailing vehicles in bus lanes (Lynn Fayed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lynn Fayed, Gustav Nilsson, Nikolas Geroliminis. (2024)<br><strong>Macroscopic pricing schemes for the utilization of pool ride-hailing vehicles in bus lanes</strong><br><button class=copy-to-clipboard title="Macroscopic pricing schemes for the utilization of pool ride-hailing vehicles in bus lanes" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13651v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13651v1.pdf filename=2403.13651v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing popularity of ride-hailing services, new modes of transportation are having a significant impact on the overall performance of transportation networks. As a result, there is a need to ensure that both the various transportation alternatives and the spatial network resources are used efficiently. In this work, we analyze a network configuration where part of the urban transportation network is devoted to dedicated bus lanes. Apart from buses, we let pool ride-hailing trips use the dedicated bus lanes which, contingent upon the demand for the remaining modes, may result in faster trips for users opting for the pooling alternative. Under an aggregated modelling framework, we characterize the spatial configuration and the <b>multi-modal</b> demand split for which this strategy achieves a system optimum. For these specific scenarios, we compute the equilibrium when ride-hailing users can choose between solo and pool services, and we provide a pricing scheme for mitigating the gap between total user delays of the system optimum and user equilibrium solutions, when needed.</p></p class="citation"></blockquote><h2 id=physicschem-ph-1>physics.chem-ph (1)</h2><h3 id=11--231286-considerations-in-the-use-of-ml-interaction-potentials-for-free-energy-calculations-orlando-a-mendible-et-al-2024>(1/1 | 231/286) Considerations in the use of ML interaction potentials for free energy calculations (Orlando A. Mendible et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Orlando A. Mendible, Jonathan K. Whitmer, Yamil J. Colón. (2024)<br><strong>Considerations in the use of ML interaction potentials for free energy calculations</strong><br><button class=copy-to-clipboard title="Considerations in the use of ML interaction potentials for free energy calculations" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.chem-ph<br>Categories: cond-mat-mtrl-sci, cs-LG, physics-chem-ph, physics.chem-ph<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13952v1.pdf filename=2403.13952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning potentials (MLPs) offer the potential to accurately model the energy and free energy landscapes of molecules with the precision of quantum mechanics and an efficiency similar to classical <b>simulations.</b> This research focuses on using equivariant <b>graph</b> <b>neural</b> <b>networks</b> MLPs due to their proven effectiveness in modeling equilibrium molecular trajectories. A key issue addressed is the capability of MLPs to accurately predict free energies and transition states by considering both the energy and the diversity of molecular configurations. We examined how the distribution of collective variables (CVs) in the training data affects MLP accuracy in determining the free energy surface (FES) of systems, using Metadynamics <b>simulations</b> for butane and alanine dipeptide (ADP). The study involved training forty-three MLPs, half based on classical molecular dynamics data and the rest on ab initio computed energies. The MLPs were trained using different distributions that aim to replicate hypothetical scenarios of sampled CVs obtained if the underlying FES of the system was unknown. Findings for butane revealed that training data coverage of key FES regions ensures model accuracy regardless of CV distribution. However, missing significant FES regions led to correct potential energy predictions but failed free energy reconstruction. For ADP, models trained on classical dynamics data were notably less accurate, while ab initio-based MLPs predicted potential energy well but faltered on free energy predictions. These results emphasize the challenge of assembling an all-encompassing training set for accurate FES prediction and highlight the importance of understanding the FES in preparing training data. The study points out the limitations of MLPs in free energy calculations, stressing the need for comprehensive data that encompasses the system&rsquo;s full FES for effective model training.</p></p class="citation"></blockquote><h2 id=csma-3>cs.MA (3)</h2><h3 id=13--232286-motion-prediction-of-multi-agent-systems-with-multi-view-clustering-anegi-james-et-al-2024>(1/3 | 232/286) Motion Prediction of Multi-agent systems with Multi-view clustering (Anegi James et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anegi James, Efstathios Bakolas. (2024)<br><strong>Motion Prediction of Multi-agent systems with Multi-view clustering</strong><br><button class=copy-to-clipboard title="Motion Prediction of Multi-agent systems with Multi-view clustering" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 33<br>Keywords: Clustering, Hierarchical Clustering, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13905v1.pdf filename=2403.13905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a method for future motion prediction of multi-agent systems by including group formation information and future intent. Formation of groups depends on a physics-based <b>clustering</b> method that follows the agglomerative <b>hierarchical</b> <b>clustering</b> algorithm. We identify clusters that incorporate the minimum cost-to-go function of a relevant optimal control problem as a metric for <b>clustering</b> between the groups among agents, where groups with similar associated costs are assumed to be likely to move together. The cost metric accounts for proximity to other agents as well as the intended goal of each agent. An unscented Kalman filter based approach is used to update the established clusters as well as add new clusters when new information is obtained. Our approach is verified through non-trivial numerical <b>simulations</b> implementing the proposed algorithm on different datasets pertaining to a variety of scenarios and agents.</p></p class="citation"></blockquote><h3 id=23--233286-hyper-strategy-logic-raven-beutner-et-al-2024>(2/3 | 233/286) Hyper Strategy Logic (Raven Beutner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raven Beutner, Bernd Finkbeiner. (2024)<br><strong>Hyper Strategy Logic</strong><br><button class=copy-to-clipboard title="Hyper Strategy Logic" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-LO, cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13741v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13741v1.pdf filename=2403.13741v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Strategy logic (SL) is a powerful temporal logic that enables strategic <b>reasoning</b> in multi-agent systems. SL supports explicit (first-order) quantification over strategies and provides a logical framework to express many important properties such as Nash equilibria, dominant strategies, etc. While in SL the same strategy can be used in multiple strategy profiles, each such profile is evaluated w.r.t. a path-property, i.e., a property that considers the single path resulting from a particular strategic interaction. In this paper, we present Hyper Strategy Logic (HyperSL), a strategy logic where the outcome of multiple strategy profiles can be compared w.r.t. a hyperproperty, i.e., a property that relates multiple paths. We show that HyperSL can capture important properties that cannot be expressed in SL, including non-interference, quantitative Nash equilibria, optimal adversarial planning, and <b>reasoning</b> under imperfect information. On the algorithmic side, we identify an expressive fragment of HyperSL with decidable model checking and present a model-checking algorithm. We contribute a prototype implementation of our algorithm and report on encouraging experimental results.</p></p class="citation"></blockquote><h3 id=33--234286-multi-agent-reinforcement-traffic-signal-control-based-on-interpretable-influence-mechanism-and-biased-relu-approximation-zhiyue-luo-et-al-2024>(3/3 | 234/286) Multi-agent Reinforcement Traffic Signal Control based on Interpretable Influence Mechanism and Biased ReLU Approximation (Zhiyue Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyue Luo, Jun Xu, Fanglin Chen. (2024)<br><strong>Multi-agent Reinforcement Traffic Signal Control based on Interpretable Influence Mechanism and Biased ReLU Approximation</strong><br><button class=copy-to-clipboard title="Multi-agent Reinforcement Traffic Signal Control based on Interpretable Influence Mechanism and Biased ReLU Approximation" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13639v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13639v1.pdf filename=2403.13639v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traffic signal control is important in intelligent transportation system, of which cooperative control is difficult to realize but yet vital. Many methods model multi-intersection traffic networks as grids and address the problem using multi-agent <b>reinforcement</b> <b>learning</b> (RL). Despite these existing studies, there is an opportunity to further enhance our understanding of the connectivity and globality of the traffic networks by capturing the spatiotemporal traffic information with efficient neural networks in deep RL. In this paper, we propose a novel multi-agent actor-critic framework based on an interpretable influence mechanism with a centralized learning and decentralized execution method. Specifically, we first construct an actor-critic framework, for which the piecewise linear neural network (PWLNN), named biased ReLU (BReLU), is used as the function approximator to obtain a more accurate and theoretically grounded approximation. Finally, our proposed framework is validated on two synthetic traffic networks to coordinate signal control between intersections, achieving lower traffic delays across the entire traffic network compared to state-of-the-art (SOTA) performance.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=11--235286-a-comparative-study-of-machine-learning-models-predicting-energetics-of-interacting-defects-hao-yu-2024>(1/1 | 235/286) A Comparative Study of Machine Learning Models Predicting Energetics of Interacting Defects (Hao Yu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Yu. (2024)<br><strong>A Comparative Study of Machine Learning Models Predicting Energetics of Interacting Defects</strong><br><button class=copy-to-clipboard title="A Comparative Study of Machine Learning Models Predicting Energetics of Interacting Defects" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-LG, physics-comp-ph<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13243v1.pdf filename=2403.13243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interacting defect systems are ubiquitous in materials under realistic scenarios, yet gaining an atomic-level understanding of these systems from a computational perspective is challenging - it often demands substantial resources due to the necessity of employing supercell calculations. While machine learning techniques have shown potential in accelerating materials <b>simulations,</b> their application to systems involving interacting defects remains relatively rare. In this work, we present a comparative study of three different methods to predict the free energy change of systems with interacting defects. We leveraging a limited dataset from Density Functional Theory(DFT) calculations to assess the performance models using materials descriptors, <b>graph</b> <b>neural</b> <b>networks</b> and cluster expansion. Our findings indicate that the cluster expansion model can achieve precise energetics predictions even with this limited dataset. Furthermore, with synthetic data generate from cluster expansion model at near-DFT levels, we obtained enlarged dataset to assess the demands on data for training accurate prediction models using <b>graph</b> <b>neural</b> <b>networks</b> for systems featuring interacting defects. A brief discussion of the computational cost for each method is provided at the end. This research provide a preliminary evaluation of applying machine learning techniques in imperfect surface systems.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--236286-antithetic-multilevel-methods-for-elliptic-and-hypo-elliptic-diffusions-with-applications-yuga-iguchi-et-al-2024>(1/2 | 236/286) Antithetic Multilevel Methods for Elliptic and Hypo-Elliptic Diffusions with Applications (Yuga Iguchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuga Iguchi, Ajay Jasra, Mohamed Maama, Alexandros Beskos. (2024)<br><strong>Antithetic Multilevel Methods for Elliptic and Hypo-Elliptic Diffusions with Applications</strong><br><button class=copy-to-clipboard title="Antithetic Multilevel Methods for Elliptic and Hypo-Elliptic Diffusions with Applications" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, stat-CO, stat-ME<br>Keyword Score: 30<br>Keywords: Discrete Time, Discrete Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13489v1.pdf filename=2403.13489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a new antithetic multilevel Monte Carlo (MLMC) method for the estimation of expectations with respect to laws of diffusion processes that can be elliptic or hypo-elliptic. In particular, we consider the case where one has to resort to time discretization of the diffusion and numerical <b>simulation</b> of such schemes. Motivated by recent developments, we introduce a new MLMC estimator of expectations, which does not require <b>simulation</b> of intractable L'evy areas but has a weak error of order 2 and achieves the optimal computational complexity. We then show how this approach can be used in the context of the filtering problem associated to partially observed diffusions with <b>discrete</b> <b>time</b> observations. We illustrate with numerical <b>simulations</b> that our new approaches provide efficiency gains for several problems relative to some existing methods.</p></p class="citation"></blockquote><h3 id=22--237286-a-high-fidelity-material-point-method-for-frictional-contact-problems-emmanouil-g-kakouris-et-al-2024>(2/2 | 237/286) A high-fidelity material point method for frictional contact problems (Emmanouil G. Kakouris et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emmanouil G. Kakouris, Manolis N. Chatzis, Savvas P. Triantafyllou. (2024)<br><strong>A high-fidelity material point method for frictional contact problems</strong><br><button class=copy-to-clipboard title="A high-fidelity material point method for frictional contact problems" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13534v1.pdf filename=2403.13534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A novel Material Point Method (MPM) is introduced for addressing frictional contact problems. In contrast to the standard multi-velocity field approach, this method employs a penalty method to evaluate contact forces at the discretised boundaries of their respective physical domains. This enhances <b>simulation</b> fidelity by accurately considering the deformability of the contact surface, preventing fictitious gaps between bodies in contact. Additionally, the method utilises the Extended B-Splines (EBSs) domain approximation, providing two key advantages. First, EBSs robustly mitigate grid cell-crossing errors by offering continuous gradients of the basis functions on the interface between adjacent grid cells. Second, numerical integration errors are minimised, even with small physical domains in occupied grid cells. The proposed method&rsquo;s robustness and accuracy are evaluated through <b>benchmarks,</b> including comparisons with analytical solutions, other MPM-based contact algorithms, and experimental observations from the literature. Notably, the method demonstrates effective mitigation of stress errors inherent in contact <b>simulations.</b></p></p class="citation"></blockquote><h2 id=cssi-5>cs.SI (5)</h2><h3 id=15--238286-incentivizing-news-consumption-on-social-media-platforms-using-large-language-models-and-realistic-bot-accounts-hadi-askari-et-al-2024>(1/5 | 238/286) Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts (Hadi Askari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hadi Askari, Anshuman Chhabra, Bernhard Clemm von Hohenberg, Michael Heseltine, Magdalena Wojcieszak. (2024)<br><strong>Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts</strong><br><button class=copy-to-clipboard title="Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-CL, cs-SI, cs.SI<br>Keyword Score: 30<br>Keywords: GPT, GPT-2, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13362v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13362v2.pdf filename=2403.13362v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Polarization, declining trust, and wavering support for democratic norms are pressing threats to U.S. democracy. Exposure to verified and quality news may lower individual susceptibility to these threats and make citizens more resilient to misinformation, populism, and hyperpartisan rhetoric. This project examines how to enhance users&rsquo; exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. We rely on a <b>large-scale</b> <b>two-week</b> <b>long</b> field experiment (from 1/19/2023 to 2/3/2023) on 28,457 Twitter users. We created 28 bots utilizing <b>GPT-2</b> that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing two hardcoded elements: a URL to the topic-relevant section of quality news organization and an encouragement to follow its Twitter account. To further test differential effects by gender of the bots, treated users were randomly assigned to receive responses by bots presented as female or male. We examine whether our over-time intervention enhances the following of news media organization, the sharing and the liking of news content and the tweeting about politics and the liking of political content. We find that the treated users followed more news accounts and the users in the female bot treatment were more likely to like news content than the control. Most of these results, however, were small in magnitude and confined to the already politically interested Twitter users, as indicated by their pre-treatment tweeting about politics. These findings have implications for social media and news organizations, and also offer direction for future work on how <b>Large</b> <b>Language</b> <b>Models</b> and other computational interventions can effectively enhance individual on-platform engagement with quality news and public affairs.</p></p class="citation"></blockquote><h3 id=25--239286-graph-neural-network-for-crawling-target-nodes-in-social-networks-kirill-lukyanov-et-al-2024>(2/5 | 239/286) Graph Neural Network for Crawling Target Nodes in Social Networks (Kirill Lukyanov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kirill Lukyanov, Mikhail Drobyshevskiy, Danil Shaikhelislamov, Denis Turdakov. (2024)<br><strong>Graph Neural Network for Crawling Target Nodes in Social Networks</strong><br><button class=copy-to-clipboard title="Graph Neural Network for Crawling Target Nodes in Social Networks" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: I-2-6, cs-LG, cs-SI, cs.SI<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13865v1.pdf filename=2403.13865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social networks crawling is in the focus of active research the last years. One of the challenging task is to collect target nodes in an initially unknown <b>graph</b> <b>given</b> <b>a</b> budget of crawling steps. Predicting a node property based on its partially known neighbourhood is at the heart of a successful crawler. In this paper we adopt <b>graph</b> <b>neural</b> <b>networks</b> for this purpose and show they are competitive to traditional classifiers and are better for individual cases. Additionally we suggest a training sample boosting technique, which helps to diversify the training set at early stages of crawling and thus improves the predictor quality. The experimental study on three types of target set topology indicates <b>GNN</b> based approach has a potential in crawling task, especially in the case of distributed target nodes.</p></p class="citation"></blockquote><h3 id=35--240286-what-makes-a-small-world-network-leveraging-machine-learning-for-the-robust-prediction-and-classification-of-networks-raima-carol-appaw-et-al-2024>(3/5 | 240/286) What makes a small-world network? Leveraging machine learning for the robust prediction and classification of networks (Raima Carol Appaw et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raima Carol Appaw, Nicholas Fountain-Jones, Michael A. Charleston. (2024)<br><strong>What makes a small-world network? Leveraging machine learning for the robust prediction and classification of networks</strong><br><button class=copy-to-clipboard title="What makes a small-world network? Leveraging machine learning for the robust prediction and classification of networks" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI, math-SP, stat-ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13215v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13215v1.pdf filename=2403.13215v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to simulate realistic networks based on empirical data is an important task across scientific disciplines, from epidemiology to computer science. Often <b>simulation</b> approaches involve selecting a suitable network generative model such as Erd"os-R'enyi or small-world. However, few tools are available to quantify if a particular generative model is suitable for capturing a given network structure or organization. We utilize advances in interpretable machine learning to classify simulated networks by our generative models based on various network attributes, using both primary features and their interactions. Our study underscores the significance of specific network features and their interactions in distinguishing generative models, comprehending complex network structures, and forming real-world networks</p></p class="citation"></blockquote><h3 id=45--241286-use-dynamic-user-modeling-with-stateful-sequence-models-zhihan-zhou-et-al-2024>(4/5 | 241/286) USE: Dynamic User Modeling with Stateful Sequence Models (Zhihan Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihan Zhou, Qixiang Fang, Leonardo Neves, Francesco Barbieri, Yozen Liu, Han Liu, Maarten W. Bos, Ron Dotsch. (2024)<br><strong>USE: Dynamic User Modeling with Stateful Sequence Models</strong><br><button class=copy-to-clipboard title="USE: Dynamic User Modeling with Stateful Sequence Models" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-CL, cs-HC, cs-IR, cs-LG, cs-SI, cs.SI<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13344v1.pdf filename=2403.13344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>User embeddings play a crucial role in user engagement forecasting and personalized services. Recent advances in sequence modeling have sparked interest in learning user embeddings from behavioral data. Yet behavior-based user embedding learning faces the unique challenge of dynamic user modeling. As users continuously interact with the apps, user embeddings should be periodically updated to account for users&rsquo; recent and long-term behavior patterns. Existing methods highly rely on stateless sequence models that lack memory of historical behavior. They have to either discard historical data and use only the most recent data or reprocess the old and new data jointly. Both cases incur substantial computational overhead. To address this limitation, we introduce User Stateful Embedding (USE). USE generates user embeddings and reflects users&rsquo; evolving behaviors without the need for exhaustive reprocessing by storing previous model states and revisiting them in the future. Furthermore, we introduce a novel training objective named future W-behavior prediction to transcend the limitations of next-token prediction by forecasting a broader horizon of upcoming user behaviors. By combining it with the Same User Prediction, a <b>contrastive</b> <b>learning-based</b> objective that predicts whether different segments of behavior sequences belong to the same user, we further improve the embeddings&rsquo; distinctiveness and representativeness. We conducted experiments on 8 downstream tasks using Snapchat users&rsquo; behavioral logs in both static (i.e., fixed user behavior sequences) and dynamic (i.e., periodically updated user behavior sequences) settings. We demonstrate USE&rsquo;s superior performance over established baselines. The results underscore USE&rsquo;s effectiveness and efficiency in integrating historical and recent user behavior sequences into user embeddings in dynamic user modeling.</p></p class="citation"></blockquote><h3 id=55--242286-mathematical-model-of-information-bubbles-on-networks-pál-burai-et-al-2024>(5/5 | 242/286) Mathematical model of information bubbles on networks (Pál Burai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pál Burai, Paweł Pasteczka. (2024)<br><strong>Mathematical model of information bubbles on networks</strong><br><button class=copy-to-clipboard title="Mathematical model of information bubbles on networks" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: 94C15, 05C82, 26E60, 91D30, cs-SI, cs.SI, math-PR, physics-soc-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13875v1.pdf filename=2403.13875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The main goal of this paper to introduce a new model of evolvement of narratives (common opinions, information bubble) on networks. Our main tools come from invariant mean theory and <b>graph</b> theory. The case, when the root set of the network (influencers, news agencies, etc.) is ergodic is fully discussed. The other possibility, when the root contains more than one component is partially discussed and it could be a motivation for further research.</p></p class="citation"></blockquote><h2 id=cssd-5>cs.SD (5)</h2><h3 id=15--243286-building-speech-corpus-with-diverse-voice-characteristics-for-its-prompt-based-representation-aya-watanabe-et-al-2024>(1/5 | 243/286) Building speech corpus with diverse voice characteristics for its prompt-based representation (Aya Watanabe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aya Watanabe, Shinnosuke Takamichi, Yuki Saito, Wataru Nakata, Detai Xin, Hiroshi Saruwatari. (2024)<br><strong>Building speech corpus with diverse voice characteristics for its prompt-based representation</strong><br><button class=copy-to-clipboard title="Building speech corpus with diverse voice characteristics for its prompt-based representation" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Text-to-speech, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13353v1.pdf filename=2403.13353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>text-to-speech</b> synthesis, the ability to control voice characteristics is vital for various applications. By leveraging thriving text <b>prompt-based</b> generation techniques, it should be possible to enhance the nuanced control of voice characteristics. While previous research has explored the <b>prompt-based</b> manipulation of voice characteristics, most studies have used pre-recorded speech, which limits the diversity of voice characteristics available. Thus, we aim to address this gap by creating a novel corpus and developing a model for <b>prompt-based</b> manipulation of voice characteristics in <b>text-to-speech</b> synthesis, facilitating a broader range of voice characteristics. Specifically, we propose a method to build a sizable corpus pairing voice characteristics descriptions with corresponding speech samples. This involves automatically gathering voice-related speech data from the Internet, ensuring its quality, and manually annotating it using crowdsourcing. We implement this method with Japanese language data and analyze the results to validate its effectiveness. Subsequently, we propose a construction method of the model to retrieve speech from voice characteristics descriptions based on a <b>contrastive</b> <b>learning</b> method. We train the model using not only conservative <b>contrastive</b> <b>learning</b> but also feature prediction learning to predict quantitative speech features corresponding to voice characteristics. We evaluate the model performance via experiments with the corpus we constructed above.</p></p class="citation"></blockquote><h3 id=25--244286-frequency-aware-convolution-for-sound-event-detection-tao-song-2024>(2/5 | 244/286) Frequency-aware convolution for sound event detection (Tao Song, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Song. (2024)<br><strong>Frequency-aware convolution for sound event detection</strong><br><button class=copy-to-clipboard title="Frequency-aware convolution for sound event detection" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13252v1.pdf filename=2403.13252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In sound <b>event</b> <b>detection</b> (SED), <b>convolution</b> neural networks <b>(CNNs)</b> are widely used to extract time-frequency patterns from the input spectrogram. However, features extracted by <b>CNN</b> can be insensitive to the shift of time-frequency patterns along the frequency axis. To address this issue, frequency dynamic <b>convolution</b> (FDY) has been proposed, which applies different kernels to different frequency components. Compared to the vannila <b>CNN,</b> FDY requires several times more parameters. In this paper, a more efficient solution named frequency-aware <b>convolution</b> (FAC) is proposed. In FAC, frequency-positional information is encoded in a vector and added to the input spectrogram. To match the amplitude of input, the encoding vector is scaled adaptively and channel-independently. Experiments are carried out in the context of DCASE 2022 task 4, and the results demonstrate that FAC can achieve comparable performance to that of FDY with only 515 additional parameters, while FDY requires 8.02 million additional parameters. The ablation study shows that scaling the encoding vector adaptively and channel-independently is critical to the performance of FAC.</p></p class="citation"></blockquote><h3 id=35--245286-utduss-utokyo-sarulab-system-for-interspeech2024-speech-processing-using-discrete-speech-unit-challenge-wataru-nakata-et-al-2024>(3/5 | 245/286) UTDUSS: UTokyo-SaruLab System for Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge (Wataru Nakata et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wataru Nakata, Kazuki Yamauchi, Dong Yang, Hiroaki Hyodo, Yuki Saito. (2024)<br><strong>UTDUSS: UTokyo-SaruLab System for Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge</strong><br><button class=copy-to-clipboard title="UTDUSS: UTokyo-SaruLab System for Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Transformer, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13720v1.pdf filename=2403.13720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present UTDUSS, the UTokyo-SaruLab system submitted to Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge. The challenge focuses on using discrete speech unit learned from large speech corpora for some tasks. We submitted our UTDUSS system to two <b>text-to-speech</b> tracks: Vocoder and Acoustic+Vocoder. Our system incorporates neural audio codec (NAC) pre-trained on only speech corpora, which makes the learned codec represent rich acoustic features that are necessary for high-fidelity speech reconstruction. For the acoustic+vocoder track, we trained an acoustic model based on <b>Transformer</b> encoder-decoder that predicted the pre-trained NAC tokens from text input. We describe our strategies to build these models, such as data selection, downsampling, and hyper-parameter tuning. Our system ranked in second and first for the Vocoder and Acoustic+Vocoder tracks, respectively.</p></p class="citation"></blockquote><h3 id=45--246286-advanced-long-content-speech-recognition-with-factorized-neural-transducer-xun-gong-et-al-2024>(4/5 | 246/286) Advanced Long-Content Speech Recognition With Factorized Neural Transducer (Xun Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xun Gong, Yu Wu, Jinyu Li, Shujie Liu, Rui Zhao, Xie Chen, Yanmin Qian. (2024)<br><strong>Advanced Long-Content Speech Recognition With Factorized Neural Transducer</strong><br><button class=copy-to-clipboard title="Advanced Long-Content Speech Recognition With Factorized Neural Transducer" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13423v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13423v1.pdf filename=2403.13423v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose two novel approaches, which integrate long-content information into the factorized neural transducer (FNT) based architecture in both non-streaming (referred to as LongFNT ) and streaming (referred to as SLongFNT ) scenarios. We first investigate whether long-content transcriptions can improve the vanilla conformer transducer (C-T) models. Our experiments indicate that the vanilla C-T models do not exhibit improved performance when utilizing long-content transcriptions, possibly due to the predictor network of C-T models not functioning as a pure language model. Instead, FNT shows its potential in utilizing long-content information, where we propose the LongFNT model and explore the impact of long-content information in both text (LongFNT-Text) and <b>speech</b> <b>(LongFNT-Speech).</b> The proposed LongFNT-Text and LongFNT-Speech models further complement each other to achieve better performance, with transcription history proving more valuable to the model. The effectiveness of our LongFNT approach is evaluated on LibriSpeech and GigaSpeech corpora, and obtains relative 19% and 12% word error rate reduction, respectively. Furthermore, we extend the LongFNT model to the streaming scenario, which is named SLongFNT , consisting of SLongFNT-Text and SLongFNT-Speech approaches to utilize long-content text and <b>speech</b> <b>information.</b> Experiments show that the proposed SLongFNT model achieves relative 26% and 17% WER reduction on LibriSpeech and GigaSpeech respectively while keeping a good latency, compared to the FNT baseline. Overall, our proposed LongFNT and SLongFNT highlight the significance of considering long-content <b>speech</b> <b>and</b> transcription knowledge for improving both non-streaming and streaming <b>speech</b> <b>recognition</b> systems.</p></p class="citation"></blockquote><h3 id=55--247286-onset-and-offset-weighted-loss-function-for-sound-event-detection-tao-song-2024>(5/5 | 247/286) Onset and offset weighted loss function for sound event detection (Tao Song, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Song. (2024)<br><strong>Onset and offset weighted loss function for sound event detection</strong><br><button class=copy-to-clipboard title="Onset and offset weighted loss function for sound event detection" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13254v1.pdf filename=2403.13254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a typical sound <b>event</b> <b>detection</b> (SED) system, the existence of a sound <b>event</b> <b>is</b> detected at a frame level, and consecutive frames with the same <b>event</b> <b>detected</b> are combined as one sound <b>event.</b> <b>The</b> median filter is applied as a post-processing step to remove detection errors as much as possible. However, detection errors occurring around the onset and offset of a sound <b>event</b> <b>are</b> beyond the capacity of the median filter. To address this issue, an onset and offset weighted binary cross-entropy (OWBCE) loss function is proposed in this paper, which trains the DNN model to be more robust on frames around (a) onsets and offsets. Experiments are carried out in the context of DCASE 2022 task 4. Results show that OWBCE outperforms BCE when different models are considered. For a basic CRNN, relative improvements of 6.43% in <b>event-F1,</b> <b>1.96%</b> in PSDS1, and 2.43% in PSDS2 can be achieved by OWBCE.</p></p class="citation"></blockquote><h2 id=csdb-2>cs.DB (2)</h2><h3 id=12--248286-no-more-optimization-rules-llm-enabled-policy-based-multi-modal-query-optimizer-version-1-yifan-wang-et-al-2024>(1/2 | 248/286) No more optimization rules: LLM-enabled policy-based multi-modal query optimizer (version 1) (Yifan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Wang, Haodi Ma, Daisy Zhe Wang. (2024)<br><strong>No more optimization rules: LLM-enabled policy-based multi-modal query optimizer (version 1)</strong><br><button class=copy-to-clipboard title="No more optimization rules: LLM-enabled policy-based multi-modal query optimizer (version 1)" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-DB, cs-IR, cs.DB<br>Keyword Score: 23<br>Keywords: Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13597v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13597v1.pdf filename=2403.13597v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>model</b> <b>(LLM)</b> has marked a pivotal moment in the field of machine learning and deep learning. Recently its capability for query planning has been investigated, including both single-modal and <b>multi-modal</b> queries. However, there is no work on the query optimization capability of <b>LLM.</b> As a critical (or could even be the most important) step that significantly impacts the execution performance of the query plan, such analysis and attempts should not be missed. From another aspect, existing query optimizers are usually rule-based or rule-based + cost-based, i.e., they are dependent on manually created rules to complete the query plan rewrite/transformation. Given the fact that modern optimizers include hundreds to thousands of rules, designing a <b>multi-modal</b> query optimizer following a similar way is significantly time-consuming since we will have to enumerate as many <b>multi-modal</b> optimization rules as possible, which has not been well addressed today. In this paper, we investigate the query optimization ability of <b>LLM</b> and use <b>LLM</b> to design LaPuda, a novel <b>LLM</b> and Policy based <b>multi-modal</b> query optimizer. Instead of enumerating specific and detailed rules, LaPuda only needs a few abstract policies to guide <b>LLM</b> in the optimization, by which much time and human effort are saved. Furthermore, to prevent <b>LLM</b> from making mistakes or negative optimization, we borrow the idea of gradient descent and propose a guided cost descent (GCD) algorithm to perform the optimization, such that the optimization can be kept in the correct direction. In our evaluation, our methods consistently outperform the baselines in most cases. For example, the optimized plans generated by our methods result in 1~3x higher execution speed than those by the baselines.</p></p class="citation"></blockquote><h3 id=22--249286-distance-comparison-operators-for-approximate-nearest-neighbor-search-exploration-and-benchmark-zeyu-wang-et-al-2024>(2/2 | 249/286) Distance Comparison Operators for Approximate Nearest Neighbor Search: Exploration and Benchmark (Zeyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Wang, Haoran Xiong, Zhenying He, Peng Wang, Wei wang. (2024)<br><strong>Distance Comparison Operators for Approximate Nearest Neighbor Search: Exploration and Benchmark</strong><br><button class=copy-to-clipboard title="Distance Comparison Operators for Approximate Nearest Neighbor Search: Exploration and Benchmark" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13491v1.pdf filename=2403.13491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Approximate nearest neighbor search (ANNS) on high-dimensional vectors has become a fundamental and essential component in various machine learning tasks. Prior research has shown that the distance comparison operation is the bottleneck of ANNS, which determines the query and indexing performance. To overcome this challenge, some novel methods have been proposed recently. The basic idea is to estimate the actual distance with fewer calculations, at the cost of accuracy loss. Inspired by this, we also propose that some classical techniques and deep learning models can also be adapted to this purpose. In this paper, we systematically categorize the techniques that have been or can be used to accelerate distance approximation. And to help the users understand the pros and cons of different techniques, we design a fair and comprehensive <b>benchmark,</b> Fudist implements these techniques with the same base index and evaluates them on 16 real datasets with several evaluation metrics. Designed as an independent and portable library, Fudist is orthogonal to the specific index structure and thus can be easily utilized in the current ANNS library to achieve significant improvements.</p></p class="citation"></blockquote><h2 id=csdc-5>cs.DC (5)</h2><h3 id=15--250286-regent-based-parallel-meshfree-lskum-solver-for-heterogenous-hpc-platforms-sanath-salil-et-al-2024>(1/5 | 250/286) Regent based parallel meshfree LSKUM solver for heterogenous HPC platforms (Sanath Salil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanath Salil, Nischay Ram Mamidi, Anil Nemili, Elliott Slaughter. (2024)<br><strong>Regent based parallel meshfree LSKUM solver for heterogenous HPC platforms</strong><br><button class=copy-to-clipboard title="Regent based parallel meshfree LSKUM solver for heterogenous HPC platforms" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13287v1.pdf filename=2403.13287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Regent is an implicitly parallel programming language that allows the development of a single codebase for heterogeneous platforms targeting CPUs and GPUs. This paper presents the development of a parallel meshfree solver in Regent for two-dimensional inviscid compressible flows. The meshfree solver is based on the least squares kinetic upwind method. Example codes are presented to show the difference between the Regent and CUDA-C implementations of the meshfree solver on a GPU node. For CPU parallel computations, details are presented on how the data communication and synchronisation are handled by Regent and Fortran+MPI codes. The Regent solver is verified by applying it to the standard test cases for inviscid flows. <b>Benchmark</b> <b>simulations</b> are performed on coarse to very fine point distributions to assess the solver&rsquo;s performance. The computational efficiency of the Regent solver on an A100 GPU is compared with an equivalent meshfree solver written in CUDA-C. The codes are then profiled to investigate the differences in their performance. The performance of the Regent solver on CPU cores is compared with an equivalent explicitly parallel Fortran meshfree solver based on MPI. Scalability results are shown to offer insights into performance.</p></p class="citation"></blockquote><h3 id=25--251286-automated-calibration-of-parallel-and-distributed-computing-simulators-a-case-study-jesse-mcdonald-et-al-2024>(2/5 | 251/286) Automated Calibration of Parallel and Distributed Computing Simulators: A Case Study (Jesse McDonald et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jesse McDonald, Maximilian Horzela, Frédéric Suter, Henri Casanova. (2024)<br><strong>Automated Calibration of Parallel and Distributed Computing Simulators: A Case Study</strong><br><button class=copy-to-clipboard title="Automated Calibration of Parallel and Distributed Computing Simulators: A Case Study" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-PF, cs.DC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13918v1.pdf filename=2403.13918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many parallel and distributed computing research results are obtained in <b>simulation,</b> using simulators that mimic real-world executions on some target system. Each such simulator is configured by picking values for parameters that define the behavior of the underlying <b>simulation</b> models it implements. The main concern for a simulator is accuracy: simulated behaviors should be as close as possible to those observed in the real-world target system. This requires that values for each of the simulator&rsquo;s parameters be carefully picked, or &ldquo;calibrated,&rdquo; based on ground-truth real-world executions. Examining the current state of the art shows that simulator calibration, at least in the field of parallel and distributed computing, is often undocumented (and thus perhaps often not performed) and, when documented, is described as a labor-intensive, manual process. In this work we evaluate the benefit of automating <b>simulation</b> calibration using simple algorithms. Specifically, we use a real-world case study from the field of High Energy Physics and compare automated calibration to calibration performed by a domain scientist. Our main finding is that automated calibration is on par with or significantly outperforms the calibration performed by the domain scientist. Furthermore, automated calibration makes it straightforward to operate desirable trade-offs between <b>simulation</b> accuracy and <b>simulation</b> speed.</p></p class="citation"></blockquote><h3 id=35--252286-optimal-fixed-priority-scheduling-in-multi-stage-multi-resource-distributed-real-time-systems-niraj-kumar-et-al-2024>(3/5 | 252/286) Optimal Fixed Priority Scheduling in Multi-Stage Multi-Resource Distributed Real-Time Systems (Niraj Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niraj Kumar, Chuanchao Gao, Arvind Easwaran. (2024)<br><strong>Optimal Fixed Priority Scheduling in Multi-Stage Multi-Resource Distributed Real-Time Systems</strong><br><button class=copy-to-clipboard title="Optimal Fixed Priority Scheduling in Multi-Stage Multi-Resource Distributed Real-Time Systems" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13411v1.pdf filename=2403.13411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work studies fixed priority (FP) scheduling of real-time jobs with end-to-end deadlines in a distributed system. Specifically, given a multi-stage pipeline with multiple heterogeneous resources of the same type at each stage, the problem is to assign priorities to a set of real-time jobs with different release times to access a resource at each stage of the pipeline subject to the end-to-end deadline constraints. Note, in such a system, jobs may compete with different sets of jobs at different stages of the pipeline depending on the job-to-resource mapping. To this end, following are the two major contributions of this work. We show that an OPA-compatible schedulability test based on the delay composition algebra can be constructed, which we then use with an optimal priority assignment algorithm to compute a priority ordering. Further, we establish the versatility of pairwise priority assignment in such a multi-stage multi-resource system, compared to a total priority ordering. In particular, we show that a pairwise priority assignment may be feasible even if a priority ordering does not exist. We propose an integer linear programming formulation and a scalable heuristic to compute a pairwise priority assignment. We also show through <b>simulation</b> experiments that the proposed approaches can be used for the holistic scheduling of real-time jobs in edge computing systems.</p></p class="citation"></blockquote><h3 id=45--253286-agent-based-mst-construction-ajay-d-kshemkalyani-et-al-2024>(4/5 | 253/286) Agent-based MST Construction (Ajay D. Kshemkalyani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ajay D. Kshemkalyani, Manish Kumar, Anisur Rahaman Molla, Gokarna Sharma. (2024)<br><strong>Agent-based MST Construction</strong><br><button class=copy-to-clipboard title="Agent-based MST Construction" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-DS, cs-MA, cs.DC<br>Keyword Score: 13<br>Keywords: Message-Passing, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13716v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13716v1.pdf filename=2403.13716v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>{\em Minimum-weight spanning tree} (MST) is one of the fundamental and well-studied problems in distributed computing. In this paper, we initiate the study of constructing MST using mobile agents (aka robots). Suppose $n$ agents are positioned initially arbitrarily on the nodes of a connected, undirected, arbitrary, anonymous, port-labeled, weighted $n$-node, $m$-edge <b>graph</b> $G$ of diameter $D$ and maximum degree $\Delta$. The agents relocate themselves autonomously and compute an MST of $G$ such that exactly one agent positions on a node and tracks in its memory which of its adjacent edges belong to the MST. The objective is to minimize time and memory requirements. Following the literature, we consider the synchronous setting in which each agent performs its operations synchronously with others and hence time can be measured in rounds. We first establish a generic result: if $n$ and $\Delta$ are known a priori and memory per agent is as much as node memory in the <b>message-passing</b> model (of distributed computing), agents can simulate any $O(T)$-round deterministic algorithm for any problem in the <b>message-passing</b> model to the agent model in $O(\Delta T \log n+n\log^2n)$ rounds. As a corollary, MST can be constructed in the agent model in $O(\max{\Delta \sqrt{n} \log n \log^*n, \Delta D \log n,n\log^2n})$ rounds simulating the celebrated $O(\sqrt{n} \log^*n +D)$-round GKP algorithm for MST in the <b>message-passing</b> model. We then establish that, without knowing any <b>graph</b> parameter a priori, there exists a deterministic algorithm to construct MST in the agent model in $O(m+n\log n)$ rounds with $O(n \log n)$ bits memory at each agent. The presented algorithm needs to overcome highly non-trivial challenges on how to synchronize agents in computing MST as they may initially be positioned arbitrarily on the <b>graph</b> nodes.</p></p class="citation"></blockquote><h3 id=55--254286-causal-graph-dynamics-and-kan-extensions-luidnel-maignan-et-al-2024>(5/5 | 254/286) Causal Graph Dynamics and Kan Extensions (Luidnel Maignan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luidnel Maignan, Antoine Spicher. (2024)<br><strong>Causal Graph Dynamics and Kan Extensions</strong><br><button class=copy-to-clipboard title="Causal Graph Dynamics and Kan Extensions" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-DM, cs-MA, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13393v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13393v1.pdf filename=2403.13393v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>On the one side, the formalism of Global Transformations comes with the claim of capturing any transformation of space that is local, synchronous and deterministic.The claim has been proven for different classes of models such as mesh refinements from computer graphics, Lindenmayer systems from morphogenesis modeling and cellular automata from biological, physical and parallel computation modeling.The Global Transformation formalism achieves this by using category theory for its genericity, and more precisely the notion of Kan extension to determine the global behaviors based on the local ones.On the other side, Causal <b>Graph</b> Dynamics describe the transformation of port <b>graphs</b> in a synchronous and deterministic way and has not yet being tackled.In this paper, we show the precise sense in which the claim of Global Transformations holds for them as well.This is done by showing different ways in which they can be expressed as Kan extensions, each of them highlighting different features of Causal <b>Graph</b> Dynamics.Along the way, this work uncovers the interesting class of Monotonic Causal <b>Graph</b> Dynamics and their universality among General Causal <b>Graph</b> Dynamics.</p></p class="citation"></blockquote><h2 id=csdl-1>cs.DL (1)</h2><h3 id=11--255286-automated-extraction-and-maturity-analysis-of-open-source-clinical-informatics-repositories-from-scientific-literature-jeremy-r-harper-2024>(1/1 | 255/286) Automated Extraction and Maturity Analysis of Open Source Clinical Informatics Repositories from Scientific Literature (Jeremy R. Harper, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeremy R. Harper. (2024)<br><strong>Automated Extraction and Maturity Analysis of Open Source Clinical Informatics Repositories from Scientific Literature</strong><br><button class=copy-to-clipboard title="Automated Extraction and Maturity Analysis of Open Source Clinical Informatics Repositories from Scientific Literature" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-DL, cs-SE, cs.DL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14721v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14721v1.pdf filename=2403.14721v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving landscape of clinical informatics, the integration and utilization of software tools developed through governmental funding represent a pivotal advancement in research and application. However, the dispersion of these tools across various repositories, with no centralized knowledge base, poses significant challenges to leveraging their full potential. This study introduces an automated methodology to bridge this gap by systematically extracting GitHub repository URLs from academic papers indexed in arXiv, focusing on the field of clinical informatics. Our approach encompasses querying the arXiv API for relevant papers, cleaning extracted GitHub URLs, fetching comprehensive repository information via the GitHub API, and analyzing repository maturity based on defined metrics such as stars, forks, open issues, and contributors. The process is designed to be robust, incorporating error handling and rate limiting to ensure compliance with API constraints. Preliminary findings demonstrate the efficacy of this methodology in compiling a centralized knowledge base of NIH-funded software tools, laying the groundwork for an enriched understanding and utilization of these resources within the clinical informatics community. We propose the future integration of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to generate concise summaries and evaluations of the tools. This approach facilitates the discovery and assessment of clinical informatics tools and also enables ongoing monitoring of new and actively updated repositories, revolutionizing how researchers access and leverage federally funded software. The implications of this study extend beyond simplification of access to valuable resources; it proposes a scalable model for the dynamic aggregation and evaluation of scientific software, encouraging more collaborative, transparent, and efficient research practices in clinical informatics and beyond.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--256286-large-language-models-meet-network-slicing-management-and-orchestration-abdulhalim-dandoush-et-al-2024>(1/2 | 256/286) Large Language Models meet Network Slicing Management and Orchestration (Abdulhalim Dandoush et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdulhalim Dandoush, Viswanath Kumarskandpriya, Mueen Uddin, Usman Khalil. (2024)<br><strong>Large Language Models meet Network Slicing Management and Orchestration</strong><br><button class=copy-to-clipboard title="Large Language Models meet Network Slicing Management and Orchestration" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13721v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13721v1.pdf filename=2403.13721v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Network slicing, a cornerstone technology for future networks, enables the creation of customized virtual networks on a shared physical infrastructure. This fosters innovation and agility by providing dedicated resources tailored to specific applications. However, current orchestration and management approaches face limitations in handling the complexity of new service demands within multi-administrative domain environments. This paper proposes a future vision for network slicing powered by <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and multi-agent systems, offering a framework that can be integrated with existing Management and Orchestration (MANO) frameworks. This framework leverages <b>LLMs</b> to translate user intent into technical requirements, map network functions to infrastructure, and manage the entire slice lifecycle, while multi-agent systems facilitate collaboration across different administrative domains. We also discuss the challenges associated with implementing this framework and potential solutions to mitigate them.</p></p class="citation"></blockquote><h3 id=22--257286-bft-poloc-a-byzantine-fortified-trigonometric-proof-of-location-protocol-using-internet-delays-peiyao-sheng-et-al-2024>(2/2 | 257/286) BFT-PoLoc: A Byzantine Fortified Trigonometric Proof of Location Protocol using Internet Delays (Peiyao Sheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peiyao Sheng, Vishal Sevani, Himanshu Tyagi, Pramod Viswanath. (2024)<br><strong>BFT-PoLoc: A Byzantine Fortified Trigonometric Proof of Location Protocol using Internet Delays</strong><br><button class=copy-to-clipboard title="BFT-PoLoc: A Byzantine Fortified Trigonometric Proof of Location Protocol using Internet Delays" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13230v1.pdf filename=2403.13230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Internet platforms depend on accurately determining the geographical locations of online users to deliver targeted services (e.g., advertising). The advent of decentralized platforms (blockchains) emphasizes the importance of geographically distributed nodes, making the validation of locations more crucial. In these decentralized settings, mutually non-trusting participants need to {\em prove} their locations to each other. The incentives for claiming desired location include decentralization properties (validators of a blockchain), explicit rewards for improving coverage (physical infrastructure blockchains) and regulatory compliance &ndash; and entice participants towards prevaricating their true location malicious via VPNs, tampering with internet delays, or compromising other parties (challengers) to misrepresent their location. Traditional delay-based geolocation methods focus on reducing the noise in measurements and are very vulnerable to wilful divergences from prescribed protocol. In this paper we use Internet delay measurements to securely prove the location of IP addresses while being immune to a large fraction of Byzantine actions. Our core methods are to endow Internet telemetry tools (e.g., ping) with cryptographic primitives (signatures and hash functions) together with Byzantine resistant data inferences subject to Euclidean geometric constraints. We introduce two new networking protocols, robust against Byzantine actions: Proof of Internet <b>Geometry</b> (PoIG) converts delay measurements into precise distance estimates across the Internet; Proof of Location (PoLoc) enables accurate and efficient multilateration of a specific IP address. The key algorithmic innovations are in conducting ``Byzantine fortified trigonometry" (BFT) inferences of data, endowing low rank matrix completion methods with Byzantine resistance.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--258286-mimo-channel-as-a-neural-function-implicit-neural-representations-for-extreme-csi-compression-in-massive-mimo-systems-haotian-wu-et-al-2024>(1/2 | 258/286) MIMO Channel as a Neural Function: Implicit Neural Representations for Extreme CSI Compression in Massive MIMO Systems (Haotian Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotian Wu, Maojun Zhang, Yulin Shao, Krystian Mikolajczyk, Deniz Gündüz. (2024)<br><strong>MIMO Channel as a Neural Function: Implicit Neural Representations for Extreme CSI Compression in Massive MIMO Systems</strong><br><button class=copy-to-clipboard title="MIMO Channel as a Neural Function: Implicit Neural Representations for Extreme CSI Compression in Massive MIMO Systems" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: 94A24, E-4, cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Meta Learning, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13615v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13615v1.pdf filename=2403.13615v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Acquiring and utilizing accurate channel state information (CSI) can significantly improve transmission performance, thereby holding a crucial role in realizing the potential advantages of massive multiple-input multiple-output (MIMO) technology. Current prevailing CSI feedback approaches improve precision by employing advanced deep-learning methods to learn representative CSI features for a subsequent compression process. Diverging from previous works, we treat the CSI compression problem in the context of implicit neural representations. Specifically, each CSI matrix is viewed as a neural function that maps the CSI coordinates (antenna number and subchannel) to the corresponding channel gains. Instead of transmitting the parameters of the implicit neural functions directly, we transmit modulations based on the CSI matrix derived through a <b>meta-learning</b> <b>algorithm.</b> Modulations are then applied to a shared base network to generate the elements of the CSI matrix. Modulations corresponding to the CSI matrix are <b>quantized</b> and entropy-coded to further reduce the communication bandwidth, thus achieving extreme CSI compression ratios. Numerical results show that our proposed approach achieves state-of-the-art performance and showcases flexibility in feedback strategies.</p></p class="citation"></blockquote><h3 id=22--259286-massive-mimo-csi-feedback-using-channel-prediction-how-to-avoid-machine-learning-at-ue-muhammad-karam-shehzad-et-al-2024>(2/2 | 259/286) Massive MIMO CSI Feedback using Channel Prediction: How to Avoid Machine Learning at UE? (Muhammad Karam Shehzad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Karam Shehzad, Luca Rose, Mohamad Assaad. (2024)<br><strong>Massive MIMO CSI Feedback using Channel Prediction: How to Avoid Machine Learning at UE?</strong><br><button class=copy-to-clipboard title="Massive MIMO CSI Feedback using Channel Prediction: How to Avoid Machine Learning at UE?" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13363v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13363v1.pdf filename=2403.13363v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the literature, machine learning (ML) has been implemented at the base station (BS) and user equipment (UE) to improve the precision of downlink channel state information (CSI). However, ML implementation at the UE can be infeasible for various reasons, such as UE power consumption. Motivated by this issue, we propose a CSI learning mechanism at BS, called CSILaBS, to avoid ML at UE. To this end, by exploiting channel predictor (CP) at BS, a light-weight predictor function (PF) is considered for feedback evaluation at the UE. CSILaBS reduces over-the-air feedback overhead, improves CSI quality, and lowers the computation cost of UE. Besides, in a multiuser environment, we propose various mechanisms to select the feedback by exploiting PF while aiming to improve CSI accuracy. We also address various ML-based CPs, such as NeuralProphet (NP), an ML-inspired statistical algorithm. Furthermore, inspired to use a statistical model and ML together, we propose a novel hybrid framework composed of a <b>recurrent</b> <b>neural</b> <b>network</b> and NP, which yields better prediction accuracy than individual models. The performance of CSILaBS is evaluated through an empirical dataset recorded at Nokia Bell-Labs. The outcomes show that ML elimination at UE can retain performance gains, for example, precoding quality.</p></p class="citation"></blockquote><h2 id=statml-4>stat.ML (4)</h2><h3 id=14--260286-analysing-heavy-tail-properties-of-stochastic-gradient-descent-by-means-of-stochastic-recurrence-equations-ewa-damek-et-al-2024>(1/4 | 260/286) Analysing heavy-tail properties of Stochastic Gradient Descent by means of Stochastic Recurrence Equations (Ewa Damek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ewa Damek, Sebastian Mentemeier. (2024)<br><strong>Analysing heavy-tail properties of Stochastic Gradient Descent by means of Stochastic Recurrence Equations</strong><br><button class=copy-to-clipboard title="Analysing heavy-tail properties of Stochastic Gradient Descent by means of Stochastic Recurrence Equations" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: 60J05, 90C15, 60B20, cs-LG, math-OC, math-PR, math-ST, stat-ML, stat-TH, stat.ML<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13868v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13868v1.pdf filename=2403.13868v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent works on the theory of machine learning, it has been observed that heavy tail properties of <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> <b>(SGD)</b> can be studied in the probabilistic framework of <b>stochastic</b> <b>recursions.</b> <b>In</b> particular, G"{u}rb"{u}zbalaban et al. (arXiv:2006.04740) considered a setup corresponding to linear regression for which iterations of <b>SGD</b> can be modelled by a multivariate affine <b>stochastic</b> <b>recursion</b> <b>$X_k=A_k</b> X_{k-1}+B_k$, for independent and identically distributed pairs $(A_k, B_k)$, where $A_k$ is a random symmetric matrix and $B_k$ is a random vector. In this work, we will answer several open questions of the quoted paper and extend their results by applying the theory of irreducible-proximal (i-p) matrices.</p></p class="citation"></blockquote><h3 id=24--261286-adatrans-feature-wise-and-sample-wise-adaptive-transfer-learning-for-high-dimensional-regression-zelin-he-et-al-2024>(2/4 | 261/286) AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression (Zelin He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zelin He, Ying Sun, Jingyuan Liu, Runze Li. (2024)<br><strong>AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression</strong><br><button class=copy-to-clipboard title="AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-ST, stat-ME, stat-ML, stat-TH, stat.ML<br>Keyword Score: 13<br>Keywords: Sample Size, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13565v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13565v1.pdf filename=2403.13565v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the <b>transfer</b> <b>learning</b> problem in the high dimensional setting, where the feature dimension is larger than the <b>sample</b> <b>size.</b> To learn transferable information, which may vary across features or the source <b>samples,</b> <b>we</b> propose an adaptive <b>transfer</b> <b>learning</b> method that can detect and aggregate the feature-wise (F-AdaTrans) or <b>sample-wise</b> <b>(S-AdaTrans)</b> transferable structures. We achieve this by employing a novel fused-penalty, coupled with weights that can adapt according to the transferable structure. To choose the weight, we propose a theoretically informed, data-driven procedure, enabling F-AdaTrans to selectively fuse the transferable signals with the target while filtering out non-transferable signals, and S-AdaTrans to obtain the optimal combination of information transferred from each source <b>sample.</b> <b>The</b> non-asymptotic rates are established, which recover existing near-minimax optimal rates in special cases. The effectiveness of the proposed method is validated using both synthetic and real data.</p></p class="citation"></blockquote><h3 id=34--262286-kernel-multigrid-accelerate-back-fitting-via-sparse-gaussian-process-regression-lu-zou-et-al-2024>(3/4 | 262/286) Kernel Multigrid: Accelerate Back-fitting via Sparse Gaussian Process Regression (Lu Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lu Zou, Liang Ding. (2024)<br><strong>Kernel Multigrid: Accelerate Back-fitting via Sparse Gaussian Process Regression</strong><br><button class=copy-to-clipboard title="Kernel Multigrid: Accelerate Back-fitting via Sparse Gaussian Process Regression" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13300v1.pdf filename=2403.13300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Additive <b>Gaussian</b> <b>Processes</b> (GPs) are popular approaches for nonparametric feature selection. The common training method for these models is Bayesian Back-fitting. However, the convergence rate of Back-fitting in training additive GPs is still an open problem. By utilizing a technique called Kernel Packets (KP), we prove that the convergence rate of Back-fitting is no faster than $(1-\mathcal{O}(\frac{1}{n}))^t$, where $n$ and $t$ denote the data size and the iteration number, respectively. Consequently, Back-fitting requires a minimum of $\mathcal{O}(n\log n)$ iterations to achieve convergence. Based on KPs, we further propose an algorithm called Kernel Multigrid (KMG). This algorithm enhances Back-fitting by incorporating a sparse <b>Gaussian</b> <b>Process</b> Regression (GPR) to process the residuals subsequent to each Back-fitting iteration. It is applicable to additive GPs with both structured and scattered data. Theoretically, we prove that KMG reduces the required iterations to $\mathcal{O}(\log n)$ while preserving the time and space complexities at $\mathcal{O}(n\log n)$ and $\mathcal{O}(n)$ per iteration, respectively. Numerically, by employing a sparse GPR with merely 10 inducing points, KMG can produce accurate approximations of high-dimensional targets within 5 iterations.</p></p class="citation"></blockquote><h3 id=44--263286-a-sampling-based-framework-for-hypothesis-testing-on-large-attributed-graphs-yun-wang-et-al-2024>(4/4 | 263/286) A Sampling-based Framework for Hypothesis Testing on Large Attributed Graphs (Yun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yun Wang, Chrysanthi Kosyfaki, Sihem Amer-Yahia, Reynold Cheng. (2024)<br><strong>A Sampling-based Framework for Hypothesis Testing on Large Attributed Graphs</strong><br><button class=copy-to-clipboard title="A Sampling-based Framework for Hypothesis Testing on Large Attributed Graphs" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-DB, cs-LG, stat-ML, stat.ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13286v1.pdf filename=2403.13286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hypothesis testing is a statistical method used to draw conclusions about populations from sample data, typically represented in tables. With the prevalence of <b>graph</b> representations in real-life applications, hypothesis testing in <b>graphs</b> is gaining importance. In this work, we formalize node, edge, and path hypotheses in attributed <b>graphs.</b> We develop a sampling-based hypothesis testing framework, which can accommodate existing hypothesis-agnostic <b>graph</b> sampling methods. To achieve accurate and efficient sampling, we then propose a Path-Hypothesis-Aware SamplEr, PHASE, an m- dimensional random walk that accounts for the paths specified in a hypothesis. We further optimize its time efficiency and propose PHASEopt. Experiments on real datasets demonstrate the ability of our framework to leverage common <b>graph</b> sampling methods for hypothesis testing, and the superiority of hypothesis-aware sampling in terms of accuracy and time efficiency.</p></p class="citation"></blockquote><h2 id=cscy-4>cs.CY (4)</h2><h3 id=14--264286-the-future-of-generative-ai-chatbots-in-higher-education-joshua-ebere-chukwuere-2024>(1/4 | 264/286) The future of generative AI chatbots in higher education (Joshua Ebere Chukwuere, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joshua Ebere Chukwuere. (2024)<br><strong>The future of generative AI chatbots in higher education</strong><br><button class=copy-to-clipboard title="The future of generative AI chatbots in higher education" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Generative AI, Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13487v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13487v1.pdf filename=2403.13487v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of <b>generative</b> <b>Artificial</b> Intelligence (AI) <b>chatbots</b> in higher education institutions (HEIs) is reshaping the educational landscape, offering opportunities for enhanced student support, and administrative and research efficiency. This study explores the future implications of <b>generative</b> <b>AI</b> <b>chatbots</b> in HEIs, aiming to understand their potential impact on teaching and learning, and research processes. Utilizing a narrative literature review (NLR) methodology, this study synthesizes existing research on <b>generative</b> <b>AI</b> <b>chatbots</b> in higher education from diverse sources, including academic databases and scholarly publications. The findings highlight the transformative potential of <b>generative</b> <b>AI</b> <b>chatbots</b> in streamlining administrative tasks, enhancing student learning experiences, and supporting research activities. However, challenges such as academic integrity concerns, user input understanding, and resource allocation pose significant obstacles to the effective integration of <b>generative</b> <b>AI</b> <b>chatbots</b> in HEIs. This study underscores the importance of proactive measures to address ethical considerations, provide comprehensive training for stakeholders, and establish clear guidelines for the responsible use of <b>generative</b> <b>AI</b> <b>chatbots</b> in higher education. By navigating these challenges, and leveraging the benefits of <b>generative</b> <b>AI</b> technologies, HEIs can harness the full potential of <b>generative</b> <b>AI</b> <b>chatbots</b> to create a more efficient, effective, inclusive, and innovative educational environment.</p></p class="citation"></blockquote><h3 id=24--265286-inditag-an-online-media-bias-analysis-and-annotation-system-using-fine-grained-bias-indicators-luyang-lin-et-al-2024>(2/4 | 265/286) IndiTag: An Online Media Bias Analysis and Annotation System Using Fine-Grained Bias Indicators (Luyang Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luyang Lin, Lingzhi Wang, Jinsong Guo, Jing Li, Kam-Fai Wong. (2024)<br><strong>IndiTag: An Online Media Bias Analysis and Annotation System Using Fine-Grained Bias Indicators</strong><br><button class=copy-to-clipboard title="IndiTag: An Online Media Bias Analysis and Annotation System Using Fine-Grained Bias Indicators" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Fact Verification, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13446v1.pdf filename=2403.13446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the age of information overload and polarized discourse, understanding media bias has become imperative for informed decision-making and fostering a balanced public discourse. This paper presents IndiTag, an innovative online media bias analysis and annotation system that leverages fine-grained bias indicators to dissect and annotate bias in digital content. IndiTag offers a novel approach by incorporating <b>large</b> <b>language</b> <b>models,</b> bias indicator, vector database to automatically detect and interpret bias. Complemented by a user-friendly interface facilitating both automated bias analysis and manual annotation, IndiTag offers a comprehensive platform for in-depth bias examination. We demonstrate the efficacy and versatility of IndiTag through experiments on four datasets encompassing news articles from diverse platforms. Furthermore, we discuss potential applications of IndiTag in fostering media literacy, facilitating <b>fact-checking</b> <b>initiatives,</b> and enhancing the transparency and accountability of digital media platforms. IndiTag stands as a valuable tool in the pursuit of fostering a more informed, discerning, and inclusive public discourse in the digital age. The demonstration video can be accessed from <a href=https://youtu.be/Gt2T4T7DYqs>https://youtu.be/Gt2T4T7DYqs</a>. We release an online system for end users and the source code is available at <a href=https://github.com/lylin0/IndiTag>https://github.com/lylin0/IndiTag</a>.</p></p class="citation"></blockquote><h3 id=34--266286-community-needs-and-assets-a-computational-analysis-of-community-conversations-md-towhidul-absar-chowdhury-et-al-2024>(3/4 | 266/286) Community Needs and Assets: A Computational Analysis of Community Conversations (Md Towhidul Absar Chowdhury et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Towhidul Absar Chowdhury, Naveen Sharma, Ashiqur R. KhudaBukhsh. (2024)<br><strong>Community Needs and Assets: A Computational Analysis of Community Conversations</strong><br><button class=copy-to-clipboard title="Community Needs and Assets: A Computational Analysis of Community Conversations" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CL, cs-CY, cs-SI, cs.CY<br>Keyword Score: 20<br>Keywords: Zero-shot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13272v1.pdf filename=2403.13272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A community needs assessment is a tool used by non-profits and government agencies to quantify the strengths and issues of a community, allowing them to allocate their resources better. Such approaches are transitioning towards leveraging social media conversations to analyze the needs of communities and the assets already present within them. However, manual analysis of exponentially increasing social media conversations is challenging. There is a gap in the present literature in computationally analyzing how community members discuss the strengths and needs of the community. To address this gap, we introduce the task of identifying, extracting, and categorizing community needs and assets from conversational data using sophisticated natural language processing methods. To facilitate this task, we introduce the first dataset about community needs and assets consisting of 3,511 conversations from Reddit, annotated using crowdsourced workers. Using this dataset, we evaluate an utterance-level classification model compared to sentiment classification and a popular <b>large</b> <b>language</b> <b>model</b> (in a <b>zero-shot</b> setting), where we find that our model outperforms both baselines at an F1 score of 94% compared to 49% and 61% respectively. Furthermore, we observe through our study that conversations about needs have negative sentiments and emotions, while conversations about assets focus on location and entities. The dataset is available at <a href=https://github.com/towhidabsar/CommunityNeeds>https://github.com/towhidabsar/CommunityNeeds</a>.</p></p class="citation"></blockquote><h3 id=44--267286-spatial-fairness-the-case-for-its-importance-limitations-of-existing-work-and-guidelines-for-future-research-nripsuta-ani-saxena-et-al-2024>(4/4 | 267/286) Spatial Fairness: The Case for its Importance, Limitations of Existing Work, and Guidelines for Future Research (Nripsuta Ani Saxena et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nripsuta Ani Saxena, Wenbin Zhang, Cyrus Shahabi. (2024)<br><strong>Spatial Fairness: The Case for its Importance, Limitations of Existing Work, and Guidelines for Future Research</strong><br><button class=copy-to-clipboard title="Spatial Fairness: The Case for its Importance, Limitations of Existing Work, and Guidelines for Future Research" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14040v1.pdf filename=2403.14040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite location being increasingly used in decision-making systems employed in many sensitive domains such as mortgages and insurance, astonishingly little attention has been paid to unfairness that may seep in due to the correlation of location with characteristics considered protected under anti-discrimination law, such as race or national origin. This position paper argues for the urgent need to consider <b>fairness</b> with respect to location, termed \textit{spatial fairness}, by outlining the harms that continue to be perpetuated due to location&rsquo;s correlation with protected characteristics. This interdisciplinary work connects knowledge from fields such as public policy, economic development, and geography to highlight how fair-AI research currently falls short of correcting for spatial biases, and does not consider challenges unique to spatial data. Furthermore, we identify limitations of the handful of spatial <b>fairness</b> work proposed so far, and finally, detail guidelines for future research so subsequent work may avoid such issues and help correct spatial biases.</p></p class="citation"></blockquote><h2 id=csce-3>cs.CE (3)</h2><h3 id=13--268286-application-of-advanced-ultrasonic-testing-methods-to-dissimilar-metal-welds----comparison-of-simulated-and-experimental-results-audrey-gardahaut-et-al-2024>(1/3 | 268/286) Application of advanced ultrasonic testing methods to Dissimilar Metal Welds &ndash; Comparison of simulated and experimental results (Audrey Gardahaut et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Audrey Gardahaut, Hugues Lourme, Steve Mahaut, Masaki Nagai, Shan Lin. (2024)<br><strong>Application of advanced ultrasonic testing methods to Dissimilar Metal Welds &ndash; Comparison of simulated and experimental results</strong><br><button class=copy-to-clipboard title="Application of advanced ultrasonic testing methods to Dissimilar Metal Welds -- Comparison of simulated and experimental results" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13379v1.pdf filename=2403.13379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Widely present in the primary circuit of Nuclear Power Plants (NPP), Dissimilar Metal Welds (DMW) are inspected using Ultrasonic nondestructive Testing (UT) techniques to ensure the integrity of the structure and detect defects such as Stress Corrosion Cracking (SCC).In a previous collaborative research, CRIEPI and CEA have worked on the understanding of the propagation of ultrasonic waves in complex materials. Indeed, the ultrasonic propagation can be disturbed due to the anisotropic and inhomogeneous properties of the medium and the interpretation of inspection results can then be difficult. An analytical model, based on a dynamic ray theory, developed by CEA-LIST and implemented in the CIVA software had been used to predict the ultrasonic propagation in a DMW. The model evaluates the ray trajectories, the travel-time and the computation of the amplitude along the ray tube in a medium described thanks to a continuously varying description of its physical properties. In this study, the weld had been described by an analytical law of the crystallographic orientation. The simulated results of the detection of calibrated notches located in the buttering and the weld had been compared with experimental data and had shown a good agreement.The new collaborative program presented in this paper aims at detecting a real SCC defect located close to the root of the DMW. Thus, <b>simulations</b> have been performed for a DMW described with an analytical law and a smooth cartography of the crystallographic orientation. Furthermore, advanced ultrasonic testing methods have been used to inspect the specimen and detect the real SCC defect. Experimental and simulated results of the mock-up inspection have been compared.</p></p class="citation"></blockquote><h3 id=23--269286-stochastic-geometry-models-for-texture-synthesis-of-machined-metallic-surfaces-sandblasting-and-milling-natascha-jeziorski-et-al-2024>(2/3 | 269/286) Stochastic Geometry Models for Texture Synthesis of Machined Metallic Surfaces: Sandblasting and Milling (Natascha Jeziorski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Natascha Jeziorski, Claudia Redenbach. (2024)<br><strong>Stochastic Geometry Models for Texture Synthesis of Machined Metallic Surfaces: Sandblasting and Milling</strong><br><button class=copy-to-clipboard title="Stochastic Geometry Models for Texture Synthesis of Machined Metallic Surfaces: Sandblasting and Milling" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs-CV, cs.CE<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13439v1.pdf filename=2403.13439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training defect detection algorithms for visual surface inspection systems requires a large and representative set of training data. Often there is not enough real data available which additionally cannot cover the variety of possible defects. Synthetic data generated by a synthetic visual surface inspection environment can overcome this problem. Therefore, a digital twin of the object is needed, whose micro-scale surface topography is modeled by texture synthesis models. We develop stochastic texture models for sandblasted and milled surfaces based on topography measurements of such surfaces. As the surface patterns differ significantly, we use separate modeling approaches for the two cases. Sandblasted surfaces are modeled by a combination of data-based texture synthesis methods that rely entirely on the measurements. In contrast, the model for milled surfaces is procedural and includes all process-related parameters known from the machine settings.</p></p class="citation"></blockquote><h3 id=33--270286-canonical-descriptors-for-periodic-lattice-truss-materials-ge-qi-et-al-2024>(3/3 | 270/286) Canonical Descriptors for Periodic Lattice Truss Materials (Ge Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ge Qi, Huai-Liang Zheng, Chen-xi Liu, Li MA, Kai-Uwe Schröder. (2024)<br><strong>Canonical Descriptors for Periodic Lattice Truss Materials</strong><br><button class=copy-to-clipboard title="Canonical Descriptors for Periodic Lattice Truss Materials" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: I-1-1, cs-CE, cs.CE<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13270v1.pdf filename=2403.13270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For decades, aspects of the topological architecture, and of the mechanical as well as other physical behaviors of periodic lattice truss materials (PLTMs) have been massively studied. Their approximate infinite design space presents a double-edged sword, implying on one hand dramatic designability in fulfilling the requirement of various performance, but on the other hand unexpected intractability in determining the best candidate with tailoring properties. In recent years, the development of additive manufacturing and artificial intelligence spurs an explosion in the methods exploring the design space and searching its boundaries. However, regrettably, a normative description with sufficient information of PLTMs applying to machine learning has not yet been constructed, which confines the inverse design to some discrete and small scrutinized space. In the current paper, we develop a system of canonical descriptors for PLTMs, encoding not only the geometrical configurations but also mechanical properties into matrix forms to establish good quantitative correlations between structures and mechanical behaviors. The system mainly consists of the <b>geometry</b> matrix for the lattice node configuration, density, stretching and bending stiffness matrices for the lattice strut properties, as well as packing matrix for the principal periodic orientation. All these matrices are theoretically derived based on the intrinsic nature of PLTMs, leading to concise descriptions and sufficient information. The characteristics, including the completeness and uniqueness, of the descriptors are analyzed. In addition, we discuss how the current system of descriptors can be applied to the database construction and material discovery, and indicate the possible open problems.</p></p class="citation"></blockquote><h2 id=physicsbio-ph-1>physics.bio-ph (1)</h2><h3 id=11--271286-a-machine-learning-approach-for-multiscale-modeling-of-the-facet-capsular-ligament-jacob-s-merson-et-al-2024>(1/1 | 271/286) A Machine Learning Approach for Multiscale Modeling of the Facet Capsular Ligament (Jacob S. Merson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacob S. Merson, Nishan Parvez. (2024)<br><strong>A Machine Learning Approach for Multiscale Modeling of the Facet Capsular Ligament</strong><br><button class=copy-to-clipboard title="A Machine Learning Approach for Multiscale Modeling of the Facet Capsular Ligament" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.bio-ph<br>Categories: cond-mat-soft, cs-CE, physics-bio-ph, physics.bio-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13357v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13357v1.pdf filename=2403.13357v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop a new neural network architecture that strictly enforces constitutive constraints such as polyconvexity, frame-indifference, zero strain energy with zero deformations, and the symmetry of the stress and material stiffness. Additionally, we show that for this neural network, the accuracy is significantly improved by using a Sobolev minimization strategy that includes derivative terms. Using our network and Sobolev minimization, we obtain a NMSE of 0.15% for the energy, 0.815% averaged across the components of the stress, and 5.4% averaged across the components of the stiffness. This machine learned constitutive model was deployed in a finite element <b>simulation</b> of a facet capsular ligament. The displacement fields and stress-strain curves where compared to a multiscale <b>simulation</b> that required running on a GPU based supercomputer. At 70% strain, the model using the neural network had less than 10% relative error in the mean stress value.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=12--272286-tdt-kws-fast-and-accurate-keyword-spotting-using-token-and-duration-transducer-yu-xi-et-al-2024>(1/2 | 272/286) TDT-KWS: Fast And Accurate Keyword Spotting Using Token-and-duration Transducer (Yu Xi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Xi, Hao Li, Baochen Yang, Haoyu Li, Hainan Xu, Kai Yu. (2024)<br><strong>TDT-KWS: Fast And Accurate Keyword Spotting Using Token-and-duration Transducer</strong><br><button class=copy-to-clipboard title="TDT-KWS: Fast And Accurate Keyword Spotting Using Token-and-duration Transducer" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13332v1.pdf filename=2403.13332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Designing an efficient keyword spotting (KWS) system that delivers exceptional performance on resource-constrained edge devices has long been a subject of significant attention. Existing KWS search algorithms typically follow a frame-synchronous approach, where search decisions are made repeatedly at each frame despite the fact that most frames are keyword-irrelevant. In this paper, we propose TDT-KWS, which leverages token-and-duration Transducers (TDT) for KWS tasks. We also propose a novel KWS task-specific decoding algorithm for Transducer-based models, which supports highly effective frame-asynchronous keyword search in streaming speech scenarios. With evaluations conducted on both the public Hey Snips and self-constructed LibriKWS-20 datasets, our proposed KWS-decoding algorithm produces more accurate results than conventional <b>ASR</b> decoding algorithms. Additionally, TDT-KWS achieves on-par or better wake word detection performance than both <b>RNN-T</b> and traditional TDT-ASR systems while achieving significant inference speed-up. Furthermore, experiments show that TDT-KWS is more robust to noisy environments compared to <b>RNN-T</b> KWS.</p></p class="citation"></blockquote><h3 id=22--273286-kunqudb-an-attempt-for-speaker-verification-in-the-chinese-opera-scenario-huali-zhou-et-al-2024>(2/2 | 273/286) KunquDB: An Attempt for Speaker Verification in the Chinese Opera Scenario (Huali Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huali Zhou, Yuke Lin, Dong Liu, Ming Li. (2024)<br><strong>KunquDB: An Attempt for Speaker Verification in the Chinese Opera Scenario</strong><br><button class=copy-to-clipboard title="KunquDB: An Attempt for Speaker Verification in the Chinese Opera Scenario" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess-IV, eess.AS<br>Keyword Score: 13<br>Keywords: Benchmarking, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13356v1.pdf filename=2403.13356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work aims to promote Chinese opera research in both musical and speech <b>domains,</b> <b>with</b> a primary focus on overcoming the data limitations. We introduce KunquDB, a relatively large-scale, well-annotated audio-visual dataset comprising 339 speakers and 128 hours of content. Originating from the Kunqu Opera Art Canon (Kunqu yishu dadian), KunquDB is meticulously structured by dialogue lines, providing explicit annotations including character names, speaker names, gender information, vocal manner classifications, and accompanied by preliminary text transcriptions. KunquDB provides a versatile foundation for role-centric acoustic studies and advancements in speech-related research, including Automatic Speaker Verification (ASV). Beyond enriching opera research, this dataset bridges the gap between artistic expression and technological innovation. Pioneering the exploration of ASV in Chinese opera, we construct four test trials considering two distinct vocal manners in opera voices: stage speech (ST) and singing (S). Implementing <b>domain</b> <b>adaptation</b> methods effectively mitigates <b>domain</b> <b>mismatches</b> induced by these vocal manner variations while there is still room for further improvement as a <b>benchmark.</b></p></p class="citation"></blockquote><h2 id=mathoc-4>math.OC (4)</h2><h3 id=14--274286-projection-free-computation-of-robust-controllable-sets-with-constrained-zonotopes-abraham-p-vinod-et-al-2024>(1/4 | 274/286) Projection-free computation of robust controllable sets with constrained zonotopes (Abraham P. Vinod et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abraham P. Vinod, Avishai Weiss, Stefano Di Cairano. (2024)<br><strong>Projection-free computation of robust controllable sets with constrained zonotopes</strong><br><button class=copy-to-clipboard title="Projection-free computation of robust controllable sets with constrained zonotopes" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-RO, cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13730v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13730v1.pdf filename=2403.13730v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of computing robust controllable sets for <b>discrete-time</b> <b>linear</b> systems with additive uncertainty. We propose a tractable and scalable approach to inner- and outer-approximate robust controllable sets using constrained zonotopes, when the additive uncertainty set is a symmetric, convex, and compact set. Our least-squares-based approach uses novel closed-form approximations of the Pontryagin difference between a constrained zonotopic minuend and a symmetric, convex, and compact subtrahend. Unlike existing approaches, our approach does not rely on convex optimization solvers, and is projection-free for ellipsoidal and zonotopic uncertainty sets. We also propose a least-squares-based approach to compute a convex, polyhedral outer-approximation to constrained zonotopes, and characterize sufficient conditions under which all these approximations are exact. We demonstrate the computational efficiency and scalability of our approach in several case studies, including the design of abort-safe rendezvous trajectories for a spacecraft in near-rectilinear halo orbit under uncertainty. Our approach can inner-approximate a 20-step robust controllable set for a 100-dimensional linear system in under 15 seconds on a standard computer.</p></p class="citation"></blockquote><h3 id=24--275286-optimal-control-of-continuous-time-symmetric-systems-with-unknown-dynamics-and-noisy-measurements-hamed-taghavian-et-al-2024>(2/4 | 275/286) Optimal control of continuous-time symmetric systems with unknown dynamics and noisy measurements (Hamed Taghavian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamed Taghavian, Florian Dorfler, Mikael Johansson. (2024)<br><strong>Optimal control of continuous-time symmetric systems with unknown dynamics and noisy measurements</strong><br><button class=copy-to-clipboard title="Optimal control of continuous-time symmetric systems with unknown dynamics and noisy measurements" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13605v1.pdf filename=2403.13605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An iterative learning algorithm is presented for <b>continuous-time</b> <b>linear-quadratic</b> optimal control problems where the system is externally symmetric with unknown dynamics. Both finite-horizon and infinite-horizon problems are considered. It is shown that the proposed algorithm is globally convergent to the optimal solution and has some advantages over adaptive dynamic programming, including being unbiased under noisy measurements and having a relatively low computational burden. Numerical experiments show the effectiveness of the results.</p></p class="citation"></blockquote><h3 id=34--276286-towards-a-connection-between-the-capacitated-vehicle-routing-problem-and-the-constrained-centroid-based-clustering-abdelhakim-abdellaoui-et-al-2024>(3/4 | 276/286) Towards a connection between the capacitated vehicle routing problem and the constrained centroid-based clustering (Abdelhakim Abdellaoui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdelhakim Abdellaoui, Loubna Benabbou, Issmail El Hallaoui. (2024)<br><strong>Towards a connection between the capacitated vehicle routing problem and the constrained centroid-based clustering</strong><br><button class=copy-to-clipboard title="Towards a connection between the capacitated vehicle routing problem and the constrained centroid-based clustering" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 6<br>Keywords: Benchmarking, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14013v1.pdf filename=2403.14013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficiently solving a vehicle routing problem (VRP) in a practical runtime is a critical challenge for delivery management companies. This paper explores both a theoretical and experimental connection between the Capacitated Vehicle Routing Problem (CVRP) and the Constrained Centroid-Based <b>Clustering</b> (CCBC). Reducing a CVRP to a CCBC is a synonym for a transition from an exponential to a polynomial complexity using commonly known algorithms for <b>clustering,</b> i.e K-means. At the beginning, we conduct an exploratory analysis to highlight the existence of such a relationship between the two problems through illustrative small-size examples and simultaneously deduce some mathematically-related formulations and properties. On a second level, the paper proposes a CCBC based approach endowed with some enhancements. The proposed framework consists of three stages. At the first step, a constrained centroid-based <b>clustering</b> algorithm generates feasible clusters of customers. This methodology incorporates three enhancement tools to achieve near-optimal clusters, namely: a multi-start procedure for initial centroids, a customer assignment metric, and a self-adjustment mechanism for choosing the number of clusters. At the second step, a traveling salesman problem (T SP) solver is used to optimize the order of customers within each cluster. Finally, we introduce a process relying on routes cutting and relinking procedure, which calls upon solving a linear and integer programming model to further improve the obtained routes. This step is inspired by the ruin & recreate algorithm. This approach is an extension of the classical cluster-first, route-second method and provides near-optimal solutions on well-known <b>benchmark</b> instances in terms of solution quality and computational runtime, offering a milestone in solving VRP.</p></p class="citation"></blockquote><h3 id=44--277286-a-log-domain-interior-point-method-for-convex-quadratic-games-bingqi-liu-et-al-2024>(4/4 | 277/286) A Log-domain Interior Point Method for Convex Quadratic Games (Bingqi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingqi Liu, Dominic Liao-McPherson. (2024)<br><strong>A Log-domain Interior Point Method for Convex Quadratic Games</strong><br><button class=copy-to-clipboard title="A Log-domain Interior Point Method for Convex Quadratic Games" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13290v1.pdf filename=2403.13290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose an equilibrium-seeking algorithm for finding generalized Nash equilibria of non-cooperative monotone convex quadratic games. Specifically, we recast the Nash equilibrium-seeking problem as variational inequality problem that we solve using a log-domain interior point method and provide a general purpose solver based on this algorithm. This approach is suitable for non-potential, general sum games and does not require extensive structural assumptions. We demonstrate the efficiency and versatility of our method using three <b>benchmark</b> games and demonstrate our algorithm is especially effective on small to medium scale problems.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--278286-network-bottlenecks-and-task-structure-control-the-evolution-of-interpretable-learning-rules-in-a-foraging-agent-emmanouil-giannakakis-et-al-2024>(1/1 | 278/286) Network bottlenecks and task structure control the evolution of interpretable learning rules in a foraging agent (Emmanouil Giannakakis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emmanouil Giannakakis, Sina Khajehabdollahi, Anna Levina. (2024)<br><strong>Network bottlenecks and task structure control the evolution of interpretable learning rules in a foraging agent</strong><br><button class=copy-to-clipboard title="Network bottlenecks and task structure control the evolution of interpretable learning rules in a foraging agent" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-NE, nlin-AO, q-bio-NC, q-bio.NC<br>Keyword Score: 10<br>Keywords: Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13649v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13649v1.pdf filename=2403.13649v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing reliable mechanisms for continuous local learning is a central challenge faced by biological and artificial systems. Yet, how the environmental factors and structural constraints on the learning network influence the optimal plasticity mechanisms remains obscure even for simple settings. To elucidate these dependencies, we study <b>meta-learning</b> <b>via</b> evolutionary optimization of simple reward-modulated plasticity rules in embodied agents solving a foraging task. We show that unconstrained <b>meta-learning</b> <b>leads</b> to the emergence of diverse plasticity rules. However, regularization and bottlenecks to the model help reduce this variability, resulting in interpretable rules. Our findings indicate that the <b>meta-learning</b> <b>of</b> plasticity rules is very sensitive to various parameters, with this sensitivity possibly reflected in the learning rules found in biological networks. When included in models, these dependencies can be used to discover potential objective functions and details of biological learning via comparisons with experimental observations.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--279286-extremality-of-stabilizer-states-kaifeng-bu-2024>(1/1 | 279/286) Extremality of stabilizer states (Kaifeng Bu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaifeng Bu. (2024)<br><strong>Extremality of stabilizer states</strong><br><button class=copy-to-clipboard title="Extremality of stabilizer states" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-IT, math-IT, math-MP, math-ph, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13632v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13632v1.pdf filename=2403.13632v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the extremality of stabilizer states to reveal their exceptional role in the space of all $n$-qubit/qudit states. We establish uncertainty principles for the characteristic function and the Wigner function of states, respectively. We find that only stabilizer states achieve saturation in these principles. Furthermore, we prove a general theorem that stabilizer states are extremal for convex information measures invariant under local unitaries. We explore this extremality in the context of various quantum information and correlation measures, including entanglement entropy, conditional entropy and other entanglement measures. Additionally, leveraging the recent discovery that stabilizer states are the limit states under quantum <b>convolution,</b> we establish the monotonicity of the entanglement entropy and conditional entropy under quantum <b>convolution.</b> These results highlight the remarkable information-theoretic properties of stabilizer states. Their extremality provides valuable insights into their ability to capture information content and correlations, paving the way for further exploration of their potential in quantum information processing.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--280286-hcim-adc-less-hybrid-analog-digital-compute-in-memory-accelerator-for-deep-learning-workloads-shubham-negi-et-al-2024>(1/1 | 280/286) HCiM: ADC-Less Hybrid Analog-Digital Compute in Memory Accelerator for Deep Learning Workloads (Shubham Negi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubham Negi, Utkarsh Saxena, Deepika Sharma, Kaushik Roy. (2024)<br><strong>HCiM: ADC-Less Hybrid Analog-Digital Compute in Memory Accelerator for Deep Learning Workloads</strong><br><button class=copy-to-clipboard title="HCiM: ADC-Less Hybrid Analog-Digital Compute in Memory Accelerator for Deep Learning Workloads" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13577v1.pdf filename=2403.13577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Analog Compute-in-Memory (CiM) accelerators are increasingly recognized for their efficiency in accelerating Deep Neural Networks (DNN). However, their dependence on Analog-to-Digital Converters (ADCs) for accumulating partial sums from crossbars leads to substantial power and area overhead. Moreover, the high area overhead of ADCs constrains the throughput due to the limited number of ADCs that can be integrated per crossbar. An approach to mitigate this issue involves the adoption of extreme low-precision <b>quantization</b> (binary or ternary) for partial sums. Training based on such an approach eliminates the need for ADCs. While this strategy effectively reduces ADC costs, it introduces the challenge of managing numerous floating-point scale factors, which are trainable parameters like DNN weights. These scale factors must be multiplied with the binary or ternary outputs at the columns of the crossbar to ensure system accuracy. To that effect, we propose an algorithm-hardware co-design approach, where DNNs are first trained with <b>quantization-aware</b> training. Subsequently, we introduce HCiM, an ADC-Less Hybrid Analog-Digital CiM accelerator. HCiM uses analog CiM crossbars for performing Matrix-Vector Multiplication operations coupled with a digital CiM array dedicated to processing scale factors. This digital CiM array can execute both addition and subtraction operations within the memory array, thus enhancing processing speed. Additionally, it exploits the inherent sparsity in ternary <b>quantization</b> to achieve further energy savings. Compared to an analog CiM baseline architecture using 7 and 4-bit ADC, HCiM achieves energy reductions up to 28% and 12%, respectively</p></p class="citation"></blockquote><h2 id=cslo-3>cs.LO (3)</h2><h3 id=13--281286-reasoning-about-distributive-laws-in-a-concurrent-refinement-algebra-larissa-a-meinicke-et-al-2024>(1/3 | 281/286) Reasoning about distributive laws in a concurrent refinement algebra (Larissa A. Meinicke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Larissa A. Meinicke, Ian J. Hayes. (2024)<br><strong>Reasoning about distributive laws in a concurrent refinement algebra</strong><br><button class=copy-to-clipboard title="Reasoning about distributive laws in a concurrent refinement algebra" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: F-3-1; D-1-3, cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13425v1.pdf filename=2403.13425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributive laws are important for algebraic <b>reasoning</b> in arithmetic and logic. They are equally important for algebraic <b>reasoning</b> about concurrent programs. In existing theories such as Concurrent Kleene Algebra, only partial correctness is handled, and many of its distributive laws are weak, in the sense that they are only refinements in one direction, rather than equalities. The focus of this paper is on strengthening our theory to support the proof of strong distributive laws that are equalities, and in doing so come up with laws that are quite general. Our concurrent refinement algebra supports total correctness by allowing both finite and infinite behaviours. It supports the rely/guarantee approach of Jones by encoding rely and guarantee conditions as rely and guarantee commands. The strong distributive laws may then be used to distribute rely and guarantee commands over sequential compositions and into (and out of) iterations. For handling data refinement of concurrent programs, strong distributive laws are essential.</p></p class="citation"></blockquote><h3 id=23--282286-mechanized-hol-reasoning-in-set-theory-simon-guilloud-et-al-2024>(2/3 | 282/286) Mechanized HOL Reasoning in Set Theory (Simon Guilloud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Guilloud, Sankalp Gambhir, Andrea Gilot, Viktor Kunčak. (2024)<br><strong>Mechanized HOL Reasoning in Set Theory</strong><br><button class=copy-to-clipboard title="Mechanized HOL Reasoning in Set Theory" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: I-2-3; F-4-1, cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13403v1.pdf filename=2403.13403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a mechanized embedding of higher-order logic (HOL) and algebraic data types (ADT) into first-order logic with ZFC axioms. We implement this in the Lisa proof assistant for schematic first-order logic and its library based on axiomatic set theory. HOL proof steps are implemented as proof producing tactics in Lisa, and the types are interpreted as sets, with function (or arrow) types coinciding with set-theoretic function spaces. The embedded HOL proofs, as opposed to being a layer over the existing proofs, are interoperable with the existing library. This yields a form of soft type system supporting top-level polymorphism and ADTs over set theory, and offer tools to reason about functions in set theory.</p></p class="citation"></blockquote><h3 id=33--283286-the-equational-theory-of-the-weihrauch-lattice-with-multiplication-eike-neumann-et-al-2024>(3/3 | 283/286) The equational theory of the Weihrauch lattice with multiplication (Eike Neumann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eike Neumann, Arno Pauly, Cécilia Pradic. (2024)<br><strong>The equational theory of the Weihrauch lattice with multiplication</strong><br><button class=copy-to-clipboard title="The equational theory of the Weihrauch lattice with multiplication" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: 03D30, 08A50, 68Q17, cs-LO, cs.LO, math-LO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13975v1.pdf filename=2403.13975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the equational theory of the Weihrauch lattice with multiplication, meaning the collection of equations between terms built from variables, the lattice operations $\sqcup$, $\sqcap$, the product $\times$, and the finite parallelization $(-)^*$ which are true however we substitute Weihrauch degrees for the variables. We provide a combinatorial description of these in terms of a reducibility between finite <b>graphs,</b> and moreover, show that deciding which equations are true in this sense is complete for the third level of the polynomial hierarchy.</p></p class="citation"></blockquote><h2 id=cssc-1>cs.SC (1)</h2><h3 id=11--284286-gröbner-bases-over-polytopal-affinoid-algebras-moulay-a-barkatou-et-al-2024>(1/1 | 284/286) Gr{ö}bner bases over polytopal affinoid algebras (Moulay A. Barkatou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Moulay A. Barkatou, Lucas Legrand, Tristan Vaccon. (2024)<br><strong>Gr{ö}bner bases over polytopal affinoid algebras</strong><br><button class=copy-to-clipboard title="Gr{ö}bner bases over polytopal affinoid algebras" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SC<br>Categories: cs-SC, cs.SC, math-AC, math-NT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13382v1.pdf filename=2403.13382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Polyhedral affinoid algebras have been introduced by Einsiedler, Kapranov and Lind to connect rigid analytic <b>geometry</b> (analytic <b>geometry</b> over non-archimedean fields) and tropical <b>geometry.In</b> this article, we present a theory of Gr{"o}bner bases for polytopal affinoid algebras that extends both Caruso et al.&rsquo;s theory of Gr{"o}bner bases on Tate algebras and Pauer et al.&rsquo;s theory of Gr{"o}bner bases on Laurent polynomials.We provide effective algorithms to compute Gr{"o}bner bases for both ideals of Laurent polynomials and ideals in polytopal affinoid algebras. Experiments with a Sagemath implementation are provided.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--285286-from-primary-hpv-infection-to-carcinoma-in-situ-a-mathematical-approach-of-cervical-intraepithelial-neoplasia-vasiliki-bitsouni-et-al-2024>(1/1 | 285/286) From primary HPV infection to carcinoma in situ: a mathematical approach of cervical intraepithelial neoplasia (Vasiliki Bitsouni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vasiliki Bitsouni, Nikolaos Gialelis, Ioannis G. Stratis, Vasilis Tsilidis. (2024)<br><strong>From primary HPV infection to carcinoma in situ: a mathematical approach of cervical intraepithelial neoplasia</strong><br><button class=copy-to-clipboard title="From primary HPV infection to carcinoma in situ: a mathematical approach of cervical intraepithelial neoplasia" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: 35Q92, 37N25, 92C17, 92-10, cs-NA, math-AP, math-NA, q-bio-QM, q-bio.QM<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13302v1.pdf filename=2403.13302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cervical intraepithelial neoplasia (CIN) is the development of abnormal cells on the surface of the cervix, caused by a human papillomavirus (HPV) infection. Although in most of the cases it is resolved by the immune system, a small percentage of people might develop a more serious CIN which, if left untreated, can develop into cervical cancer. Cervical cancer is the fourth most common cancer in women globally, for which the World Health Organization (WHO) recently adopted the Global Strategy for cervical cancer elimination by 2030. With this research topic being more imperative than ever, in this paper, we develop a nonlinear mathematical model describing the CIN progression. The model consists of partial differential equations describing the dynamics of epithelial, dysplastic and immune cells, as well as the dynamics of viral particles. We use our model to explore numerically three important factors of dysplasia progression, namely the <b>geometry</b> of the cervix, the strength of the immune response and the frequency of viral exposure.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--286286-constrained-and-ordered-level-planarity-parameterized-by-the-number-of-levels-václav-blažej-et-al-2024>(1/1 | 286/286) Constrained and Ordered Level Planarity Parameterized by the Number of Levels (Václav Blažej et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Václav Blažej, Boris Klemz, Felix Klesen, Marie Diana Sieper, Alexander Wolff, Johannes Zink. (2024)<br><strong>Constrained and Ordered Level Planarity Parameterized by the Number of Levels</strong><br><button class=copy-to-clipboard title="Constrained and Ordered Level Planarity Parameterized by the Number of Levels" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13702v1.pdf filename=2403.13702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The problem Level Planarity asks for a crossing-free drawing of a <b>graph</b> in the plane such that vertices are placed at prescribed y-coordinates (called levels) and such that every edge is realized as a y-monotone curve. In the variant Constrained Level Planarity (CLP), each level $y$ is equipped with a partial order $\prec_y$ on its vertices and in the desired drawing the left-to-right order of vertices on level $y$ has to be a linear extension of $\prec_y$. Ordered Level Planarity (OLP) corresponds to the special case of CLP where the given partial orders $\prec_y$ are total orders. Previous results by Br"uckner and Rutter [SODA 2017] and Klemz and Rote [ACM Trans. Alg. 2019] state that both CLP and OLP are NP-hard even in severely restricted cases. In particular, they remain NP-hard even when restricted to instances whose width (the maximum number of vertices that may share a common level) is at most two. In this paper, we focus on the other dimension: we study the parameterized complexity of CLP and OLP with respect to the height (the number of levels). We show that OLP parameterized by the height is complete with respect to the complexity class XNLP, which was first studied by Elberfeld et al. [Algorithmica 2015] (under a different name) and recently made more prominent by Bodlaender et al. [FOCS 2021]. It contains all parameterized problems that can be solved nondeterministically in time $f(k) n^{O(1)}$ and space $f(k) \log n$ (where $f$ is a computable function, $n$ is the input size, and $k$ is the parameter). If a problem is XNLP-complete, it lies in XP, but is W[$t$]-hard for every $t$. In contrast to the fact that OLP parameterized by the height lies in XP, it turns out that CLP is NP-hard even when restricted to instances of height 4. We complement this result by showing that CLP can be solved in polynomial time for instances of height at most 3.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.21</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.23</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#csir-6>cs.IR (6)</a><ul><li><a href=#16--1286-harnessing-large-language-models-for-text-rich-sequential-recommendation-zhi-zheng-et-al-2024>(1/6 | 1/286) Harnessing Large Language Models for Text-Rich Sequential Recommendation (Zhi Zheng et al., 2024)</a></li><li><a href=#26--2286-a-large-language-model-enhanced-sequential-recommender-for-joint-video-and-comment-recommendation-bowen-zheng-et-al-2024>(2/6 | 2/286) A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation (Bowen Zheng et al., 2024)</a></li><li><a href=#36--3286-desire-me-domain-enhanced-supervised-information-retrieval-using-mixture-of-experts-pranav-kasela-et-al-2024>(3/6 | 3/286) DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using Mixture-of-Experts (Pranav Kasela et al., 2024)</a></li><li><a href=#46--4286-flickr30k-cfq-a-compact-and-fragmented-query-dataset-for-text-image-retrieval-haoyu-liu-et-al-2024>(4/6 | 4/286) Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image Retrieval (Haoyu Liu et al., 2024)</a></li><li><a href=#56--5286-an-analysis-on-matching-mechanisms-and-token-pruning-for-late-interaction-models-qi-liu-et-al-2024>(5/6 | 5/286) An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models (Qi Liu et al., 2024)</a></li><li><a href=#66--6286-a-semantic-search-engine-for-mathlib4-guoxiong-gao-et-al-2024>(6/6 | 6/286) A Semantic Search Engine for Mathlib4 (Guoxiong Gao et al., 2024)</a></li></ul></li><li><a href=#cscv-89>cs.CV (89)</a><ul><li><a href=#189--7286-mtp-advancing-remote-sensing-foundation-model-via-multi-task-pretraining-di-wang-et-al-2024>(1/89 | 7/286) MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining (Di Wang et al., 2024)</a></li><li><a href=#289--8286-rar-retrieving-and-ranking-augmented-mllms-for-visual-recognition-ziyu-liu-et-al-2024>(2/89 | 8/286) RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition (Ziyu Liu et al., 2024)</a></li><li><a href=#389--9286-learning-from-models-and-data-for-visual-grounding-ruozhen-he-et-al-2024>(3/89 | 9/286) Learning from Models and Data for Visual Grounding (Ruozhen He et al., 2024)</a></li><li><a href=#489--10286-multi-modal-hallucination-control-by-visual-information-grounding-alessandro-favero-et-al-2024>(4/89 | 10/286) Multi-Modal Hallucination Control by Visual Information Grounding (Alessandro Favero et al., 2024)</a></li><li><a href=#589--11286-agfsync-leveraging-ai-generated-feedback-for-preference-optimization-in-text-to-image-generation-jingkun-an-et-al-2024>(5/89 | 11/286) AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in Text-to-Image Generation (Jingkun An et al., 2024)</a></li><li><a href=#689--12286-enhancing-fingerprint-image-synthesis-with-gans-diffusion-models-and-style-transfer-techniques-w-tang-et-al-2024>(6/89 | 12/286) Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and Style Transfer Techniques (W. Tang et al., 2024)</a></li><li><a href=#789--13286-reground-improving-textual-and-spatial-grounding-at-no-cost-yuseung-lee-et-al-2024>(7/89 | 13/286) ReGround: Improving Textual and Spatial Grounding at No Cost (Yuseung Lee et al., 2024)</a></li><li><a href=#889--14286-t-pixel2mesh-combining-global-and-local-transformer-for-3d-mesh-generation-from-a-single-image-shijie-zhang-et-al-2024>(8/89 | 14/286) T-Pixel2Mesh: Combining Global and Local Transformer for 3D Mesh Generation from a Single Image (Shijie Zhang et al., 2024)</a></li><li><a href=#989--15286-ground-a-score-scaling-up-the-score-distillation-for-multi-attribute-editing-hangeol-chang-et-al-2024>(9/89 | 15/286) Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute Editing (Hangeol Chang et al., 2024)</a></li><li><a href=#1089--16286-improved-baselines-for-data-efficient-perceptual-augmentation-of-llms-théophane-vallaeys-et-al-2024>(10/89 | 16/286) Improved Baselines for Data-efficient Perceptual Augmentation of LLMs (Théophane Vallaeys et al., 2024)</a></li><li><a href=#1189--17286-editing-massive-concepts-in-text-to-image-diffusion-models-tianwei-xiong-et-al-2024>(11/89 | 17/286) Editing Massive Concepts in Text-to-Image Diffusion Models (Tianwei Xiong et al., 2024)</a></li><li><a href=#1289--18286-fmm-attack-a-flow-based-multi-modal-adversarial-attack-on-video-based-llms-jinmin-li-et-al-2024>(12/89 | 18/286) FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs (Jinmin Li et al., 2024)</a></li><li><a href=#1389--19286-ecosense-energy-efficient-intelligent-sensing-for-in-shore-ship-detection-through-edge-cloud-collaboration-wenjun-huang-et-al-2024>(13/89 | 19/286) EcoSense: Energy-Efficient Intelligent Sensing for In-Shore Ship Detection through Edge-Cloud Collaboration (Wenjun Huang et al., 2024)</a></li><li><a href=#1489--20286-when-cars-meet-drones-hyperbolic-federated-learning-for-source-free-domain-adaptation-in-adverse-weather-giulia-rizzoli-et-al-2024>(14/89 | 20/286) When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather (Giulia Rizzoli et al., 2024)</a></li><li><a href=#1589--21286-h-vmunet-high-order-vision-mamba-unet-for-medical-image-segmentation-renkai-wu-et-al-2024>(15/89 | 21/286) H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation (Renkai Wu et al., 2024)</a></li><li><a href=#1689--22286-an-ai-assisted-skincare-routine-recommendation-system-in-xr-gowravi-malalur-rajegowda-et-al-2024>(16/89 | 22/286) An AI-Assisted Skincare Routine Recommendation System in XR (Gowravi Malalur Rajegowda et al., 2024)</a></li><li><a href=#1789--23286-few-shot-oriented-object-detection-with-memorable-contrastive-learning-in-remote-sensing-images-jiawei-zhou-et-al-2024>(17/89 | 23/286) Few-shot Oriented Object Detection with Memorable Contrastive Learning in Remote Sensing Images (Jiawei Zhou et al., 2024)</a></li><li><a href=#1889--24286-detdiffusion-synergizing-generative-and-perceptive-models-for-enhanced-data-generation-and-perception-yibo-wang-et-al-2024>(18/89 | 24/286) DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception (Yibo Wang et al., 2024)</a></li><li><a href=#1989--25286-vl-mamba-exploring-state-space-models-for-multimodal-learning-yanyuan-qiao-et-al-2024>(19/89 | 25/286) VL-Mamba: Exploring State Space Models for Multimodal Learning (Yanyuan Qiao et al., 2024)</a></li><li><a href=#2089--26286-aud-tgn-advancing-action-unit-detection-with-temporal-convolution-and-gpt-2-in-wild-audiovisual-contexts-jun-yu-et-al-2024>(20/89 | 26/286) AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and GPT-2 in Wild Audiovisual Contexts (Jun Yu et al., 2024)</a></li><li><a href=#2189--27286-recursive-cross-modal-attention-for-multimodal-fusion-in-dimensional-emotion-recognition-r-gnana-praveen-et-al-2024>(21/89 | 27/286) Recursive Cross-Modal Attention for Multimodal Fusion in Dimensional Emotion Recognition (R. Gnana Praveen et al., 2024)</a></li><li><a href=#2289--28286-what-if-counterfactual-inception-to-mitigate-hallucination-effects-in-large-multimodal-models-junho-kim-et-al-2024>(22/89 | 28/286) What if&mldr;?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models (Junho Kim et al., 2024)</a></li><li><a href=#2389--29286-puzzlevqa-diagnosing-multimodal-reasoning-challenges-of-language-models-with-abstract-visual-patterns-yew-ken-chia-et-al-2024>(23/89 | 29/286) PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns (Yew Ken Chia et al., 2024)</a></li><li><a href=#2489--30286-portrait4d-v2-pseudo-multi-view-data-creates-better-4d-head-synthesizer-yu-deng-et-al-2024>(24/89 | 30/286) Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer (Yu Deng et al., 2024)</a></li><li><a href=#2589--31286-leveraging-high-resolution-features-for-improved-deep-hashing-based-image-retrieval-aymene-berriche-et-al-2024>(25/89 | 31/286) Leveraging High-Resolution Features for Improved Deep Hashing-based Image Retrieval (Aymene Berriche et al., 2024)</a></li><li><a href=#2689--32286-progressive-trajectory-matching-for-medical-dataset-distillation-zhen-yu-et-al-2024>(26/89 | 32/286) Progressive trajectory matching for medical dataset distillation (Zhen Yu et al., 2024)</a></li><li><a href=#2789--33286-out-of-distribution-detection-using-peer-class-generated-by-large-language-model-k-huang-et-al-2024>(27/89 | 33/286) Out-of-Distribution Detection Using Peer-Class Generated by Large Language Model (K Huang et al., 2024)</a></li><li><a href=#2889--34286-dd-robustbench-an-adversarial-robustness-benchmark-for-dataset-distillation-yifan-wu-et-al-2024>(28/89 | 34/286) DD-RobustBench: An Adversarial Robustness Benchmark for Dataset Distillation (Yifan Wu et al., 2024)</a></li><li><a href=#2989--35286-on-pretraining-data-diversity-for-self-supervised-learning-hasan-abed-al-kader-hammoud-et-al-2024>(29/89 | 35/286) On Pretraining Data Diversity for Self-Supervised Learning (Hasan Abed Al Kader Hammoud et al., 2024)</a></li><li><a href=#3089--36286-enhancing-gait-video-analysis-in-neurodegenerative-diseases-by-knowledge-augmentation-in-vision-language-model-diwei-wang-et-al-2024>(30/89 | 36/286) Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge Augmentation in Vision Language Model (Diwei Wang et al., 2024)</a></li><li><a href=#3189--37286-fostc3neta-lightweight-yolov5-based-on-the-network-structure-optimization-danqing-ma-et-al-2024>(31/89 | 37/286) Fostc3net:A Lightweight YOLOv5 Based On the Network Structure Optimization (Danqing Ma et al., 2024)</a></li><li><a href=#3289--38286-retina-vision-transformer-retinavit-introducing-scaled-patches-into-vision-transformers-yuyang-shu-et-al-2024>(32/89 | 38/286) Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into Vision Transformers (Yuyang Shu et al., 2024)</a></li><li><a href=#3389--39286-zodi-zero-shot-domain-adaptation-with-diffusion-based-image-transfer-hiroki-azuma-et-al-2024>(33/89 | 39/286) ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer (Hiroki Azuma et al., 2024)</a></li><li><a href=#3489--40286-idadapter-learning-mixed-features-for-tuning-free-personalization-of-text-to-image-models-siying-cui-et-al-2024>(34/89 | 40/286) IDAdapter: Learning Mixed Features for Tuning-Free Personalization of Text-to-Image Models (Siying Cui et al., 2024)</a></li><li><a href=#3589--41286-vstar-generative-temporal-nursing-for-longer-dynamic-video-synthesis-yumeng-li-et-al-2024>(35/89 | 41/286) VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis (Yumeng Li et al., 2024)</a></li><li><a href=#3689--42286-iidm-image-to-image-diffusion-model-for-semantic-image-synthesis-feng-liu-et-al-2024>(36/89 | 42/286) IIDM: Image-to-Image Diffusion Model for Semantic Image Synthesis (Feng Liu et al., 2024)</a></li><li><a href=#3789--43286-fissionfusion-fast-geometric-generation-and-hierarchical-souping-for-medical-image-analysis-santosh-sanjeev-et-al-2024>(37/89 | 43/286) FissionFusion: Fast Geometric Generation and Hierarchical Souping for Medical Image Analysis (Santosh Sanjeev et al., 2024)</a></li><li><a href=#3889--44286-rotary-position-embedding-for-vision-transformer-byeongho-heo-et-al-2024>(38/89 | 44/286) Rotary Position Embedding for Vision Transformer (Byeongho Heo et al., 2024)</a></li><li><a href=#3989--45286-adavipro-region-based-adaptive-visual-prompt-for-large-scale-models-adapting-mengyu-yang-et-al-2024>(39/89 | 45/286) AdaViPro: Region-based Adaptive Visual Prompt for Large-Scale Models Adapting (Mengyu Yang et al., 2024)</a></li><li><a href=#4089--46286-samct-segment-any-ct-allowing-labor-free-task-indicator-prompts-xian-lin-et-al-2024>(40/89 | 46/286) SAMCT: Segment Any CT Allowing Labor-Free Task-Indicator Prompts (Xian Lin et al., 2024)</a></li><li><a href=#4189--47286-zigma-zigzag-mamba-diffusion-model-vincent-tao-hu-et-al-2024>(41/89 | 47/286) ZigMa: Zigzag Mamba Diffusion Model (Vincent Tao Hu et al., 2024)</a></li><li><a href=#4289--48286-compress3d-a-compressed-latent-space-for-3d-generation-from-a-single-image-bowen-zhang-et-al-2024>(42/89 | 48/286) Compress3D: a Compressed Latent Space for 3D Generation from a Single Image (Bowen Zhang et al., 2024)</a></li><li><a href=#4389--49286-text-to-3d-shape-generation-han-hung-lee-et-al-2024>(43/89 | 49/286) Text-to-3D Shape Generation (Han-Hung Lee et al., 2024)</a></li><li><a href=#4489--50286-beyond-skeletons-integrative-latent-mapping-for-coherent-4d-sequence-generation-qitong-yang-et-al-2024>(44/89 | 50/286) Beyond Skeletons: Integrative Latent Mapping for Coherent 4D Sequence Generation (Qitong Yang et al., 2024)</a></li><li><a href=#4589--51286-sptnet-an-efficient-alternative-framework-for-generalized-category-discovery-with-spatial-prompt-tuning-hongjun-wang-et-al-2024>(45/89 | 51/286) SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning (Hongjun Wang et al., 2024)</a></li><li><a href=#4689--52286-dancecamera3d-3d-camera-movement-synthesis-with-music-and-dance-zixuan-wang-et-al-2024>(46/89 | 52/286) DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance (Zixuan Wang et al., 2024)</a></li><li><a href=#4789--53286-exmap-leveraging-explainability-heatmaps-for-unsupervised-group-robustness-to-spurious-correlations-rwiddhi-chakraborty-et-al-2024>(47/89 | 53/286) ExMap: Leveraging Explainability Heatmaps for Unsupervised Group Robustness to Spurious Correlations (Rwiddhi Chakraborty et al., 2024)</a></li><li><a href=#4889--54286-find-n-propagate-open-vocabulary-3d-object-detection-in-urban-environments-djamahl-etchegaray-et-al-2024>(48/89 | 54/286) Find n&rsquo; Propagate: Open-Vocabulary 3D Object Detection in Urban Environments (Djamahl Etchegaray et al., 2024)</a></li><li><a href=#4989--55286-scale-decoupled-distillation-shicai-wei-chunbo-luo-yang-luo-2024>(49/89 | 55/286) Scale Decoupled Distillation (Shicai Wei Chunbo Luo Yang Luo, 2024)</a></li><li><a href=#5089--56286-sc-tune-unleashing-self-consistent-referential-comprehension-in-large-vision-language-models-tongtian-yue-et-al-2024>(50/89 | 56/286) SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models (Tongtian Yue et al., 2024)</a></li><li><a href=#5189--57286-nellie-automated-organelle-segmentation-tracking-and-hierarchical-feature-extraction-in-2d3d-live-cell-microscopy-austin-e-y-t-lefebvre-et-al-2024>(51/89 | 57/286) Nellie: Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy (Austin E. Y. T. Lefebvre et al., 2024)</a></li><li><a href=#5289--58286-uncertainty-driven-active-learning-for-image-segmentation-in-underwater-inspection-luiza-ribeiro-marnet-et-al-2024>(52/89 | 58/286) Uncertainty Driven Active Learning for Image Segmentation in Underwater Inspection (Luiza Ribeiro Marnet et al., 2024)</a></li><li><a href=#5389--59286-como-controllable-motion-generation-through-language-guided-pose-code-editing-yiming-huang-et-al-2024>(53/89 | 59/286) CoMo: Controllable Motion Generation through Language Guided Pose Code Editing (Yiming Huang et al., 2024)</a></li><li><a href=#5489--60286-self-attention-based-semantic-decomposition-in-vector-symbolic-architectures-calvin-yeung-et-al-2024>(54/89 | 60/286) Self-Attention Based Semantic Decomposition in Vector Symbolic Architectures (Calvin Yeung et al., 2024)</a></li><li><a href=#5589--61286-practical-end-to-end-optical-music-recognition-for-pianoform-music-jiří-mayer-et-al-2024>(55/89 | 61/286) Practical End-to-End Optical Music Recognition for Pianoform Music (Jiří Mayer et al., 2024)</a></li><li><a href=#5689--62286-a-unified-optimal-transport-framework-for-cross-modal-retrieval-with-noisy-labels-haochen-han-et-al-2024>(56/89 | 62/286) A Unified Optimal Transport Framework for Cross-Modal Retrieval with Noisy Labels (Haochen Han et al., 2024)</a></li><li><a href=#5789--63286-depthfm-fast-monocular-depth-estimation-with-flow-matching-ming-gui-et-al-2024>(57/89 | 63/286) DepthFM: Fast Monocular Depth Estimation with Flow Matching (Ming Gui et al., 2024)</a></li><li><a href=#5889--64286-hiercode-a-lightweight-hierarchical-codebook-for-zero-shot-chinese-text-recognition-yuyi-zhang-et-al-2024>(58/89 | 64/286) HierCode: A Lightweight Hierarchical Codebook for Zero-shot Chinese Text Recognition (Yuyi Zhang et al., 2024)</a></li><li><a href=#5989--65286-be-your-outpainter-mastering-video-outpainting-through-input-specific-adaptation-fu-yun-wang-et-al-2024>(59/89 | 65/286) Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation (Fu-Yun Wang et al., 2024)</a></li><li><a href=#6089--66286-orthcaps-an-orthogonal-capsnet-with-sparse-attention-routing-and-pruning-xinyu-geng-et-al-2024>(60/89 | 66/286) OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and Pruning (Xinyu Geng et al., 2024)</a></li><li><a href=#6189--67286-tibix-leveraging-temporal-information-for-bidirectional-x-ray-and-report-generation-santosh-sanjeev-et-al-2024>(61/89 | 67/286) TiBiX: Leveraging Temporal Information for Bidirectional X-ray and Report Generation (Santosh Sanjeev et al., 2024)</a></li><li><a href=#6289--68286-efficient-scene-text-image-super-resolution-with-semantic-guidance-leowu-tomyenrique-et-al-2024>(62/89 | 68/286) Efficient scene text image super-resolution with semantic guidance (LeoWu TomyEnrique et al., 2024)</a></li><li><a href=#6389--69286-self-supervised-class-agnostic-motion-prediction-with-spatial-and-temporal-consistency-regularizations-kewei-wang-et-al-2024>(63/89 | 69/286) Self-Supervised Class-Agnostic Motion Prediction with Spatial and Temporal Consistency Regularizations (Kewei Wang et al., 2024)</a></li><li><a href=#6489--70286-seffec-semantic-facial-feature-control-for-fine-grained-face-editing-florian-strohm-et-al-2024>(64/89 | 70/286) SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing (Florian Strohm et al., 2024)</a></li><li><a href=#6589--71286-radsplat-radiance-field-informed-gaussian-splatting-for-robust-real-time-rendering-with-900-fps-michael-niemeyer-et-al-2024>(65/89 | 71/286) RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS (Michael Niemeyer et al., 2024)</a></li><li><a href=#6689--72286-bounding-box-stability-against-feature-dropout-reflects-detector-generalization-across-environments-yang-yang-et-al-2024>(66/89 | 72/286) Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments (Yang Yang et al., 2024)</a></li><li><a href=#6789--73286-timerewind-rewinding-time-with-image-and-events-video-diffusion-jingxi-chen-et-al-2024>(67/89 | 73/286) TimeRewind: Rewinding Time with Image-and-Events Video Diffusion (Jingxi Chen et al., 2024)</a></li><li><a href=#6889--74286-certified-human-trajectory-prediction-mohammadhossein-bahari-et-al-2024>(68/89 | 74/286) Certified Human Trajectory Prediction (Mohammadhossein Bahari et al., 2024)</a></li><li><a href=#6989--75286-promamba-prompt-mamba-for-polyp-segmentation-jianhao-xie-et-al-2024>(69/89 | 75/286) ProMamba: Prompt-Mamba for polyp segmentation (Jianhao Xie et al., 2024)</a></li><li><a href=#7089--76286-learning-user-embeddings-from-human-gaze-for-personalised-saliency-prediction-florian-strohm-et-al-2024>(70/89 | 76/286) Learning User Embeddings from Human Gaze for Personalised Saliency Prediction (Florian Strohm et al., 2024)</a></li><li><a href=#7189--77286-leveraging-feature-communication-in-federated-learning-for-remote-sensing-image-classification-anh-kiet-duong-et-al-2024>(71/89 | 77/286) Leveraging feature communication in federated learning for remote sensing image classification (Anh-Kiet Duong et al., 2024)</a></li><li><a href=#7289--78286-diversity-aware-channel-pruning-for-stylegan-compression-jiwoo-chung-et-al-2024>(72/89 | 78/286) Diversity-aware Channel Pruning for StyleGAN Compression (Jiwoo Chung et al., 2024)</a></li><li><a href=#7389--79286-scaling-diffusion-models-to-real-world-3d-lidar-scene-completion-lucas-nunes-et-al-2024>(73/89 | 79/286) Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion (Lucas Nunes et al., 2024)</a></li><li><a href=#7489--80286-diversified-and-personalized-multi-rater-medical-image-segmentation-yicheng-wu-et-al-2024>(74/89 | 80/286) Diversified and Personalized Multi-rater Medical Image Segmentation (Yicheng Wu et al., 2024)</a></li><li><a href=#7589--81286-cell-tracking-in-c-elegans-with-cell-position-heatmap-based-alignment-and-pairwise-detection-kaito-shiku-et-al-2024>(75/89 | 81/286) Cell Tracking in C. elegans with Cell Position Heatmap-Based Alignment and Pairwise Detection (Kaito Shiku et al., 2024)</a></li><li><a href=#7689--82286-s2dm-sector-shaped-diffusion-models-for-video-generation-haoran-lang-et-al-2024>(76/89 | 82/286) S2DM: Sector-Shaped Diffusion Models for Video Generation (Haoran Lang et al., 2024)</a></li><li><a href=#7789--83286-counting-network-for-learning-from-majority-label-kaito-shiku-et-al-2024>(77/89 | 83/286) Counting Network for Learning from Majority Label (Kaito Shiku et al., 2024)</a></li><li><a href=#7889--84286-vid-tldr-training-free-token-merging-for-light-weight-video-transformer-joonmyung-choi-et-al-2024>(78/89 | 84/286) vid-TLDR: Training Free Token merging for Light-weight Video Transformer (Joonmyung Choi et al., 2024)</a></li><li><a href=#7989--85286-amp-autoregressive-motion-prediction-revisited-with-next-token-prediction-for-autonomous-driving-xiaosong-jia-et-al-2024>(79/89 | 85/286) AMP: Autoregressive Motion Prediction Revisited with Next Token Prediction for Autonomous Driving (Xiaosong Jia et al., 2024)</a></li><li><a href=#8089--86286-laserhuman-language-guided-scene-aware-human-motion-generation-in-free-environment-peishan-cong-et-al-2024>(80/89 | 86/286) LaserHuman: Language-guided Scene-aware Human Motion Generation in Free Environment (Peishan Cong et al., 2024)</a></li><li><a href=#8189--87286-building-optimal-neural-architectures-using-interpretable-knowledge-keith-g-mills-et-al-2024>(81/89 | 87/286) Building Optimal Neural Architectures using Interpretable Knowledge (Keith G. Mills et al., 2024)</a></li><li><a href=#8289--88286-mora-enabling-generalist-video-generation-via-a-multi-agent-framework-zhengqing-yuan-et-al-2024>(82/89 | 88/286) Mora: Enabling Generalist Video Generation via A Multi-Agent Framework (Zhengqing Yuan et al., 2024)</a></li><li><a href=#8389--89286-correlation-clustering-of-organoid-images-jannik-presberger-et-al-2024>(83/89 | 89/286) Correlation Clustering of Organoid Images (Jannik Presberger et al., 2024)</a></li><li><a href=#8489--90286-describe-and-dissect-interpreting-neurons-in-vision-networks-with-language-models-nicholas-bai-et-al-2024>(84/89 | 90/286) Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models (Nicholas Bai et al., 2024)</a></li><li><a href=#8589--91286-unifying-local-and-global-multimodal-features-for-place-recognition-in-aliased-and-low-texture-environments-alberto-garcía-hernández-et-al-2024>(85/89 | 91/286) Unifying Local and Global Multimodal Features for Place Recognition in Aliased and Low-Texture Environments (Alberto García-Hernández et al., 2024)</a></li><li><a href=#8689--92286-hyperfusion-a-hypernetwork-approach-to-multimodal-integration-of-tabular-and-medical-imaging-data-for-predictive-modeling-daniel-duenias-et-al-2024>(86/89 | 92/286) HyperFusion: A Hypernetwork Approach to Multimodal Integration of Tabular and Medical Imaging Data for Predictive Modeling (Daniel Duenias et al., 2024)</a></li><li><a href=#8789--93286-congeo-robust-cross-view-geo-localization-across-ground-view-variations-li-mi-et-al-2024>(87/89 | 93/286) ConGeo: Robust Cross-view Geo-localization across Ground View Variations (Li Mi et al., 2024)</a></li><li><a href=#8889--94286-fast-poly-a-fast-polyhedral-framework-for-3d-multi-object-tracking-xiaoyu-li-et-al-2024>(88/89 | 94/286) Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking (Xiaoyu Li et al., 2024)</a></li><li><a href=#8989--95286-adaptive-critical-subgraph-mining-for-cognitive-impairment-conversion-prediction-with-t1-mri-based-brain-network-yilin-leng-et-al-2024>(89/89 | 95/286) Adaptive Critical Subgraph Mining for Cognitive Impairment Conversion Prediction with T1-MRI-based Brain Network (Yilin Leng et al., 2024)</a></li></ul></li><li><a href=#cscl-33>cs.CL (33)</a><ul><li><a href=#133--96286-paramanu-ayn-an-efficient-novel-generative-and-instruction-tuned-language-model-for-indian-legal-case-documents-mitodru-niyogi-et-al-2024>(1/33 | 96/286) PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents (Mitodru Niyogi et al., 2024)</a></li><li><a href=#233--97286-facilitating-pornographic-text-detection-for-open-domain-dialogue-systems-via-knowledge-distillation-of-large-language-models-huachuan-qiu-et-al-2024>(2/33 | 97/286) Facilitating Pornographic Text Detection for Open-Domain Dialogue Systems via Knowledge Distillation of Large Language Models (Huachuan Qiu et al., 2024)</a></li><li><a href=#333--98286-information-theoretic-distillation-for-reference-less-summarization-jaehun-jung-et-al-2024>(3/33 | 98/286) Information-Theoretic Distillation for Reference-less Summarization (Jaehun Jung et al., 2024)</a></li><li><a href=#433--99286-clinical-information-extraction-for-low-resource-languages-with-few-shot-learning-using-pre-trained-language-models-and-prompting-phillip-richter-pechanski-et-al-2024>(4/33 | 99/286) Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting (Phillip Richter-Pechanski et al., 2024)</a></li><li><a href=#533--100286-llama-meets-eu-investigating-the-european-political-spectrum-through-the-lens-of-llms-ilias-chalkidis-et-al-2024>(5/33 | 100/286) Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs (Ilias Chalkidis et al., 2024)</a></li><li><a href=#633--101286-sumtra-a-differentiable-pipeline-for-few-shot-cross-lingual-summarization-jacob-parnell-et-al-2024>(6/33 | 101/286) SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization (Jacob Parnell et al., 2024)</a></li><li><a href=#733--102286-teacher-student-training-for-debiasing-general-permutation-debiasing-for-large-language-models-adian-liusie-et-al-2024>(7/33 | 102/286) Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models (Adian Liusie et al., 2024)</a></li><li><a href=#833--103286-on-prompt-sensitivity-of-chatgpt-in-affective-computing-mostafa-m-amin-et-al-2024>(8/33 | 103/286) On Prompt Sensitivity of ChatGPT in Affective Computing (Mostafa M. Amin et al., 2024)</a></li><li><a href=#933--104286-leveraging-linguistically-enhanced-embeddings-for-open-information-extraction-fauzan-farooqui-et-al-2024>(9/33 | 104/286) Leveraging Linguistically Enhanced Embeddings for Open Information Extraction (Fauzan Farooqui et al., 2024)</a></li><li><a href=#1033--105286-train--constrain-phonologically-informed-tongue-twister-generation-from-topics-and-paraphrases-tyler-loakman-et-al-2024>(10/33 | 105/286) Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases (Tyler Loakman et al., 2024)</a></li><li><a href=#1133--106286-do-not-worry-if-you-do-not-have-data-building-pretrained-language-models-using-translationese-meet-doshi-et-al-2024>(11/33 | 106/286) Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese (Meet Doshi et al., 2024)</a></li><li><a href=#1233--107286-instruction-multi-constraint-molecular-generation-using-a-teacher-student-large-language-model-peng-zhou-et-al-2024>(12/33 | 107/286) Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model (Peng Zhou et al., 2024)</a></li><li><a href=#1333--108286-ax-to-grind-urdu-benchmark-dataset-for-urdu-fake-news-detection-sheetal-harris-et-al-2024>(13/33 | 108/286) Ax-to-Grind Urdu: Benchmark Dataset for Urdu Fake News Detection (Sheetal Harris et al., 2024)</a></li><li><a href=#1433--109286-ethiollm-multilingual-large-language-models-for-ethiopian-languages-with-task-evaluation-atnafu-lambebo-tonja-et-al-2024>(14/33 | 109/286) EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation (Atnafu Lambebo Tonja et al., 2024)</a></li><li><a href=#1533--110286-evaluating-unsupervised-dimensionality-reduction-methods-for-pretrained-sentence-embeddings-gaifan-zhang-et-al-2024>(15/33 | 110/286) Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained Sentence Embeddings (Gaifan Zhang et al., 2024)</a></li><li><a href=#1633--111286-chain-of-interaction-enhancing-large-language-models-for-psychiatric-behavior-understanding-by-dyadic-contexts-guangzeng-han-et-al-2024>(16/33 | 111/286) Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts (Guangzeng Han et al., 2024)</a></li><li><a href=#1733--112286-dynamic-reward-adjustment-in-multi-reward-reinforcement-learning-for-counselor-reflection-generation-do-june-min-et-al-2024>(17/33 | 112/286) Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for Counselor Reflection Generation (Do June Min et al., 2024)</a></li><li><a href=#1833--113286-llamafactory-unified-efficient-fine-tuning-of-100-language-models-yaowei-zheng-et-al-2024>(18/33 | 113/286) LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models (Yaowei Zheng et al., 2024)</a></li><li><a href=#1933--114286-leanreasoner-boosting-complex-logical-reasoning-with-lean-dongwei-jiang-et-al-2024>(19/33 | 114/286) LeanReasoner: Boosting Complex Logical Reasoning with Lean (Dongwei Jiang et al., 2024)</a></li><li><a href=#2033--115286-arcees-mergekit-a-toolkit-for-merging-large-language-models-charles-goddard-et-al-2024>(20/33 | 115/286) Arcee&rsquo;s MergeKit: A Toolkit for Merging Large Language Models (Charles Goddard et al., 2024)</a></li><li><a href=#2133--116286-roleinteract-evaluating-the-social-interaction-of-role-playing-agents-hongzhan-chen-et-al-2024>(21/33 | 116/286) RoleInteract: Evaluating the Social Interaction of Role-Playing Agents (Hongzhan Chen et al., 2024)</a></li><li><a href=#2233--117286-grounding-spatial-relations-in-text-only-language-models-gorka-azkune-et-al-2024>(22/33 | 117/286) Grounding Spatial Relations in Text-Only Language Models (Gorka Azkune et al., 2024)</a></li><li><a href=#2333--118286-hyacinth6b-a-large-language-model-for-traditional-chinese-chih-wei-song-et-al-2024>(23/33 | 118/286) Hyacinth6B: A large language model for Traditional Chinese (Chih-Wei Song et al., 2024)</a></li><li><a href=#2433--119286-aflora-adaptive-freezing-of-low-rank-adaptation-in-parameter-efficient-fine-tuning-of-large-models-zeyu-liu-et-al-2024>(24/33 | 119/286) AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models (Zeyu Liu et al., 2024)</a></li><li><a href=#2533--120286-reverse-training-to-nurse-the-reversal-curse-olga-golovneva-et-al-2024>(25/33 | 120/286) Reverse Training to Nurse the Reversal Curse (Olga Golovneva et al., 2024)</a></li><li><a href=#2633--121286-what-explains-the-success-of-cross-modal-fine-tuning-with-orca-paloma-garcía-de-herreros-et-al-2024>(26/33 | 121/286) What explains the success of cross-modal fine-tuning with ORCA? (Paloma García-de-Herreros et al., 2024)</a></li><li><a href=#2733--122286-how-gender-interacts-with-political-values-a-case-study-on-czech-bert-models-adnan-al-ali-et-al-2024>(27/33 | 122/286) How Gender Interacts with Political Values: A Case Study on Czech BERT Models (Adnan Al Ali et al., 2024)</a></li><li><a href=#2833--123286-an-entropy-based-text-watermarking-detection-method-yijian-lu-et-al-2024>(28/33 | 123/286) An Entropy-based Text Watermarking Detection Method (Yijian Lu et al., 2024)</a></li><li><a href=#2933--124286-technical-report-competition-solution-for-bettermixture-shuaijiang-zhao-et-al-2024>(29/33 | 124/286) Technical Report: Competition Solution For BetterMixture (Shuaijiang Zhao et al., 2024)</a></li><li><a href=#3033--125286-a-new-massive-multilingual-dataset-for-high-performance-language-technologies-ona-de-gibert-et-al-2024>(30/33 | 125/286) A New Massive Multilingual Dataset for High-Performance Language Technologies (Ona de Gibert et al., 2024)</a></li><li><a href=#3133--126286-reducing-large-language-model-bias-with-emphasis-on-restricted-industries-automated-dataset-augmentation-and-prejudice-quantification-devam-mondal-et-al-2024>(31/33 | 126/286) Reducing Large Language Model Bias with Emphasis on &lsquo;Restricted Industries&rsquo;: Automated Dataset Augmentation and Prejudice Quantification (Devam Mondal et al., 2024)</a></li><li><a href=#3233--127286-different-tokenization-schemes-lead-to-comparable-performance-in-spanish-number-agreement-catherine-arnett-et-al-2024>(32/33 | 127/286) Different Tokenization Schemes Lead to Comparable Performance in Spanish Number Agreement (Catherine Arnett et al., 2024)</a></li><li><a href=#3333--128286-erst-a-signaled-graph-theory-of-discourse-relations-and-organization-amir-zeldes-et-al-2024>(33/33 | 128/286) eRST: A Signaled Graph Theory of Discourse Relations and Organization (Amir Zeldes et al., 2024)</a></li></ul></li><li><a href=#csse-9>cs.SE (9)</a><ul><li><a href=#19--129286-enhancing-code-generation-performance-of-smaller-models-by-distilling-the-reasoning-ability-of-llms-zhihong-sun-et-al-2024>(1/9 | 129/286) Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning Ability of LLMs (Zhihong Sun et al., 2024)</a></li><li><a href=#29--130286-genetic-auto-prompt-learning-for-pre-trained-code-intelligence-language-models-chengzhe-feng-et-al-2024>(2/9 | 130/286) Genetic Auto-prompt Learning for Pre-trained Code Intelligence Language Models (Chengzhe Feng et al., 2024)</a></li><li><a href=#39--131286-conline-complex-code-generation-and-refinement-with-online-searching-and-correctness-testing-xinyi-he-et-al-2024>(3/9 | 131/286) CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing (Xinyi He et al., 2024)</a></li><li><a href=#49--132286-creative-and-correct-requesting-diverse-code-solutions-from-ai-foundation-models-scott-blyth-et-al-2024>(4/9 | 132/286) Creative and Correct: Requesting Diverse Code Solutions from AI Foundation Models (Scott Blyth et al., 2024)</a></li><li><a href=#59--133286-reinforcement-learning-for-online-testing-of-autonomous-driving-systems-a-replication-and-extension-study-luca-giamattei-et-al-2024>(5/9 | 133/286) Reinforcement Learning for Online Testing of Autonomous Driving Systems: a Replication and Extension Study (Luca Giamattei et al., 2024)</a></li><li><a href=#69--134286-hylimo-a-hybrid-live-synchronized-modular-diagramming-editor-as-ide-extension-for-technical-and-scientific-publications-niklas-krieger-et-al-2024>(6/9 | 134/286) HyLiMo: A Hybrid Live-Synchronized Modular Diagramming Editor as IDE Extension for Technical and Scientific Publications (Niklas Krieger et al., 2024)</a></li><li><a href=#79--135286-fastflip-compositional-error-injection-analysis-keyur-joshi-et-al-2024>(7/9 | 135/286) FastFlip: Compositional Error Injection Analysis (Keyur Joshi et al., 2024)</a></li><li><a href=#89--136286-motorease-automated-detection-of-motor-impairment-accessibility-issues-in-mobile-app-uis-arun-krishnavajjala-et-al-2024>(8/9 | 136/286) MotorEase: Automated Detection of Motor Impairment Accessibility Issues in Mobile App UIs (Arun Krishnavajjala et al., 2024)</a></li><li><a href=#99--137286-specification-mining-for-smart-contracts-with-trace-slicing-and-predicate-abstraction-ye-liu-et-al-2024>(9/9 | 137/286) Specification Mining for Smart Contracts with Trace Slicing and Predicate Abstraction (Ye Liu et al., 2024)</a></li></ul></li><li><a href=#cslg-32>cs.LG (32)</a><ul><li><a href=#132--138286-from-representational-harms-to-quality-of-service-harms-a-case-study-on-llama-2-safety-safeguards-khaoula-chehbouni-et-al-2024>(1/32 | 138/286) From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards (Khaoula Chehbouni et al., 2024)</a></li><li><a href=#232--139286-rewardbench-evaluating-reward-models-for-language-modeling-nathan-lambert-et-al-2024>(2/32 | 139/286) RewardBench: Evaluating Reward Models for Language Modeling (Nathan Lambert et al., 2024)</a></li><li><a href=#332--140286-real-representation-enhanced-analytic-learning-for-exemplar-free-class-incremental-learning-run-he-et-al-2024>(3/32 | 140/286) REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning (Run He et al., 2024)</a></li><li><a href=#432--141286-adaptive-ensembles-of-fine-tuned-transformers-for-llm-generated-text-detection-zhixin-lai-et-al-2024>(4/32 | 141/286) Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection (Zhixin Lai et al., 2024)</a></li><li><a href=#532--142286-multimodal-variational-autoencoder-for-low-cost-cardiac-hemodynamics-instability-detection-mohammod-n-i-suvon-et-al-2024>(5/32 | 142/286) Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics Instability Detection (Mohammod N. I. Suvon et al., 2024)</a></li><li><a href=#632--143286-capsule-neural-networks-as-noise-stabilizer-for-time-series-data-soyeon-kim-et-al-2024>(6/32 | 143/286) Capsule Neural Networks as Noise Stabilizer for Time Series Data (Soyeon Kim et al., 2024)</a></li><li><a href=#732--144286-diffusion-model-for-data-driven-black-box-optimization-zihao-li-et-al-2024>(7/32 | 144/286) Diffusion Model for Data-Driven Black-Box Optimization (Zihao Li et al., 2024)</a></li><li><a href=#832--145286-machine-learning-optimized-approach-for-parameter-selection-in-meshfree-simulations-paulami-banerjee-et-al-2024>(8/32 | 145/286) Machine Learning Optimized Approach for Parameter Selection in MESHFREE Simulations (Paulami Banerjee et al., 2024)</a></li><li><a href=#932--146286-towards-principled-representation-learning-from-videos-for-reinforcement-learning-dipendra-misra-et-al-2024>(9/32 | 146/286) Towards Principled Representation Learning from Videos for Reinforcement Learning (Dipendra Misra et al., 2024)</a></li><li><a href=#1032--147286-sparse-implementation-of-versatile-graph-informed-layers-francesco-della-santa-2024>(10/32 | 147/286) Sparse Implementation of Versatile Graph-Informed Layers (Francesco Della Santa, 2024)</a></li><li><a href=#1132--148286-hierarchical-gaussian-mixture-normalizing-flow-modeling-for-unified-anomaly-detection-xincheng-yao-et-al-2024>(11/32 | 148/286) Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection (Xincheng Yao et al., 2024)</a></li><li><a href=#1232--149286-unifews-unified-entry-wise-sparsification-for-efficient-graph-neural-network-ningyi-liao-et-al-2024>(12/32 | 149/286) Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural Network (Ningyi Liao et al., 2024)</a></li><li><a href=#1332--150286-decentralized-federated-learning-model-update-tracking-under-imperfect-information-sharing-vishnu-pandi-chellapandi-et-al-2024>(13/32 | 150/286) Decentralized Federated Learning: Model Update Tracking Under Imperfect Information Sharing (Vishnu Pandi Chellapandi et al., 2024)</a></li><li><a href=#1432--151286-bridge-the-modality-and-capacity-gaps-in-vision-language-model-selection-chao-yi-et-al-2024>(14/32 | 151/286) Bridge the Modality and Capacity Gaps in Vision-Language Model Selection (Chao Yi et al., 2024)</a></li><li><a href=#1532--152286-does-differentially-private-synthetic-data-lead-to-synthetic-discoveries-ileana-montoya-perez-et-al-2024>(15/32 | 152/286) Does Differentially Private Synthetic Data Lead to Synthetic Discoveries? (Ileana Montoya Perez et al., 2024)</a></li><li><a href=#1632--153286-the-bid-picture-auction-inspired-multi-player-generative-adversarial-networks-training-joo-yong-shim-et-al-2024>(16/32 | 153/286) The Bid Picture: Auction-Inspired Multi-player Generative Adversarial Networks Training (Joo Yong Shim et al., 2024)</a></li><li><a href=#1732--154286-diffimpute-tabular-data-imputation-with-denoising-diffusion-probabilistic-model-yizhu-wen-et-al-2024>(17/32 | 154/286) DiffImpute: Tabular Data Imputation With Denoising Diffusion Probabilistic Model (Yizhu Wen et al., 2024)</a></li><li><a href=#1832--155286-divide-conquer-transformer-learning-for-predicting-electric-vehicle-charging-events-using-smart-meter-data-fucai-ke-et-al-2024>(18/32 | 155/286) Divide-Conquer Transformer Learning for Predicting Electric Vehicle Charging Events Using Smart Meter Data (Fucai Ke et al., 2024)</a></li><li><a href=#1932--156286-spatial-temporal-graph-representation-learning-for-tactical-networks-future-state-prediction-liu-junhua-et-al-2024>(19/32 | 156/286) Spatial-Temporal Graph Representation Learning for Tactical Networks Future State Prediction (Liu Junhua et al., 2024)</a></li><li><a href=#2032--157286-weisfeiler-and-leman-go-loopy-a-new-hierarchy-for-graph-representational-learning-raffaele-paolino-et-al-2024>(20/32 | 157/286) Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning (Raffaele Paolino et al., 2024)</a></li><li><a href=#2132--158286-adversarial-attacks-and-defenses-in-automated-control-systems-a-comprehensive-benchmark-vitaliy-pozdnyakov-et-al-2024>(21/32 | 158/286) Adversarial Attacks and Defenses in Automated Control Systems: A Comprehensive Benchmark (Vitaliy Pozdnyakov et al., 2024)</a></li><li><a href=#2232--159286-optimal-transport-for-fairness-archival-data-repair-using-small-research-data-sets-abigail-langbridge-et-al-2024>(22/32 | 159/286) Optimal Transport for Fairness: Archival Data Repair using Small Research Data Sets (Abigail Langbridge et al., 2024)</a></li><li><a href=#2332--160286-a-unified-and-general-framework-for-continual-learning-zhenyi-wang-et-al-2024>(23/32 | 160/286) A Unified and General Framework for Continual Learning (Zhenyi Wang et al., 2024)</a></li><li><a href=#2432--161286-machine-learning-based-layer-wise-detection-of-overheating-anomaly-in-lpbf-using-photodiode-data-nazmul-hasan-et-al-2024>(24/32 | 161/286) Machine Learning-based Layer-wise Detection of Overheating Anomaly in LPBF using Photodiode Data (Nazmul Hasan et al., 2024)</a></li><li><a href=#2532--162286-multi-criteria-approach-for-selecting-an-explanation-from-the-set-of-counterfactuals-produced-by-an-ensemble-of-explainers-ignacy-stępka-et-al-2024>(25/32 | 162/286) Multi-criteria approach for selecting an explanation from the set of counterfactuals produced by an ensemble of explainers (Ignacy Stępka et al., 2024)</a></li><li><a href=#2632--163286-evaluating-frontier-models-for-dangerous-capabilities-mary-phuong-et-al-2024>(26/32 | 163/286) Evaluating Frontier Models for Dangerous Capabilities (Mary Phuong et al., 2024)</a></li><li><a href=#2732--164286-the-model-openness-framework-promoting-completeness-and-openness-for-reproducibility-transparency-and-usability-in-ai-matt-white-et-al-2024>(27/32 | 164/286) The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI (Matt White et al., 2024)</a></li><li><a href=#2832--165286-integrating-large-language-models-for-severity-classification-in-traffic-incident-management-a-machine-learning-approach-artur-grigorev-et-al-2024>(28/32 | 165/286) Integrating Large Language Models for Severity Classification in Traffic Incident Management: A Machine Learning Approach (Artur Grigorev et al., 2024)</a></li><li><a href=#2932--166286-have-you-poisoned-my-data-defending-neural-networks-against-data-poisoning-fabio-de-gaspari-et-al-2024>(29/32 | 166/286) Have You Poisoned My Data? Defending Neural Networks against Data Poisoning (Fabio De Gaspari et al., 2024)</a></li><li><a href=#3032--167286-byzantine-resilient-federated-learning-with-adaptivity-to-data-heterogeneity-shiyuan-zuo-et-al-2024>(30/32 | 167/286) Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity (Shiyuan Zuo et al., 2024)</a></li><li><a href=#3132--168286-probabilistic-forecasting-with-stochastic-interpolants-and-föllmer-processes-yifan-chen-et-al-2024>(31/32 | 168/286) Probabilistic Forecasting with Stochastic Interpolants and Föllmer Processes (Yifan Chen et al., 2024)</a></li><li><a href=#3232--169286-tackling-noisy-labels-with-network-parameter-additive-decomposition-jingyi-wang-et-al-2024>(32/32 | 169/286) Tackling Noisy Labels with Network Parameter Additive Decomposition (Jingyi Wang et al., 2024)</a></li></ul></li><li><a href=#csro-23>cs.RO (23)</a><ul><li><a href=#123--170286-natural-language-as-polices-reasoning-for-coordinate-level-embodied-control-with-llms-yusuke-mikami-et-al-2024>(1/23 | 170/286) Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs (Yusuke Mikami et al., 2024)</a></li><li><a href=#223--171286-waypoint-based-reinforcement-learning-for-robot-manipulation-tasks-shaunak-a-mehta-et-al-2024>(2/23 | 171/286) Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks (Shaunak A. Mehta et al., 2024)</a></li><li><a href=#323--172286-manipose-a-comprehensive-benchmark-for-pose-aware-object-manipulation-in-robotics-qiaojun-yu-et-al-2024>(3/23 | 172/286) ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation in Robotics (Qiaojun Yu et al., 2024)</a></li><li><a href=#423--173286-loss-regularizing-robotic-terrain-classification-shakti-deo-kumar-et-al-2024>(4/23 | 173/286) Loss Regularizing Robotic Terrain Classification (Shakti Deo Kumar et al., 2024)</a></li><li><a href=#523--174286-clipswarm-generating-drone-shows-from-text-prompts-with-vision-language-models-pablo-pueyo-et-al-2024>(5/23 | 174/286) CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language Models (Pablo Pueyo et al., 2024)</a></li><li><a href=#623--175286-germ-a-generalist-robotic-model-with-mixture-of-experts-for-quadruped-robot-wenxuan-song-et-al-2024>(6/23 | 175/286) GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot (Wenxuan Song et al., 2024)</a></li><li><a href=#723--176286-visual-imitation-learning-of-task-oriented-object-grasping-and-rearrangement-yichen-cai-et-al-2024>(7/23 | 176/286) Visual Imitation Learning of Task-Oriented Object Grasping and Rearrangement (Yichen Cai et al., 2024)</a></li><li><a href=#823--177286-a-contact-model-based-on-denoising-diffusion-to-learn-variable-impedance-control-for-contact-rich-manipulation-masashi-okada-et-al-2024>(8/23 | 177/286) A Contact Model based on Denoising Diffusion to Learn Variable Impedance Control for Contact-rich Manipulation (Masashi Okada et al., 2024)</a></li><li><a href=#923--178286-embedding-pose-graph-enabling-3d-foundation-model-capabilities-with-a-compact-representation-hugues-thomas-et-al-2024>(9/23 | 178/286) Embedding Pose Graph, Enabling 3D Foundation Model Capabilities with a Compact Representation (Hugues Thomas et al., 2024)</a></li><li><a href=#1023--179286-open-access-nao-oan-a-ros2-based-software-framework-for-hri-applications-with-the-nao-robot-antonio-bono-et-al-2024>(10/23 | 179/286) Open Access NAO (OAN): a ROS2-based software framework for HRI applications with the NAO robot (Antonio Bono et al., 2024)</a></li><li><a href=#1123--180286-a-convex-formulation-of-frictional-contact-for-the-material-point-method-and-rigid-bodies-zeshun-zong-et-al-2024>(11/23 | 180/286) A Convex Formulation of Frictional Contact for the Material Point Method and Rigid Bodies (Zeshun Zong et al., 2024)</a></li><li><a href=#1223--181286-what-matters-for-active-texture-recognition-with-vision-based-tactile-sensors-alina-böhm-et-al-2024>(12/23 | 181/286) What Matters for Active Texture Recognition With Vision-Based Tactile Sensors (Alina Böhm et al., 2024)</a></li><li><a href=#1323--182286-reward-driven-automated-curriculum-learning-for-interaction-aware-self-driving-at-unsignalized-intersections-zengqi-peng-et-al-2024>(13/23 | 182/286) Reward-Driven Automated Curriculum Learning for Interaction-Aware Self-Driving at Unsignalized Intersections (Zengqi Peng et al., 2024)</a></li><li><a href=#1423--183286-centroidal-state-estimation-based-on-the-koopman-embedding-for-dynamic-legged-locomotion-shahram-khorshidi-et-al-2024>(14/23 | 183/286) Centroidal State Estimation based on the Koopman Embedding for Dynamic Legged Locomotion (Shahram Khorshidi et al., 2024)</a></li><li><a href=#1523--184286-robotics-meets-fluid-dynamics-a-characterization-of-the-induced-airflow-around-a-quadrotor-leonard-bauersfeld-et-al-2024>(15/23 | 184/286) Robotics meets Fluid Dynamics: A Characterization of the Induced Airflow around a Quadrotor (Leonard Bauersfeld et al., 2024)</a></li><li><a href=#1623--185286-look-before-you-leap-socially-acceptable-high-speed-ground-robot-navigation-in-crowded-hallways-lakshay-sharma-et-al-2024>(16/23 | 185/286) Look Before You Leap: Socially Acceptable High-Speed Ground Robot Navigation in Crowded Hallways (Lakshay Sharma et al., 2024)</a></li><li><a href=#1723--186286-a-rule-compliance-path-planner-for-lane-merge-scenarios-based-on-responsibility-sensitive-safety-pengfei-lin-et-al-2024>(17/23 | 186/286) A Rule-Compliance Path Planner for Lane-Merge Scenarios Based on Responsibility-Sensitive Safety (Pengfei Lin et al., 2024)</a></li><li><a href=#1823--187286-workload-estimation-for-unknown-tasks-a-survey-of-machine-learning-under-distribution-shift-josh-bhagat-smith-et-al-2024>(18/23 | 187/286) Workload Estimation for Unknown Tasks: A Survey of Machine Learning Under Distribution Shift (Josh Bhagat Smith et al., 2024)</a></li><li><a href=#1923--188286-amco-adaptive-multimodal-coupling-of-vision-and-proprioception-for-quadruped-robot-navigation-in-outdoor-environments-mohamed-elnoor-et-al-2024>(19/23 | 188/286) AMCO: Adaptive Multimodal Coupling of Vision and Proprioception for Quadruped Robot Navigation in Outdoor Environments (Mohamed Elnoor et al., 2024)</a></li><li><a href=#2023--189286-policed-rl-learning-closed-loop-robot-control-policies-with-provable-satisfaction-of-hard-constraints-jean-baptiste-bouvier-et-al-2024>(20/23 | 189/286) POLICEd RL: Learning Closed-Loop Robot Control Policies with Provable Satisfaction of Hard Constraints (Jean-Baptiste Bouvier et al., 2024)</a></li><li><a href=#2123--190286-dba-fusion-tightly-integrating-deep-dense-visual-bundle-adjustment-with-multiple-sensors-for-large-scale-localization-and-mapping-yuxuan-zhou-et-al-2024>(21/23 | 190/286) DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with Multiple Sensors for Large-Scale Localization and Mapping (Yuxuan Zhou et al., 2024)</a></li><li><a href=#2223--191286-lace-lhmp-airflow-modelling-inspired-long-term-human-motion-prediction-by-enhancing-laminar-characteristics-in-human-flow-yufei-zhu-et-al-2024>(22/23 | 191/286) LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction By Enhancing Laminar Characteristics in Human Flow (Yufei Zhu et al., 2024)</a></li><li><a href=#2323--192286-enhancing-security-in-multi-robot-systems-through-co-observation-planning-reachability-analysis-and-network-flow-ziqi-yang-et-al-2024>(23/23 | 192/286) Enhancing Security in Multi-Robot Systems through Co-Observation Planning, Reachability Analysis, and Network Flow (Ziqi Yang et al., 2024)</a></li></ul></li><li><a href=#csai-5>cs.AI (5)</a><ul><li><a href=#15--193286-polaris-a-safety-focused-llm-constellation-architecture-for-healthcare-subhabrata-mukherjee-et-al-2024>(1/5 | 193/286) Polaris: A Safety-focused LLM Constellation Architecture for Healthcare (Subhabrata Mukherjee et al., 2024)</a></li><li><a href=#25--194286-hyperllava-dynamic-visual-and-language-expert-tuning-for-multimodal-large-language-models-wenqiao-zhang-et-al-2024>(2/5 | 194/286) HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models (Wenqiao Zhang et al., 2024)</a></li><li><a href=#35--195286-motion-generation-from-fine-grained-textual-descriptions-kunhang-li-et-al-2024>(3/5 | 195/286) Motion Generation from Fine-grained Textual Descriptions (Kunhang Li et al., 2024)</a></li><li><a href=#45--196286-agent-group-chat-an-interactive-group-chat-simulacra-for-better-eliciting-collective-emergent-behavior-zhouhong-gu-et-al-2024>(4/5 | 196/286) Agent Group Chat: An Interactive Group Chat Simulacra For Better Eliciting Collective Emergent Behavior (Zhouhong Gu et al., 2024)</a></li><li><a href=#55--197286-multi-robot-connected-fermat-spiral-coverage-jingtao-tang-et-al-2024>(5/5 | 197/286) Multi-Robot Connected Fermat Spiral Coverage (Jingtao Tang et al., 2024)</a></li></ul></li><li><a href=#cshc-5>cs.HC (5)</a><ul><li><a href=#15--198286-reading-users-minds-from-what-they-say-an-investigation-into-llm-based-empathic-mental-inference-qihao-zhu-et-al-2024>(1/5 | 198/286) Reading Users&rsquo; Minds from What They Say: An Investigation into LLM-based Empathic Mental Inference (Qihao Zhu et al., 2024)</a></li><li><a href=#25--199286-vcounselor-a-psychological-intervention-chat-agent-based-on-a-knowledge-enhanced-large-language-model-h-zhang-et-al-2024>(2/5 | 199/286) VCounselor: A Psychological Intervention Chat Agent Based on a Knowledge-Enhanced Large Language Model (H. Zhang et al., 2024)</a></li><li><a href=#35--200286-blendscape-enabling-unified-and-personalized-video-conferencing-environments-through-generative-ai-shwetha-rajaram-et-al-2024>(3/5 | 200/286) BlendScape: Enabling Unified and Personalized Video-Conferencing Environments through Generative AI (Shwetha Rajaram et al., 2024)</a></li><li><a href=#45--201286-its-not-a-replacement-enabling-parent-robot-collaboration-to-support-in-home-learning-experiences-of-young-children-hui-ru-ho-et-al-2024>(4/5 | 201/286) &lsquo;It&rsquo;s Not a Replacement:&rsquo; Enabling Parent-Robot Collaboration to Support In-Home Learning Experiences of Young Children (Hui-Ru Ho et al., 2024)</a></li><li><a href=#55--202286-the-tribal-theater-model-social-regulation-for-dynamic-user-adaptation-in-virtual-interactive-environments-h-zhang-et-al-2024>(5/5 | 202/286) The Tribal Theater Model: Social Regulation for Dynamic User Adaptation in Virtual Interactive Environments (H. Zhang et al., 2024)</a></li></ul></li><li><a href=#cscr-9>cs.CR (9)</a><ul><li><a href=#19--203286-defending-against-indirect-prompt-injection-attacks-with-spotlighting-keegan-hines-et-al-2024>(1/9 | 203/286) Defending Against Indirect Prompt Injection Attacks With Spotlighting (Keegan Hines et al., 2024)</a></li><li><a href=#29--204286-badedit-backdooring-large-language-models-by-model-editing-yanzhou-li-et-al-2024>(2/9 | 204/286) BadEdit: Backdooring large language models by model editing (Yanzhou Li et al., 2024)</a></li><li><a href=#39--205286-mapping-llm-security-landscapes-a-comprehensive-stakeholder-risk-assessment-proposal-rahul-pankajakshan-et-al-2024>(3/9 | 205/286) Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal (Rahul Pankajakshan et al., 2024)</a></li><li><a href=#49--206286-graph-attention-network-based-block-propagation-with-optimal-aoi-and-reputation-in-web-30-jiana-liao-et-al-2024>(4/9 | 206/286) Graph Attention Network-based Block Propagation with Optimal AoI and Reputation in Web 3.0 (Jiana Liao et al., 2024)</a></li><li><a href=#59--207286-zero-knowledge-proof-of-distinct-identity-a-standard-compatible-sybil-resistant-pseudonym-extension-for-c-its-ye-tao-et-al-2024>(5/9 | 207/286) Zero-Knowledge Proof of Distinct Identity: a Standard-compatible Sybil-resistant Pseudonym Extension for C-ITS (Ye Tao et al., 2024)</a></li><li><a href=#69--208286-dl2fence-integrating-deep-learning-and-frame-fusion-for-enhanced-detection-and-localization-of-refined-denial-of-service-in-large-scale-nocs-haoyu-wang-et-al-2024>(6/9 | 208/286) DL2Fence: Integrating Deep Learning and Frame Fusion for Enhanced Detection and Localization of Refined Denial-of-Service in Large-Scale NoCs (Haoyu Wang et al., 2024)</a></li><li><a href=#79--209286-private-aggregate-queries-to-untrusted-databases-syed-mahbub-hafiz-et-al-2024>(7/9 | 209/286) Private Aggregate Queries to Untrusted Databases (Syed Mahbub Hafiz et al., 2024)</a></li><li><a href=#89--210286-jailbreaking-is-best-solved-by-definition-taeyoun-kim-et-al-2024>(8/9 | 210/286) Jailbreaking is Best Solved by Definition (Taeyoun Kim et al., 2024)</a></li><li><a href=#99--211286-threats-attacks-and-defenses-in-machine-unlearning-a-survey-ziyao-liu-et-al-2024>(9/9 | 211/286) Threats, Attacks, and Defenses in Machine Unlearning: A Survey (Ziyao Liu et al., 2024)</a></li></ul></li><li><a href=#eessiv-3>eess.IV (3)</a><ul><li><a href=#13--212286-high-confidence-pseudo-labels-for-domain-adaptation-in-covid-19-detection-robert-turnbull-et-al-2024>(1/3 | 212/286) High-confidence pseudo-labels for domain adaptation in COVID-19 detection (Robert Turnbull et al., 2024)</a></li><li><a href=#23--213286-towards-learning-contrast-kinetics-with-multi-condition-latent-diffusion-models-richard-osuala-et-al-2024>(2/3 | 213/286) Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models (Richard Osuala et al., 2024)</a></li><li><a href=#33--214286-p-count-persistence-based-counting-of-white-matter-hyperintensities-in-brain-mri-xiaoling-hu-et-al-2024>(3/3 | 214/286) P-Count: Persistence-based Counting of White Matter Hyperintensities in Brain MRI (Xiaoling Hu et al., 2024)</a></li></ul></li><li><a href=#q-fintr-1>q-fin.TR (1)</a><ul><li><a href=#11--215286-detecting-and-triaging-spoofing-using-temporal-convolutional-networks-kaushalya-kularatnam-et-al-2024>(1/1 | 215/286) Detecting and Triaging Spoofing using Temporal Convolutional Networks (Kaushalya Kularatnam et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--216286-bridging-scales-in-multiscale-bubble-growth-dynamics-with-correlated-fluctuations-using-neural-operator-learning-minglei-lu-et-al-2024>(1/1 | 216/286) Bridging scales in multiscale bubble growth dynamics with correlated fluctuations using neural operator learning (Minglei Lu et al., 2024)</a></li></ul></li><li><a href=#eesssy-14>eess.SY (14)</a><ul><li><a href=#114--217286-federated-reinforcement-learning-for-robot-motion-planning-with-zero-shot-generalization-zhenyuan-yuan-et-al-2024>(1/14 | 217/286) Federated reinforcement learning for robot motion planning with zero-shot generalization (Zhenyuan Yuan et al., 2024)</a></li><li><a href=#214--218286-a-control-recoverable-added-noise-based-privacy-scheme-for-lq-control-in-networked-control-systems-xuening-tang-et-al-2024>(2/14 | 218/286) A Control-Recoverable Added-Noise-based Privacy Scheme for LQ Control in Networked Control Systems (Xuening Tang et al., 2024)</a></li><li><a href=#314--219286-safety-aware-reinforcement-learning-for-electric-vehicle-charging-station-management-in-distribution-network-jiarong-fan-et-al-2024>(3/14 | 219/286) Safety-Aware Reinforcement Learning for Electric Vehicle Charging Station Management in Distribution Network (Jiarong Fan et al., 2024)</a></li><li><a href=#414--220286-3d-directed-formation-control-with-global-shape-convergence-using-bispherical-coordinates-omid-mirzaeedodangeh-et-al-2024>(4/14 | 220/286) 3D Directed Formation Control with Global Shape Convergence using Bispherical Coordinates (Omid Mirzaeedodangeh et al., 2024)</a></li><li><a href=#514--221286-distributed-cooperative-formation-control-of-nonlinear-multi-agent-system-ugv-using-neural-network-si-kheang-moeurn-2024>(5/14 | 221/286) Distributed Cooperative Formation Control of Nonlinear Multi-Agent System (UGV) Using Neural Network (Si Kheang Moeurn, 2024)</a></li><li><a href=#614--222286-on-optimal-management-of-energy-storage-systems-in-renewable-energy-communities-giovanni-gino-zanvettor-et-al-2024>(6/14 | 222/286) On Optimal Management of Energy Storage Systems in Renewable Energy Communities (Giovanni Gino Zanvettor et al., 2024)</a></li><li><a href=#714--223286-priority-based-energy-allocation-in-buildings-for-distributed-model-predictive-control-hongyi-li-et-al-2024>(7/14 | 223/286) Priority-based Energy Allocation in Buildings for Distributed Model Predictive Control (Hongyi Li et al., 2024)</a></li><li><a href=#814--224286-lattice-piecewise-affine-approximation-of-explicit-model-predictive-control-with-application-to-satellite-attitude-control-zhengqi-xu-et-al-2024>(8/14 | 224/286) Lattice piecewise affine approximation of explicit model predictive control with application to satellite attitude control (Zhengqi Xu et al., 2024)</a></li><li><a href=#914--225286-augmented-labeled-random-finite-sets-and-its-application-to-group-target-tracking-chaoqun-yang-et-al-2024>(9/14 | 225/286) Augmented Labeled Random Finite Sets and Its Application to Group Target Tracking (Chaoqun Yang et al., 2024)</a></li><li><a href=#1014--226286-an-extended-kuramoto-model-for-frequency-and-phase-synchronization-in-delay-free-networks-with-finite-number-of-agents-andreas-bathelt-et-al-2024>(10/14 | 226/286) An Extended Kuramoto Model for Frequency and Phase Synchronization in Delay-Free Networks with Finite Number of Agents (Andreas Bathelt et al., 2024)</a></li><li><a href=#1114--227286-network-aware-value-stacking-of-community-battery-via-asynchronous-distributed-optimization-canchen-jiang-et-al-2024>(11/14 | 227/286) Network-Aware Value Stacking of Community Battery via Asynchronous Distributed Optimization (Canchen Jiang et al., 2024)</a></li><li><a href=#1214--228286-bayesian-physics-informed-neural-networks-for-system-identification-of-inverter-dominated-power-systems-simon-stock-et-al-2024>(12/14 | 228/286) Bayesian Physics-informed Neural Networks for System Identification of Inverter-dominated Power Systems (Simon Stock et al., 2024)</a></li><li><a href=#1314--229286-clustering-heuristics-for-robust-energy-capacitated-vehicle-routing-problem-ecvrp-mark-pustilnik-et-al-2024>(13/14 | 229/286) Clustering Heuristics for Robust Energy Capacitated Vehicle Routing Problem (ECVRP) (Mark Pustilnik et al., 2024)</a></li><li><a href=#1414--230286-macroscopic-pricing-schemes-for-the-utilization-of-pool-ride-hailing-vehicles-in-bus-lanes-lynn-fayed-et-al-2024>(14/14 | 230/286) Macroscopic pricing schemes for the utilization of pool ride-hailing vehicles in bus lanes (Lynn Fayed et al., 2024)</a></li></ul></li><li><a href=#physicschem-ph-1>physics.chem-ph (1)</a><ul><li><a href=#11--231286-considerations-in-the-use-of-ml-interaction-potentials-for-free-energy-calculations-orlando-a-mendible-et-al-2024>(1/1 | 231/286) Considerations in the use of ML interaction potentials for free energy calculations (Orlando A. Mendible et al., 2024)</a></li></ul></li><li><a href=#csma-3>cs.MA (3)</a><ul><li><a href=#13--232286-motion-prediction-of-multi-agent-systems-with-multi-view-clustering-anegi-james-et-al-2024>(1/3 | 232/286) Motion Prediction of Multi-agent systems with Multi-view clustering (Anegi James et al., 2024)</a></li><li><a href=#23--233286-hyper-strategy-logic-raven-beutner-et-al-2024>(2/3 | 233/286) Hyper Strategy Logic (Raven Beutner et al., 2024)</a></li><li><a href=#33--234286-multi-agent-reinforcement-traffic-signal-control-based-on-interpretable-influence-mechanism-and-biased-relu-approximation-zhiyue-luo-et-al-2024>(3/3 | 234/286) Multi-agent Reinforcement Traffic Signal Control based on Interpretable Influence Mechanism and Biased ReLU Approximation (Zhiyue Luo et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#11--235286-a-comparative-study-of-machine-learning-models-predicting-energetics-of-interacting-defects-hao-yu-2024>(1/1 | 235/286) A Comparative Study of Machine Learning Models Predicting Energetics of Interacting Defects (Hao Yu, 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--236286-antithetic-multilevel-methods-for-elliptic-and-hypo-elliptic-diffusions-with-applications-yuga-iguchi-et-al-2024>(1/2 | 236/286) Antithetic Multilevel Methods for Elliptic and Hypo-Elliptic Diffusions with Applications (Yuga Iguchi et al., 2024)</a></li><li><a href=#22--237286-a-high-fidelity-material-point-method-for-frictional-contact-problems-emmanouil-g-kakouris-et-al-2024>(2/2 | 237/286) A high-fidelity material point method for frictional contact problems (Emmanouil G. Kakouris et al., 2024)</a></li></ul></li><li><a href=#cssi-5>cs.SI (5)</a><ul><li><a href=#15--238286-incentivizing-news-consumption-on-social-media-platforms-using-large-language-models-and-realistic-bot-accounts-hadi-askari-et-al-2024>(1/5 | 238/286) Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts (Hadi Askari et al., 2024)</a></li><li><a href=#25--239286-graph-neural-network-for-crawling-target-nodes-in-social-networks-kirill-lukyanov-et-al-2024>(2/5 | 239/286) Graph Neural Network for Crawling Target Nodes in Social Networks (Kirill Lukyanov et al., 2024)</a></li><li><a href=#35--240286-what-makes-a-small-world-network-leveraging-machine-learning-for-the-robust-prediction-and-classification-of-networks-raima-carol-appaw-et-al-2024>(3/5 | 240/286) What makes a small-world network? Leveraging machine learning for the robust prediction and classification of networks (Raima Carol Appaw et al., 2024)</a></li><li><a href=#45--241286-use-dynamic-user-modeling-with-stateful-sequence-models-zhihan-zhou-et-al-2024>(4/5 | 241/286) USE: Dynamic User Modeling with Stateful Sequence Models (Zhihan Zhou et al., 2024)</a></li><li><a href=#55--242286-mathematical-model-of-information-bubbles-on-networks-pál-burai-et-al-2024>(5/5 | 242/286) Mathematical model of information bubbles on networks (Pál Burai et al., 2024)</a></li></ul></li><li><a href=#cssd-5>cs.SD (5)</a><ul><li><a href=#15--243286-building-speech-corpus-with-diverse-voice-characteristics-for-its-prompt-based-representation-aya-watanabe-et-al-2024>(1/5 | 243/286) Building speech corpus with diverse voice characteristics for its prompt-based representation (Aya Watanabe et al., 2024)</a></li><li><a href=#25--244286-frequency-aware-convolution-for-sound-event-detection-tao-song-2024>(2/5 | 244/286) Frequency-aware convolution for sound event detection (Tao Song, 2024)</a></li><li><a href=#35--245286-utduss-utokyo-sarulab-system-for-interspeech2024-speech-processing-using-discrete-speech-unit-challenge-wataru-nakata-et-al-2024>(3/5 | 245/286) UTDUSS: UTokyo-SaruLab System for Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge (Wataru Nakata et al., 2024)</a></li><li><a href=#45--246286-advanced-long-content-speech-recognition-with-factorized-neural-transducer-xun-gong-et-al-2024>(4/5 | 246/286) Advanced Long-Content Speech Recognition With Factorized Neural Transducer (Xun Gong et al., 2024)</a></li><li><a href=#55--247286-onset-and-offset-weighted-loss-function-for-sound-event-detection-tao-song-2024>(5/5 | 247/286) Onset and offset weighted loss function for sound event detection (Tao Song, 2024)</a></li></ul></li><li><a href=#csdb-2>cs.DB (2)</a><ul><li><a href=#12--248286-no-more-optimization-rules-llm-enabled-policy-based-multi-modal-query-optimizer-version-1-yifan-wang-et-al-2024>(1/2 | 248/286) No more optimization rules: LLM-enabled policy-based multi-modal query optimizer (version 1) (Yifan Wang et al., 2024)</a></li><li><a href=#22--249286-distance-comparison-operators-for-approximate-nearest-neighbor-search-exploration-and-benchmark-zeyu-wang-et-al-2024>(2/2 | 249/286) Distance Comparison Operators for Approximate Nearest Neighbor Search: Exploration and Benchmark (Zeyu Wang et al., 2024)</a></li></ul></li><li><a href=#csdc-5>cs.DC (5)</a><ul><li><a href=#15--250286-regent-based-parallel-meshfree-lskum-solver-for-heterogenous-hpc-platforms-sanath-salil-et-al-2024>(1/5 | 250/286) Regent based parallel meshfree LSKUM solver for heterogenous HPC platforms (Sanath Salil et al., 2024)</a></li><li><a href=#25--251286-automated-calibration-of-parallel-and-distributed-computing-simulators-a-case-study-jesse-mcdonald-et-al-2024>(2/5 | 251/286) Automated Calibration of Parallel and Distributed Computing Simulators: A Case Study (Jesse McDonald et al., 2024)</a></li><li><a href=#35--252286-optimal-fixed-priority-scheduling-in-multi-stage-multi-resource-distributed-real-time-systems-niraj-kumar-et-al-2024>(3/5 | 252/286) Optimal Fixed Priority Scheduling in Multi-Stage Multi-Resource Distributed Real-Time Systems (Niraj Kumar et al., 2024)</a></li><li><a href=#45--253286-agent-based-mst-construction-ajay-d-kshemkalyani-et-al-2024>(4/5 | 253/286) Agent-based MST Construction (Ajay D. Kshemkalyani et al., 2024)</a></li><li><a href=#55--254286-causal-graph-dynamics-and-kan-extensions-luidnel-maignan-et-al-2024>(5/5 | 254/286) Causal Graph Dynamics and Kan Extensions (Luidnel Maignan et al., 2024)</a></li></ul></li><li><a href=#csdl-1>cs.DL (1)</a><ul><li><a href=#11--255286-automated-extraction-and-maturity-analysis-of-open-source-clinical-informatics-repositories-from-scientific-literature-jeremy-r-harper-2024>(1/1 | 255/286) Automated Extraction and Maturity Analysis of Open Source Clinical Informatics Repositories from Scientific Literature (Jeremy R. Harper, 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--256286-large-language-models-meet-network-slicing-management-and-orchestration-abdulhalim-dandoush-et-al-2024>(1/2 | 256/286) Large Language Models meet Network Slicing Management and Orchestration (Abdulhalim Dandoush et al., 2024)</a></li><li><a href=#22--257286-bft-poloc-a-byzantine-fortified-trigonometric-proof-of-location-protocol-using-internet-delays-peiyao-sheng-et-al-2024>(2/2 | 257/286) BFT-PoLoc: A Byzantine Fortified Trigonometric Proof of Location Protocol using Internet Delays (Peiyao Sheng et al., 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--258286-mimo-channel-as-a-neural-function-implicit-neural-representations-for-extreme-csi-compression-in-massive-mimo-systems-haotian-wu-et-al-2024>(1/2 | 258/286) MIMO Channel as a Neural Function: Implicit Neural Representations for Extreme CSI Compression in Massive MIMO Systems (Haotian Wu et al., 2024)</a></li><li><a href=#22--259286-massive-mimo-csi-feedback-using-channel-prediction-how-to-avoid-machine-learning-at-ue-muhammad-karam-shehzad-et-al-2024>(2/2 | 259/286) Massive MIMO CSI Feedback using Channel Prediction: How to Avoid Machine Learning at UE? (Muhammad Karam Shehzad et al., 2024)</a></li></ul></li><li><a href=#statml-4>stat.ML (4)</a><ul><li><a href=#14--260286-analysing-heavy-tail-properties-of-stochastic-gradient-descent-by-means-of-stochastic-recurrence-equations-ewa-damek-et-al-2024>(1/4 | 260/286) Analysing heavy-tail properties of Stochastic Gradient Descent by means of Stochastic Recurrence Equations (Ewa Damek et al., 2024)</a></li><li><a href=#24--261286-adatrans-feature-wise-and-sample-wise-adaptive-transfer-learning-for-high-dimensional-regression-zelin-he-et-al-2024>(2/4 | 261/286) AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression (Zelin He et al., 2024)</a></li><li><a href=#34--262286-kernel-multigrid-accelerate-back-fitting-via-sparse-gaussian-process-regression-lu-zou-et-al-2024>(3/4 | 262/286) Kernel Multigrid: Accelerate Back-fitting via Sparse Gaussian Process Regression (Lu Zou et al., 2024)</a></li><li><a href=#44--263286-a-sampling-based-framework-for-hypothesis-testing-on-large-attributed-graphs-yun-wang-et-al-2024>(4/4 | 263/286) A Sampling-based Framework for Hypothesis Testing on Large Attributed Graphs (Yun Wang et al., 2024)</a></li></ul></li><li><a href=#cscy-4>cs.CY (4)</a><ul><li><a href=#14--264286-the-future-of-generative-ai-chatbots-in-higher-education-joshua-ebere-chukwuere-2024>(1/4 | 264/286) The future of generative AI chatbots in higher education (Joshua Ebere Chukwuere, 2024)</a></li><li><a href=#24--265286-inditag-an-online-media-bias-analysis-and-annotation-system-using-fine-grained-bias-indicators-luyang-lin-et-al-2024>(2/4 | 265/286) IndiTag: An Online Media Bias Analysis and Annotation System Using Fine-Grained Bias Indicators (Luyang Lin et al., 2024)</a></li><li><a href=#34--266286-community-needs-and-assets-a-computational-analysis-of-community-conversations-md-towhidul-absar-chowdhury-et-al-2024>(3/4 | 266/286) Community Needs and Assets: A Computational Analysis of Community Conversations (Md Towhidul Absar Chowdhury et al., 2024)</a></li><li><a href=#44--267286-spatial-fairness-the-case-for-its-importance-limitations-of-existing-work-and-guidelines-for-future-research-nripsuta-ani-saxena-et-al-2024>(4/4 | 267/286) Spatial Fairness: The Case for its Importance, Limitations of Existing Work, and Guidelines for Future Research (Nripsuta Ani Saxena et al., 2024)</a></li></ul></li><li><a href=#csce-3>cs.CE (3)</a><ul><li><a href=#13--268286-application-of-advanced-ultrasonic-testing-methods-to-dissimilar-metal-welds----comparison-of-simulated-and-experimental-results-audrey-gardahaut-et-al-2024>(1/3 | 268/286) Application of advanced ultrasonic testing methods to Dissimilar Metal Welds &ndash; Comparison of simulated and experimental results (Audrey Gardahaut et al., 2024)</a></li><li><a href=#23--269286-stochastic-geometry-models-for-texture-synthesis-of-machined-metallic-surfaces-sandblasting-and-milling-natascha-jeziorski-et-al-2024>(2/3 | 269/286) Stochastic Geometry Models for Texture Synthesis of Machined Metallic Surfaces: Sandblasting and Milling (Natascha Jeziorski et al., 2024)</a></li><li><a href=#33--270286-canonical-descriptors-for-periodic-lattice-truss-materials-ge-qi-et-al-2024>(3/3 | 270/286) Canonical Descriptors for Periodic Lattice Truss Materials (Ge Qi et al., 2024)</a></li></ul></li><li><a href=#physicsbio-ph-1>physics.bio-ph (1)</a><ul><li><a href=#11--271286-a-machine-learning-approach-for-multiscale-modeling-of-the-facet-capsular-ligament-jacob-s-merson-et-al-2024>(1/1 | 271/286) A Machine Learning Approach for Multiscale Modeling of the Facet Capsular Ligament (Jacob S. Merson et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#12--272286-tdt-kws-fast-and-accurate-keyword-spotting-using-token-and-duration-transducer-yu-xi-et-al-2024>(1/2 | 272/286) TDT-KWS: Fast And Accurate Keyword Spotting Using Token-and-duration Transducer (Yu Xi et al., 2024)</a></li><li><a href=#22--273286-kunqudb-an-attempt-for-speaker-verification-in-the-chinese-opera-scenario-huali-zhou-et-al-2024>(2/2 | 273/286) KunquDB: An Attempt for Speaker Verification in the Chinese Opera Scenario (Huali Zhou et al., 2024)</a></li></ul></li><li><a href=#mathoc-4>math.OC (4)</a><ul><li><a href=#14--274286-projection-free-computation-of-robust-controllable-sets-with-constrained-zonotopes-abraham-p-vinod-et-al-2024>(1/4 | 274/286) Projection-free computation of robust controllable sets with constrained zonotopes (Abraham P. Vinod et al., 2024)</a></li><li><a href=#24--275286-optimal-control-of-continuous-time-symmetric-systems-with-unknown-dynamics-and-noisy-measurements-hamed-taghavian-et-al-2024>(2/4 | 275/286) Optimal control of continuous-time symmetric systems with unknown dynamics and noisy measurements (Hamed Taghavian et al., 2024)</a></li><li><a href=#34--276286-towards-a-connection-between-the-capacitated-vehicle-routing-problem-and-the-constrained-centroid-based-clustering-abdelhakim-abdellaoui-et-al-2024>(3/4 | 276/286) Towards a connection between the capacitated vehicle routing problem and the constrained centroid-based clustering (Abdelhakim Abdellaoui et al., 2024)</a></li><li><a href=#44--277286-a-log-domain-interior-point-method-for-convex-quadratic-games-bingqi-liu-et-al-2024>(4/4 | 277/286) A Log-domain Interior Point Method for Convex Quadratic Games (Bingqi Liu et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--278286-network-bottlenecks-and-task-structure-control-the-evolution-of-interpretable-learning-rules-in-a-foraging-agent-emmanouil-giannakakis-et-al-2024>(1/1 | 278/286) Network bottlenecks and task structure control the evolution of interpretable learning rules in a foraging agent (Emmanouil Giannakakis et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--279286-extremality-of-stabilizer-states-kaifeng-bu-2024>(1/1 | 279/286) Extremality of stabilizer states (Kaifeng Bu, 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--280286-hcim-adc-less-hybrid-analog-digital-compute-in-memory-accelerator-for-deep-learning-workloads-shubham-negi-et-al-2024>(1/1 | 280/286) HCiM: ADC-Less Hybrid Analog-Digital Compute in Memory Accelerator for Deep Learning Workloads (Shubham Negi et al., 2024)</a></li></ul></li><li><a href=#cslo-3>cs.LO (3)</a><ul><li><a href=#13--281286-reasoning-about-distributive-laws-in-a-concurrent-refinement-algebra-larissa-a-meinicke-et-al-2024>(1/3 | 281/286) Reasoning about distributive laws in a concurrent refinement algebra (Larissa A. Meinicke et al., 2024)</a></li><li><a href=#23--282286-mechanized-hol-reasoning-in-set-theory-simon-guilloud-et-al-2024>(2/3 | 282/286) Mechanized HOL Reasoning in Set Theory (Simon Guilloud et al., 2024)</a></li><li><a href=#33--283286-the-equational-theory-of-the-weihrauch-lattice-with-multiplication-eike-neumann-et-al-2024>(3/3 | 283/286) The equational theory of the Weihrauch lattice with multiplication (Eike Neumann et al., 2024)</a></li></ul></li><li><a href=#cssc-1>cs.SC (1)</a><ul><li><a href=#11--284286-gröbner-bases-over-polytopal-affinoid-algebras-moulay-a-barkatou-et-al-2024>(1/1 | 284/286) Gr{ö}bner bases over polytopal affinoid algebras (Moulay A. Barkatou et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--285286-from-primary-hpv-infection-to-carcinoma-in-situ-a-mathematical-approach-of-cervical-intraepithelial-neoplasia-vasiliki-bitsouni-et-al-2024>(1/1 | 285/286) From primary HPV infection to carcinoma in situ: a mathematical approach of cervical intraepithelial neoplasia (Vasiliki Bitsouni et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--286286-constrained-and-ordered-level-planarity-parameterized-by-the-number-of-levels-václav-blažej-et-al-2024>(1/1 | 286/286) Constrained and Ordered Level Planarity Parameterized by the Number of Levels (Václav Blažej et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>